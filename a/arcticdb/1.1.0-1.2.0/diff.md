# Comparing `tmp/arcticdb-1.1.0-cp39-cp39-win_amd64.whl.zip` & `tmp/arcticdb-1.2.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,73 +1,73 @@
-Zip file size: 6161040 bytes, number of entries: 71
--rw-rw-rw-  2.0 fat 21822464 b- defN 23-May-08 14:16 arcticdb_ext.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat       64 b- defN 23-May-08 13:59 arcticc/__init__.py
--rw-rw-rw-  2.0 fat      583 b- defN 23-May-08 13:59 arcticc/pb2/__init__.py
--rw-rw-rw-  2.0 fat      401 b- defN 23-May-08 13:59 arcticdb/__init__.py
--rw-rw-rw-  2.0 fat      447 b- defN 23-May-08 13:59 arcticdb/_msgpack_compat.py
--rw-rw-rw-  2.0 fat    10405 b- defN 23-May-08 13:59 arcticdb/arctic.py
--rw-rw-rw-  2.0 fat     7970 b- defN 23-May-08 13:59 arcticdb/config.py
--rw-rw-rw-  2.0 fat      765 b- defN 23-May-08 13:59 arcticdb/exceptions.py
--rw-rw-rw-  2.0 fat     9363 b- defN 23-May-08 13:59 arcticdb/flattener.py
--rw-rw-rw-  2.0 fat     2071 b- defN 23-May-08 13:59 arcticdb/log.py
--rw-rw-rw-  2.0 fat     6436 b- defN 23-May-08 13:59 arcticdb/options.py
--rw-rw-rw-  2.0 fat      519 b- defN 23-May-08 13:59 arcticdb/preconditions.py
--rw-rw-rw-  2.0 fat     1612 b- defN 23-May-08 13:59 arcticdb/supported_types.py
--rw-rw-rw-  2.0 fat     3119 b- defN 23-May-08 13:59 arcticdb/tools.py
--rw-rw-rw-  2.0 fat      138 b- defN 23-May-08 13:59 arcticdb/adapters/__init__.py
--rw-rw-rw-  2.0 fat     2211 b- defN 23-May-08 13:59 arcticdb/adapters/arctic_library_adapter.py
--rw-rw-rw-  2.0 fat     2394 b- defN 23-May-08 13:59 arcticdb/adapters/lmdb_library_adapter.py
--rw-rw-rw-  2.0 fat     6434 b- defN 23-May-08 13:59 arcticdb/adapters/s3_library_adapter.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-08 13:59 arcticdb/authorization/__init__.py
--rw-rw-rw-  2.0 fat      969 b- defN 23-May-08 13:59 arcticdb/authorization/permissions.py
--rw-rw-rw-  2.0 fat     8935 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/config_pb2.py
--rw-rw-rw-  2.0 fat    96954 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/descriptors_pb2.py
--rw-rw-rw-  2.0 fat    28646 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/encoding_pb2.py
--rw-rw-rw-  2.0 fat    13225 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/generation_pb2.py
--rw-rw-rw-  2.0 fat     1990 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/in_memory_storage_pb2.py
--rw-rw-rw-  2.0 fat     4634 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/lmdb_storage_pb2.py
--rw-rw-rw-  2.0 fat    24224 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/logger_pb2.py
--rw-rw-rw-  2.0 fat     2931 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/mongo_storage_pb2.py
--rw-rw-rw-  2.0 fat     6878 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/nfs_backed_storage_pb2.py
--rw-rw-rw-  2.0 fat     3821 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/processors_pb2.py
--rw-rw-rw-  2.0 fat    49227 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/request_pb2.py
--rw-rw-rw-  2.0 fat     6729 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/s3_storage_pb2.py
--rw-rw-rw-  2.0 fat    67292 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/storage_pb2.py
--rw-rw-rw-  2.0 fat    12621 b- defN 23-May-08 14:09 arcticdb/proto/3/arcticc/pb2/utils_pb2.py
--rw-rw-rw-  2.0 fat     2298 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/config_pb2.py
--rw-rw-rw-  2.0 fat    14870 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/descriptors_pb2.py
--rw-rw-rw-  2.0 fat     4895 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/encoding_pb2.py
--rw-rw-rw-  2.0 fat     2998 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/generation_pb2.py
--rw-rw-rw-  2.0 fat     1038 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/in_memory_storage_pb2.py
--rw-rw-rw-  2.0 fat     1367 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/lmdb_storage_pb2.py
--rw-rw-rw-  2.0 fat     4114 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/logger_pb2.py
--rw-rw-rw-  2.0 fat     1195 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/mongo_storage_pb2.py
--rw-rw-rw-  2.0 fat     1517 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/nfs_backed_storage_pb2.py
--rw-rw-rw-  2.0 fat     1259 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/processors_pb2.py
--rw-rw-rw-  2.0 fat     6801 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/request_pb2.py
--rw-rw-rw-  2.0 fat     1496 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/s3_storage_pb2.py
--rw-rw-rw-  2.0 fat     9907 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/storage_pb2.py
--rw-rw-rw-  2.0 fat     2310 b- defN 23-May-08 14:09 arcticdb/proto/4/arcticc/pb2/utils_pb2.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-08 13:59 arcticdb/toolbox/__init__.py
--rw-rw-rw-  2.0 fat     5669 b- defN 23-May-08 13:59 arcticdb/toolbox/library_tool.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-08 13:59 arcticdb/util/__init__.py
--rw-rw-rw-  2.0 fat     2705 b- defN 23-May-08 13:59 arcticdb/util/errors.py
--rw-rw-rw-  2.0 fat     8464 b- defN 23-May-08 13:59 arcticdb/util/hypothesis.py
--rw-rw-rw-  2.0 fat      719 b- defN 23-May-08 13:59 arcticdb/util/memory.py
--rw-rw-rw-  2.0 fat     6042 b- defN 23-May-08 13:59 arcticdb/util/tasks.py
--rw-rw-rw-  2.0 fat    18235 b- defN 23-May-08 13:59 arcticdb/util/test.py
--rw-rw-rw-  2.0 fat      131 b- defN 23-May-08 13:59 arcticdb/version_store/__init__.py
--rw-rw-rw-  2.0 fat     7142 b- defN 23-May-08 13:59 arcticdb/version_store/_common.py
--rw-rw-rw-  2.0 fat     4193 b- defN 23-May-08 13:59 arcticdb/version_store/_custom_normalizers.py
--rw-rw-rw-  2.0 fat    52304 b- defN 23-May-08 13:59 arcticdb/version_store/_normalization.py
--rw-rw-rw-  2.0 fat   103093 b- defN 23-May-08 13:59 arcticdb/version_store/_store.py
--rw-rw-rw-  2.0 fat    10399 b- defN 23-May-08 13:59 arcticdb/version_store/helper.py
--rw-rw-rw-  2.0 fat    54049 b- defN 23-May-08 13:59 arcticdb/version_store/library.py
--rw-rw-rw-  2.0 fat    24193 b- defN 23-May-08 13:59 arcticdb/version_store/processing.py
--rw-rw-rw-  2.0 fat      603 b- defN 23-May-08 13:59 arcticdb/version_store/read_result.py
--rw-rw-rw-  2.0 fat     4851 b- defN 23-May-08 14:16 arcticdb-1.1.0.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     8348 b- defN 23-May-08 14:16 arcticdb-1.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat    18089 b- defN 23-May-08 14:16 arcticdb-1.1.0.dist-info/NOTICE.txt
--rw-rw-rw-  2.0 fat      100 b- defN 23-May-08 14:16 arcticdb-1.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       30 b- defN 23-May-08 14:16 arcticdb-1.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6529 b- defN 23-May-08 14:16 arcticdb-1.1.0.dist-info/RECORD
-71 files, 22598425 bytes uncompressed, 6150544 bytes compressed:  72.8%
+Zip file size: 6256497 bytes, number of entries: 71
+-rw-rw-rw-  2.0 fat 22184448 b- defN 23-May-22 11:59 arcticdb_ext.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat       64 b- defN 23-May-22 11:43 arcticc/__init__.py
+-rw-rw-rw-  2.0 fat      583 b- defN 23-May-22 11:43 arcticc/pb2/__init__.py
+-rw-rw-rw-  2.0 fat      424 b- defN 23-May-22 11:43 arcticdb/__init__.py
+-rw-rw-rw-  2.0 fat      447 b- defN 23-May-22 11:43 arcticdb/_msgpack_compat.py
+-rw-rw-rw-  2.0 fat    10405 b- defN 23-May-22 11:43 arcticdb/arctic.py
+-rw-rw-rw-  2.0 fat     7970 b- defN 23-May-22 11:43 arcticdb/config.py
+-rw-rw-rw-  2.0 fat      765 b- defN 23-May-22 11:43 arcticdb/exceptions.py
+-rw-rw-rw-  2.0 fat     9363 b- defN 23-May-22 11:43 arcticdb/flattener.py
+-rw-rw-rw-  2.0 fat     2071 b- defN 23-May-22 11:43 arcticdb/log.py
+-rw-rw-rw-  2.0 fat     6436 b- defN 23-May-22 11:43 arcticdb/options.py
+-rw-rw-rw-  2.0 fat      519 b- defN 23-May-22 11:43 arcticdb/preconditions.py
+-rw-rw-rw-  2.0 fat     1612 b- defN 23-May-22 11:43 arcticdb/supported_types.py
+-rw-rw-rw-  2.0 fat     3142 b- defN 23-May-22 11:43 arcticdb/tools.py
+-rw-rw-rw-  2.0 fat      138 b- defN 23-May-22 11:43 arcticdb/adapters/__init__.py
+-rw-rw-rw-  2.0 fat     2211 b- defN 23-May-22 11:43 arcticdb/adapters/arctic_library_adapter.py
+-rw-rw-rw-  2.0 fat     2394 b- defN 23-May-22 11:43 arcticdb/adapters/lmdb_library_adapter.py
+-rw-rw-rw-  2.0 fat     6434 b- defN 23-May-22 11:43 arcticdb/adapters/s3_library_adapter.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-22 11:43 arcticdb/authorization/__init__.py
+-rw-rw-rw-  2.0 fat      969 b- defN 23-May-22 11:43 arcticdb/authorization/permissions.py
+-rw-rw-rw-  2.0 fat     8935 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/config_pb2.py
+-rw-rw-rw-  2.0 fat    96954 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/descriptors_pb2.py
+-rw-rw-rw-  2.0 fat    28646 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/encoding_pb2.py
+-rw-rw-rw-  2.0 fat    13225 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/generation_pb2.py
+-rw-rw-rw-  2.0 fat     1990 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/in_memory_storage_pb2.py
+-rw-rw-rw-  2.0 fat     4634 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/lmdb_storage_pb2.py
+-rw-rw-rw-  2.0 fat    24224 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/logger_pb2.py
+-rw-rw-rw-  2.0 fat     2931 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/mongo_storage_pb2.py
+-rw-rw-rw-  2.0 fat     6878 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/nfs_backed_storage_pb2.py
+-rw-rw-rw-  2.0 fat     3821 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/processors_pb2.py
+-rw-rw-rw-  2.0 fat    49227 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/request_pb2.py
+-rw-rw-rw-  2.0 fat     6729 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/s3_storage_pb2.py
+-rw-rw-rw-  2.0 fat    67292 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/storage_pb2.py
+-rw-rw-rw-  2.0 fat    12621 b- defN 23-May-22 11:52 arcticdb/proto/3/arcticc/pb2/utils_pb2.py
+-rw-rw-rw-  2.0 fat     2298 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/config_pb2.py
+-rw-rw-rw-  2.0 fat    14870 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/descriptors_pb2.py
+-rw-rw-rw-  2.0 fat     4895 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/encoding_pb2.py
+-rw-rw-rw-  2.0 fat     2998 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/generation_pb2.py
+-rw-rw-rw-  2.0 fat     1038 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/in_memory_storage_pb2.py
+-rw-rw-rw-  2.0 fat     1367 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/lmdb_storage_pb2.py
+-rw-rw-rw-  2.0 fat     4114 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/logger_pb2.py
+-rw-rw-rw-  2.0 fat     1195 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/mongo_storage_pb2.py
+-rw-rw-rw-  2.0 fat     1517 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/nfs_backed_storage_pb2.py
+-rw-rw-rw-  2.0 fat     1259 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/processors_pb2.py
+-rw-rw-rw-  2.0 fat     6801 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/request_pb2.py
+-rw-rw-rw-  2.0 fat     1496 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/s3_storage_pb2.py
+-rw-rw-rw-  2.0 fat     9907 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/storage_pb2.py
+-rw-rw-rw-  2.0 fat     2310 b- defN 23-May-22 11:52 arcticdb/proto/4/arcticc/pb2/utils_pb2.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-22 11:43 arcticdb/toolbox/__init__.py
+-rw-rw-rw-  2.0 fat     5669 b- defN 23-May-22 11:43 arcticdb/toolbox/library_tool.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-May-22 11:43 arcticdb/util/__init__.py
+-rw-rw-rw-  2.0 fat     2705 b- defN 23-May-22 11:43 arcticdb/util/errors.py
+-rw-rw-rw-  2.0 fat     8769 b- defN 23-May-22 11:43 arcticdb/util/hypothesis.py
+-rw-rw-rw-  2.0 fat      719 b- defN 23-May-22 11:43 arcticdb/util/memory.py
+-rw-rw-rw-  2.0 fat     6042 b- defN 23-May-22 11:43 arcticdb/util/tasks.py
+-rw-rw-rw-  2.0 fat    18195 b- defN 23-May-22 11:43 arcticdb/util/test.py
+-rw-rw-rw-  2.0 fat      131 b- defN 23-May-22 11:43 arcticdb/version_store/__init__.py
+-rw-rw-rw-  2.0 fat     7142 b- defN 23-May-22 11:43 arcticdb/version_store/_common.py
+-rw-rw-rw-  2.0 fat     4193 b- defN 23-May-22 11:43 arcticdb/version_store/_custom_normalizers.py
+-rw-rw-rw-  2.0 fat    52604 b- defN 23-May-22 11:43 arcticdb/version_store/_normalization.py
+-rw-rw-rw-  2.0 fat   113852 b- defN 23-May-22 11:43 arcticdb/version_store/_store.py
+-rw-rw-rw-  2.0 fat    10879 b- defN 23-May-22 11:43 arcticdb/version_store/helper.py
+-rw-rw-rw-  2.0 fat    59225 b- defN 23-May-22 11:43 arcticdb/version_store/library.py
+-rw-rw-rw-  2.0 fat    24592 b- defN 23-May-22 11:43 arcticdb/version_store/processing.py
+-rw-rw-rw-  2.0 fat      603 b- defN 23-May-22 11:43 arcticdb/version_store/read_result.py
+-rw-rw-rw-  2.0 fat     4851 b- defN 23-May-22 11:59 arcticdb-1.2.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     8393 b- defN 23-May-22 11:59 arcticdb-1.2.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat    18186 b- defN 23-May-22 11:59 arcticdb-1.2.0.dist-info/NOTICE.txt
+-rw-rw-rw-  2.0 fat      100 b- defN 23-May-22 11:59 arcticdb-1.2.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       30 b- defN 23-May-22 11:59 arcticdb-1.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6529 b- defN 23-May-22 11:59 arcticdb-1.2.0.dist-info/RECORD
+71 files, 22977976 bytes uncompressed, 6246001 bytes compressed:  72.8%
```

## zipnote {}

```diff
@@ -189,26 +189,26 @@
 
 Filename: arcticdb/version_store/processing.py
 Comment: 
 
 Filename: arcticdb/version_store/read_result.py
 Comment: 
 
-Filename: arcticdb-1.1.0.dist-info/LICENSE.txt
+Filename: arcticdb-1.2.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: arcticdb-1.1.0.dist-info/METADATA
+Filename: arcticdb-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: arcticdb-1.1.0.dist-info/NOTICE.txt
+Filename: arcticdb-1.2.0.dist-info/NOTICE.txt
 Comment: 
 
-Filename: arcticdb-1.1.0.dist-info/WHEEL
+Filename: arcticdb-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: arcticdb-1.1.0.dist-info/top_level.txt
+Filename: arcticdb-1.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: arcticdb-1.1.0.dist-info/RECORD
+Filename: arcticdb-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## arcticdb/__init__.py

```diff
@@ -7,7 +7,8 @@
 from arcticdb.version_store.processing import QueryBuilder
 import arcticdb.version_store.library as library
 from arcticdb.options import LibraryOptions
 from arcticdb.tools import set_config_from_env_vars
 
 set_config_from_env_vars(_os.environ)
 
+__version__ = "1.2.0"
```

## arcticdb/tools.py

```diff
@@ -10,18 +10,18 @@
 from typing import Dict
 from arcticdb.config import set_log_level, Defaults
 
 from arcticdb_ext import set_config_int, set_config_string, set_config_double
 
 # Setting config from environment variables. This code is used in the package __init__.py
 _ARCTICDB_ENV_VAR_PREFIX = "ARCTICDB_"
-_TYPE_INT = "int"
-_TYPE_FLOAT = "float"
-_TYPE_STR = "str"
-_TYPE_LOGLEVEL = "loglevel"
+_TYPE_INT = "INT"
+_TYPE_FLOAT = "FLOAT"
+_TYPE_STR = "STR"
+_TYPE_LOGLEVEL = "LOGLEVEL"
 _DEFAULT_TYPE = _TYPE_STR
 _TYPE_SET = {_TYPE_INT, _TYPE_FLOAT, _TYPE_STR, _TYPE_LOGLEVEL}
 
 
 def set_config_from_env_vars(env_vars: Dict[str, str]):
     """
     The env_vars dictionary is a set of a key-value pairs. The values should all be strings.
@@ -32,14 +32,15 @@
     """
     if env_vars is None:
         return
 
     log_level_changes = {}
     default_log_level = Defaults.DEFAULT_LOG_LEVEL
     for k, v in env_vars.items():
+        k = k.upper()
         start_index = None
         if k.startswith(_ARCTICDB_ENV_VAR_PREFIX):  # 1 underscore in prefix
             start_index = 1
 
         if start_index is not None:
             w = k.split("_")
             var_type_raw = w[-1] if w[-1] in _TYPE_SET else None
```

## arcticdb/util/hypothesis.py

```diff
@@ -60,14 +60,21 @@
     return non_infinite(x) and x != 0
 
 
 @st.composite
 def integral_type_strategies(draw):
     return draw(from_dtype(draw(st.one_of([unsigned_integer_dtypes(), integer_dtypes()]))).filter(non_infinite))
 
+@st.composite
+def signed_integral_type_strategies(draw):
+    return draw(from_dtype(draw(st.one_of([integer_dtypes()]))).filter(non_infinite))
+
+@st.composite
+def unsigned_integral_type_strategies(draw):
+    return draw(from_dtype(draw(st.one_of([unsigned_integer_dtypes()]))).filter(non_infinite))
 
 @st.composite
 def dataframes_with_names_and_dtypes(draw, names, dtype_strategy):
     cols = [hs_pd.column(name, dtype=draw(dtype_strategy)) for name in names]
     return draw(hs_pd.data_frames(cols, index=hs_pd.range_indexes()))
```

## arcticdb/util/test.py

```diff
@@ -181,15 +181,15 @@
         params = params + xfail_marker
         ids = ids + xfail_ids
 
     return pytest.mark.parametrize(fields, params, ids=ids)
 
 
 def configure_test_logger(level="INFO"):
-    level = os.getenv("ARCTICC_TEST_LOG_LEVEL", "INFO")
+    level = os.getenv("ARCTICC_TEST_LOG_LEVEL", level)
     if os.getenv("ARCTICC_TEST_FILE_LOGGING"):
         outputs = ["file", "console"]
     else:
         outputs = ["console"]
     configure(get_test_logger_config(level=level, outputs=outputs), force=True)
 
 
@@ -458,27 +458,26 @@
     cols = df.columns
     num_cols = len(cols)
     num_rows = len(df)
     rows_per_slice = int(num_rows / num_slices)
     rows_per_slice = 1 if rows_per_slice == 0 else rows_per_slice
 
     slices = []
-    expected = pd.DataFrame()
     column_index = 0
 
     for step in range(0, num_rows, rows_per_slice):
         df_slice = df.iloc[step : step + rows_per_slice]
         col_to_drop_i = (column_index + 1) % num_cols
         if col_to_drop_i != 0:
             col_to_drop = cols[col_to_drop_i]
             df_slice = df_slice.drop(columns=[col_to_drop])
         column_index += 1
         slices.append(df_slice)
-        expected = expected.append(df_slice)
-
+    
+    expected = pd.concat(slices)
     return expected, slices
 
 
 def regularize_dataframe(df):
     output = df.copy(deep=True)
     for col in output.select_dtypes(include=["object"]).columns:
         output[col] = output[col].fillna("")
```

## arcticdb/version_store/_normalization.py

```diff
@@ -3,14 +3,15 @@
 
 Use of this software is governed by the Business Source License 1.1 included in the file licenses/BSL.txt.
 
 As of the Change Date specified in that file, in accordance with the Business Source License, use of this software will be governed by the Apache License, version 2.0.
 """
 import copy
 import datetime
+from datetime import timedelta
 import math
 
 import numpy as np
 import os
 import sys
 import pandas as pd
 import pickle
@@ -721,18 +722,18 @@
 
                 # Restore the timezone on the series used to construct the index level
                 tz = midx.timezone.get(index_level_num, "")
                 if tz != "":
                     index_col = index_col.dt.tz_localize(tz)
 
                 levels.append(index_col)
-            if pd.__version__.startswith("1"):
-                index = pd.MultiIndex(levels=levels, codes=[[]] * len(levels), names=index_names)
-            else:
+            if pd.__version__.startswith("0"):
                 index = pd.MultiIndex(levels=levels, labels=[[]] * len(levels), names=index_names)
+            else:
+                index = pd.MultiIndex(levels=levels, codes=[[]] * len(levels), names=index_names)
             df = df.iloc[:, midx.field_count :]
             df.index = index
         else:
             df.set_index(list(df.columns[: midx.field_count]), append=True, inplace=True)
 
             # Restore the timezones in all but the first index which is fixed in _index_from_records.
             for key in midx.timezone:
@@ -1251,14 +1252,18 @@
     def _strip_tz(s, e):
         return s.tz_localize(None), e.tz_localize(None)
 
     if hasattr(data, "loc"):
         if not data.index.get_level_values(0).tz:
             start, end = _strip_tz(start, end)
         data = data.loc[pd.to_datetime(start) : pd.to_datetime(end)]
+    else:  # non-Pandas, try to slice it anyway
+        if not getattr(data, "timezone", None):
+            start, end = _strip_tz(start, end)
+        data = data[start.to_pydatetime() - timedelta(microseconds=1) : end.to_pydatetime() + timedelta(microseconds=1)]
     return data
 
 
 def normalize_dt_range_to_ts(dtr: "DateRangeInput") -> Tuple[Timestamp, Timestamp]:
     def _to_utc_ts(v: "ExplicitlySupportedDates", bound_name: str) -> Timestamp:
         if not isinstance(v, supported_time_types):
             raise TypeError(
```

## arcticdb/version_store/_store.py

```diff
@@ -15,15 +15,15 @@
 import itertools
 import attr
 import warnings
 import difflib
 from datetime import datetime
 from numpy import datetime64
 from pandas import Timestamp, to_datetime, Timedelta
-from typing import Any, Optional, Union, List, Mapping, Iterable, Sequence, Tuple, Dict, TYPE_CHECKING
+from typing import Any, Optional, Union, List, Mapping, Iterable, Sequence, Tuple, Dict, Set, TYPE_CHECKING
 from contextlib import contextmanager
 
 from arcticc.pb2.descriptors_pb2 import TypeDescriptor, SortedValue
 from arcticc.pb2.storage_pb2 import LibraryConfig, EnvironmentConfigsMap
 from arcticdb.preconditions import check
 from arcticdb.supported_types import time_types as supported_time_types
 from arcticdb.toolbox.library_tool import LibraryTool
@@ -41,14 +41,15 @@
 from arcticdb_ext.version_store import TailRange as _TailRange
 from arcticdb_ext.version_store import SignedRowRange as _SignedRowRange
 from arcticdb_ext.version_store import PythonVersionStore as _PythonVersionStore
 from arcticdb_ext.version_store import PythonVersionStoreReadQuery as _PythonVersionStoreReadQuery
 from arcticdb_ext.version_store import PythonVersionStoreUpdateQuery as _PythonVersionStoreUpdateQuery
 from arcticdb_ext.version_store import PythonVersionStoreReadOptions as _PythonVersionStoreReadOptions
 from arcticdb_ext.version_store import PythonVersionStoreVersionQuery as _PythonVersionStoreVersionQuery
+from arcticdb_ext.version_store import ColumnStats as _ColumnStats
 from arcticdb_ext.version_store import StreamDescriptorMismatch
 from arcticdb.authorization.permissions import OpenMode
 from arcticdb.exceptions import ArcticNativeNotYetImplemented, ArcticNativeException
 from arcticdb.flattener import Flattener
 from arcticdb.log import version as log
 from arcticdb.version_store._custom_normalizers import get_custom_normalizer, CompositeCustomNormalizer
 from arcticdb.version_store._normalization import (
@@ -794,14 +795,124 @@
                 library=self._library.library_path,
                 version=vit.version,
                 metadata=metadata,
                 data=None,
                 host=self.env,
             )
 
+    def create_column_stats(
+            self,
+            symbol: str,
+            column_stats: Dict[str, Set[str]],
+            as_of: VersionQueryInput = None,
+    ) -> None:
+        """
+        Calculates the specified column statistics for each row-slice for the given symbol. In the future, these
+        statistics will be used by `QueryBuilder` filtering operations to reduce the number of data segments read out
+        of storage.
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        column_stats: `Dict[str, Set[str]]`
+            The column stats to create.
+            Keys are column names.
+            Values are sets of statistic types to build for that column. Options are:
+                "MINMAX" : store the minimum and maximum value for the column in each row-slice
+        as_of : `str` or `int` or `datetime.datetime`
+            Create the column stats for the version as it was as_of the point in time.
+            `int` : specific version number
+            `str` : snapshot name which contains the version
+            `datetime.datetime` : the version of the data that existed as_of the requested point in time
+
+        Returns
+        -------
+        None
+        """
+        column_stats = self._get_column_stats(column_stats)
+        version_query = self._get_version_query(as_of)
+        self.version_store.create_column_stats_version(symbol, column_stats, version_query)
+
+    def drop_column_stats(
+            self,
+            symbol: str,
+            column_stats: Optional[Dict[str, Set[str]]] = None,
+            as_of: VersionQueryInput = None,
+    ) -> None:
+        """
+        Deletes the specified column statistics for the given symbol.
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        column_stats: `Optional[Dict[str, Set[str]]], default=None`
+            The column stats to drop. If not provided, all column stats will be dropped.
+            See documentation of `create_column_stats` method for more details.
+        as_of : `str` or `int` or `datetime.datetime`
+            Create the column stats for the version as it was as_of the point in time.
+            `int` : specific version number
+            `str` : snapshot name which contains the version
+            `datetime.datetime` : the version of the data that existed as_of the requested point in time
+
+        Returns
+        -------
+        None
+        """
+        column_stats = self._get_column_stats(column_stats)
+        version_query = self._get_version_query(as_of)
+        self.version_store.drop_column_stats_version(symbol, column_stats, version_query)
+
+    def read_column_stats(self, symbol: str, as_of: VersionQueryInput = None, **kwargs) -> pd.DataFrame:
+        """
+        Read all the column statistics data that has been generated for the given symbol.
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        as_of : `str` or `int` or `datetime.datetime`
+            Create the column stats for the version as it was as_of the point in time.
+            `int` : specific version number
+            `str` : snapshot name which contains the version
+            `datetime.datetime` : the version of the data that existed as_of the requested point in time
+
+        Returns
+        -------
+        `pandas.DataFrame`
+            DataFrame representing the stored column statistics for each row-slice in a human-readable format.
+        """
+        version_query = self._get_version_query(as_of, **kwargs)
+        data = denormalize_dataframe(self.version_store.read_column_stats_version(symbol, version_query))
+        return data
+
+    def get_column_stats_info(self, symbol: str, as_of: VersionQueryInput = None, **kwargs) -> Dict[str, Set[str]]:
+        """
+        Read the column statistics dictionary for the given symbol.
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        as_of : `str` or `int` or `datetime.datetime`
+            Create the column stats for the version as it was as_of the point in time.
+            `int` : specific version number
+            `str` : snapshot name which contains the version
+            `datetime.datetime` : the version of the data that existed as_of the requested point in time
+
+        Returns
+        -------
+        `Dict[str, Set[str]]`
+            A dict from column names to sets of column stats that have been generated for that column.
+            In the same format as the `column_stats` argument provided to `create_column_stats` and `drop_column_stats`.
+        """
+        version_query = self._get_version_query(as_of, **kwargs)
+        return self.version_store.get_column_stats_info_version(symbol, version_query).to_map()
+
     def _batch_read_keys(self, atom_keys):
         for result in self.version_store.batch_read_keys(atom_keys):
             read_result = ReadResult(*result)
             vitem = self._adapt_read_res(read_result)
             yield vitem
 
     def batch_read(
@@ -1359,14 +1470,17 @@
 
     def _get_queries(self, as_of, date_range, row_range, columns, query_builder, **kwargs):
         version_query = self._get_version_query(as_of, **kwargs)
         read_options = self._get_read_options(**kwargs)
         read_query = self._get_read_query(date_range, row_range, columns, query_builder)
         return version_query, read_options, read_query
 
+    def _get_column_stats(self, column_stats):
+        return None if column_stats is None else _ColumnStats(column_stats)
+
     def read(
         self,
         symbol: str,
         as_of: Optional[VersionQueryInput] = None,
         date_range: Optional[DateRangeInput] = None,
         row_range: Optional[Tuple[int, int]] = None,
         columns: Optional[List[str]] = None,
@@ -2390,12 +2504,109 @@
 
             print(
                 "{} \tObjects: {}\tTotal Size: {}\tAvg Size {}".format(
                     key, value[0], format_bytes(value[1]), format_bytes(value[1] / value[0])
                 )
             )
 
+    def is_symbol_fragmented(self, symbol: str, segment_size: Optional[int] = None) -> bool:
+        """
+        Check whether the number of segments that would be reduced by compaction is more than or equal to the
+        value specified by the configuration option "SymbolDataCompact.SegmentCount" (defaults to 100).
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        segment_size: `int`
+            Target for maximum no. of rows per segment, after compaction.
+            If parameter is not provided, library option for segments's maximum row size will be used
+
+        Notes
+        ----------
+        Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting
+        in the future. This API will allow overriding the setting as well.
+        
+        Returns
+        -------
+        bool
+        """
+        return self.version_store.is_symbol_fragmented(symbol, segment_size)
+
+    def defragment_symbol_data(self, symbol: str, segment_size: Optional[int] = None) -> VersionedItem:
+        """
+        Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer).
+        This method calls `is_symbol_fragmented` to determine whether to proceed with the defragmentation operation. 
+
+        CAUTION - Please note that a major restriction of this method at present is that any column slicing present on the data will be
+        removed in the new version created as a result of this method. 
+        As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of 
+        the symbol (by using the `columns` parameter) may be negatively impacted in the defragmented version. 
+        If your symbol has less than 127 columns this caveat does not apply. 
+        For more information, please see `columns_per_segment` here:
+
+        https://docs.arcticdb.io/api/arcticdb/arcticdb.LibraryOptions
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        segment_size: `int`
+            Target for maximum no. of rows per segment, after compaction.
+            If parameter is not provided, library option - "segment_row_size" will be used
+            Note that no. of rows per segment, after compaction, may exceed the target.
+            It is for achieving smallest no. of segment after compaction. Please refer to below example for further explantion.
+
+        Returns
+        -------
+        VersionedItem
+            Structure containing metadata and version number of the defragmented symbol in the store.
+
+        Raises
+        ------
+        1002 ErrorCategory.INTERNAL:E_ASSERTION_FAILURE
+            If `is_symbol_fragmented` returns false.
+        2001 ErrorCategory.NORMALIZATION:E_UNIMPLEMENTED_INPUT_TYPE
+            If library option - "bucketize_dynamic" is ON.
+
+        Examples
+        --------
+        >>> lib.write("symbol", pd.DataFrame({"A": [0]}, index=[pd.Timestamp(0)]))
+        >>> lib.append("symbol", pd.DataFrame({"A": [1, 2]}, index=[pd.Timestamp(1), pd.Timestamp(2)]))
+        >>> lib.append("symbol", pd.DataFrame({"A": [3]}, index=[pd.Timestamp(3)]))
+        >>> lib.read_index(sym)
+                            start_index                     end_index  version_id stream_id          creation_ts          content_hash  index_type  key_type  start_col  end_col  start_row  end_row
+        1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000001          20    b'sym'  1678974096622685727   6872717287607530038          84         2          1        2          0        1
+        1970-01-01 00:00:00.000000001 1970-01-01 00:00:00.000000003          21    b'sym'  1678974096931527858  12345256156783683504          84         2          1        2          1        3
+        1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          22    b'sym'  1678974096970045987   7952936283266921920          84         2          1        2          3        4
+        >>> lib.version_store.defragment_symbol_data("symbol", 2)
+        >>> lib.read_index(sym)  # Returns two segments rather than three as a result of the defragmentation operation
+                            start_index                     end_index  version_id stream_id          creation_ts         content_hash  index_type  key_type  start_col  end_col  start_row  end_row
+        1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000003          23    b'sym'  1678974097067271451  5576804837479525884          84         2          1        2          0        3
+        1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          23    b'sym'  1678974097067427062  7952936283266921920          84         2          1        2          3        4
+
+        Notes
+        ----------
+        Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting
+        in the future. This API will allow overriding the setting as well.
+        """
+
+        if self._lib_cfg.lib_desc.version.write_options.bucketize_dynamic:
+            raise ArcticNativeNotYetImplemented(
+                f"Support for library with 'bucketize_dynamic' ON is not implemented yet"
+            )
+
+        result = self.version_store.defragment_symbol_data(symbol, segment_size)
+        return VersionedItem(
+            symbol=result.symbol,
+            library=self._library.library_path,
+            version=result.version,
+            metadata=None,
+            data=None,
+            host=self.env,
+        )
+
     def library(self):
         return self._library
 
     def library_tool(self) -> LibraryTool:
         return LibraryTool(self.library())
```

## arcticdb/version_store/helper.py

```diff
@@ -8,14 +8,15 @@
 import re
 import time
 
 
 from arcticc.pb2.lmdb_storage_pb2 import Config as LmdbConfig
 from arcticc.pb2.s3_storage_pb2 import Config as S3Config
 from arcticc.pb2.in_memory_storage_pb2 import Config as MemoryConfig
+from arcticc.pb2.mongo_storage_pb2 import Config as MongoConfig
 from arcticc.pb2.storage_pb2 import (
     EnvironmentConfigsMap,
     EnvironmentConfig,
     LibraryConfig,
     LibraryDescriptor,
     VariantStorage,
     Permissions,
@@ -143,15 +144,14 @@
 def get_secondary_storage_for_lib_name(lib_name, env):
     # type: (LibName, EnvironmentConfigsMap)->(StorageId, VariantStorage)
     sid = "{}_store_2".format(lib_name)
     return sid, env.storage_by_id[sid]
 
 
 def _add_lib_desc_to_env(env, lib_name, sid, description=None):
-    # type: (EnvironmentConfigsMap, LibName, StorageId, Optional[AnyStr], Optional[AnyStr], bool)->None
     if lib_name in env.lib_by_path:
         raise ArcticNativeException("Library {} already configured in {}".format(lib_name, env))
     lib_desc = env.lib_by_path[lib_name]
     lib_desc.storage_ids.append(sid)
     lib_desc.name = lib_name
     if description:
         lib_desc.description = description
@@ -177,21 +177,33 @@
 
     sid, storage = get_storage_for_lib_name(lib_name, env)
     storage.config.Pack(lmdb, type_url_prefix="cxx.arctic.org")
     _add_lib_desc_to_env(env, lib_name, sid, description)
 
 
 def add_memory_library_to_env(cfg, lib_name, env_name, description=None):
-    # type: (EnvironmentConfigsMap, LibName, EnvName, Optional[FilePath], Optional[AnyStr],Optional[AnyStr], bool)->None
     env = cfg.env_by_id[env_name]
     in_mem = MemoryConfig()
 
     sid, storage = get_storage_for_lib_name(lib_name, env)
     storage.config.Pack(in_mem, type_url_prefix="cxx.arctic.org")
-    _add_lib_desc_to_env(env, lib_name, sid, description, lib_type, prefer_native)
+    _add_lib_desc_to_env(env, lib_name, sid, description)
+
+
+def add_mongo_library_to_env(
+        cfg, lib_name, env_name, uri=None, description=None
+):
+    env = cfg.env_by_id[env_name]
+    mongo = MongoConfig()
+    if uri is not None:
+        mongo.uri = uri
+
+    sid, storage = get_storage_for_lib_name(lib_name, env)
+    storage.config.Pack(mongo, type_url_prefix="cxx.arctic.org")
+    _add_lib_desc_to_env(env, lib_name, sid, description)
 
 
 def get_s3_proto(
     cfg,
     lib_name,
     env_name,
     credential_name,
@@ -283,14 +295,20 @@
 
 def create_test_memory_cfg(lib_name=Defaults.LIB, description=None):
     cfg = EnvironmentConfigsMap()
     add_memory_library_to_env(cfg, lib_name=lib_name, env_name=Defaults.ENV, description=description)
     return cfg
 
 
+def create_test_mongo_cfg(lib_name=Defaults.LIB, uri="mongodb://localhost:27017", description=None):
+    cfg = EnvironmentConfigsMap()
+    add_mongo_library_to_env(cfg, lib_name=lib_name, env_name=Defaults.ENV, uri=uri, description=description)
+    return cfg
+
+
 # see https://regex101.com/r/mBCS80/1
 _LIB_PATH_REGEX = re.compile(
     r"""\b
 (
 (?:\w+) # lib name first fragment as non capturing group
 \.
 (?:\w+) # lib name second fragment as non capturing group
```

## arcticdb/version_store/library.py

```diff
@@ -1415,11 +1415,94 @@
 
     def reload_symbol_list(self):
         """
         Forces the symbol list cache to be reloaded
         """
         self._nvs.version_store.reload_symbol_list()
 
+    def is_symbol_fragmented(self, symbol: str, segment_size: Optional[int] = None) -> bool:
+        """
+        Check whether the number of segments that would be reduced by compaction is more than or equal to the
+        value specified by the configuration option "SymbolDataCompact.SegmentCount" (defaults to 100).
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        segment_size: `int`
+            Target for maximum no. of rows per segment, after compaction.
+            If parameter is not provided, library option for segments's maximum row size will be used
+
+        Notes
+        ----------
+        Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting
+        in the future. This API will allow overriding the setting as well.
+        
+        Returns
+        -------
+        bool
+        """
+        return self._nvs.is_symbol_fragmented(symbol, segment_size)
+
+    def defragment_symbol_data(self, symbol: str, segment_size: Optional[int] = None) -> VersionedItem:
+        """
+        Compacts fragmented segments by merging row-sliced segments (https://docs.arcticdb.io/technical/on_disk_storage/#data-layer).
+        This method calls `is_symbol_fragmented` to determine whether to proceed with the defragmentation operation. 
+
+        CAUTION - Please note that a major restriction of this method at present is that any column slicing present on the data will be
+        removed in the new version created as a result of this method. 
+        As a result, if the impacted symbol has more than 127 columns (default value), the performance of selecting individual columns of 
+        the symbol (by using the `columns` parameter) may be negatively impacted in the defragmented version. 
+        If your symbol has less than 127 columns this caveat does not apply. 
+        For more information, please see `columns_per_segment` here:
+
+        https://docs.arcticdb.io/api/arcticdb/arcticdb.LibraryOptions
+
+        Parameters
+        ----------
+        symbol: `str`
+            Symbol name.
+        segment_size: `int`
+            Target for maximum no. of rows per segment, after compaction.
+            If parameter is not provided, library option - "segment_row_size" will be used
+            Note that no. of rows per segment, after compaction, may exceed the target.
+            It is for achieving smallest no. of segment after compaction. Please refer to below example for further explantion.
+
+        Returns
+        -------
+        VersionedItem
+            Structure containing metadata and version number of the defragmented symbol in the store.
+
+        Raises
+        ------
+        1002 ErrorCategory.INTERNAL:E_ASSERTION_FAILURE
+            If `is_symbol_fragmented` returns false.
+        2001 ErrorCategory.NORMALIZATION:E_UNIMPLEMENTED_INPUT_TYPE
+            If library option - "bucketize_dynamic" is ON
+
+        Examples
+        --------
+        >>> lib.write("symbol", pd.DataFrame({"A": [0]}, index=[pd.Timestamp(0)]))
+        >>> lib.append("symbol", pd.DataFrame({"A": [1, 2]}, index=[pd.Timestamp(1), pd.Timestamp(2)]))
+        >>> lib.append("symbol", pd.DataFrame({"A": [3]}, index=[pd.Timestamp(3)]))
+        >>> lib.read_index(sym)
+                            start_index                     end_index  version_id stream_id          creation_ts          content_hash  index_type  key_type  start_col  end_col  start_row  end_row
+        1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000001          20    b'sym'  1678974096622685727   6872717287607530038          84         2          1        2          0        1
+        1970-01-01 00:00:00.000000001 1970-01-01 00:00:00.000000003          21    b'sym'  1678974096931527858  12345256156783683504          84         2          1        2          1        3
+        1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          22    b'sym'  1678974096970045987   7952936283266921920          84         2          1        2          3        4
+        >>> lib.version_store.defragment_symbol_data("symbol", 2)
+        >>> lib.read_index(sym)  # Returns two segments rather than three as a result of the defragmentation operation
+                            start_index                     end_index  version_id stream_id          creation_ts         content_hash  index_type  key_type  start_col  end_col  start_row  end_row
+        1970-01-01 00:00:00.000000000 1970-01-01 00:00:00.000000003          23    b'sym'  1678974097067271451  5576804837479525884          84         2          1        2          0        3
+        1970-01-01 00:00:00.000000003 1970-01-01 00:00:00.000000004          23    b'sym'  1678974097067427062  7952936283266921920          84         2          1        2          3        4
+
+        Notes
+        ----------
+        Config map setting - SymbolDataCompact.SegmentCount will be replaced by a library setting
+        in the future. This API will allow overriding the setting as well.
+        """
+        return self._nvs.defragment_symbol_data(symbol, segment_size)
+
     @property
     def name(self):
         """The name of this library."""
         return self._nvs.name()
```

## arcticdb/version_store/processing.py

```diff
@@ -10,15 +10,15 @@
 from math import inf
 
 import numpy as np
 import pandas as pd
 
 from abc import ABC, abstractmethod
 
-from arcticdb.exceptions import ArcticNativeException
+from arcticdb.exceptions import ArcticNativeException, UserInputException
 from arcticdb.supported_types import time_types as supported_time_types
 
 from arcticdb_ext.version_store import ExecutionContextOptimisation as _Optimisation
 from arcticdb_ext.version_store import ExecutionContext as _ExecutionContext
 from arcticdb_ext.version_store import ExpressionName as _ExpressionName
 from arcticdb_ext.version_store import ColumnName as _ColumnName
 from arcticdb_ext.version_store import ValueName as _ValueName
@@ -226,23 +226,27 @@
 def value_list_from_args(*args):
     if len(args) == 1 and is_supported_sequence(args[0]):
         collection = args[0]
     else:
         collection = args
     array_list = []
     value_set = set()
+    contains_integer = False
     if len(collection) > 0:
         for value in collection:
             if value not in value_set:
                 value_set.add(value)
                 if isinstance(value, supported_time_types):
                     value = int(value.timestamp() * 1_000_000_000)
-                dtype = np.min_scalar_type(value) if isinstance(value, (int, np.integer)) else None
-                array_list.append(np.full(1, value, dtype=dtype))
+                elem = np.array([value]) if isinstance(value, (int, np.integer)) else np.full(1, value, dtype=None)
+                array_list.append(elem)
+                contains_integer = contains_integer or isinstance(value, (int, np.integer))
         value_list = np.concatenate(array_list)
+        if contains_integer and value_list.dtype == np.float64:
+            raise UserInputException("Invalid datatype conversion to double")
         if value_list.dtype == np.float16:
             value_list = value_list.astype(np.float32)
         elif value_list.dtype.kind == "U":
             value_list = value_list.tolist()
     else:
         # Return an empty list. This will call the string ctor for ValueSet, but also set a bool flag so that numeric
         # types also behave as expected
@@ -617,20 +621,21 @@
         raise ArcticNativeException("Infinite values not supported in queries")
 
     if isinstance(value, np.floating):
         f = CONSTRUCTOR_MAP.get(value.dtype.kind).get(value.dtype.itemsize)
     elif isinstance(value, np.integer):
         min_scalar_type = np.min_scalar_type(value)
         f = CONSTRUCTOR_MAP.get(min_scalar_type.kind).get(min_scalar_type.itemsize)
+    elif isinstance(value, (pd.Timestamp, pd.Timedelta)):
+        # pd.Timestamp is in supported_time_types, but its timestamp() method can't provide ns precision
+        value = value.value
+        f = ValueInt64
     elif isinstance(value, supported_time_types):
         value = int(value.timestamp() * 1_000_000_000)
         f = ValueInt64
-    elif isinstance(value, pd.Timedelta):
-        value = value.value
-        f = ValueInt64
     elif isinstance(value, datetime.timedelta):
         value = int(value.total_seconds() * 1_000_000_000)
         f = ValueInt64
     elif isinstance(value, bool):
         f = ValueBool
     else:
         f = _Value
```

## Comparing `arcticdb-1.1.0.dist-info/LICENSE.txt` & `arcticdb-1.2.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `arcticdb-1.1.0.dist-info/METADATA` & `arcticdb-1.2.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: arcticdb
-Version: 1.1.0
+Version: 1.2.0
 Summary: ArcticDB DataFrame Database
 Home-page: https://github.com/man-group/arcticdb
 Author: Man Alpha Technology
 Author-email: arcticdb@man.com
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: Microsoft :: Windows
@@ -36,14 +36,15 @@
 Requires-Dist: pytest-server-fixtures ; extra == 'testing'
 Requires-Dist: mock ; extra == 'testing'
 Requires-Dist: boto3 ; extra == 'testing'
 Requires-Dist: moto ; extra == 'testing'
 Requires-Dist: flask ; extra == 'testing'
 Requires-Dist: flask-cors ; extra == 'testing'
 Requires-Dist: hypothesis (<6.73) ; extra == 'testing'
+Requires-Dist: pymongo ; extra == 'testing'
 
 <p align="center">
 <img src="https://github.com/man-group/ArcticDB/raw/master/static/ArcticDBCropped.png" width="40%">
 </p>
 
 ---
```

### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: arcticdb Version: 1.1.0 Summary: ArcticDB DataFrame
+Metadata-Version: 2.1 Name: arcticdb Version: 1.2.0 Summary: ArcticDB DataFrame
 Database Home-page: https://github.com/man-group/arcticdb Author: Man Alpha
 Technology Author-email: arcticdb@man.com Classifier: Programming Language ::
 Python :: 3 Classifier: Operating System :: POSIX :: Linux Classifier:
 Operating System :: Microsoft :: Windows Classifier: Topic :: Database
 Classifier: Topic :: Database :: Database Engines/Servers Description-Content-
 Type: text/markdown License-File: LICENSE.txt License-File: NOTICE.txt
 Requires-Dist: numpy Requires-Dist: pandas (<2) Requires-Dist: attrs Requires-
@@ -13,15 +13,16 @@
 Provides-Extra: testing Requires-Dist: pytest ; extra == 'testing' Requires-
 Dist: pytest-cpp ; extra == 'testing' Requires-Dist: pytest-timeout ; extra ==
 'testing' Requires-Dist: packaging ; extra == 'testing' Requires-Dist: future ;
 extra == 'testing' Requires-Dist: pytest-server-fixtures ; extra == 'testing'
 Requires-Dist: mock ; extra == 'testing' Requires-Dist: boto3 ; extra ==
 'testing' Requires-Dist: moto ; extra == 'testing' Requires-Dist: flask ; extra
 == 'testing' Requires-Dist: flask-cors ; extra == 'testing' Requires-Dist:
-hypothesis (<6.73) ; extra == 'testing'
+hypothesis (<6.73) ; extra == 'testing' Requires-Dist: pymongo ; extra ==
+'testing'
  [https://github.com/man-group/ArcticDB/raw/master/static/ArcticDBCropped.png]
 ---
      [https://raw.githubusercontent.com/man-group/ArcticDB/master/static/
                              ArcticDBTerminal.gif]
 ---
  ArcticDB_Website | ArcticDB_Blog | Press_Release | Press_Release | Community
```

## Comparing `arcticdb-1.1.0.dist-info/NOTICE.txt` & `arcticdb-1.2.0.dist-info/NOTICE.txt`

 * *Files 1% similar despite different names*

```diff
@@ -3,14 +3,17 @@
 
 This product includes software from the JEMalloc project (BSD, 2-Clause).
 * Copyright (C) 2002-present Jason Evans <jasone@canonware.com>.
 All rights reserved.
 * Copyright (C) 2007-2012 Mozilla Foundation.  All rights reserved.
 * Copyright (C) 2009-present Facebook, Inc.  All rights reserved.
 
+This product includes software from the PRCE project (BSD).
+https://www.pcre.org/licence.txt
+
 This product includes software from the LZ4 project (BSD, 2-Clause).
 https://github.com/lz4/lz4/blob/dev/lib/LICENSE
 
 This product includes software from the ZSTD project (BSD).
 https://github.com/facebook/zstd/blob/dev/LICENSE
 
 This product includes software from the OpenSSL (Apache 2).
```

## Comparing `arcticdb-1.1.0.dist-info/RECORD` & `arcticdb-1.2.0.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-arcticdb_ext.cp39-win_amd64.pyd,sha256=HZTQgAQHKECDZ8YQlvMI9JUZyZ-bMHz02r4Za_cyDYw,21822464
+arcticdb_ext.cp39-win_amd64.pyd,sha256=ijicGM3o4-37Qi1rphBF4aT9V-R2aOLPy9Cz0njj2u4,22184448
 arcticc/__init__.py,sha256=h-QyvMVzDNpT3jyVskcSbUVFXxGCRxieFPrvTveZG9k,64
 arcticc/pb2/__init__.py,sha256=pfsson0mxPSudMCFd_HpaUTeFROIFD4yA_XQgqAYMiU,583
-arcticdb/__init__.py,sha256=XO7gSiA7WeUQEAEH0jWpmIAjSm9-e81uji2QhGalsog,401
+arcticdb/__init__.py,sha256=nnHi9Zc2DT30JDIU7PfZngcTN_RhGXv-8Sh2Yd6CNJI,424
 arcticdb/_msgpack_compat.py,sha256=i_3HluY89KVSXFnxC-UjcdK0zNsIcSBLmY3YpKFeLl8,447
 arcticdb/arctic.py,sha256=p9qG4ZfHo0zI-ZE2OIZOmyNoV3ZI2xoNzcvFlOwwWSs,10405
 arcticdb/config.py,sha256=EzKNvld8Y6dA5R7CkbZ03mMnlOQSYlPen-pzK5e9u-E,7970
 arcticdb/exceptions.py,sha256=ArcFJwbH8PKD5CfC5Aq5LEKwcjIx-ftsocuGtwZjUrU,765
 arcticdb/flattener.py,sha256=1BBY-zD4FBD40DxhfIwXFpP9Glcgx5xaa2B098OKV7s,9363
 arcticdb/log.py,sha256=HTRQyuTNxr5g9a3vFRpGxgGZlhs9LJRzbtJQ64sP5YY,2071
 arcticdb/options.py,sha256=lYScTfHS-JGUZFomGjkLeLS459v_PrFADZhL9AV24Zw,6436
 arcticdb/preconditions.py,sha256=85PtbfJEUGvVeRethePECPEhwAY6ZTZ3oy8Fx-KePyY,519
 arcticdb/supported_types.py,sha256=FUr7Slxn5c1v_wQ20H3l5wlK1cwpDoQKJfwDEjtnhEI,1612
-arcticdb/tools.py,sha256=c1vbgaPREY1pNqxX4Uovct4LUC0OC9WDmz4gHAQXOk0,3119
+arcticdb/tools.py,sha256=umjEygBZDw6_IWrj5gew-werTfTW-UGsqp4TdtfQt6g,3142
 arcticdb/adapters/__init__.py,sha256=wmFWqLdci5GAilj5KQGwGPYLnDRRKDOxrc80r27lYlg,138
 arcticdb/adapters/arctic_library_adapter.py,sha256=brVvjPTkxUlWCVhpNQp2nl-V368reG_1qNa57QAoG3s,2211
 arcticdb/adapters/lmdb_library_adapter.py,sha256=YmygJR8j1JowEkxfaDO2lFXRp7GRpAuXt-zikw9OSaA,2394
 arcticdb/adapters/s3_library_adapter.py,sha256=6auNlcU_IldvXaYW48j1wRJ9jNyxrdjf_A2ZcE9bFGw,6434
 arcticdb/authorization/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 arcticdb/authorization/permissions.py,sha256=3E91GMrL6xSEKloOWdIGdaKrHfHoqhkSEqJwjJZJ3HQ,969
 arcticdb/proto/3/arcticc/pb2/config_pb2.py,sha256=W1pb3T1ArzmExVkjgF3xWn5WMWL5l9fR7rR0unnKhxE,8935
@@ -46,26 +46,26 @@
 arcticdb/proto/4/arcticc/pb2/s3_storage_pb2.py,sha256=MK5ZPt9oik4TyWxLke4VC3BM3KfQVwkdcudbd1G7RK4,1496
 arcticdb/proto/4/arcticc/pb2/storage_pb2.py,sha256=liHmX8x8V6jRhFwzALwdYGsCx-npRpkbpg4pRL3mTcM,9907
 arcticdb/proto/4/arcticc/pb2/utils_pb2.py,sha256=ksfhHFjEdosK8mwT1fcaiMfVTal7C9SAviBxFYDbnHI,2310
 arcticdb/toolbox/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 arcticdb/toolbox/library_tool.py,sha256=5jBjyB2-VMczywWrEjeWvnRHQ0pzSU5gCsEdusximQc,5669
 arcticdb/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 arcticdb/util/errors.py,sha256=vmVvTdFvJJP0cofApzcor6Mn6lSmoE5ih3zvEf1Flkc,2705
-arcticdb/util/hypothesis.py,sha256=o4OXd2uromxRL8Lxsz4ApTSnod7NYSLJRq8dlz2qBTo,8464
+arcticdb/util/hypothesis.py,sha256=RA7ty7sFUl_p1B85FFAnQZfFd52f7KYxMkEFLJaMzTs,8769
 arcticdb/util/memory.py,sha256=-YYUz_ATxFGPov8mAOWsvwgulxZng1hgEB4xkAvUkPg,719
 arcticdb/util/tasks.py,sha256=_tIl4ZfwGyxmnquKcc14TlqMbLBflmctkIIn2uTJAjo,6042
-arcticdb/util/test.py,sha256=LcUrt_z6vao8MNWqa7hQytfTbWe94TnF6FIVim1V2x0,18235
+arcticdb/util/test.py,sha256=AdtnFnoN379lBL2FMlX2hGrYZ6Wp_dzIjDsWmwZcK0Q,18195
 arcticdb/version_store/__init__.py,sha256=uGAsgCsoyGmR-F7zvOx8GuMs1QdnVHdg5l7RSm6XiBY,131
 arcticdb/version_store/_common.py,sha256=XMTAUSUAPWxOZEsnbc44egIX41AAHSIUMT78KignaRY,7142
 arcticdb/version_store/_custom_normalizers.py,sha256=EKqxg39qV8BJPjeSCZ9BEOnZkixCXJW796F91bR8wws,4193
-arcticdb/version_store/_normalization.py,sha256=F3u8vMJKJkZ_kakrz-UUGgE2VHw6iOyrc7sGenyahsI,52304
-arcticdb/version_store/_store.py,sha256=dPiCyR47MJZ0CKBW9T-Cs38PJ6l9qb5fhMM6W023RfM,103093
-arcticdb/version_store/helper.py,sha256=-xeQSScRJJQc7doiyyoY8MpjZMc8dTkBB_COHdy3jLc,10399
-arcticdb/version_store/library.py,sha256=albnZ3FUdcz54InZxLWEd1Fn1QdVmIin0zjXyTrglVE,54049
-arcticdb/version_store/processing.py,sha256=ZcOasG-xJcgUfCdBbgMtkBIlJLZydF6UQdYjZkij95A,24193
+arcticdb/version_store/_normalization.py,sha256=vup8gPRBD53hfrYpAvDr8CxKvTnTQ1KdxPt16zVbgd4,52604
+arcticdb/version_store/_store.py,sha256=TagrLpunpd_4DJYnnTDbYhNsAqvpRVdJY3b0I7y9iH4,113852
+arcticdb/version_store/helper.py,sha256=4e3bmX-6i9UJ8LS48zXsKarQCeFDl3-A8uDd1QgGU2w,10879
+arcticdb/version_store/library.py,sha256=U1P_7Y6su6J95hhEPeKBOsVlOn8ZHuCO8ciK3UlzX7s,59225
+arcticdb/version_store/processing.py,sha256=VSEm8kg-XTG5fhEc_PKpsraTJCjaXVWEOf3yjtbndPk,24592
 arcticdb/version_store/read_result.py,sha256=5HhAJ0Wh01f111qA5XvWqOXABO-2H0jZ78kx-iiQzOk,603
-arcticdb-1.1.0.dist-info/LICENSE.txt,sha256=ruvCXWZm0cgyb-XAEjFcfdJkJ_nGbv9gsYNeps514Ys,4851
-arcticdb-1.1.0.dist-info/METADATA,sha256=9_iT8sF0d99XRV83pZQB5ZoyC9MTuVabgX-N3xChKxg,8348
-arcticdb-1.1.0.dist-info/NOTICE.txt,sha256=d4F-smeSIdGKIqB7RbLhbzGkHYCmeZtnBkUY0HmMguk,18089
-arcticdb-1.1.0.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
-arcticdb-1.1.0.dist-info/top_level.txt,sha256=jDxz3uFaLYFuxxPf3h6-sqPQqEFbMkWGP-9vE8Dbi1w,30
-arcticdb-1.1.0.dist-info/RECORD,,
+arcticdb-1.2.0.dist-info/LICENSE.txt,sha256=ruvCXWZm0cgyb-XAEjFcfdJkJ_nGbv9gsYNeps514Ys,4851
+arcticdb-1.2.0.dist-info/METADATA,sha256=rZoU5bdSEfuGvsoaHnmzw4XMY4Vavvj72JNxsXkv4r0,8393
+arcticdb-1.2.0.dist-info/NOTICE.txt,sha256=TsVpAVXueJjRq_zV86A3v_TqJRsXCQtYjrEvycwyaaY,18186
+arcticdb-1.2.0.dist-info/WHEEL,sha256=J_4V_gB-O6Y7Pn6lk91K27JaIhI-q07YM5J8Ufzqla4,100
+arcticdb-1.2.0.dist-info/top_level.txt,sha256=jDxz3uFaLYFuxxPf3h6-sqPQqEFbMkWGP-9vE8Dbi1w,30
+arcticdb-1.2.0.dist-info/RECORD,,
```

