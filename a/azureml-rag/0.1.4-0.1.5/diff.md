# Comparing `tmp/azureml_rag-0.1.4-py3-none-any.whl.zip` & `tmp/azureml_rag-0.1.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,59 +1,59 @@
-Zip file size: 166769 bytes, number of entries: 57
--rw-rw-rw-  2.0 fat      246 b- defN 23-May-18 00:06 azureml/rag/__init__.py
--rw-rw-rw-  2.0 fat    39993 b- defN 23-May-18 00:06 azureml/rag/documents.py
--rw-rw-rw-  2.0 fat    27596 b- defN 23-May-18 00:06 azureml/rag/embeddings.py
--rw-rw-rw-  2.0 fat     5058 b- defN 23-May-18 00:06 azureml/rag/mlindex.py
--rw-rw-rw-  2.0 fat     4149 b- defN 23-May-18 00:06 azureml/rag/models.py
--rw-rw-rw-  2.0 fat     4646 b- defN 23-May-18 00:07 azureml/rag/_asset_client/client.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-May-18 00:08 azureml/rag/_asset_client/_restclient/__init__.py
--rw-rw-rw-  2.0 fat     4381 b- defN 23-May-18 00:08 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3538 b- defN 23-May-18 00:08 azureml/rag/_asset_client/_restclient/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-May-18 00:08 azureml/rag/_asset_client/_restclient/_patch.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-18 00:08 azureml/rag/_asset_client/_restclient/_version.py
--rw-rw-rw-  2.0 fat      399 b- defN 23-May-18 00:08 azureml/rag/_asset_client/_restclient/models.py
--rw-rw-rw-  2.0 fat      957 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/__init__.py
--rw-rw-rw-  2.0 fat     3802 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3144 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/_patch.py
--rw-rw-rw-  2.0 fat    81019 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
--rw-rw-rw-  2.0 fat     1833 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/_version.py
--rw-rw-rw-  2.0 fat     6787 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
--rw-rw-rw-  2.0 fat     4635 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   131448 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
--rw-rw-rw-  2.0 fat      585 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
--rw-rw-rw-  2.0 fat   104636 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
--rw-rw-rw-  2.0 fat    80785 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
--rw-rw-rw-  2.0 fat     3902 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
--rw-rw-rw-  2.0 fat     3226 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
--rw-rw-rw-  2.0 fat     1561 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
--rw-rw-rw-  2.0 fat     1255 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/_version.py
--rw-rw-rw-  2.0 fat    11705 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
--rw-rw-rw-  2.0 fat     2566 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
--rw-rw-rw-  2.0 fat   175170 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
--rw-rw-rw-  2.0 fat   188776 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
--rw-rw-rw-  2.0 fat      563 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
--rw-rw-rw-  2.0 fat   162796 b- defN 23-May-18 00:09 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
--rw-rw-rw-  2.0 fat    10785 b- defN 23-May-18 00:07 azureml/rag/langchain/acs.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-18 00:07 azureml/rag/tasks/__init__.py
--rw-rw-rw-  2.0 fat     2094 b- defN 23-May-18 00:07 azureml/rag/tasks/build_faiss.py
--rw-rw-rw-  2.0 fat     7190 b- defN 23-May-18 00:07 azureml/rag/tasks/crack_and_chunk.py
--rw-rw-rw-  2.0 fat     7330 b- defN 23-May-18 00:07 azureml/rag/tasks/embed.py
--rw-rw-rw-  2.0 fat     5410 b- defN 23-May-18 00:07 azureml/rag/tasks/embed_prs.py
--rw-rw-rw-  2.0 fat     2194 b- defN 23-May-18 00:07 azureml/rag/tasks/git_clone.py
--rw-rw-rw-  2.0 fat     2791 b- defN 23-May-18 00:07 azureml/rag/tasks/register_mlindex.py
--rw-rw-rw-  2.0 fat    17977 b- defN 23-May-18 00:07 azureml/rag/tasks/update_acs.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-May-18 00:07 azureml/rag/utils/__init__.py
--rw-rw-rw-  2.0 fat     1588 b- defN 23-May-18 00:07 azureml/rag/utils/azureml.py
--rw-rw-rw-  2.0 fat     7435 b- defN 23-May-18 00:07 azureml/rag/utils/connections.py
--rw-rw-rw-  2.0 fat     2350 b- defN 23-May-18 00:07 azureml/rag/utils/git.py
--rw-rw-rw-  2.0 fat     2330 b- defN 23-May-18 00:07 azureml/rag/utils/logging.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-May-18 00:13 azureml_rag-0.1.4.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     4968 b- defN 23-May-18 00:13 azureml_rag-0.1.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-May-18 00:13 azureml_rag-0.1.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-May-18 00:13 azureml_rag-0.1.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6063 b- defN 23-May-18 00:13 azureml_rag-0.1.4.dist-info/RECORD
-57 files, 1149399 bytes uncompressed, 156675 bytes compressed:  86.4%
+Zip file size: 169364 bytes, number of entries: 57
+-rw-rw-rw-  2.0 fat      246 b- defN 23-May-22 06:18 azureml/rag/__init__.py
+-rw-rw-rw-  2.0 fat    39993 b- defN 23-May-22 06:18 azureml/rag/documents.py
+-rw-rw-rw-  2.0 fat    29244 b- defN 23-May-22 06:18 azureml/rag/embeddings.py
+-rw-rw-rw-  2.0 fat     5058 b- defN 23-May-22 06:18 azureml/rag/mlindex.py
+-rw-rw-rw-  2.0 fat     4149 b- defN 23-May-22 06:18 azureml/rag/models.py
+-rw-rw-rw-  2.0 fat     4646 b- defN 23-May-22 06:19 azureml/rag/_asset_client/client.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/__init__.py
+-rw-rw-rw-  2.0 fat     4381 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3538 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/_patch.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/_version.py
+-rw-rw-rw-  2.0 fat      399 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/models.py
+-rw-rw-rw-  2.0 fat      957 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/__init__.py
+-rw-rw-rw-  2.0 fat     3802 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3144 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/_configuration.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/_patch.py
+-rw-rw-rw-  2.0 fat    81019 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/_serialization.py
+-rw-rw-rw-  2.0 fat     1833 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/dataset/_version.py
+-rw-rw-rw-  2.0 fat     6787 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/models/__init__.py
+-rw-rw-rw-  2.0 fat     4635 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   131448 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/models/_patch.py
+-rw-rw-rw-  2.0 fat      585 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/operations/__init__.py
+-rw-rw-rw-  2.0 fat   104636 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/operations/_data_version_operations.py
+-rw-rw-rw-  2.0 fat    80785 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/dataset/operations/_mlindex_operations.py
+-rw-rw-rw-  2.0 fat      893 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/runhistory/__init__.py
+-rw-rw-rw-  2.0 fat     3902 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/runhistory/_azure_machine_learning_workspaces.py
+-rw-rw-rw-  2.0 fat     3226 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/runhistory/_configuration.py
+-rw-rw-rw-  2.0 fat     1561 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/runhistory/_patch.py
+-rw-rw-rw-  2.0 fat     1255 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/runhistory/_vendor.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-May-22 06:20 azureml/rag/_asset_client/_restclient/runhistory/_version.py
+-rw-rw-rw-  2.0 fat    11705 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/runhistory/models/__init__.py
+-rw-rw-rw-  2.0 fat     2566 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/runhistory/models/_azure_machine_learning_workspaces_enums.py
+-rw-rw-rw-  2.0 fat   175170 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py
+-rw-rw-rw-  2.0 fat   188776 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py
+-rw-rw-rw-  2.0 fat      563 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py
+-rw-rw-rw-  2.0 fat   162796 b- defN 23-May-22 06:21 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py
+-rw-rw-rw-  2.0 fat    10785 b- defN 23-May-22 06:19 azureml/rag/langchain/acs.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-22 06:19 azureml/rag/tasks/__init__.py
+-rw-rw-rw-  2.0 fat     2094 b- defN 23-May-22 06:19 azureml/rag/tasks/build_faiss.py
+-rw-rw-rw-  2.0 fat     7544 b- defN 23-May-22 06:19 azureml/rag/tasks/crack_and_chunk.py
+-rw-rw-rw-  2.0 fat     7330 b- defN 23-May-22 06:19 azureml/rag/tasks/embed.py
+-rw-rw-rw-  2.0 fat     6328 b- defN 23-May-22 06:19 azureml/rag/tasks/embed_prs.py
+-rw-rw-rw-  2.0 fat     2340 b- defN 23-May-22 06:19 azureml/rag/tasks/git_clone.py
+-rw-rw-rw-  2.0 fat     3002 b- defN 23-May-22 06:19 azureml/rag/tasks/register_mlindex.py
+-rw-rw-rw-  2.0 fat    18964 b- defN 23-May-22 06:19 azureml/rag/tasks/update_acs.py
+-rw-rw-rw-  2.0 fat      208 b- defN 23-May-22 06:19 azureml/rag/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1588 b- defN 23-May-22 06:19 azureml/rag/utils/azureml.py
+-rw-rw-rw-  2.0 fat     7435 b- defN 23-May-22 06:19 azureml/rag/utils/connections.py
+-rw-rw-rw-  2.0 fat     2350 b- defN 23-May-22 06:19 azureml/rag/utils/git.py
+-rw-rw-rw-  2.0 fat     8529 b- defN 23-May-22 06:19 azureml/rag/utils/logging.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-May-22 06:25 azureml_rag-0.1.5.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     5365 b- defN 23-May-22 06:25 azureml_rag-0.1.5.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-May-22 06:25 azureml_rag-0.1.5.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-May-22 06:25 azureml_rag-0.1.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6063 b- defN 23-May-22 06:25 azureml_rag-0.1.5.dist-info/RECORD
+57 files, 1160259 bytes uncompressed, 159270 bytes compressed:  86.3%
```

## zipnote {}

```diff
@@ -150,23 +150,23 @@
 
 Filename: azureml/rag/utils/git.py
 Comment: 
 
 Filename: azureml/rag/utils/logging.py
 Comment: 
 
-Filename: azureml_rag-0.1.4.dist-info/LICENSE.txt
+Filename: azureml_rag-0.1.5.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_rag-0.1.4.dist-info/METADATA
+Filename: azureml_rag-0.1.5.dist-info/METADATA
 Comment: 
 
-Filename: azureml_rag-0.1.4.dist-info/WHEEL
+Filename: azureml_rag-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_rag-0.1.4.dist-info/top_level.txt
+Filename: azureml_rag-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_rag-0.1.4.dist-info/RECORD
+Filename: azureml_rag-0.1.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/rag/embeddings.py

```diff
@@ -23,17 +23,16 @@
 import pyarrow as pa
 import pyarrow.parquet as pq
 import time
 import yaml
 
 from azureml.rag.documents import LazyDocument
 from azureml.rag.models import parse_model_uri
-from azureml.rag.utils.azureml import get_workspace_from_environment
 from azureml.rag.utils.connections import get_connection_credential, get_connection_by_id_v2, workspace_connection_to_credential
-from azureml.rag.utils.logging import get_logger
+from azureml.rag.utils.logging import get_logger, track_activity, track_error
 
 
 logger = get_logger(__name__)
 
 
 def _parse_open_ai_args(arguments: dict):
     from langchain.embeddings.openai import OpenAIEmbeddings
@@ -41,15 +40,15 @@
 
     logger.info('OpenAI arguments: \n')
     logger.info('\n'.join(f'{k}={v}' for k, v in arguments.items()))
 
     if "api_type" in arguments:
         openai.api_type = arguments["api_type"]
 
-    if openai.api_type == "azure":
+    if "azure" in openai.api_type:
         openai.api_version = arguments.get("api_version", "2023-03-15-preview")
 
     if "endpoint" in arguments:
         openai.api_base = arguments["endpoint"]
 
     if "api_base" in arguments:
         openai.api_base = arguments["api_base"]
@@ -58,15 +57,20 @@
         openai.api_key = os.environ["OPENAI_API_KEY"]
     else:
         if arguments["connection_type"] == "workspace_connection":
             connection_id = arguments.get('connection', {}).get('id', '')
             connection = get_connection_by_id_v2(connection_id)
             openai.api_base = connection.get('properties', {}).get('target')
             openai.api_version = connection.get('properties', {}).get('metadata', {}).get('apiVersion', '2023-03-15-preview')
-            credential = workspace_connection_to_credential(connection)
+            openai.api_type = connection.get('properties', {}).get('metadata', {}).get('apiType', openai.api_type)
+            if openai.api_type == 'azure_ad' or openai.api_type == 'azuread':
+                from azure.identity import DefaultAzureCredential
+                credential = DefaultAzureCredential()
+            else:
+                credential = workspace_connection_to_credential(connection)
         else:
             credential = get_connection_credential(arguments)
         if hasattr(credential, 'key'):
             openai.api_key = credential.key
         else:
             # Add hack to check for "BAKER-OPENAI-API-KEY"
             if arguments.get("connection_type", "workspace_keyvault") == "workspace_keyvault":
@@ -138,20 +142,27 @@
     """Get an embedding function from the given arguments."""
     if "open_ai" in embedding_kind:
         embedder = _parse_open_ai_args(arguments)
 
         def embed(texts: List[str]) -> List[List[float]]:
             # AOAI doesn't allow batch_size > 1 so we serialize embedding here to improve error handling
             embeddings = []
+            pre_batch = None
             for i in range(0, len(texts), embedder.chunk_size):
                 texts_chunk = texts[i:i + embedder.chunk_size]
                 try:
+                    pre_batch = time.time()
                     embeddings.extend(embedder.embed_documents(texts_chunk))
                 except Exception as e:
-                    logger.error(f'Failed to embed:\n{e}.', exc_info=e, extra={'print': True})
+                    if pre_batch:
+                        duration = time.time() - pre_batch
+                    else:
+                        duration = 0
+                    logger.error(f'Failed to embed after {duration}s:\n{e}.', exc_info=e, extra={'print': True})
+                    track_error(logger, 'open_ai.embed', {'batch_size': embedder.chunk_size, 'duration': duration})
                     print(f'Failed texts: {texts_chunk}\nlengths: {[len(t) for t in texts_chunk]}\n')
                     raise e
             return embeddings
 
         return embed
     elif embedding_kind == "hugging_face":
         embedder = get_langchain_embeddings(embedding_kind, arguments)
@@ -296,14 +307,22 @@
     }
     kind: str
     arguments: dict
     _embed_fn: Callable[[List[str]], List[List[float]]]
 
     _document_embeddings: OrderedDict
 
+    def __getitem__(self, key):
+        """Get document by doc_id"""
+        return self._document_embeddings[key]
+
+    def __len__(self):
+        """Get the number of documents in the embeddings."""
+        return len(self._document_embeddings)
+
     def __init__(self, kind: str, **kwargs):
         """Initialize the embeddings."""
         self.kind = kind
         self.arguments = kwargs
         self._embed_fn = get_embed_fn(kind, kwargs)
         self._document_embeddings = OrderedDict()
         self.dimension = kwargs.get('dimension', None)
@@ -320,14 +339,18 @@
             arguments = copy.deepcopy(self.arguments)
             arguments["pickled_embedding_fn"] = gzip.compress(
                 cloudpickle.dumps(arguments["embedding_fn"]))
             del arguments["embedding_fn"]
         else:
             arguments = self.arguments
 
+        if "open_ai" in self.kind:
+            import openai
+            arguments["api_base"] = openai.api_base
+
         metadata = {
             "schema_version": "2",
             "kind": self.kind,
             "dimension": self.get_embedding_dimensions(),
             **self.arguments
         }
 
@@ -507,35 +530,41 @@
             return lambda text: text
 
     def embed(self, input_documents: Union[Iterator[LazyDocument], BaseLoader]):
         """
         Embeds inout documents if they are new or changed and mutates current instance to drop embeddings for documents that are no longer present.
         """
         self._document_embeddings = self._get_embeddings_internal(input_documents)
+        return self
 
     def embed_and_create_new_instance(self, input_documents: Union[Iterator[LazyDocument], BaseLoader]) -> 'Embeddings':
         """
         Embeds input documents if they are new or changed and returns a new instance with the new embeddings. Current instance is not mutated.
         """
         document_embeddings = self._get_embeddings_internal(input_documents)
         new_embeddings = Embeddings(self.kind, **self.arguments)
         new_embeddings._document_embeddings = document_embeddings
         return new_embeddings
 
     def _get_embeddings_internal(self, input_documents: Union[Iterator[LazyDocument], BaseLoader]) -> OrderedDict:
         if self._embed_fn is None:
             raise ValueError("No embed function provided.")
 
+        # TODO: mlflow.set_tracking_uri if not set already?
+
         if isinstance(input_documents, BaseLoader):
             input_documents = iter([LangChainDocument(d)
                                    for d in input_documents.load()])
 
         documents_to_embed = []
         documents_embedded = OrderedDict()
         for document in input_documents:
+            if isinstance(document, Document):
+                document = LangChainDocument(document)
+
             logger.info(f'Processing document: {document.document_id}')
             mtime = document.modified_time()
             current_embedded_document = self._document_embeddings.get(
                 document.document_id)
             if mtime \
                     and current_embedded_document \
                     and current_embedded_document.mtime \
@@ -574,15 +603,25 @@
 
         truncate_func = self.get_embedding_ctx_length_truncate_func()
 
         data_to_embed = [truncate_func(t) for (_, _, t, _, _) in documents_to_embed]
 
         embeddings = []
         try:
-            embeddings = self._embed_fn(data_to_embed)
+            with track_activity(
+                logger,
+                'Embeddings.embed',
+                custom_dimensions={
+                    'documents_to_embed': len(documents_to_embed),
+                    'reused_documents': len(documents_embedded.keys()),
+                    'kind': self.kind,
+                    'model': self.arguments.get('model', ''),
+                }
+            ):
+                embeddings = self._embed_fn(data_to_embed)
         except Exception as e:
             logger.error(f'Failed to get embeddings with error: {e}')
             raise
 
         for ((doc_id, mtime, document_data, document_hash, document_metadata), embeddings) in zip(documents_to_embed, embeddings):
             documents_embedded[doc_id] = \
                 DataEmbeddedDocument(doc_id, mtime, document_hash,
```

## azureml/rag/tasks/crack_and_chunk.py

```diff
@@ -2,15 +2,15 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import openai
 import pandas as pd
 from pathlib import Path
 import time
 
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
 from azureml.rag.documents import SUPPORTED_EXTENSIONS, DocumentChunksIterator, split_documents, crack_documents
 from azureml.rag.models import parse_model_uri
 from azureml.rag.utils.azureml import get_secret_from_workspace
 
 
 logger = get_logger('crack_and_chunk')
 
@@ -59,14 +59,15 @@
     parser.add_argument("--openai_api_type", type=str, default=None)
     parser.add_argument("--use_rcts", type=bool, default=True)
 
     args = parser.parse_args()
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
+    enable_appinsights_logging()
 
     # Get all files in directory, recursively
     files = Path(args.input_data).rglob(args.input_glob)
 
     summary_model_config = None
     include_summary = (args.include_summary == "True" or args.include_summary == "true")
     if include_summary:
@@ -94,45 +95,48 @@
             summary_model_config["api_version"] = args.openai_api_version
             summary_model_config["api_base"] = summary_model_config.get('endpoint') if summary_model_config.get('endpoint') is not None else get_secret_from_workspace("OPENAI-API-BASE")
             openai.api_version = summary_model_config["api_version"]
             openai.api_type = summary_model_config["api_type"]
             openai.api_base = summary_model_config["api_base"]
             openai.api_key = summary_model_config["key"]
 
-    chunked_documents = DocumentChunksIterator(
-        files_source=args.input_data,
-        glob=args.input_glob,
-        base_url=args.data_source_url,
-        document_path_replacement_regex=args.document_path_replacement_regex,
-        chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts})],
-    )
-    file_count = 0
-    for document in chunked_documents:
-        file_count += 1
-        logger.info(f'Processing file: {document.source.filename}', extra={'print': True})
-        # TODO: Ideally make it easy to limit number of files with a `- take: n` operation on input URI in MLTable
-        if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
-            logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
-            break
-        write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_title_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
-    logger.info(f"Processed {file_count} files", extra={'print': True})
+    splitter_args = {'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts}
 
-    file_count = 0
-    if include_summary:
+    with track_activity(logger, 'crack_and_chunk', {**splitter_args}) as activity_logger:
         chunked_documents = DocumentChunksIterator(
             files_source=args.input_data,
             glob=args.input_glob,
             base_url=args.data_source_url,
             document_path_replacement_regex=args.document_path_replacement_regex,
-            source_loader=lambda sources: crack_documents(sources, summary_model_config=summary_model_config),
-            chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts})]
+            chunked_document_processors = [lambda docs: split_documents(docs, splitter_args=splitter_args)],
         )
-        total_time = 0
+        file_count = 0
         for document in chunked_documents:
-            file_start_time = time.time()
+            file_count += 1
+            logger.info(f'Processing file: {document.source.filename}', extra={'print': True})
+            # TODO: Ideally make it easy to limit number of files with a `- take: n` operation on input URI in MLTable
             if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
                 logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
                 break
-            write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_summary_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
-            file_end_time = time.time()
-            file_time = file_end_time - file_start_time
-        logger.info(f"Write chunks to {file_count} files in {total_time} seconds (chunk generation time excluded)", extra={'print': True})
+            write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_title_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
+        logger.info(f"Processed {file_count} files", extra={'print': True})
+
+        file_count = 0
+        if include_summary:
+            chunked_documents = DocumentChunksIterator(
+                files_source=args.input_data,
+                glob=args.input_glob,
+                base_url=args.data_source_url,
+                document_path_replacement_regex=args.document_path_replacement_regex,
+                source_loader=lambda sources: crack_documents(sources, summary_model_config=summary_model_config),
+                chunked_document_processors = [lambda docs: split_documents(docs, splitter_args={'chunk_size': args.chunk_size, 'chunk_overlap': args.chunk_overlap, 'use_rcts': args.use_rcts})]
+            )
+            total_time = 0
+            for document in chunked_documents:
+                file_start_time = time.time()
+                if (args.max_sample_files != -1 and file_count >= args.max_sample_files):
+                    logger.info(f"file count: {file_count} - reached max sample file count: {args.max_sample_files}", extra={'print': True})
+                    break
+                write_chunks_to_csv(chunks_to_dataframe(document.chunks), Path(args.output_summary_chunk) / f"Chunks_{Path(document.source.filename).name}.csv")
+                file_end_time = time.time()
+                file_time = file_end_time - file_start_time
+            logger.info(f"Write chunks to {file_count} files in {total_time} seconds (chunk generation time excluded)", extra={'print': True})
```

## azureml/rag/tasks/embed_prs.py

```diff
@@ -7,15 +7,15 @@
 import pandas as pd
 import pathlib
 import time
 
 from azureml.rag.embeddings import Embeddings
 from azureml.rag.tasks.embed import read_chunks_into_documents
 from azureml.rag.utils.azureml import get_workspace_from_environment
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
 
 logger = get_logger('embed')
 
 
 def init():
     """Load previous embeddings if provided."""
     global output_data
@@ -25,45 +25,54 @@
     parser.add_argument("--embeddings_model", type=str)
     parser.add_argument("--embeddings_container", required=False, type=str, default=None)
     args, _ = parser.parse_known_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
+    enable_appinsights_logging()
 
     output_data = args.output_data
 
     embeddings_container = None
     if args.embeddings_container is not None:
-        from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount
-        mnt_options = MountOptions(
-            default_permission=0o555, allow_other=False, read_only=True)
-        try:
-            with rslex_uri_volume_mount(args.embeddings_container, f'{os.getcwd()}/embeddings_container', options=mnt_options) as mount_context:
-                embeddings_container_dir_name = None
-                # list all folders in embeddings_container and find the latest one
-                try:
-                    embeddings_container_dir_name = str(max([dir for dir in pathlib.Path(
-                        mount_context.mount_point).glob('*') if dir.is_dir() and dir.name != os.environ['AZUREML_RUN_ID']], key=os.path.getmtime).name)
-                except Exception as e:
-                    logger.warn(
-                        f'failed to get latest folder from {mount_context.mount_point} with {e}.', extra={'print': True})
-                    pass
-
-                if embeddings_container_dir_name is not None:
-                    logger.info(
-                        f'loading from previous embeddings from {embeddings_container_dir_name} in {mount_context.mount_point}', extra={'print': True})
+        with track_activity(logger, 'init.load_embeddings_container') as activity_logger:
+            if hasattr(activity_logger, 'activity_info'):
+                activity_logger.activity_info["completionStatus"] = "Failure"
+            from azureml.dataprep.fuse.dprepfuse import MountOptions, rslex_uri_volume_mount
+            mnt_options = MountOptions(
+                default_permission=0o555, allow_other=False, read_only=True)
+            try:
+                with rslex_uri_volume_mount(args.embeddings_container, f'{os.getcwd()}/embeddings_container', options=mnt_options) as mount_context:
+                    embeddings_container_dir_name = None
+                    # list all folders in embeddings_container and find the latest one
                     try:
-                        embeddings_container = Embeddings.load(
-                            embeddings_container_dir_name, mount_context.mount_point)
+                        embeddings_container_dir_name = str(max([dir for dir in pathlib.Path(
+                            mount_context.mount_point).glob('*') if dir.is_dir() and dir.name != os.environ['AZUREML_RUN_ID']], key=os.path.getmtime).name)
                     except Exception as e:
+                        activity_logger.warn('Failed to get latest folder from embeddings_container.')
                         logger.warn(
-                            f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.', extra={'print': True})
-        except Exception as e:
-            logger.warn(f'Failed to load previous embeddings from mount with {e}, proceeding to create new embeddings.', extra={'print': True})
+                            f'failed to get latest folder from {mount_context.mount_point} with {e}.', extra={'print': True})
+                        pass
+
+                    if embeddings_container_dir_name is not None:
+                        logger.info(
+                            f'loading from previous embeddings from {embeddings_container_dir_name} in {mount_context.mount_point}', extra={'print': True})
+                        try:
+                            embeddings_container = Embeddings.load(
+                                embeddings_container_dir_name, mount_context.mount_point)
+                            if hasattr(activity_logger, 'activity_info'):
+                                activity_logger.activity_info["completionStatus"] = "Success"
+                        except Exception as e:
+                            activity_logger.warn('Failed to load from embeddings_container_dir_name. Creating new Embeddings.')
+                            logger.warn(
+                                f'Failed to load from previous embeddings with {e}.\nCreating new Embeddings.', extra={'print': True})
+            except Exception as e:
+                activity_logger.warn('Failed to load from embeddings_container. Creating new Embeddings.')
+                logger.warn(f'Failed to load previous embeddings from mount with {e}, proceeding to create new embeddings.', extra={'print': True})
 
     connection_args = {}
     connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_AOAI')
     if connection_id is not None:
         connection_args['connection_type'] = 'workspace_connection'
         connection_args['connection'] = {'id': connection_id}
     else:
```

## azureml/rag/tasks/git_clone.py

```diff
@@ -1,28 +1,29 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import argparse
 import os
 
 from azureml.rag.utils.git import clone_repo, get_keyvault_authentication
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
 
 logger = get_logger('git_clone')
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument("--git-repository", type=str, required=True, dest='git_repository')
     parser.add_argument("--branch-name", type=str, required=False, default=None)
     parser.add_argument("--authentication-key-prefix", type=str, required=False, default=None, help="<PREFIX>-USER and <PREFIX>-PASS are the expected names of two Secrets in the Workspace Key Vault which will be used for authenticated when pulling the given git repo.")
     parser.add_argument("--output-data", type=str, required=True, dest='output_data')
     args = parser.parse_args()
 
     enable_stdout_logging()
+    enable_appinsights_logging()
 
     connection_id = os.environ.get('AZUREML_WORKSPACE_CONNECTION_ID_GIT')
     if connection_id is not None and connection_id != '':
         from azureml.rag.utils.connections import get_connection_by_id_v2
 
         connection = get_connection_by_id_v2(connection_id)
         if args.git_repository != connection['properties']['target']:
@@ -30,11 +31,11 @@
         args.git_repository = connection['properties']['target']
         authentication = {'username': connection['properties']['metadata']['username'], 'password': connection['properties']['credentials']['pat']}
     elif args.authentication_key_prefix is not None:
         authentication = get_keyvault_authentication(args.authentication_key_prefix)
     else:
         authentication = None
 
-
-    clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
+    with track_activity(logger, 'git_clone') as activity_logger:
+        clone_repo(args.git_repository, args.output_data, args.branch_name, authentication)
 
     logger.info('Finished cloning.')
```

## azureml/rag/tasks/register_mlindex.py

```diff
@@ -4,15 +4,15 @@
 """File for registering ML Indexes."""
 import argparse
 from azureml.core import Run
 import fsspec
 import re
 import yaml
 
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_info
 from azureml.rag._asset_client.client import get_rest_client, register_new_data_asset_version
 
 logger = get_logger('register_mlindex')
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
@@ -20,14 +20,15 @@
     parser.add_argument("--asset-name", type=str, required=False, dest='asset_name', default='MLIndexAsset')
     parser.add_argument("--output-asset-id", type=str, dest='output_asset_id')
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
+    enable_appinsights_logging()
 
     run: Run = Run.get_context()
     ws = run.experiment.workspace
 
     logger.info(f'Checking for MLIndex at: {args.storage_uri.strip("/")}/MLIndex')
     index_kind = None
     mlindex_yaml = None
@@ -63,8 +64,9 @@
     asset_id = re.sub('azureml://locations/(.*)/workspaces/(.*)/data', f'azureml://subscriptions/{ws._subscription_id}/resourcegroups/{ws._resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{ws._workspace_name}/data', data_version.asset_id)
 
     print(asset_id)
 
     with open(args.output_asset_id, 'w') as f:
         f.write(asset_id)
 
+    track_info(logger, 'register_mlindex', {'kind': index_kind, 'source': run.properties.get('azureml.mlIndexAssetSource', 'Unknown')})
     logger.info(f"Finished Registering MLIndex Asset '{args.asset_name}', version = {data_version.version_id}")
```

## azureml/rag/tasks/update_acs.py

```diff
@@ -16,15 +16,15 @@
 from azure.core.credentials import AzureKeyCredential
 from azure.search.documents.indexes import SearchIndexClient
 from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchableField, SemanticSettings, SemanticConfiguration, PrioritizedFields, SemanticField, SearchField, ComplexField
 from azure.search.documents import SearchClient
 
 from azureml.rag.embeddings import Embeddings
 from azureml.rag.utils.connections import get_connection_credential
-from azureml.rag.utils.logging import get_logger, enable_stdout_logging
+from azureml.rag.utils.logging import get_logger, enable_stdout_logging, enable_appinsights_logging, track_activity
 
 
 logger = get_logger('update_acs')
 
 _azure_logger = logging.getLogger('azure.core.pipeline')
 _azure_logger.setLevel(logging.WARNING)
 
@@ -197,154 +197,160 @@
         logger.error(f"Request failed: {e}\nResponse: {e.response.text}")
         raise e
 
     return response.json()['value']
 
 
 def create_index_from_raw_embeddings(emb: Embeddings, acs_config={}, connection={}, output_path: Optional[str] = None):
-    logger.info("Updating ACS index", extra={'print': True})
+    with track_activity(logger, 'update_acs', {'num_embeddings': len(emb._document_embeddings)}) as activity_logger:
+        logger.info("Updating ACS index", extra={'print': True})
 
-    credential = get_connection_credential(connection)
+        credential = get_connection_credential(connection)
 
-    if str(acs_config.get('push_embeddings')).lower() == "true":
-        create_search_index_rest(acs_config, credential, emb)
+        if str(acs_config.get('push_embeddings')).lower() == "true":
+            create_search_index_rest(acs_config, credential, emb)
 
-        def upload_docs_batch(batch):
-            return upload_documents_rest(acs_config, credential, batch)
-    else:
-        create_search_index_sdk(acs_config, credential)
-        search_client = search_client_from_config(acs_config, credential)
+            def upload_docs_batch(batch):
+                return upload_documents_rest(acs_config, credential, batch)
+        else:
+            create_search_index_sdk(acs_config, credential)
+            search_client = search_client_from_config(acs_config, credential)
 
-        def upload_docs_batch(batch):
-            return search_client.upload_documents(documents=batch)
+            def upload_docs_batch(batch):
+                return search_client.upload_documents(documents=batch)
 
-    batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100
+        batch_size = acs_config['batch_size'] if 'batch_size' in acs_config else 100
 
-    def process_upload_results(results):
-        succeeded = []
-        failed = []
-        for r in results:
-            if isinstance(r, dict):
-                if r['status'] is False:
-                    failed.append(r)
-                else:
-                    succeeded.append(r)
-            else:
-                if r.succeeded:
-                    succeeded.append(r)
-                else:
-                    failed.append(r)
-        logger.info(f"Uploaded {len(succeeded)} documents to ACS in {time.time() - start_time:.4f} seconds, {len(failed)} failed", extra={'print': True})
-        if len(failed) > 0:
-            for r in failed:
+        def process_upload_results(results, start_time):
+            succeeded = []
+            failed = []
+            for r in results:
                 if isinstance(r, dict):
-                    error = r['errorMessage']
+                    if r['status'] is False:
+                        failed.append(r)
+                    else:
+                        succeeded.append(r)
                 else:
-                    error = r.error_message
-                logger.error(f"Failed document reason: {error}", extra={'print': True})
-            return failed
-        return []
-
-    t1 = time.time()
-    num_source_docs = 0
-    batch = []
-    for doc_id, emb_doc in emb._document_embeddings.items():
-        logger.info(f'Adding document: {doc_id}', extra={'print': True})
-        acs_doc = {
-            "@search.action": "mergeOrUpload",
-            "id": base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'),
-            "content": emb_doc.get_data(),
-            "category": "document",
-            "sourcepage": emb_doc.metadata.get("source", {}).get("url"),
-            "sourcefile": emb_doc.metadata.get("source", {}).get("filename"),
-            "title": emb_doc.metadata.get("source", {}).get("title"),
-            "content_hash": emb_doc.document_hash,
-            "meta_json_string": json.dumps(emb_doc.metadata),
-        }
+                    if r.succeeded:
+                        succeeded.append(r)
+                    else:
+                        failed.append(r)
+            duration = time.time() - start_time
+            logger.info(f"Uploaded {len(succeeded)} documents to ACS in {duration:.4f} seconds, {len(failed)} failed", extra={'print': True})
+            activity_logger.info("Uploaded documents", extra={'succeeded': len(succeeded), 'failed': len(failed), 'duration': duration})
+            if len(failed) > 0:
+                for r in failed:
+                    if isinstance(r, dict):
+                        error = r['errorMessage']
+                    else:
+                        error = r.error_message
+                    logger.error(f"Failed document reason: {error}", extra={'print': True})
+                return failed
+            return []
+
+        t1 = time.time()
+        num_source_docs = 0
+        batch = []
+        for doc_id, emb_doc in emb._document_embeddings.items():
+            logger.info(f'Adding document: {doc_id}', extra={'print': True})
+            acs_doc = {
+                "@search.action": "mergeOrUpload",
+                "id": base64.urlsafe_b64encode(doc_id.encode('utf-8')).decode('utf-8'),
+                "content": emb_doc.get_data(),
+                "category": "document",
+                "sourcepage": emb_doc.metadata.get("source", {}).get("url"),
+                "sourcefile": emb_doc.metadata.get("source", {}).get("filename"),
+                "title": emb_doc.metadata.get("source", {}).get("title"),
+                "content_hash": emb_doc.document_hash,
+                "meta_json_string": json.dumps(emb_doc.metadata),
+            }
 
-        if str(acs_config.get('push_embeddings')).lower() == "true":
-            acs_doc[f"content_vector_{emb.kind}"] = emb_doc.get_embeddings()
+            if str(acs_config.get('push_embeddings')).lower() == "true":
+                acs_doc[f"content_vector_{emb.kind}"] = emb_doc.get_embeddings()
+
+            batch.append(acs_doc)
+            if len(batch) % batch_size == 0:
+                logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
+                start_time = time.time()
+                results = upload_docs_batch(batch)
+                failed = process_upload_results(results, start_time)
+                if len(failed) > 0:
+                    logger.info(f"Retrying {len(failed)} documents", extra={'print': True})
+                    failed_ids = [fail['key'] for fail in failed]
+                    results = upload_docs_batch([doc for doc in batch if doc['id'] in failed_ids])
+                    failed = process_upload_results(results, start_time)
+                    if len(failed) > 0:
+                        raise RuntimeError(f"Failed to upload {len(failed)} documents.")
+                batch = []
+                num_source_docs += batch_size
 
-        batch.append(acs_doc)
-        if len(batch) % batch_size == 0:
+        if len(batch) > 0:
             logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
             start_time = time.time()
             results = upload_docs_batch(batch)
-            failed = process_upload_results(results)
+            failed = process_upload_results(results, start_time)
             if len(failed) > 0:
                 logger.info(f"Retrying {len(failed)} documents", extra={'print': True})
                 failed_ids = [fail['key'] for fail in failed]
                 results = upload_docs_batch([doc for doc in batch if doc['id'] in failed_ids])
-                failed = process_upload_results(results)
+                failed = process_upload_results(results, start_time)
                 if len(failed) > 0:
                     raise RuntimeError(f"Failed to upload {len(failed)} documents.")
-            batch = []
-            num_source_docs += batch_size
 
-    if len(batch) > 0:
-        logger.info(f"Sending {len(batch)} documents to ACS", extra={'print': True})
-        start_time = time.time()
-        results = upload_docs_batch(batch)
-        failed = process_upload_results(results)
-        if len(failed) > 0:
-                logger.info(f"Retrying {len(failed)} documents", extra={'print': True})
-                failed_ids = [fail['key'] for fail in failed]
-                results = upload_docs_batch([doc for doc in batch if doc['id'] in failed_ids])
-                failed = process_upload_results(results)
-                if len(failed) > 0:
-                    raise RuntimeError(f"Failed to upload {len(failed)} documents.")
-
-        num_source_docs += len(batch)
+            num_source_docs += len(batch)
 
-    logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {time.time()-t1:.4f} seconds", extra={'print': True})
-
-    if output_path is not None:
-        logger.info('Writing MLIndex yaml', extra={'print': True})
-        mlindex_config = {
-            "embeddings": emb.get_metadata()
-        }
-        mlindex_config["index"] = {
-            "kind": "acs",
-            "engine": "azure-sdk",
-            "index": acs_config['index_name'],
-            "api_version": acs_config['api_version'],
-            "field_mapping": {
-                "content": "content",
-                "url": "sourcepage",
-                "filename": "sourcefile",
-                "title": "title",
-                "metadata": "meta_json_string",
+        duration = time.time()-t1
+        logger.info(f"Built index from {num_source_docs} documents and {len(emb._document_embeddings)} chunks, took {duration:.4f} seconds", extra={'print': True})
+        activity_logger.info("Built index", extra={'num_source_docs': num_source_docs, 'duration': duration})
+
+        if output_path is not None:
+            logger.info('Writing MLIndex yaml', extra={'print': True})
+            mlindex_config = {
+                "embeddings": emb.get_metadata()
             }
-        }
-        if str(acs_config.get('push_embeddings')).lower() == "true":
-            mlindex_config["index"]["field_mapping"]["embedding"] = f"content_vector_{emb.kind}"
+            mlindex_config["index"] = {
+                "kind": "acs",
+                "engine": "azure-sdk",
+                "index": acs_config['index_name'],
+                "api_version": acs_config['api_version'],
+                "field_mapping": {
+                    "content": "content",
+                    "url": "sourcepage",
+                    "filename": "sourcefile",
+                    "title": "title",
+                    "metadata": "meta_json_string",
+                }
+            }
+            if str(acs_config.get('push_embeddings')).lower() == "true":
+                mlindex_config["index"]["field_mapping"]["embedding"] = f"content_vector_{emb.kind}"
 
-        if not isinstance(connection, DefaultAzureCredential):
-            mlindex_config["index"] = {**mlindex_config["index"], **connection}
+            if not isinstance(connection, DefaultAzureCredential):
+                mlindex_config["index"] = {**mlindex_config["index"], **connection}
 
-        # Keyvault auth and Default ambient auth need the endpoint, Workspace Connection auth could get endpoint.
-        mlindex_config["index"]["endpoint"] = acs_config['endpoint']
-        output = Path(output_path)
-        output.mkdir(parents=True, exist_ok=True)
-        with open(output / "MLIndex", "w") as f:
-            yaml.dump(mlindex_config, f)
+            # Keyvault auth and Default ambient auth need the endpoint, Workspace Connection auth could get endpoint.
+            mlindex_config["index"]["endpoint"] = acs_config['endpoint']
+            output = Path(output_path)
+            output.mkdir(parents=True, exist_ok=True)
+            with open(output / "MLIndex", "w") as f:
+                yaml.dump(mlindex_config, f)
 
 
 if __name__ == '__main__':
     from argparse import ArgumentParser
 
     parser = ArgumentParser()
     parser.add_argument("--embeddings", type=str)
     parser.add_argument("--acs_config", type=str)
     parser.add_argument("--output", type=str)
     args = parser.parse_args()
 
     print('\n'.join(f'{k}={v}' for k, v in vars(args).items()))
 
     enable_stdout_logging()
+    enable_appinsights_logging()
 
     raw_embeddings_uri = args.embeddings
     logger.info(f'got embeddings uri as input: {raw_embeddings_uri}', extra={'print': True})
     splits = raw_embeddings_uri.split('/')
     embeddings_dir_name = splits.pop(len(splits)-2)
     logger.info(f'extracted embeddings directory name: {embeddings_dir_name}', extra={'print': True})
     parent = '/'.join(splits)
```

## azureml/rag/utils/logging.py

```diff
@@ -1,61 +1,214 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 """Logging utilities."""
+from functools import lru_cache
+import inspect
 import logging
+import pkg_resources
 import sys
 
 
 # class ExtraPrintHandler(logging.StreamHandler):
 #     LEVELS = {0: 'DBUG', 1: 'INFO', 2: 'WARN', 3: 'ERRR'}
 
 #     def emit(self, record):
 #         if hasattr(record, 'print') and record.print:
 #             print(f'[{datetime.utcnow()}] {record.levelname} - {record.msg}')
 #         super().emit(record)
 
 # handler = ExtraPrintHandler()
 
+COMPONENT_NAME = 'azureml.rag'
+instrumentation_key = ''
+try:
+    version = pkg_resources.get_distribution("azureml-rag").version
+    langchain_version = pkg_resources.get_distribution("langchain").version
+except Exception:
+    version = ''
+    langchain_version = ''
+default_custom_dimensions = {'source': COMPONENT_NAME, 'version': version, 'langchain_version': langchain_version}
+STACK_FMT = "%s, line %d in function %s."
+DEFAULT_ACTIVITY_TYPE = "InternalCall"
+
+try:
+    from azureml.telemetry import get_telemetry_log_handler, INSTRUMENTATION_KEY
+    from azureml.telemetry.activity import log_activity as _log_activity, ActivityType, ActivityLoggerAdapter
+    from azureml._base_sdk_common import _ClientSessionId
+    import os
+
+    session_id = _ClientSessionId
+    current_folder = os.path.dirname(os.path.realpath(__file__))
+    telemetry_config_path = os.path.join(current_folder, '_telemetry.json')
+
+    verbosity = logging.INFO
+    instrumentation_key = INSTRUMENTATION_KEY
+    telemetry_enabled = True
+except Exception:
+    from contextlib import contextmanager
+
+    verbosity = None
+    ActivityLoggerAdapter = None
+    telemetry_enabled = False
+
+    @contextmanager
+    def _run_without_logging(logger, activity_name, activity_type, custom_dimensions):
+        yield logger
+
 
 class LoggerFactory:
     """Factory for creating loggers"""
     def __init__(self, stdout=False):
         """Initialize the logger factory"""
         self.loggers = {}
         self.stdout = stdout
+        self.appinsights = None
+        self.with_appinsights()
 
     def with_stdout(self, stdout=True):
         """Set whether to log to stdout"""
         self.stdout = stdout
         # Add stdout handler to any loggers created before enabling stdout.
         for logger in self.loggers.values():
             if self.stdout:
                 stdout_handler = logging.StreamHandler(stream=sys.stdout)
                 stdout_handler.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)-8s %(name)s - %(message)s (%(filename)s:%(lineno)s)', "%Y-%m-%d %H:%M:%S"))
                 logger.addHandler(stdout_handler)
         return self
 
+    def with_appinsights(self):
+        """Set whether to log track_* events to appinsights"""
+        if telemetry_enabled:
+            self.appinsights = get_telemetry_log_handler(component_name=COMPONENT_NAME, path=telemetry_config_path)
+
     def get_logger(self, name, level=logging.INFO):
         """Get a logger with the given name and level"""
         if name not in self.loggers:
             logger = logging.getLogger(f'azureml.rag.{name}')
             logger.setLevel(level)
             if self.stdout:
                 stdout_handler = logging.StreamHandler(stream=sys.stdout)
                 stdout_handler.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)-8s %(name)s - %(message)s (%(filename)s:%(lineno)s)', "%Y-%m-%d %H:%M:%S"))
                 logger.addHandler(stdout_handler)
             self.loggers[name] = logger
         return self.loggers[name]
 
+    def track_activity(self, logger, name, activity_type=DEFAULT_ACTIVITY_TYPE, custom_dimensions={}):
+        """Track the activity of the given logger"""
+        if self.appinsights:
+            stack = self.get_stack()
+            custom_dimensions.update({**default_custom_dimensions, 'trace': stack})
+            run_info = self._try_get_run_info()
+            if run_info is not None:
+                custom_dimensions.update(run_info)
+            child_logger = logger.getChild(name)
+            child_logger.addHandler(self.appinsights)
+            return _log_activity(child_logger, name, activity_type, custom_dimensions)
+        else:
+            return _run_without_logging(logger, name, activity_type, custom_dimensions)
+
+    def telemetry_info(self, logger, message, custom_dimensions={}):
+        """Track info with given logger"""
+        if self.appinsights:
+            payload = custom_dimensions
+            payload.update(default_custom_dimensions)
+            child_logger = logger.getChild('appinsights')
+            child_logger.addHandler(self.appinsights)
+            if ActivityLoggerAdapter:
+                activity_logger = ActivityLoggerAdapter(child_logger, payload)
+                activity_logger.info(message)
+
+    def telemetry_error(self, logger, message, custom_dimensions={}):
+        """Track error with given logger"""
+        if self.appinsights:
+            payload = custom_dimensions
+            payload.update(default_custom_dimensions)
+            child_logger = logger.getChild('appinsights')
+            child_logger.addHandler(self.appinsights)
+            if ActivityLoggerAdapter:
+                activity_logger = ActivityLoggerAdapter(child_logger, payload)
+                activity_logger.error(message)
+
+    @staticmethod
+    def get_stack(limit=3, start=1) -> str:
+        """Get the stack trace as a string"""
+        try:
+            stack = inspect.stack()
+            # The index of the first frame to print.
+            begin = start + 2
+            # The index of the last frame to print.
+            if limit:
+                end = min(begin + limit, len(stack))
+            else:
+                end = len(stack)
+
+            lines = []
+            for frame in stack[begin:end]:
+                file, line, func = frame[1:4]
+                parts = file.rsplit('\\', 4)
+                parts = parts if len(parts) > 1 else file.rsplit('/', 4)
+                file = '|'.join(parts[-3:])
+                lines.append(STACK_FMT % (file, line, func))
+            return '\n'.join(lines)
+        except Exception:
+            pass
+        return None
+
+    @staticmethod
+    @lru_cache(maxsize=1)
+    def _try_get_run_info():
+        info = {
+            "subscription": os.environ.get('AZUREML_ARM_SUBSCRIPTION', ""),
+            "run_id": os.environ.get("AZUREML_RUN_ID", ""),
+            "resource_group": os.environ.get("AZUREML_ARM_RESOURCEGROUP", ""),
+            "workspace_name": os.environ.get("AZUREML_ARM_WORKSPACE_NAME", ""),
+            "experiment_id": os.environ.get("AZUREML_EXPERIMENT_ID", "")
+        }
+        try:
+            import re
+            location = os.environ.get("AZUREML_SERVICE_ENDPOINT")
+            location = re.compile("//(.*?)\\.").search(location).group(1)
+        except Exception:
+            location = os.environ.get("AZUREML_SERVICE_ENDPOINT", "")
+        info["location"] = location
+        try:
+            from azureml.core import Run
+            run: Run = Run.get_context()
+            if hasattr(run, 'experiment'):
+                info["parent_run_id"] = run.properties.get('azureml.pipelinerunid', 'Unknown')
+        except Exception:
+            pass
+        return info
+
 
 _logger_factory = LoggerFactory()
 
 
 def enable_stdout_logging():
     """Enable logging to stdout"""
     _logger_factory.with_stdout(True)
 
 
+def enable_appinsights_logging():
+    """Enable logging to appinsights"""
+    _logger_factory.with_appinsights()
+
+
 def get_logger(name, level=logging.INFO):
     """Get a logger with the given name and level"""
     return _logger_factory.get_logger(name, level)
+
+
+def track_activity(logger, name, activity_type=DEFAULT_ACTIVITY_TYPE, custom_dimensions={}):
+    """Track the activity with given logger"""
+    return _logger_factory.track_activity(logger, name, activity_type, custom_dimensions)
+
+
+def track_info(logger, message, custom_dimensions={}):
+    """Track info with given logger"""
+    return _logger_factory.telemetry_info(logger, message, custom_dimensions)
+
+
+def track_error(logger, message, custom_dimensions={}):
+    """Track error with given logger"""
+    return _logger_factory.telemetry_error(logger, message, custom_dimensions)
```

## Comparing `azureml_rag-0.1.4.dist-info/LICENSE.txt` & `azureml_rag-0.1.5.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_rag-0.1.4.dist-info/METADATA` & `azureml_rag-0.1.5.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-rag
-Version: 0.1.4
+Version: 0.1.5
 Summary: Contains Retrieval Augmented Generation related utilities for Azure Machine Learning and OSS interoperability.
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corporation
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
@@ -20,20 +20,21 @@
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Topic :: Scientific/Engineering
 Requires-Python: >=3.7,<4.0
 Description-Content-Type: text/markdown
 Requires-Dist: azureml-dataprep[parquet] (<4.11.0a,>=4.10.0a)
 Requires-Dist: azureml-core
+Requires-Dist: azureml-telemetry
 Requires-Dist: azureml-mlflow
 Requires-Dist: azureml-fsspec
 Requires-Dist: fsspec (~=2023.3)
 Requires-Dist: openai (~=0.27.4)
 Requires-Dist: tiktoken (~=0.3.0)
-Requires-Dist: langchain (>=0.0.149)
+Requires-Dist: langchain (!=0.0.174,>=0.0.149)
 Requires-Dist: cloudpickle
 Requires-Dist: msrest (>=0.6.18)
 Requires-Dist: pyyaml (<7.0.0,>=5.1.0)
 Provides-Extra: cognitive_search
 Requires-Dist: azure-search-documents (~=11.4.0b3) ; extra == 'cognitive_search'
 Provides-Extra: document_parsing
 Requires-Dist: pandas (>=1) ; extra == 'document_parsing'
@@ -116,15 +117,24 @@
 retriever = MLIndex(uri_to_folder_with_mlindex).as_langchain_retriever()
 retriever.get_relevant_documents('What is an AzureML Compute Instance?')
 ```
 
 
 # Changelog
 
-## 0.2.0 (2023-05-12)
+## 0.1.5 (2023-05-19)
+
+- Add api_base back to MLIndex embeddings config for back-compat (until all clients start getting it from Workspace Connection).
+- Add telemetry for tasks used in pipeline components, not enabled by default for SDK usage.
+
+## 0.1.4 (2023-05-17)
+
+- Fix bug where enabling rcts option on split_documents used nltk splitter instead.
+
+## 0.1.3 (2023-05-12)
 
 - Support Workspace Connection based auth for Git, Azure OpenAI and Azure Cognitive Search usage.
 
 ## 0.1.2 (2023-05-05)
 
 - Refactored document chunking to allow insertion of custom processing logic
```

## Comparing `azureml_rag-0.1.4.dist-info/RECORD` & `azureml_rag-0.1.5.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 azureml/rag/__init__.py,sha256=zgRl-az58He0no_ZUWab0AuVQSOTmgCLlfwbEcnwKS0,246
 azureml/rag/documents.py,sha256=QgTx5P8xtBRb4OgplPlx_sB2_f3ObfeVCvqs76A18ZE,39993
-azureml/rag/embeddings.py,sha256=aadXgbZtYhmIkx69sMIEyPmNk0F5o4giM5ZBYC4fjKA,27596
+azureml/rag/embeddings.py,sha256=PgirYai7PmQkborZlyVLCNJe4oXkNq525Dpetj7XBN0,29244
 azureml/rag/mlindex.py,sha256=DNb-Z15nTzjmGZH5h0RGjqxQRvnIDg756W8gJDbObhA,5058
 azureml/rag/models.py,sha256=Ga9tvV0FsVhXprEeWpPVakNJ9q96byc0OjDESqkMuCw,4149
 azureml/rag/_asset_client/client.py,sha256=LpsDsedlQRxytezdvnxx1zfEQUb9DcYDY6RfkRSlXT0,4646
 azureml/rag/_asset_client/_restclient/__init__.py,sha256=38lKUIqL59KqhES7ZGBUGcrELWICWet0VFLxwY4W0fo,893
 azureml/rag/_asset_client/_restclient/_azure_machine_learning_workspaces.py,sha256=7XqTcSuvXHcKXu4FE71E4rBQAQ4Fmpdq8SXrhU7_AAg,4381
 azureml/rag/_asset_client/_restclient/_configuration.py,sha256=E52K3BmA4ZqAE6oP5VjRK32HsBrl6W9Y1oOsWCJu8Ig,3538
 azureml/rag/_asset_client/_restclient/_patch.py,sha256=wuqrJGWK488sJvWwSq6iwPTqil7TPaRadoxE7BMK0tA,1561
@@ -35,23 +35,23 @@
 azureml/rag/_asset_client/_restclient/runhistory/models/_models.py,sha256=CqQccTVrcm5_9uBZOJ6AMubF9K8GphuIzmF1j46G84Q,175170
 azureml/rag/_asset_client/_restclient/runhistory/models/_models_py3.py,sha256=G5x7IlIzWN98TsXpY80M5nyLjXIwBWWeL9Fk6Piwpzw,188776
 azureml/rag/_asset_client/_restclient/runhistory/operations/__init__.py,sha256=FFrjEyJdO53e7YDI8o_8W9BgTTEVt1ZzS8FwwnNp7-g,563
 azureml/rag/_asset_client/_restclient/runhistory/operations/_runs_operations.py,sha256=1nl-ETYrSQSZH3B-a2QXTKIUCx-Hx3XeQPmgAwKSGoA,162796
 azureml/rag/langchain/acs.py,sha256=xyJ5u7RlZ8FUVAQZg7z4LJv5pXACddLcRLihUL6sEQI,10785
 azureml/rag/tasks/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 azureml/rag/tasks/build_faiss.py,sha256=dXETVJJT77PGrav6OiUBzve5EjqOutNcD5cpP1vIHxU,2094
-azureml/rag/tasks/crack_and_chunk.py,sha256=uK4Q1eU2OgDI2ey9snpFT3xdpHRz11bdU4IhSDhoJyI,7190
+azureml/rag/tasks/crack_and_chunk.py,sha256=vqLCHcJGmUuv1pLdT9L1CrO-yiT5MXF3LiOLwsKBX8c,7544
 azureml/rag/tasks/embed.py,sha256=yr_PnSq1a-BLh7EX7TvIVeusk2iD4Udw-xXlvpiCCI0,7330
-azureml/rag/tasks/embed_prs.py,sha256=PmtPLaopfPhW2RC1EIQRuHzEAj-jJFIGZie3psQIcR8,5410
-azureml/rag/tasks/git_clone.py,sha256=Y_f4EiPLG000asZWsYMvhheLPHsKJHsXyvmHyGiEoPA,2194
-azureml/rag/tasks/register_mlindex.py,sha256=4T7UEav88naHA-bzKWija8pdAwOlRV2kAWmVXyJXi6A,2791
-azureml/rag/tasks/update_acs.py,sha256=3J2_qZJIiRhKQ4As-BzHOmSeL0mQHP9wce1f6PROXl4,17977
+azureml/rag/tasks/embed_prs.py,sha256=DkK9I2kT1qIetgYGo-Bx6fPrDDbe3RrFJdVHrVadPGM,6328
+azureml/rag/tasks/git_clone.py,sha256=L7-gEP5r24AAq-aLxomURtDL7qfuWpusoRC9cm6iBjc,2340
+azureml/rag/tasks/register_mlindex.py,sha256=Gu2KUsyFBTfl0O_PxMOFNU-ROz-EinqowG_kkoiWxqY,3002
+azureml/rag/tasks/update_acs.py,sha256=ta42cSvBakCZNt7RTvKpAh0zxRo4WJJW-Y4KOwBKDp8,18964
 azureml/rag/utils/__init__.py,sha256=FTY5BaTKeythd7R1SXsf3midWHdshZj-bdFZLmZ7J50,208
 azureml/rag/utils/azureml.py,sha256=dOfvDTgqjTWHg-I9Ho7_o6e7I8kNcVJdl53O0Ts7xQs,1588
 azureml/rag/utils/connections.py,sha256=zInLr6twjfHW4fF2zgChtNSI7YqnxY3eOVnu74xpq-Y,7435
 azureml/rag/utils/git.py,sha256=C_79CuCdBNLrid1eUf2nruPynpIgghbf9d9ArnpI_Wc,2350
-azureml/rag/utils/logging.py,sha256=fusY7czHGANkXnZdtdKc9TQjaeH-dSsC0Vts5xWQbfQ,2330
-azureml_rag-0.1.4.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_rag-0.1.4.dist-info/METADATA,sha256=kVg8gHPh6OqzGQ29HanvTIHB5Pd_vIaZI8Jiq_s3iJU,4968
-azureml_rag-0.1.4.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_rag-0.1.4.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
-azureml_rag-0.1.4.dist-info/RECORD,,
+azureml/rag/utils/logging.py,sha256=0pnj9K8j7KC7IlJmu-HIDZETW1JKiWT2PjEzscycdis,8529
+azureml_rag-0.1.5.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_rag-0.1.5.dist-info/METADATA,sha256=94d1D-mbWpwjC5mT3oFYEgHmzbsOtw1dnD8HJwGsYmk,5365
+azureml_rag-0.1.5.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_rag-0.1.5.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_rag-0.1.5.dist-info/RECORD,,
```

