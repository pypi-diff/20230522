# Comparing `tmp/datarobot_model_metrics-0.1.4-py2.py3-none-any.whl.zip` & `tmp/datarobot_model_metrics-0.1.5-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,24 @@
-Zip file size: 24292 bytes, number of entries: 21
--rw-r--r--  2.0 unx      385 b- defN 23-Apr-19 08:30 dmm/__init__.py
--rw-r--r--  2.0 unx      606 b- defN 23-Apr-19 08:30 dmm/constants.py
--rw-r--r--  2.0 unx     4733 b- defN 23-Apr-19 08:30 dmm/example_data_helper.py
--rw-r--r--  2.0 unx    13088 b- defN 23-Apr-19 08:30 dmm/metric_evaluator.py
--rw-r--r--  2.0 unx     3164 b- defN 23-Apr-19 08:30 dmm/time_bucket.py
--rw-r--r--  2.0 unx     1690 b- defN 23-Apr-19 08:30 dmm/utils.py
--rw-r--r--  2.0 unx      471 b- defN 23-Apr-19 08:30 dmm/data_source/__init__.py
--rw-r--r--  2.0 unx     2096 b- defN 23-Apr-19 08:30 dmm/data_source/data_source_base.py
--rw-r--r--  2.0 unx     2698 b- defN 23-Apr-19 08:30 dmm/data_source/dataframe_source.py
--rw-r--r--  2.0 unx    41964 b- defN 23-Apr-19 08:30 dmm/data_source/datarobot_source.py
--rw-r--r--  2.0 unx     3003 b- defN 23-Apr-19 08:30 dmm/data_source/generator_source.py
--rw-r--r--  2.0 unx      396 b- defN 23-Apr-19 08:30 dmm/metric/__init__.py
--rw-r--r--  2.0 unx     1620 b- defN 23-Apr-19 08:30 dmm/metric/asymmetric_error.py
--rw-r--r--  2.0 unx      320 b- defN 23-Apr-19 08:30 dmm/metric/median_absolute_error.py
--rw-r--r--  2.0 unx     3569 b- defN 23-Apr-19 08:30 dmm/metric/metric_base.py
--rw-r--r--  2.0 unx      957 b- defN 23-Apr-19 08:30 dmm/metric/missing_values.py
--rw-r--r--  2.0 unx     2014 b- defN 23-Apr-19 08:30 dmm/metric/sklearn_metric.py
--rw-r--r--  2.0 unx      886 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1771 b- defN 23-Apr-19 08:31 datarobot_model_metrics-0.1.4.dist-info/RECORD
-21 files, 85545 bytes uncompressed, 21400 bytes compressed:  75.0%
+Zip file size: 28095 bytes, number of entries: 22
+-rw-r--r--  2.0 unx      385 b- defN 23-May-22 09:29 dmm/__init__.py
+-rw-r--r--  2.0 unx     8663 b- defN 23-May-22 09:29 dmm/batch_metric_evaluator.py
+-rw-r--r--  2.0 unx      687 b- defN 23-May-22 09:29 dmm/constants.py
+-rw-r--r--  2.0 unx     6656 b- defN 23-May-22 09:29 dmm/example_data_helper.py
+-rw-r--r--  2.0 unx    12824 b- defN 23-May-22 09:29 dmm/metric_evaluator.py
+-rw-r--r--  2.0 unx     3164 b- defN 23-May-22 09:29 dmm/time_bucket.py
+-rw-r--r--  2.0 unx     1690 b- defN 23-May-22 09:29 dmm/utils.py
+-rw-r--r--  2.0 unx      525 b- defN 23-May-22 09:29 dmm/data_source/__init__.py
+-rw-r--r--  2.0 unx     2096 b- defN 23-May-22 09:29 dmm/data_source/data_source_base.py
+-rw-r--r--  2.0 unx     2698 b- defN 23-May-22 09:29 dmm/data_source/dataframe_source.py
+-rw-r--r--  2.0 unx    55894 b- defN 23-May-22 09:29 dmm/data_source/datarobot_source.py
+-rw-r--r--  2.0 unx     3003 b- defN 23-May-22 09:29 dmm/data_source/generator_source.py
+-rw-r--r--  2.0 unx      396 b- defN 23-May-22 09:29 dmm/metric/__init__.py
+-rw-r--r--  2.0 unx     1620 b- defN 23-May-22 09:29 dmm/metric/asymmetric_error.py
+-rw-r--r--  2.0 unx      320 b- defN 23-May-22 09:29 dmm/metric/median_absolute_error.py
+-rw-r--r--  2.0 unx     3569 b- defN 23-May-22 09:29 dmm/metric/metric_base.py
+-rw-r--r--  2.0 unx      957 b- defN 23-May-22 09:29 dmm/metric/missing_values.py
+-rw-r--r--  2.0 unx     2014 b- defN 23-May-22 09:29 dmm/metric/sklearn_metric.py
+-rw-r--r--  2.0 unx      886 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1857 b- defN 23-May-22 09:30 datarobot_model_metrics-0.1.5.dist-info/RECORD
+22 files, 110018 bytes uncompressed, 25069 bytes compressed:  77.2%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: dmm/__init__.py
 Comment: 
 
+Filename: dmm/batch_metric_evaluator.py
+Comment: 
+
 Filename: dmm/constants.py
 Comment: 
 
 Filename: dmm/example_data_helper.py
 Comment: 
 
 Filename: dmm/metric_evaluator.py
@@ -45,20 +48,20 @@
 
 Filename: dmm/metric/missing_values.py
 Comment: 
 
 Filename: dmm/metric/sklearn_metric.py
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.4.dist-info/METADATA
+Filename: datarobot_model_metrics-0.1.5.dist-info/METADATA
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.4.dist-info/WHEEL
+Filename: datarobot_model_metrics-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.4.dist-info/top_level.txt
+Filename: datarobot_model_metrics-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobot_model_metrics-0.1.4.dist-info/RECORD
+Filename: datarobot_model_metrics-0.1.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dmm/constants.py

```diff
@@ -3,15 +3,17 @@
     TARGET_VALUE = "target_value"
     ACTUALS = "actuals"
     TIMESTAMP = "timestamp"
     NR_SAMPLES = "samples"
     METRIC_VALUE = "value"
     DR_TIMESTAMP_COLUMN = "DR_RESERVED_PREDICTION_TIMESTAMP"
     DR_PREDICTION_COLUMN = "DR_RESERVED_PREDICTION_VALUE"
+    DR_BATCH_ID_COLUMN = "DR_RESERVED_BATCH_ID"
     ASSOCIATION_ID_COLUMN = "association_id"
+    BATCH_ID_COLUMN = "batch_id"
 
 
 class TimeBucket:
     SECOND = "second"
     MINUTE = "minute"
     HOUR = "hour"
     DAY = "day"
```

## dmm/example_data_helper.py

```diff
@@ -97,14 +97,62 @@
         df[association_id_col] = [x for x in range(nr_rows)]
     if with_dr_timestamp_column:
         df[dr_timestamp_column] = df[timestamp_col]
 
     return df
 
 
+def gen_dataframe_for_batch_tests(
+    nr_rows: int = 1000,
+    batch_id_col: ColumnName = ColumnName.DR_BATCH_ID_COLUMN,
+    timestamp_col: ColumnName = ColumnName.TIMESTAMP,
+    with_predictions: bool = True,
+    prediction_col: ColumnName = ColumnName.PREDICTIONS,
+    with_dr_timestamp_column: bool = False,
+    dr_timestamp_column: ColumnName = ColumnName.DR_TIMESTAMP_COLUMN,
+    prediction_value: int = None,
+    random_predictions: bool = False,
+    rows_per_batch: int = 100,
+) -> (pd.DataFrame, List[str]):
+    """
+    Generate a dataframe for testing
+    :param nr_rows: Number of rows to generate
+    :param batch_id_col: Name of batch ID column
+    :param with_predictions: Add predictions to the data
+    :param prediction_col: Name of prediction column
+    :param prediction_value: A fixed value to use for predictions
+    :param random_predictions: If True generate random predictions instead of a fixed value
+    :param with_dr_timestamp_column: Add predictions timestamp column to the data
+    :param dr_timestamp_column: Name of predictions timestamp column
+    :param rows_per_batch: How many rows per batch to generate
+    :param timestamp_col: Name of timestamp column
+    :return: (Dataframe with the generated data, List of batch ids)
+    """
+    batch_ids = [f"batch {i // rows_per_batch + 1}" for i in range(nr_rows)]
+    df = pd.DataFrame(
+        ["01/06/2005 13:00:00.000000"] * nr_rows,
+        columns=[timestamp_col],
+    )
+    df[batch_id_col] = batch_ids
+
+    if prediction_value:
+        predictions = np.full(nr_rows, prediction_value)
+    elif random_predictions:
+        predictions = np.random.randint(1, 10, size=nr_rows)
+    else:
+        predictions = [x for x in range(nr_rows)]
+
+    if with_predictions:
+        df[prediction_col] = np.array(predictions)
+    if with_dr_timestamp_column:
+        df[dr_timestamp_column] = df[timestamp_col]
+
+    return df, list(set(batch_ids))
+
+
 def gen_dataframe_for_data_metrics(
     nr_rows: int = 1000,
     time_bucket: TimeBucket = TimeBucket.MINUTE,
     rows_per_time_bucket: int = 100,
     columns: Tuple = ("A", "B"),
     add_missing_values: int = 0,
 ) -> pd.DataFrame:
```

## dmm/metric_evaluator.py

```diff
@@ -23,15 +23,78 @@
 
     def __str__(self):
         return "total rows: {}, score calls: {}, reduce calls: {}".format(
             self.total_rows, self.nr_calls_to_score, self.nr_calls_to_reduce
         )
 
 
-class MetricEvaluator(object):
+class MetricEvaluatorBase:
+    def __init__(self, metric: Union[str, MetricBase]):
+        self._metrics = {
+            metric.name: metric for metric in self._validate_and_convert_metrics(metric)
+        }
+        self._stats = MetricEvaluatorStats()
+
+    def reset_stats(self) -> None:
+        """
+        Reset the metric evaluator stats
+        """
+        self._stats = MetricEvaluatorStats()
+
+    def stats(self) -> MetricEvaluatorStats:
+        """
+        Return MetricEvaluatorStats object with the current stats
+        :return: MetricEvaluatorStats
+        """
+        return self._stats
+
+    def fit(self, data: pd.DataFrame) -> None:
+        for metric in self._metrics:
+            metric.fit(data)
+
+    @staticmethod
+    def _validate_and_convert_metrics(
+        metric: Union[str, MetricBase]
+    ) -> List[MetricBase]:
+        if not metric:
+            raise ValueError("No metric(s) provided!")
+        metrics = metric if isinstance(metric, list) else [metric]
+        after_sklearn_conversion = []
+        for metric in metrics:
+            if metric is None:
+                raise ValueError(
+                    "A metric object in the list of metrics has a None value"
+                )
+            elif isinstance(metric, str):
+                after_sklearn_conversion.append(SklearnMetric(metric))
+            elif isinstance(metric, MetricBase):
+                after_sklearn_conversion.append(metric)
+            else:
+                raise ValueError(
+                    "One of the metrics provided is not based on MetricBase and is not an SKLearn metric"
+                )
+        return after_sklearn_conversion
+
+    def _reduce_bucket_metric(
+        self, bucket_metric_parts: Dict[str, List[float]]
+    ) -> Dict[str, float]:
+        """
+        :param bucket_metric_parts: Bucket to list of values mapping to be reduced.
+        :return: Dict of metric name: reduced value
+        """
+        reduced_values = {}
+        for metric_name, metric in self._metrics.items():
+            reduced_values[metric_name] = metric.reduce_func()(
+                bucket_metric_parts[metric_name]
+            )
+            self._stats.nr_calls_to_reduce += 1
+        return reduced_values
+
+
+class MetricEvaluator(MetricEvaluatorBase):
     """
     Evaluate a Model Metric using training data and scoring data. This class is
     used to "stream" data through the metric object and generate the metric values.
     """
 
     def __init__(
         self,
@@ -54,80 +117,29 @@
         :param prediction_col: The name of the prediction column
         :param actuals_col: The name of the actuals column
         :param timestamp_col: The name of the timestamp column
         :param filter_actuals: whether the metric evaluator removes missing actuals values before scoring.
         :param filter_predictions: whether the metric evaluator removes missing predictions values before scoring.
         :param filter_scoring_data: whether the metric evaluator removes missing scoring values before scoring.
         """
+        super().__init__(metric)
         self._metrics = {
             metric.name: metric for metric in self._validate_and_convert_metrics(metric)
         }
         self._data_source = source
         if self._data_source is None:
             raise Exception("Can not evaluate a metric without a data source")
 
         self._time_bucket = time_bucket
         self._prediction_col = prediction_col
         self._actuals_col = actuals_col
         self._timestamp_col = timestamp_col
         self._groups_to_filter = self._get_groups_to_filter(
             filter_actuals, filter_predictions, filter_scoring_data
         )
-        self._stats = MetricEvaluatorStats()
-
-    def _validate_and_convert_metrics(
-        self, metric: Union[str, MetricBase]
-    ) -> List[MetricBase]:
-        if not metric:
-            raise ValueError("No metric(s) provided!")
-        self._metrics = metric if isinstance(metric, list) else [metric]
-
-        after_sklearn_conversion = []
-        for metric in self._metrics:
-            if metric is None:
-                raise ValueError(
-                    "A metric object in the list of metrics has a None value"
-                )
-            elif isinstance(metric, str):
-                after_sklearn_conversion.append(SklearnMetric(metric))
-            elif isinstance(metric, MetricBase):
-                after_sklearn_conversion.append(metric)
-            else:
-                raise ValueError(
-                    "One of the metrics provided is not based on MetricBase and is not an SKLearn metric"
-                )
-        return after_sklearn_conversion
-
-    def reset_stats(self) -> None:
-        """
-        Reset the metric evaluator stats
-        """
-        self._stats = MetricEvaluatorStats()
-
-    def stats(self) -> MetricEvaluatorStats:
-        """
-        Return MetricEvaluatorStats object with the current stats
-        :return: MetricEvaluatorStats
-        """
-        return self._stats
-
-    def _reduce_time_bucket_metric(
-        self, time_bucket_metric_parts: Dict[str, List[float]]
-    ) -> Dict[str, float]:
-        """
-        :param time_bucket_metric_parts:
-        :return: Dict of metric name: reduced value
-        """
-        reduced_values = {}
-        for metric_name, metric in self._metrics.items():
-            reduced_values[metric_name] = metric.reduce_func()(
-                time_bucket_metric_parts[metric_name]
-            )
-            self._stats.nr_calls_to_reduce += 1
-        return reduced_values
 
     def _run_score(self, chunk_of_data: pd.DataFrame) -> Dict[str, float]:
         scored_values = {}
         predictions = (
             chunk_of_data[self._prediction_col].to_numpy()
             if self._prediction_col in chunk_of_data
             else None
@@ -145,29 +157,14 @@
                 scoring_data=self._get_scoring_data(chunk_of_data)
                 if metric.need_scoring_data()
                 else None,
             )
             self._stats.nr_calls_to_score += 1
         return scored_values
 
-    def fit(self, data: pd.DataFrame) -> None:
-        for metric in self._metrics:
-            metric.fit(data)
-
-    def _validate_before_score(self) -> None:
-        if self._data_source is None:
-            raise Exception("Can not evaluate a metric without a data source")
-        if self._metrics is None:
-            raise ValueError("No metric was provided for evaluation")
-        if not isinstance(self._metrics, list):
-            raise ValueError("self._metric is not a list, this should not happen")
-        for metric in self._metrics:
-            if metric is None:
-                raise ValueError("A metric object provided as None value")
-
     def _get_data_chunk(self) -> Tuple[pd.DataFrame, int, bool]:
         done = False
         chunk_of_data, time_bucket_id = self._data_source.get_data()
 
         if chunk_of_data is None:
             done = True
         else:
@@ -286,15 +283,15 @@
             data_chunk, time_bucket_id, done = self._get_data_chunk()
 
             # Moved to a new time bucket so calling reduce on the previous values
             if time_bucket_id != prev_time_bucket_id:
                 # We don't reduce in the first time we get data (as prev is None and id is not None)
                 if prev_time_bucket_id is not None:
                     # Moved to a new time bucket (or done).. need to reduce
-                    reduced_values = self._reduce_time_bucket_metric(time_bucket_scores)
+                    reduced_values = self._reduce_bucket_metric(time_bucket_scores)
                     output_nr_samples_list.append(time_bucket_metric_nr_samples)
                     output_bucket_timestamp_list.append(time_bucket_timestamp)
                     for metric, value in reduced_values.items():
                         final_scores[metric].append(value)
                         time_bucket_scores[metric].clear()
                 time_bucket_metric_nr_samples = 0
                 prev_time_bucket_id = time_bucket_id
```

## dmm/data_source/__init__.py

```diff
@@ -1,19 +1,21 @@
 from __future__ import absolute_import
 
 from .dataframe_source import DataFrameSource
 from .datarobot_source import (
     ActualsDataExportProvider,
+    BatchDataRobotSource,
     DataRobotSource,
     PredictionDataExportProvider,
     TrainingDataExportProvider,
 )
 from .generator_source import GeneratorSource
 
 __all__ = [
     "DataFrameSource",
     "DataRobotSource",
     "PredictionDataExportProvider",
     "ActualsDataExportProvider",
     "TrainingDataExportProvider",
     "GeneratorSource",
+    "BatchDataRobotSource",
 ]
```

## dmm/data_source/datarobot_source.py

```diff
@@ -1,23 +1,25 @@
 from __future__ import annotations
 
 import datetime
 import errno
+import functools
 import json
 import logging
 import os
 import tempfile
 import time
 from enum import Enum
 from typing import Callable, Generator, Iterator, List, Optional, Union
 
 import datarobot as dr
 import pandas as pd
+import pytz
 import requests
-from datarobot.errors import AsyncProcessUnsuccessfulError
+from datarobot.errors import AsyncProcessUnsuccessfulError, ClientError
 from dateutil.parser import parse
 
 from dmm.constants import ColumnName, TimeBucket
 from dmm.data_source.data_source_base import DataSourceBase
 from dmm.time_bucket import (
     check_if_in_same_time_bucket,
     check_if_in_same_time_bucket_vectorized,
@@ -388,14 +390,157 @@
         while chunk_id != -1:
             df, chunk_id = provider()
             if df is not None:
                 result = pd.concat([result, df])
         return result
 
 
+class BatchDataRobotSource(DataSourceBase):
+    def __init__(
+        self,
+        base_url: str,
+        token: str,
+        deployment_id: str,
+        batch_ids: List[str],
+        model_id: str = None,
+        max_rows: int = 100,
+    ):
+        super().__init__(max_rows)
+        if max_rows <= 0:
+            raise Exception(f"max_rows must be > 0, got {max_rows}")
+
+        self._deployment_id = deployment_id
+        self._model_id = model_id
+        self._current_chunk_id = 0
+        self._prev_chunk_batch_id = None
+        self._batch_ids = batch_ids
+
+        api = DataRobotApiClient(token, base_url)
+        self._api = api
+        self._deployment = Deployment(self._api, deployment_id)
+        self._start = self._deployment.created_at
+        self._end = datetime.datetime.utcnow().replace(tzinfo=pytz.UTC)
+        self._prediction_data_provider = self._get_prediction_data_provider()
+
+    def init(self, time_bucket: TimeBucket):
+        pass
+
+    def reset(self):
+        self._prediction_data_provider = self._get_prediction_data_provider()
+
+    def get_data(self) -> (pd.DataFrame, int):
+        """
+        Method to return a chunk of data that can be sent to a metric object to be transformed.
+        :return:
+            - DataFrame: if there is more data to process, or None
+            - int: ID of the time bucket
+        """
+        if self._prev_chunk_batch_id is None and self._current_chunk_id != -1:
+            self.reset()
+
+        prediction_df, chunk_id = self._prediction_data_provider.get_data()
+        if prediction_df is None:
+            logger.info("data export for the given time range completed")
+            return prediction_df, chunk_id
+
+        prediction_chunk_batch_id = self._prediction_data_provider.get_chunk_batch_id()
+        prediction_df = self._format_prediction_df(prediction_df)
+
+        self._update_chunk_info(prediction_chunk_batch_id)
+        return prediction_df, self._current_chunk_id
+
+    def get_prediction_data(self) -> (pd.DataFrame, int):
+        """
+        Method to return a chunk of prediction data that can be sent to a metric object to be transformed.
+        :return:
+            - DataFrame: if there is more data to process, or None
+            - int: ID of the time bucket
+        """
+        prediction_df, chunk_id = self._prediction_data_provider.get_data()
+        return prediction_df, chunk_id
+
+    def get_all_prediction_data(self) -> pd.DataFrame:
+        """
+        Returns all prediction data available for that source object in a single DataFrame.
+        :return:
+            DataFrame: Prediction data for all chunks.
+        """
+        return self._get_all(self.get_prediction_data)
+
+    def _get_prediction_data_provider(self) -> BatchPredictionDataExportProvider:
+        """
+        Retrieves a new instance of the PredictionDataExportProvider class.
+        """
+        return BatchPredictionDataExportProvider(
+            api=self._api,
+            deployment_id=self._deployment_id,
+            start=self._start,
+            end=self._end,
+            model_id=self._model_id,
+            max_rows=self.max_rows,
+            batch_ids=self._batch_ids,
+        )
+
+    def _format_prediction_df(self, pred_df: pd.DataFrame) -> pd.DataFrame:
+        """
+        Formats data from prediction data export, before merging with actuals.
+        Renames the columns to standardize the output names.
+
+        Parameters
+        ----------
+        pred_df: pd.DataFrame
+        """
+        rename_columns = {
+            ColumnName.DR_TIMESTAMP_COLUMN: ColumnName.TIMESTAMP,
+            ColumnName.DR_BATCH_ID_COLUMN: ColumnName.BATCH_ID_COLUMN,
+        }
+
+        if self._deployment.type == DeploymentType.REGRESSION:
+            prediction_col = ColumnName.DR_PREDICTION_COLUMN
+        elif self._deployment.type == DeploymentType.BINARY_CLASSIFICATION:
+            prediction_col = f"{ColumnName.DR_PREDICTION_COLUMN}_{self._deployment.positive_class_label}"
+            # apply prediction threshold
+            pred_df[prediction_col] = (
+                pred_df[prediction_col] >= self._deployment.prediction_threshold
+            )
+        else:
+            raise Exception(f"Unsupported deployment type {self._deployment.type}")
+
+        rename_columns.update({prediction_col: ColumnName.PREDICTIONS})
+        pred_df = pred_df.rename(rename_columns, axis=1)
+        return pred_df
+
+    def _update_chunk_info(self, chunk_batch_id: str) -> None:
+        """
+        Source must keep track of a small piece of state to differentiate time bucketed chunks.
+        This method updates this state.
+
+        Parameters
+        ----------
+        chunk_batch_id: str
+            batch_id of any event within the chunk
+        """
+        if self._prev_chunk_batch_id and self._prev_chunk_batch_id != chunk_batch_id:
+            self._current_chunk_id += 1
+        self._prev_chunk_batch_id = chunk_batch_id
+
+    @staticmethod
+    def _get_all(provider: Callable[[], (pd.DataFrame, int)]) -> pd.DataFrame:
+        """
+        Utility method that polls provider (matches get_prediction_data signature) and concatenates
+        all available frames.
+        """
+        result, chunk_id = provider()
+        while chunk_id != -1:
+            df, chunk_id = provider()
+            if df is not None:
+                result = pd.concat([result, df])
+        return result
+
+
 class DataRobotExportBase:
     """
     The base class for exporting data via DataRobot API in the time bucket chunk manner.
     """
 
     def __init__(
         self,
@@ -420,15 +565,15 @@
         self._max_rows = max_rows
         self._timestamp_col = timestamp_col
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
         self._time_bucket_chunks = self._get_time_bucket_chunks()
 
         self._current_chunk_id = 0
-        self._prev_chunk_datetime = None
+        self._prev_bucket_id = None
 
     def set_new_export_parameters(
         self,
         start: datetime.datetime,
         end: datetime.datetime,
         time_bucket: TimeBucket,
         max_rows: int,
@@ -438,51 +583,51 @@
         self._time_bucket = time_bucket
         self._max_rows = max_rows
         self.reset()
         return self
 
     def reset(self) -> None:
         self._current_chunk_id = 0
-        self._prev_chunk_datetime = None
+        self._prev_bucket_id = None
         self._time_bucket_chunks = self._get_time_bucket_chunks()
 
     def _get_time_bucket_chunks(self) -> TimeBucketChunks:
         return TimeBucketChunks(
             self._timestamp_col,
             PushBackFrameIterator(
                 iter(
                     DataRobotChunksIterator(
                         api=self._api,
                         deployment_id=self._deployment_id,
                         model_id=self._model_id,
                         start_dt=self._start,
                         end_dt=self._end,
-                        timestamp_col=self._timestamp_col,
                         start_export=self._start_export,
                         get_export_dataset_ids=self._get_export_dataset_ids,
+                        sort_column=self._timestamp_col,
                     )
                 )
             ),
         )
 
-    def _update_chunk_info(self, chunk_dt: datetime.datetime):
+    def _update_chunk_info(self, bucket_id: any):
         """
         Source must keep track of a small piece of state to differentiate time bucketed chunks.
         This method updates this state.
 
         Parameters
         ----------
-        chunk_dt: datetime
-            datetime of any event within the chunk
+        bucket_id: any
+            identifier of a chunk (e.g. time bucket timestamp or batch_id)
         """
-        if self._prev_chunk_datetime and not check_if_in_same_time_bucket(
-            self._prev_chunk_datetime, chunk_dt, self._time_bucket
+        if self._prev_bucket_id and not check_if_in_same_time_bucket(
+            self._prev_bucket_id, bucket_id, self._time_bucket
         ):
             self._current_chunk_id += 1
-        self._prev_chunk_datetime = chunk_dt
+        self._prev_bucket_id = bucket_id
 
 
 class PredictionDataExportProvider(DataRobotExportBase):
     def __init__(
         self,
         api: DataRobotApiClient,
         deployment_id: str,
@@ -529,15 +674,136 @@
     def get_last_prediction_timestamp(
         self, prediction_df: pd.DataFrame
     ) -> datetime.datetime:
         last_timestamp = prediction_df[self._timestamp_col].iloc[-1]
         return parse(last_timestamp)
 
     def get_chunk_timestamp(self) -> datetime.datetime:
-        return self._prev_chunk_datetime
+        return self._prev_bucket_id
+
+
+class DataRobotBatchExportBase:
+    """
+    The base class for exporting data via DataRobot API in the time bucket chunk manner.
+    """
+
+    def __init__(
+        self,
+        api: DataRobotApiClient,
+        deployment_id: str,
+        model_id: str | None,
+        start: datetime.datetime,
+        end: datetime.datetime,
+        max_rows: int,
+        start_export: Callable,
+        get_export_dataset_ids: Callable,
+    ):
+        self._api = api
+        self._deployment_id = deployment_id
+        self._model_id = model_id
+        self._deployment = Deployment(self._api, deployment_id)
+        self._start = start
+        self._end = end
+        self._max_rows = max_rows
+        self._start_export = start_export
+        self._get_export_dataset_ids = get_export_dataset_ids
+        self._batch_bucket_chunks = self._get_batch_bucket_chunks()
+
+        self._current_chunk_id = 0
+        self._prev_bucket_id = None
+
+    def set_new_export_parameters(self, max_rows: int) -> DataRobotBatchExportBase:
+        self._max_rows = max_rows
+        self.reset()
+        return self
+
+    def reset(self) -> None:
+        self._current_chunk_id = 0
+        self._prev_bucket_id = None
+        self._batch_bucket_chunks = self._get_batch_bucket_chunks()
+
+    def _get_batch_bucket_chunks(self) -> BatchBucketChunks:
+        return BatchBucketChunks(
+            ColumnName.DR_BATCH_ID_COLUMN,
+            PushBackFrameIterator(
+                iter(
+                    DataRobotChunksIterator(
+                        api=self._api,
+                        deployment_id=self._deployment_id,
+                        model_id=self._model_id,
+                        start_dt=self._start,
+                        end_dt=self._end,
+                        start_export=self._start_export,
+                        get_export_dataset_ids=self._get_export_dataset_ids,
+                    )
+                )
+            ),
+        )
+
+    def _update_chunk_info(self, bucket_id: str):
+        """
+        Source must keep track of a small piece of state to differentiate time bucketed chunks.
+        This method updates this state.
+
+        Parameters
+        ----------
+        chunk_dt: datetime
+            datetime of any event within the chunk
+        """
+        if self._prev_bucket_id and self._prev_bucket_id != bucket_id:
+            self._current_chunk_id += 1
+        self._prev_bucket_id = bucket_id
+
+
+class BatchPredictionDataExportProvider(DataRobotBatchExportBase):
+    def __init__(
+        self,
+        api: DataRobotApiClient,
+        deployment_id: str,
+        start: datetime.datetime,
+        end: datetime.datetime,
+        batch_ids: List[str],
+        model_id: str = None,
+        max_rows: int = 100,
+    ):
+        if max_rows <= 0:
+            raise Exception(f"max_rows must be > 0, got {max_rows}")
+
+        super().__init__(
+            api=api,
+            deployment_id=deployment_id,
+            model_id=model_id,
+            start=start,
+            end=end,
+            max_rows=max_rows,
+            start_export=functools.partial(
+                api.start_batch_prediction_data_export, batch_ids
+            ),
+            get_export_dataset_ids=api.get_prediction_export_dataset_ids,
+        )
+
+    def get_data(self) -> (pd.DataFrame, int):
+        """
+        Method to return a chunk of prediction data that can be sent to a metric object to be transformed.
+        :return:
+            - DataFrame: if there is more data to process, or None
+            - int: ID of the time bucket
+        """
+        if self._batch_bucket_chunks.finished():
+            return None, -1
+
+        chunk_df, bucket_id = self._batch_bucket_chunks.load_batch_bucket_chunk(
+            max_rows=self._max_rows
+        )
+
+        self._update_chunk_info(bucket_id)
+        return chunk_df, self._current_chunk_id
+
+    def get_chunk_batch_id(self) -> str:
+        return self._prev_bucket_id
 
 
 class ActualsDataExportProvider(DataRobotExportBase):
     def __init__(
         self,
         api: DataRobotApiClient,
         deployment_id: str,
@@ -664,19 +930,24 @@
 
 class Deployment:
     """
     Provides access to relevant deployment properties.
     """
 
     def __init__(self, api: DataRobotApiClient, deployment_id: str):
+        self.api = api
         self._deployment = api.get_deployment(deployment_id)
-        if "modelPackage" not in self._deployment:
-            raise Exception("We do not support deployments without model packages!")
-        model_package_id = self._deployment["modelPackage"]["id"]
-        self._model_package = api.get_model_package(model_package_id)
+
+        self._model_package = self._get_champion_model_package(deployment_id)
+        if self._model_package is None:
+            if "modelPackage" not in self._deployment:
+                raise Exception("We do not support deployments without model packages!")
+            model_package_id = self._deployment["modelPackage"]["id"]
+            self._model_package = api.get_model_package(model_package_id)
+
         if self._model_package["modelKind"]["isTimeSeries"]:
             raise Exception("Time series models are not yet supported")
 
     @property
     def target_column(self) -> str:
         return self._deployment["model"]["targetName"]
 
@@ -703,14 +974,24 @@
     def prediction_threshold(self) -> float:
         if self.type != DeploymentType.BINARY_CLASSIFICATION:
             raise Exception(
                 "Prediction threshold can only be retrieved for binary classification deployments"
             )
         return self._model_package["target"]["predictionThreshold"]
 
+    @property
+    def created_at(self) -> datetime:
+        return parse(self._deployment["createdAt"])
+
+    def _get_champion_model_package(self, deployment_id: str) -> Optional[dict]:
+        try:
+            return self.api.get_champion_model_package(deployment_id)
+        except ClientError:
+            return None
+
 
 class PushBackFrameIterator:
     """
     Extends iterator functionality to push back elements.
     """
 
     def __init__(self, iterator: Iterator[pd.DataFrame]):
@@ -769,30 +1050,61 @@
     def start_prediction_data_export(
         self,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
         model_id: Union[str, None],
     ) -> requests.Response:
+        return self._start_prediction_data_export(
+            deployment_id, start, end, model_id, batch_ids=None
+        )
+
+    def start_batch_prediction_data_export(
+        self,
+        batch_ids: List[str],
+        deployment_id: str,
+        start: datetime.datetime,
+        end: datetime.datetime,
+        model_id: Union[str, None],
+    ) -> requests.Response:
+        return self._start_prediction_data_export(
+            deployment_id, start, end, model_id, batch_ids=batch_ids
+        )
+
+    def _start_prediction_data_export(
+        self,
+        deployment_id: str,
+        start: datetime.datetime,
+        end: datetime.datetime,
+        model_id: Union[str, None],
+        batch_ids: Optional[List[str]],
+    ) -> requests.Response:
         """
         Starts prediction data export between start (inclusive) and end (exclusive).
         We cannot assume anything about the order of returned records.
 
         Parameters
         ----------
         deployment_id: str
         start: datetime
             Inclusive start of the time range.
         end: datetime
             Exclusive end of the time range.
         model_id: Optional[str]
+        batch_ids: Optional[List[str]]
+            If present, it will perform batch export for the passed IDs.
         """
         return self._client.post(
             f"deployments/{deployment_id}/predictionDataExports/",
-            data={"start": start, "end": end, "modelId": model_id},
+            data={
+                "start": start,
+                "end": end,
+                "modelId": model_id,
+                "batch_ids": batch_ids,
+            },
         )
 
     def start_actuals_export(
         self,
         deployment_id: str,
         start: datetime.datetime,
         end: datetime.datetime,
@@ -945,14 +1257,26 @@
         deployment_id: str
         """
         deployment_settings = self.get_deployment_settings(deployment_id)
         column_names = deployment_settings["associationId"]["columnNames"]
         association_id = column_names[0] if column_names else None
         return association_id
 
+    def get_champion_model_package(self, deployment_id: str) -> dict:
+        """
+        Fetches champion model package data
+
+        Parameters
+        ----------
+        deployment_id: str
+        """
+        return self._client.get(
+            f"deployments/{deployment_id}/championModelPackage"
+        ).json()
+
     @staticmethod
     def fetch_dataset_sync(dataset_id: str, max_wait_sec: int = 600) -> dr.Dataset:
         """
         Fetches dataset synchronously.
 
         Parameters
         ----------
@@ -981,23 +1305,23 @@
     def __init__(
         self,
         api: DataRobotApiClient,
         deployment_id: str,
         model_id: str | None,
         start_dt: datetime.datetime,
         end_dt: datetime.datetime,
-        timestamp_col: str,
         start_export: Callable,
         get_export_dataset_ids: Callable,
+        sort_column: Optional[str] = None,
     ):
         self._deployment_id = deployment_id
         self._model_id = model_id
         self._start_dt = start_dt
         self._end_dt = end_dt
-        self._timestamp_col = timestamp_col
+        self._sort_column = sort_column
         self._api = api
         self._start_export = start_export
         self._get_export_dataset_ids = get_export_dataset_ids
 
     def __iter__(self) -> Generator[pd.DataFrame, None, None]:
         """
         Returns an iterator over chunks for time range defined in the constructor. It returns data sorted by timestamp.
@@ -1011,15 +1335,17 @@
         interval = self._end_dt - self._start_dt
         start_dt = self._start_dt
         while start_dt != self._end_dt:
             df, interval = self._fetch_subset(start_dt, interval)
             start_dt += interval
             if df.empty:
                 continue
-            sorted_df = df.sort_values(by=self._timestamp_col)
+            sorted_df = (
+                df.sort_values(by=self._sort_column) if self._sort_column else df
+            )
             yield sorted_df
 
     def _fetch_subset(
         self, start_dt: datetime.datetime, interval: datetime.timedelta
     ) -> (pd.DataFrame, datetime.timedelta):
         export_type = self._get_export_type()
         if export_type:
@@ -1179,7 +1505,82 @@
         if self._timestamp_column not in row:
             logger.debug(
                 "Got row without %s column: %s",
                 self._timestamp_column,
                 row.to_string(header=False, index=False),
             )
             raise Exception(f"Got row without {self._timestamp_column} column")
+
+
+class BatchBucketChunks:
+    """
+    Builds chunks out of rows returned by PushBackFrameIterator.
+    """
+
+    def __init__(self, batch_id_column: str, chunks_iterator: PushBackFrameIterator):
+        self._batch_id_column = batch_id_column
+        self._chunks_iterator = chunks_iterator
+
+    def finished(self) -> bool:
+        """
+        True if the records in underlying iterator were exhausted.
+        """
+        return self._chunks_iterator.finished()
+
+    def load_batch_bucket_chunk(self, max_rows: int) -> (pd.DataFrame, str):
+        """
+        Pulls rows from rows_iterator respecting two rules
+        - all records must belong to the same batch bucket
+        - chunk cannot be larger than max_rows
+
+        Parameters
+        ----------
+        max_rows: int
+            Upperbound on the size of the chunk.
+        """
+        row = self._chunks_iterator.peek_row()
+        if row is None:
+            raise Exception(
+                "fetch_time_bucket_chunk shouldn't be called against exhausted source"
+            )
+        self._validate_row(row)
+        batch_id = row[self._batch_id_column]
+
+        chunks = []
+        total_rows = 0
+        while not self._chunks_iterator.finished() and total_rows < max_rows:
+            df = self._chunks_iterator.next()
+            same_chunk, others = self._partition_by_batch_bucket(
+                df, batch_id, max_rows - total_rows
+            )
+            chunks.append(same_chunk)
+            total_rows += len(same_chunk)
+            if not others.empty:
+                self._chunks_iterator.push_back(others)
+                break
+        return pd.concat(chunks), batch_id
+
+    def _partition_by_batch_bucket(
+        self,
+        df: pd.DataFrame,
+        batch_id: str,
+        max_rows: int,
+    ) -> (pd.DataFrame, pd.DataFrame):
+        """
+        Returns a subframe with records that belong to the bucket defined by `batch_id`.
+        It respects `max_rows` constraint - thus some records from that bucket might end up in `others` frame.
+        The remaining records are present in the latter data frame.
+        Assumes that the rows in `df` are sorted by batch ID. `same_bucket` frame is always a prefix of `df`.
+        """
+        filter_condition = df[self._batch_id_column] == batch_id
+        same_bucket = df[filter_condition].iloc[:max_rows]
+        others = df.iloc[len(same_bucket) :]
+        return same_bucket, others
+
+    def _validate_row(self, row: pd.Series) -> None:
+        if self._batch_id_column not in row:
+            logger.debug(
+                "Got row without %s column: %s",
+                self._batch_id_column,
+                row.to_string(header=False, index=False),
+            )
+            raise Exception(f"Got row without {self._batch_id_column} column")
```

## Comparing `datarobot_model_metrics-0.1.4.dist-info/METADATA` & `datarobot_model_metrics-0.1.5.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobot-model-metrics
-Version: 0.1.4
+Version: 0.1.5
 Summary: datarobot-model-metrics is a framework to compute model ML metrics
 Home-page: https://github.com/datarobot/datarobot-model-metrics
 Author: DataRobot
 Author-email: info@datarobot.com
 License: DataRobot
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

## Comparing `datarobot_model_metrics-0.1.4.dist-info/RECORD` & `datarobot_model_metrics-0.1.5.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 dmm/__init__.py,sha256=w3zRSdtt2_upBZCJNj93DqZeXKPyfMKqo3Xo1yOHLeY,385
-dmm/constants.py,sha256=t3UUKkKgWpbUX0TB3B9S1bVlcDUB_lheArpddCPkWZQ,606
-dmm/example_data_helper.py,sha256=lUSfuzt5DskwccJu3OQLOAzuTJ2nQUuVvtTBKFcjNi4,4733
-dmm/metric_evaluator.py,sha256=e5iUfGPYGTn4XjxImRSO_8WPK2KWExdjAFsXmb6MAlU,13088
+dmm/batch_metric_evaluator.py,sha256=84khhvGDGyDYi_U2YhqB_vv97b3VZ9M_nb7imM_lc_Q,8663
+dmm/constants.py,sha256=u8Vcm7TEthcgDzvGjf-s9KagpvQXgjMiasxkNHl67qc,687
+dmm/example_data_helper.py,sha256=FOQidtHYsDj6zgDLBMhSDtpgC1869r17MRlsTv_v6dE,6656
+dmm/metric_evaluator.py,sha256=L-0j6JQrDmMWWvSb1MeAvzq_yR-LPBK7jl1rxALo-HA,12824
 dmm/time_bucket.py,sha256=cg42R1U_o7is6nZf5qUuvdc-2HBH9UKuACKjKe3dLto,3164
 dmm/utils.py,sha256=kaZ7erHZofSwqZL1ddd8GXK3wrZfvYZ7I0ziz2CgbL0,1690
-dmm/data_source/__init__.py,sha256=U9h2pJ89u834vGnBJ_5jjTkQsQQhfCgvAtccXb-Pk7c,471
+dmm/data_source/__init__.py,sha256=rsK8s1quedmNYchnUodsc4gUXAENM-N_rVVohhX9HMA,525
 dmm/data_source/data_source_base.py,sha256=5zJ9SZYs1ufbku11GYa4PQRY1lwOOsXbx8kDkiG1VMk,2096
 dmm/data_source/dataframe_source.py,sha256=mKEGXyNFVAsqiocAVKQlhbGlUlwjx77sl5KAlXT6nx4,2698
-dmm/data_source/datarobot_source.py,sha256=ueVPbLGaIK8iOd-Nu-_jgO8UKR7EAK1Et4TrkZLwbTE,41964
+dmm/data_source/datarobot_source.py,sha256=lXojCRMRTI7Irz2o51YtfET6OpWSUIO3BT_xrTubqTg,55894
 dmm/data_source/generator_source.py,sha256=O-GsreX8mQognj3u6LQxHkZ3yWj3sOgx_dojvObVdlI,3003
 dmm/metric/__init__.py,sha256=nIX41kZJKfQztTw93CXVLRbgTt5W1qhdK1uakWxL9CE,396
 dmm/metric/asymmetric_error.py,sha256=h4J7nzXw9BurtGrkoe4oe7QBI2l69XeB9CiGfpcKUxc,1620
 dmm/metric/median_absolute_error.py,sha256=qV0NpJdF6daTzJOaGpd8KAPG3gZWPS43FZxOb-rR0kE,320
 dmm/metric/metric_base.py,sha256=Rzsju5eZhoyafxo7ErFmO76sDcsvkb3SyODDwcNudy4,3569
 dmm/metric/missing_values.py,sha256=i9ujXCuOWEPrUteFXTCDGX6SM8RVd7cQoI6byQPga4E,957
 dmm/metric/sklearn_metric.py,sha256=Bv4ukOSZyOKjXK-_4b7KKmlkOVrYnwSX5GST_Hf-qpc,2014
-datarobot_model_metrics-0.1.4.dist-info/METADATA,sha256=7ydRmSHTDyGIDzKxSjP0azdmuqHuq6e_iJYxN0RpgbA,886
-datarobot_model_metrics-0.1.4.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-datarobot_model_metrics-0.1.4.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
-datarobot_model_metrics-0.1.4.dist-info/RECORD,,
+datarobot_model_metrics-0.1.5.dist-info/METADATA,sha256=9tahHkZW9QGhR3SFMQx94N8ba-HeHnKs4mb1IqIPq7U,886
+datarobot_model_metrics-0.1.5.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+datarobot_model_metrics-0.1.5.dist-info/top_level.txt,sha256=69FbTyYFh17OyfaIppCUhlu4QG-prAaQ6ovJ_X0SNG8,4
+datarobot_model_metrics-0.1.5.dist-info/RECORD,,
```

