# Comparing `tmp/datarobotx-0.1.5-py3-none-any.whl.zip` & `tmp/datarobotx-0.1.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,58 +1,58 @@
-Zip file size: 141241 bytes, number of entries: 56
--rw-rw-r--  2.0 unx     1566 b- defN 23-May-04 23:52 datarobotx/__init__.py
--rw-rw-r--  2.0 unx      271 b- defN 23-May-04 23:52 datarobotx/_version.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-04 23:52 datarobotx/py.typed
--rw-rw-r--  2.0 unx        0 b- defN 23-May-04 23:52 datarobotx/client/__init__.py
--rw-rw-r--  2.0 unx     9402 b- defN 23-May-04 23:52 datarobotx/client/datasets.py
--rw-rw-r--  2.0 unx    15368 b- defN 23-May-04 23:52 datarobotx/client/deployments.py
--rw-rw-r--  2.0 unx     1002 b- defN 23-May-04 23:52 datarobotx/client/prediction_servers.py
--rw-rw-r--  2.0 unx    25695 b- defN 23-May-04 23:52 datarobotx/client/projects.py
--rw-rw-r--  2.0 unx     2420 b- defN 23-May-04 23:52 datarobotx/client/status.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-04 23:52 datarobotx/common/__init__.py
--rw-rw-r--  2.0 unx     7273 b- defN 23-May-04 23:52 datarobotx/common/client.py
--rw-rw-r--  2.0 unx     8741 b- defN 23-May-04 23:52 datarobotx/common/config.py
--rw-rw-r--  2.0 unx     4588 b- defN 23-May-04 23:52 datarobotx/common/configurator.py
--rw-rw-r--  2.0 unx   131191 b- defN 23-May-04 23:52 datarobotx/common/dr_config.py
--rw-rw-r--  2.0 unx    10494 b- defN 23-May-04 23:52 datarobotx/common/logging.py
--rw-rw-r--  2.0 unx     3980 b- defN 23-May-04 23:52 datarobotx/common/transformations.py
--rw-rw-r--  2.0 unx    15032 b- defN 23-May-04 23:52 datarobotx/common/ts_helpers.py
--rw-rw-r--  2.0 unx      605 b- defN 23-May-04 23:52 datarobotx/common/types.py
--rw-rw-r--  2.0 unx    26936 b- defN 23-May-04 23:52 datarobotx/common/utils.py
--rw-rw-r--  2.0 unx      136 b- defN 23-May-04 23:52 datarobotx/llm/__init__.py
--rw-rw-r--  2.0 unx     2119 b- defN 23-May-04 23:52 datarobotx/llm/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-04 23:52 datarobotx/llm/chains/__init__.py
--rw-rw-r--  2.0 unx    11994 b- defN 23-May-04 23:52 datarobotx/llm/chains/data_dict.py
--rw-rw-r--  2.0 unx     9398 b- defN 23-May-04 23:52 datarobotx/llm/chains/enrich.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-04 23:52 datarobotx/models/__init__.py
--rw-rw-r--  2.0 unx     2754 b- defN 23-May-04 23:52 datarobotx/models/autoanomaly.py
--rw-rw-r--  2.0 unx     3332 b- defN 23-May-04 23:52 datarobotx/models/autocluster.py
--rw-rw-r--  2.0 unx     2465 b- defN 23-May-04 23:52 datarobotx/models/automl.py
--rw-rw-r--  2.0 unx    11349 b- defN 23-May-04 23:52 datarobotx/models/autopilot.py
--rw-rw-r--  2.0 unx    11242 b- defN 23-May-04 23:52 datarobotx/models/autots.py
--rw-rw-r--  2.0 unx    14976 b- defN 23-May-04 23:52 datarobotx/models/colreduce.py
--rw-rw-r--  2.0 unx    24227 b- defN 23-May-04 23:52 datarobotx/models/deploy.py
--rw-rw-r--  2.0 unx    33947 b- defN 23-May-04 23:52 datarobotx/models/deployment.py
--rw-rw-r--  2.0 unx     7914 b- defN 23-May-04 23:52 datarobotx/models/evaluation.py
--rw-rw-r--  2.0 unx    16937 b- defN 23-May-04 23:52 datarobotx/models/featurediscovery.py
--rw-rw-r--  2.0 unx     5593 b- defN 23-May-04 23:52 datarobotx/models/intraproject.py
--rw-rw-r--  2.0 unx    28420 b- defN 23-May-04 23:52 datarobotx/models/model.py
--rw-rw-r--  2.0 unx     9736 b- defN 23-May-04 23:52 datarobotx/models/selfdiscovery.py
--rw-rw-r--  2.0 unx     4422 b- defN 23-May-04 23:52 datarobotx/models/share.py
--rw-rw-r--  2.0 unx    20800 b- defN 23-May-04 23:52 datarobotx/models/sparkingest.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-04 23:52 datarobotx/viz/__init__.py
--rw-rw-r--  2.0 unx     4906 b- defN 23-May-04 23:52 datarobotx/viz/charts.py
--rw-rw-r--  2.0 unx     6326 b- defN 23-May-04 23:52 datarobotx/viz/leaderboard.py
--rw-rw-r--  2.0 unx     9407 b- defN 23-May-04 23:52 datarobotx/viz/modelcard.py
--rw-rw-r--  2.0 unx     9508 b- defN 23-May-04 23:52 datarobotx/viz/viz.py
--rw-rw-r--  2.0 unx      926 b- defN 23-May-04 23:52 datarobotx/viz/templates/drx_button.html
--rw-rw-r--  2.0 unx     1490 b- defN 23-May-04 23:52 datarobotx/viz/templates/leaderboard.html
--rw-rw-r--  2.0 unx      414 b- defN 23-May-04 23:52 datarobotx/viz/templates/leaderboard.md
--rw-rw-r--  2.0 unx     5433 b- defN 23-May-04 23:52 datarobotx/viz/templates/model_card.html
--rw-rw-r--  2.0 unx       48 b- defN 23-May-04 23:52 datarobotx/viz/templates/model_card.md
--rw-rw-r--  2.0 unx     3140 b- defN 23-May-04 23:52 datarobotx/viz/templates/robot.svg
--rw-rw-r--  2.0 unx     3135 b- defN 23-May-04 23:52 datarobotx/viz/templates/robot_large.svg
--rw-rw-r--  2.0 unx     5330 b- defN 23-May-04 23:53 datarobotx-0.1.5.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-04 23:53 datarobotx-0.1.5.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-May-04 23:53 datarobotx-0.1.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4836 b- defN 23-May-04 23:53 datarobotx-0.1.5.dist-info/RECORD
-56 files, 541327 bytes uncompressed, 133541 bytes compressed:  75.3%
+Zip file size: 142026 bytes, number of entries: 56
+-rw-rw-r--  2.0 unx     1566 b- defN 23-May-22 21:37 datarobotx/__init__.py
+-rw-rw-r--  2.0 unx      271 b- defN 23-May-22 21:37 datarobotx/_version.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-22 21:38 datarobotx/py.typed
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-22 21:38 datarobotx/client/__init__.py
+-rw-rw-r--  2.0 unx     9402 b- defN 23-May-22 21:37 datarobotx/client/datasets.py
+-rw-rw-r--  2.0 unx    16124 b- defN 23-May-22 21:37 datarobotx/client/deployments.py
+-rw-rw-r--  2.0 unx     1002 b- defN 23-May-22 21:37 datarobotx/client/prediction_servers.py
+-rw-rw-r--  2.0 unx    25695 b- defN 23-May-22 21:37 datarobotx/client/projects.py
+-rw-rw-r--  2.0 unx     2420 b- defN 23-May-22 21:37 datarobotx/client/status.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-22 21:38 datarobotx/common/__init__.py
+-rw-rw-r--  2.0 unx     7273 b- defN 23-May-22 21:37 datarobotx/common/client.py
+-rw-rw-r--  2.0 unx     9358 b- defN 23-May-22 21:37 datarobotx/common/config.py
+-rw-rw-r--  2.0 unx     4588 b- defN 23-May-22 21:37 datarobotx/common/configurator.py
+-rw-rw-r--  2.0 unx   131191 b- defN 23-May-22 21:37 datarobotx/common/dr_config.py
+-rw-rw-r--  2.0 unx    10494 b- defN 23-May-22 21:37 datarobotx/common/logging.py
+-rw-rw-r--  2.0 unx     3980 b- defN 23-May-22 21:37 datarobotx/common/transformations.py
+-rw-rw-r--  2.0 unx    15032 b- defN 23-May-22 21:37 datarobotx/common/ts_helpers.py
+-rw-rw-r--  2.0 unx      605 b- defN 23-May-22 21:37 datarobotx/common/types.py
+-rw-rw-r--  2.0 unx    27474 b- defN 23-May-22 21:37 datarobotx/common/utils.py
+-rw-rw-r--  2.0 unx      136 b- defN 23-May-22 21:37 datarobotx/llm/__init__.py
+-rw-rw-r--  2.0 unx     2119 b- defN 23-May-22 21:37 datarobotx/llm/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-22 21:38 datarobotx/llm/chains/__init__.py
+-rw-rw-r--  2.0 unx    12665 b- defN 23-May-22 21:37 datarobotx/llm/chains/data_dict.py
+-rw-rw-r--  2.0 unx     9398 b- defN 23-May-22 21:37 datarobotx/llm/chains/enrich.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-22 21:38 datarobotx/models/__init__.py
+-rw-rw-r--  2.0 unx     2754 b- defN 23-May-22 21:37 datarobotx/models/autoanomaly.py
+-rw-rw-r--  2.0 unx     3332 b- defN 23-May-22 21:37 datarobotx/models/autocluster.py
+-rw-rw-r--  2.0 unx     2465 b- defN 23-May-22 21:37 datarobotx/models/automl.py
+-rw-rw-r--  2.0 unx    11349 b- defN 23-May-22 21:37 datarobotx/models/autopilot.py
+-rw-rw-r--  2.0 unx    11242 b- defN 23-May-22 21:37 datarobotx/models/autots.py
+-rw-rw-r--  2.0 unx    14976 b- defN 23-May-22 21:37 datarobotx/models/colreduce.py
+-rw-rw-r--  2.0 unx    24175 b- defN 23-May-22 21:37 datarobotx/models/deploy.py
+-rw-rw-r--  2.0 unx    33947 b- defN 23-May-22 21:37 datarobotx/models/deployment.py
+-rw-rw-r--  2.0 unx     7914 b- defN 23-May-22 21:37 datarobotx/models/evaluation.py
+-rw-rw-r--  2.0 unx    16937 b- defN 23-May-22 21:37 datarobotx/models/featurediscovery.py
+-rw-rw-r--  2.0 unx     5593 b- defN 23-May-22 21:37 datarobotx/models/intraproject.py
+-rw-rw-r--  2.0 unx    28420 b- defN 23-May-22 21:37 datarobotx/models/model.py
+-rw-rw-r--  2.0 unx     9736 b- defN 23-May-22 21:37 datarobotx/models/selfdiscovery.py
+-rw-rw-r--  2.0 unx     4422 b- defN 23-May-22 21:37 datarobotx/models/share.py
+-rw-rw-r--  2.0 unx    20800 b- defN 23-May-22 21:37 datarobotx/models/sparkingest.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-May-22 21:38 datarobotx/viz/__init__.py
+-rw-rw-r--  2.0 unx     4906 b- defN 23-May-22 21:37 datarobotx/viz/charts.py
+-rw-rw-r--  2.0 unx     6326 b- defN 23-May-22 21:37 datarobotx/viz/leaderboard.py
+-rw-rw-r--  2.0 unx     9407 b- defN 23-May-22 21:37 datarobotx/viz/modelcard.py
+-rw-rw-r--  2.0 unx     9508 b- defN 23-May-22 21:37 datarobotx/viz/viz.py
+-rw-rw-r--  2.0 unx      926 b- defN 23-May-22 21:37 datarobotx/viz/templates/drx_button.html
+-rw-rw-r--  2.0 unx     1490 b- defN 23-May-22 21:37 datarobotx/viz/templates/leaderboard.html
+-rw-rw-r--  2.0 unx      414 b- defN 23-May-22 21:37 datarobotx/viz/templates/leaderboard.md
+-rw-rw-r--  2.0 unx     5433 b- defN 23-May-22 21:37 datarobotx/viz/templates/model_card.html
+-rw-rw-r--  2.0 unx       48 b- defN 23-May-22 21:37 datarobotx/viz/templates/model_card.md
+-rw-rw-r--  2.0 unx     3140 b- defN 23-May-22 21:37 datarobotx/viz/templates/robot.svg
+-rw-rw-r--  2.0 unx     3135 b- defN 23-May-22 21:37 datarobotx/viz/templates/robot_large.svg
+-rw-rw-r--  2.0 unx     5478 b- defN 23-May-22 21:39 datarobotx-0.1.6.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-22 21:39 datarobotx-0.1.6.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-May-22 21:39 datarobotx-0.1.6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4836 b- defN 23-May-22 21:39 datarobotx-0.1.6.dist-info/RECORD
+56 files, 544005 bytes uncompressed, 134326 bytes compressed:  75.3%
```

## zipnote {}

```diff
@@ -150,20 +150,20 @@
 
 Filename: datarobotx/viz/templates/robot.svg
 Comment: 
 
 Filename: datarobotx/viz/templates/robot_large.svg
 Comment: 
 
-Filename: datarobotx-0.1.5.dist-info/METADATA
+Filename: datarobotx-0.1.6.dist-info/METADATA
 Comment: 
 
-Filename: datarobotx-0.1.5.dist-info/WHEEL
+Filename: datarobotx-0.1.6.dist-info/WHEEL
 Comment: 
 
-Filename: datarobotx-0.1.5.dist-info/top_level.txt
+Filename: datarobotx-0.1.6.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobotx-0.1.5.dist-info/RECORD
+Filename: datarobotx-0.1.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## datarobotx/_version.py

```diff
@@ -6,8 +6,8 @@
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 #
-__version__ = "0.1.5"
+__version__ = "0.1.6"
```

## datarobotx/client/deployments.py

```diff
@@ -84,14 +84,25 @@
     """Create a custom execution environment.
     Returns the id of the new environment"""
     url = "/executionEnvironments/"
     async with session.post(url, json=json) as resp:
         if resp.status == 201:
             json = await resp.json()
             return cast(str, json["id"])
+        await raise_value_error(resp)
+
+
+async def get_custom_env(environment_id: str) -> Dict[str, Any]:
+    """Returns the details for a custom environment"""
+    url = f"/executionEnvironments/{environment_id}/"
+    async with session.get(url) as resp:
+        if resp.status != 200:
+            await raise_value_error(resp)
+        json = await resp.json()
+        return cast(Dict[str, Any], json)
 
 
 async def create_custom_env_version(  # type: ignore[return]
     env_id: str, env_payload: Tuple[str, pd.DataFrame]
 ) -> Optional[str]:
     """Creates a custom environment version, uploading the environment archive.
     Returns the id of the auto-generated environment version."""
@@ -99,26 +110,32 @@
     url = f"/executionEnvironments/{env_id}/versions/"
     form_data = aiohttp.FormData()
     form_data.add_field("docker_context", data, filename=file_name)
     async with session.post(url, data=form_data) as resp:
         if resp.status == 202:
             url = resp.headers["Location"]
             return url.split("/")[-2]
+        await raise_value_error(resp)
 
 
 async def await_env_build(env_id: str, env_version_id: str) -> Optional[str]:
     """Wait for the build of the custom environment to complete."""
 
     async def _await_env_build(url: str) -> Optional[str]:  # type: ignore[return]
         resp = await session.get(url, allow_redirects=False)
         if resp.status == 200:
             json = await resp.json()
             status = json["buildStatus"]
-            if status in {"failed", "success"}:
+            if status == "success":
                 return cast(str, status)
+            elif status == "failed":
+                msg = f"Build of custom deployment environment '{env_id}' failed"
+                raise ValueError(msg)
+        else:
+            await raise_value_error(resp)
 
     url = f"/executionEnvironments/{env_id}/versions/{env_version_id}/"
     coro_args = [url]
     status = await poll(_await_env_build, coro_args=coro_args)
     return cast(Optional[str], status)
 
 
@@ -128,14 +145,15 @@
     Returns the id of the new custom model
     """
     url = "/customModels/"
     async with session.post(url, json=json) as resp:
         if resp.status == 201:
             json = await resp.json()
             return cast(str, json["id"])
+        await raise_value_error(resp)
 
 
 async def create_custom_model_version(  # type: ignore[return]
     env_id: str, model_id: str, model_payload: List[Tuple[str, pd.DataFrame]]
 ) -> Optional[str]:
     """Create a custom  model version, uploading custom model files.
     Returns the id of the new custom model version"""
@@ -147,14 +165,15 @@
     for file_name, _ in model_payload:
         form_data.add_field("file", sender.reader(file_name), filename=file_name)
         form_data.add_field("filePath", file_name)
     async with session.post(url, data=form_data, timeout=None) as resp:
         if resp.status == 201:
             json = await resp.json()
             return cast(str, json["id"])
+        await raise_value_error(resp)
 
 
 async def create_custom_model_package(  # type: ignore[return]
     model_id: str, model_version_id: str, env_id: str, env_version_id: str
 ) -> Optional[str]:
     """Create a model package from a custom model.
     Returns a tuple (id, name)
@@ -186,14 +205,15 @@
         "name": (model_name + f" ({model_version_label}) | {env_name} ({env_version_label})"),
         "customModelVersionId": model_version_id,
     }
     async with session.post(model_package_url, json=json) as resp:
         if resp.status == 201:
             json = await resp.json()
             return cast(str, json["id"])
+        await raise_value_error(resp)
 
 
 async def deploy_from_package(package_id: str, model_id: str) -> str:
     """Deploy a model package built from a custom model."""
     model_url = f"/customModels/{model_id}/"
     deploy_url = "/deployments/fromModelPackage/"
     resp = await session.get(model_url, allow_redirects=False)
```

## datarobotx/common/config.py

```diff
@@ -10,25 +10,25 @@
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 from __future__ import annotations
 
 import contextvars
 import os
 import pathlib
-from typing import Any, cast, Dict, Optional
+from typing import Any, cast, Dict, Optional, Union
 
 import yaml
 
 _config_initialized = contextvars.ContextVar("config_initialized", default=False)
 _token = contextvars.ContextVar("token", default="")
 _endpoint = contextvars.ContextVar("endpoint", default="")
 _pred_server_id = contextvars.ContextVar("pred_server_id", default="")
 _rest_poll_interval = contextvars.ContextVar("rest_poll_interval", default=3)
 _max_wait = contextvars.ContextVar("max_wait", default=24 * 60 * 60)
-
+_upload_file_type = contextvars.ContextVar("upload_file_type", default="csv")
 _theme = contextvars.ContextVar("theme", default="dark")
 
 _concurrency_poll_interval = contextvars.ContextVar("concurrency_poll_interval", default=0.2)
 
 _max_dr_ingest = contextvars.ContextVar("max_dr_ingest", default=5 * (10**9))
 
 # Context variable for the public entry point (function name) into threaded drx work
@@ -53,21 +53,23 @@
 
 class ConfigReader:
     """Read configuration parameters from DataRobot YAML and environment variables
     with the latter taking precedence over the former.
 
     Parameters
     ----------
-    config_path : pathlib.Path
+    config_path : Union[pathlib.Path, str]
         Path to the yaml config file
     env : bool=True
         Whether to read environment variables
     """
 
-    def __init__(self, config_path: Optional[pathlib.Path] = None, env: bool = True):
+    def __init__(self, config_path: Optional[Union[pathlib.Path, str]] = None, env: bool = True):
+        if isinstance(config_path, str):
+            config_path = pathlib.Path(config_path)
         self.config_path = config_path
         self.env = env
 
     def load_yaml(self) -> Dict[str, Any]:
         """Read DR config yaml if it exist."""
         try:
             with self.config_path.open(mode="rb") as f:  # type: ignore[union-attr]
@@ -127,14 +129,15 @@
     >>> drx.Context()
     {'token': 'foo', 'endpoint': 'bar', 'pred_server_id': 'foo_bar'}
     >>> drx.Context(config_path='my_path/my_config.yaml')  # update from file
     {'token': 'my_config_token', 'endpoint': 'my_config_endpoint', 'pred_server_id': 'my_config_pred_server_id'}
     """
 
     def __init__(self, **kwargs) -> None:  # type: ignore[no-untyped-def]
+        getattr(self, "", None)  # trigger just-in-time defaults initialization (if needed)
         # Read from kwargs to allow user to explicitly reset params to None
         if "token" in kwargs:
             self.token = kwargs["token"]
         if "endpoint" in kwargs:
             self.endpoint = kwargs["endpoint"]
         if "pred_server_id" in kwargs:
             self.pred_server_id = kwargs["pred_server_id"]
@@ -146,21 +149,21 @@
 
         Notes
         -----
         Streamlit loses contextvars across multiple heads so
         initialization must be deferred to attribute access time
         """
         if not _config_initialized.get():
+            _config_initialized.set(True)
             Context.__init__(
                 self,
                 **ConfigReader(
                     pathlib.Path("~/.config/datarobot/drconfig.yaml").expanduser().resolve()
                 ).read(),
             )
-            _config_initialized.set(True)
         return super().__getattribute__(name)
 
     def _init_from_file(self, config_path: pathlib.Path) -> None:
         """Update context from config file, env variables"""
         config = ConfigReader(config_path, env=False).read()
         self.token = config.get("token", self.token)
         self.endpoint = config.get("endpoint", self.endpoint)
@@ -236,26 +239,39 @@
         """Maximum DR upload size in bytes"""
         return _max_dr_ingest.get()
 
     @_max_dr_ingest.setter
     def _max_dr_ingest(self, value: int) -> None:
         _max_dr_ingest.set(value)
 
-    def __repr__(self) -> str:
-        d = {
-            "token": self.token,
-            "endpoint": self.endpoint,
-            "pred_server_id": self.pred_server_id,
-        }
-        return d.__repr__()
+    @property
+    def _upload_file_type(self) -> str:
+        """
+        Default file type for drx upload
+
+        Valid types are 'csv' or 'parquet'
+        """
+        return _upload_file_type.get()
+
+    @_upload_file_type.setter
+    def _upload_file_type(self, value: str) -> None:
+        _upload_file_type.set(value)
 
     @property
     def theme(self) -> str:
         """Whether charts render in dark or light"""
         return _theme.get()
 
     @theme.setter
     def theme(self, value: str) -> None:
         _theme.set(value)
 
+    def __repr__(self) -> str:
+        d = {
+            "token": self.token,
+            "endpoint": self.endpoint,
+            "pred_server_id": self.pred_server_id,
+        }
+        return d.__repr__()
+
 
 context = Context()
```

## datarobotx/common/utils.py

```diff
@@ -219,43 +219,51 @@
             result.append(item)
         return result
 
     setattr(cls, "__dir__", _dir)
     return cls
 
 
-PayloadType = Tuple[str, Union[bytes, io.BytesIO]]
+PayloadType = Tuple[str, Union[bytes, io.BytesIO, io.BufferedReader]]
 
 
 class FilesSender:
     """Async generator used to stream in-memory files to be uploaded and report progress."""
 
     def __init__(self, payload: Union[PayloadType, List[PayloadType]]) -> None:
         self.size: int = 0
         self.payload_dict: Dict[str, Any] = {}
+        self.chunk_size: int = 2**16
         if not isinstance(payload, list):
             payload = [payload]
         for file_name, data in payload:
+            self.payload_dict[file_name] = data
             if isinstance(data, bytes):
                 self.size += len(data)
                 self.payload_dict[file_name] = io.BytesIO(data)
+            elif isinstance(data, io.BufferedReader):
+                pos = data.tell()
+                data.seek(0, io.SEEK_END)
+                self.size += data.tell()
+                data.seek(pos)
+                self.chunk_size = io.DEFAULT_BUFFER_SIZE
             elif isinstance(data, io.BytesIO):
                 self.size += data.getbuffer().nbytes
-                self.payload_dict[file_name] = data
-        self.chunk_size: int = 2**16
+
         self.pbar = tqdm(total=self.size, unit="B", unit_scale=True, unit_divisor=1000)
         self.completed: Dict[str, bool] = {file_name: False for file_name, _ in payload}
 
     async def reader(self, file_name: str) -> AsyncGenerator[bytes, str]:
         buffer = self.payload_dict[file_name]
         chunk = buffer.read(self.chunk_size)
         while chunk:
             self.pbar.update(len(chunk))
             yield chunk
             chunk = buffer.read(self.chunk_size)
+        buffer.close()
         self.completed[file_name] = True
         if all(value is True for value in self.completed.values()):
             self.pbar.close()
 
 
 class SparkSender:
     """Async generators to upload rows from a spark dataframe
@@ -391,23 +399,28 @@
     zip_buffer.seek(0)
     return zip_buffer
 
 
 def prepare_df_upload(df: pd.DataFrame, filename: Optional[str] = None) -> Tuple[str, io.BytesIO]:
     """
     Prepare a pandas dataframe for upload to DataRobot.
+    Filetype can be one of 'csv' or 'parquet'
     Returns a tuple (file_name, data: io.BytesIO)
     """
-    if filename is None:
-        filename = "autogenerated_by_drx_{:%Y-%m-%d_%Hh%Mm%Ss}.csv".format(datetime.datetime.now())
 
+    if filename is None:
+        generate_date = datetime.datetime.now().strftime("%Y-%m-%d_%Hh%Mm%Ss")
+        filename = f"autogenerated_by_drx_{generate_date}.{context._upload_file_type}"
     data = io.BytesIO()
-    StreamWriter = codecs.getwriter("utf-8")
-    handle = StreamWriter(data)
-    df.to_csv(handle, encoding="utf-8", index=False, quoting=csv.QUOTE_ALL)
+    if context._upload_file_type == "parquet":
+        df.to_parquet(data, index=False, engine="pyarrow")
+    else:
+        StreamWriter = codecs.getwriter("utf-8")
+        handle = StreamWriter(data)
+        df.to_csv(handle, encoding="utf-8", index=False, quoting=csv.QUOTE_ALL)
     data.seek(0)
     return filename, data
 
 
 class DrxNull:
     """Null sentinel"""
```

## datarobotx/llm/chains/data_dict.py

```diff
@@ -106,35 +106,32 @@
     Parameters
     ----------
     as_json : bool, default = False
         Whether chain output should be returned as a natural language or json
         str
     def_feature_chain : LLMChain, optional
         Chain to be used for defining individual features. If not provided, will be
-        initialized with a DefineFeatureChain
+        initialized with a default chain that prompts and retrieves individual
+        definitions
     verbose : bool, default = False
-        Whether the chain should be run in verbose mode; only applies if
-        def_feature_chain is not specified
+        Whether the chain should be run in verbose mode; only applies if the
+        default feature definition chain is being used
 
-    Notes
-    -----
-    Chain inputs:
-
-    context : str
-        Context of the problem / use case in which a feature definition is being requested
-    features : str
-        The feature(s) for which a definition is being requested (comma separated)
-    project_id : str, optional
-        DataRobot project_id; if provided, EDA data will be retrieved from DR if available
-        and will be used to attempt to improve data dictionary completions
-
-    Chain output:
-
-    data_dict : str
-        Natural language or json string representation of data dictionary
+    Examples
+    --------
+    >>> import json
+    >>> import langchain
+    >>> import os
+    >>> from datarobotx.llm import DataDictChain
+    >>> use_case_context = "Predicting hospital readmissions"
+    >>> dr_project_id = "XXX"
+    >>> os.environ["OPENAI_API_KEY"] = "XXX"
+    >>> llm = langchain.llms.OpenAI(model_name="text-davinci-003")
+    >>> chain = DataDictChain(llm=llm)
+    >>> outputs = chain(inputs=dict(project_id=dr_project_id, context=use_case_context))
     """
 
     llm: BaseLLM
     def_feature_chain: LLMChain
     as_json: bool = False
     verbose: bool = False
 
@@ -147,18 +144,34 @@
             values["def_feature_chain"] = DefineFeatureChain(
                 llm=values["llm"], verbose=values.get("verbose", False)
             )
         return values
 
     @property
     def input_keys(self) -> List[str]:
+        """Chain inputs
+
+        context : str
+            Context of the problem / use case in which a feature definition is being requested
+        features : str
+            The feature(s) for which a definition is being requested (comma separated)
+        project_id : str, optional
+            DataRobot project_id; if provided, EDA data will be retrieved from DR if available
+            and will be used to attempt to improve data dictionary completions
+        """
         return ["context"]
 
     @property
     def output_keys(self) -> List[str]:
+        """Chain outputs
+
+        data_dict : str
+            Natural language or json string representation of data dictionary depending
+            on how the chain was initialized with parameter 'as_json'
+        """
         return ["data_dict"]
 
     @staticmethod
     def _parse_features(inputs: Dict[str, Any]) -> List[str]:
         """Parse feature names from the data dictionary request"""
         features = [feature.strip() for feature in inputs.get("features", "").split(",")]
         if len(features) == 1 and features[0] == "":
```

## datarobotx/models/deploy.py

```diff
@@ -10,41 +10,42 @@
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 from abc import ABC, abstractmethod
 import datetime
 import io
 import logging
 import os
+import pathlib
 import pickle
-import subprocess
 import sys
-import tempfile
 import textwrap
-from typing import Any, Callable, cast, Dict, Iterable, List, Optional, Tuple, Union
+from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union
 
 import pkg_resources  # type: ignore[import]
 
 import datarobotx.client.deployments as deploy_client
 from datarobotx.common import utils
 from datarobotx.common.config import context
+from datarobotx.common.utils import PayloadType
 from datarobotx.models.deployment import Deployment
 from datarobotx.models.model import Model
 
 logger = logging.getLogger("drx")
 
 
 def deploy(
-    model: Any,
-    *args: Any,
+    model: Optional[Any],
+    *args: Tuple[Any],
     target: Optional[str] = None,
     classes: Optional[List[str]] = None,
     name: Optional[str] = None,
     description: Optional[str] = None,
     hooks: Optional[Dict[str, Callable]] = None,  # type: ignore[type-arg]
     extra_requirements: Optional[List[str]] = None,
+    environment_id: Optional[str] = None,
     **kwargs: Any,
 ) -> Deployment:
     """
     Deploy a model to MLOps
 
     If model object is a non-DR model, will build and configure an appropriate
     supporting custom environment and custom inference model within DataRobot as
@@ -70,15 +71,23 @@
         Short description for the ML Ops deployment
     extra_requirements: list of str, optional
         For custom model deployments: additional python pypi package names to include
         in the custom environment. Default behavior is to include standard dependencies
         for the model type
     hooks: dict of callable, optional
         For custom model deployments: additional hooks to include with the deployment;
-        see the DataRobot User Models documentation for details on supported hooks
+        see the DataRobot User Models documentation for details on supported hooks.
+        Make sure any import statements each hook depends on have executed prior to
+        calling deploy() or are within the hook itself; add optional dependencies
+        with the extra_requirements argument.
+    environment_id: str, optional
+        Custom environment id to be used for this deployment. If provided,
+        an existing environment will be used for this deployment instead of
+        automatically detecting requirements and creating a new one. Uses the latest
+        environment version associated with the environment id.
     **kwargs
         Additional keyword arguments that may be model specific
 
     Returns
     -------
     deployment : Deployment
         Resulting ML Ops deployment; returned immediately and automatically updated
@@ -114,57 +123,61 @@
     ...                       hooks={'read_input_data': force_schema})
 
     """
 
     if isinstance(model, Model):
         return model.deploy()
     else:
+        logger.info("Deploying custom model", extra={"is_header": True})
         deployment = Deployment()
         deployer = CustomDeployer(
             deployment,
             model,
             *args,
             target=target,
             classes=classes,
             name=name,
             description=description,
             hooks=hooks,
             extra_requirements=extra_requirements,
+            environment_id=environment_id,
             **kwargs,
         )
         utils.create_task_new_thread(deployer.deploy())
         return deployment
 
 
 class CustomDeployer(ABC):
     def __init__(
         self,
         destination: Deployment,
-        model: Any,
-        *args: Any,
+        model: Optional[Any],
+        *args: Tuple[Any],
         target: Optional[str] = None,
         classes: Optional[List[str]] = None,
         name: Optional[str] = None,
         description: Optional[str] = None,
         hooks: Optional[Dict[str, Callable]] = None,  # type: ignore[type-arg]
         extra_requirements: Optional[List[str]] = None,
+        environment_id: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
         # Reference to resulting deployment object
         self.destination = destination
 
         self.model = model
         self.extra_models = args
 
         # Environment config
         self.dockerfile = _DOCKERFILE_TEMPLATE
         self.requirements_txt = ""
         self.dr_requirements_txt = _DRUM_REQ
         self.extra_requirements = extra_requirements
         self.start_server_sh = _START_SERVER_SH_TEMPLATE
+        self.environment_id = environment_id
 
         # Custom inference model spec config
         self.name = name or utils.generate_name()
         self.description = description or "Auto-deployed by drx on {:%Y-%m-%d %H:%M:%S}".format(
             datetime.datetime.now()
         )
         self.model_spec_json: Dict[str, Any] = {}
@@ -179,18 +192,18 @@
         self.env_spec_json: Dict[str, Any] = {}
         self.env_payload: Tuple[str, Union[bytes, io.BytesIO]] = None  # type: ignore[assignment]
         self.model_payload: List[Tuple[str, Any]] = []
 
         self.model_exporter = ModelExporter(model, *args, target=target, classes=classes, **kwargs)
 
     async def deploy(self) -> None:
-        logger.info("Deploying custom model", extra={"is_header": True})
         logger.info("Preparing model and environment...")
 
-        self.prepare_env()
+        if self.environment_id is None:
+            self.prepare_env()
         self.prepare_json_config()
         self.prepare_custom_hooks()
         await self.prepare_payloads()
 
         self.destination._deployment_id = await self.transmit()
         logger.info("Custom model deployment complete", extra={"is_header": True})
 
@@ -243,46 +256,47 @@
     @staticmethod
     def get_python_version() -> str:
         """Return running python interpreter version as a string e.g. '3.7'."""
         version = sys.version_info
         major = version.major
         minor = version.minor
         micro = version.micro
-        if major != 3 or minor > 9 or minor < 4:
-            raise TypeError("deploy() supports python >=3.4, <3.10")
+        if major != 3 or minor > 10 or minor < 4:
+            # limited by datarobot DRUM
+            raise TypeError("drx.deploy() and datarobot-drum supports python >=3.4, <3.11")
         return f"{major}.{minor}.{micro}"
 
     async def prepare_payloads(self) -> None:
         """
         Build a zip archive for a custom environment to be uploaded
         to DataRobot and also export the model itself.
 
         Updates dpeloyer payload attributes ready for REST API:
         env_payload, model_payload
         where
         env_payload = (env_filename, env_archive_data)
         model_payload = [(model_file1_name, file1_data),
                          (model_file2_name, file2_data), ...]
         """
-        env_contents = [
-            ("Dockerfile", self.dockerfile.encode()),
-            ("requirements.txt", self.requirements_txt.encode()),
-            ("dr_requirements.txt", self.dr_requirements_txt.encode()),
-            ("start_server.sh", self.start_server_sh.encode()),
-            ("__init__.py", "".encode()),
-        ]
-        env_filename = "drx_autogenerated_env_{:%Y-%m-%d_%Hh%Mm%Ss}.zip".format(
-            datetime.datetime.now()
-        )
-        env_data = utils.archive(env_contents)
+        if self.environment_id is None:
+            env_contents = [
+                ("Dockerfile", self.dockerfile.encode()),
+                ("requirements.txt", self.requirements_txt.encode()),
+                ("dr_requirements.txt", self.dr_requirements_txt.encode()),
+                ("start_server.sh", self.start_server_sh.encode()),
+                ("__init__.py", "".encode()),
+            ]
+            env_filename = "drx_autogenerated_env_{:%Y-%m-%d_%Hh%Mm%Ss}.zip".format(
+                datetime.datetime.now()
+            )
+            env_data = utils.archive(env_contents)
+            self.env_payload = (env_filename, env_data)
 
         raw_model_payload = await self.model_exporter.export_model(self.name)
         hooks_payload = await self.export_hooks()
-
-        self.env_payload = (env_filename, env_data)
         self.model_payload = raw_model_payload + hooks_payload
 
     def prepare_json_config(self) -> None:
         """
         Prepare the json config parameters the DataRobot
         custom inference env and model endpoints expect.
         """
@@ -315,27 +329,35 @@
         hooks_payload = [("custom.py", self.custom_py.encode())]
         for hook in self.hooks:
             hooks_payload.append((hook + ".pickle", cloudpickle.dumps(self.hooks[hook])))
         return hooks_payload
 
     async def transmit(self) -> str:
         """Transmit the custom model and environment to DataRobot and deploy."""
-        env_id = cast(str, await deploy_client.create_custom_env(self.env_spec_json))
-        env_version_id = cast(
-            str, await deploy_client.create_custom_env_version(env_id, self.env_payload)
-        )
-        self._log_created_environment(
-            environment_id=env_id,
-            environment_name=self.env_spec_json["name"],
-            python_version=self.get_python_version(),
-            requirements=self.requirements_txt,
-            dr_requirements=self.dr_requirements_txt,
-        )
-        logger.info("Awaiting custom environment build...")
-        await deploy_client.await_env_build(env_id, env_version_id)
+        if self.environment_id is None:
+            env_id = cast(str, await deploy_client.create_custom_env(self.env_spec_json))
+            env_version_id = cast(
+                str, await deploy_client.create_custom_env_version(env_id, self.env_payload)
+            )
+            self._log_created_environment(
+                environment_id=env_id,
+                environment_name=self.env_spec_json["name"],
+                python_version=self.get_python_version(),
+                requirements=self.requirements_txt,
+                dr_requirements=self.dr_requirements_txt,
+            )
+            logger.info("Awaiting custom environment build...")
+            await deploy_client.await_env_build(env_id, env_version_id)
+        else:
+            env_id = self.environment_id
+            env_json = await deploy_client.get_custom_env(env_id)
+            env_version_id = env_json["latestVersion"]["id"]
+            url = context._webui_base_url + f"/model-registry/custom-environments/{env_id}"
+            msg = f"Using environment [{env_json['name']} {env_json['latestVersion']['label']}]({url}) for deployment"
+            logger.info(msg)
 
         logger.info("Configuring and uploading custom model...")
         model_id = cast(str, await deploy_client.create_custom_model(self.model_spec_json))
         model_version_id = cast(
             str,
             await deploy_client.create_custom_model_version(env_id, model_id, self.model_payload),
         )
@@ -389,35 +411,34 @@
 
 class ModelExporter:
     """Implements export (serialization) and requirement determination for arbitrary
     models."""
 
     def __init__(
         self,
-        *models: Iterable[Any],
+        model: Optional[Any],
+        *extra_models: Tuple[Any],
         target: Optional[str] = None,
         classes: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> None:
         self.target = target
         self.classes = classes
-        self.exporter: Union[SklearnExporter, TransformersOnnxExporter] = None  # type: ignore[assignment]
+        self.exporter: BaseExporter
         import sklearn
-        from transformers import PreTrainedModel, PreTrainedTokenizerBase, TFPreTrainedModel
 
-        model = models[0]
         if isinstance(model, sklearn.base.BaseEstimator):
             self.exporter = SklearnExporter(model)
-        elif isinstance(model, (TFPreTrainedModel, PreTrainedModel, PreTrainedTokenizerBase)):
-            if "feature" not in kwargs:
-                raise TypeError("Huggingface ONNX support requires the keyword argument 'feature'")
-            self.exporter = TransformersOnnxExporter(*models, feature=kwargs["feature"])
         else:
-            model_type_str = type(model).__name__
-            raise TypeError(f"deploy() does not support models of type {model_type_str}")
+            msg = (
+                "Unable to auto-detect model type; any provided paths and files will be exported - "
+                + "dependencies should be explicitly specified using extra_requirements"
+            )
+            logger.info(msg)
+            self.exporter = DefaultUnstructuredExporter(model, *extra_models)
 
     def get_dr_config(self) -> Dict[str, Union[int, str]]:
         """Compute model-specific json config DataRobot expects for this combination
         of model objs + target + classes.
 
         Returns dict of model-specific config."""
         model_config = self.exporter.dr_model_spec
@@ -447,130 +468,99 @@
             return "Anomaly"
 
     def get_requirements(self) -> List[str]:
         """Return requirements for this model in pip requirements.txt format
         as a str object."""
         return self.exporter.requirements
 
-    async def export_model(self, model_name: str) -> List[Tuple[str, Union[bytes, io.BytesIO]]]:
+    async def export_model(self, model_name: str) -> List[PayloadType]:
         """Serialize the model(s) to bytes object(s) that
         can later be uploaded over HTTP.
 
         Returns a list of tuples [(file_name, data), ...]"""
         return await self.exporter.export_model(model_name)
 
 
 class BaseExporter(ABC):
     """Abstract base class for a model exporter."""
 
-    def __init__(self, model: Model, *extra_models: List[Model]):
+    def __init__(self, model: Any, *extra_models: Tuple[Any]):
         self.model = model
         self.extra_models = extra_models
 
     @property
     def dr_model_spec(self) -> Dict[str, Any]:
         """Returns dictionary of any additional model-specific DataRobot config
         that will be passed to DR upon custom model creation."""
         return {}
 
     @abstractmethod
-    async def export_model(self, model_name: str) -> List[Tuple[str, Union[bytes, io.BytesIO]]]:
+    async def export_model(self, model_name: str) -> List[PayloadType]:
         """Serialize the model and prepare for upload over HTTP
 
         Returns [(file_name1, data1), (file_name2, data2), ...]
         file_name is a string
         data1 is of type io.BytesIO or bytes
         """
 
     @property
     @abstractmethod
     def requirements(self) -> List[str]:
         pass
 
 
+class DefaultUnstructuredExporter(BaseExporter):
+    """Deploy arbitrary apps conforming to the DR unstructured deployment interface
+
+    In this case the model positional argument if (optionally) provided will be treated
+    as a string or path-like containing artifacts to be included in the custom model
+    version.
+    """
+
+    # requirements inferred by cloudpickle by default (must be imported in hooks)
+    requirements = []
+    dr_model_spec = {"maximumMemory": 4096 * 1024 * 1024, "targetType": "Unstructured"}
+
+    @staticmethod
+    def get_payload(path: pathlib.Path, file: str = "") -> PayloadType:
+        data = open(path / file, "rb")  # pylint: disable=consider-using-with
+        name = str(path / file)
+        return name, data
+
+    async def export_model(self, model_name: str) -> List[PayloadType]:
+        payload = []
+        if self.model is not None:
+            for path in (self.model,) + self.extra_models:
+                path = pathlib.Path(path)
+                if path.is_dir():
+                    for rel_path, _, files in os.walk(path):
+                        for f in files:
+                            payload.append(self.get_payload(pathlib.Path(rel_path), file=f))
+                else:
+                    payload.append(self.get_payload(path))
+
+        return payload
+
+
 class SklearnExporter(BaseExporter):
     # latest deps according to pip as of 6/10/2022
     requirements = [
         "scikit-learn",
         "joblib",
         "numpy",
         "scipy",
         "threadpoolctl",
     ]
 
-    async def export_model(self, model_name: str) -> List[Tuple[str, Union[bytes, io.BytesIO]]]:
+    async def export_model(self, model_name: str) -> List[PayloadType]:
         file_name = f"{model_name}.pkl"
         data = pickle.dumps(self.model)
         return [(file_name, data)]
 
 
-class TransformersOnnxExporter(BaseExporter):
-    requirements = [
-        "transformers",
-        "onnxruntime",
-    ]
-    dr_model_spec = {"maximumMemory": 4096 * 1024 * 1024, "targetType": "Unstructured"}
-
-    def __init__(self, *args: Any, feature: Optional[str] = "default", **kwargs: Any) -> None:
-        super().__init__(*args, **kwargs)
-        self.tokenizer = None
-        self.feature = feature
-
-        from transformers import (
-            FeatureExtractionMixin,
-            PreTrainedModel,
-            PreTrainedTokenizerBase,
-            TFPreTrainedModel,
-        )
-
-        models = (self.model,) + self.extra_models
-        for model in models:
-            if isinstance(model, PreTrainedTokenizerBase):
-                self.tokenizer = model
-            elif isinstance(model, (TFPreTrainedModel, PreTrainedModel)):
-                self.model = model
-            elif isinstance(model, FeatureExtractionMixin):
-                raise NotImplementedError(
-                    "Support for FeatuereExtractionMixin not implemented yet!"
-                )
-
-    async def export_model(self, model_name: str) -> List[Tuple[str, Union[bytes, io.BytesIO]]]:
-        py_executable = sys.executable
-        with tempfile.TemporaryDirectory() as path:
-            self.tokenizer.save_pretrained(path)  # type: ignore[union-attr]
-            self.model.save_pretrained(path)  # type: ignore[attr-defined]
-            command = [
-                f"{py_executable}",
-                "-m",
-                "transformers.onnx",
-                "--model",
-                f"{path}",
-                "--feature",
-                f"{self.feature}",
-                f"{path}",
-            ]
-            logger.info("Performing transformers.onnx export...")
-            subprocess.run(command, stderr=subprocess.STDOUT, check=True)
-            with open(f"{path}/model.onnx", "rb") as f:
-                model_data = io.BytesIO(f.read())
-                model_file_name = "model.onnx"
-        model_payload = [(model_file_name, model_data)]
-
-        tokenizer_payload = []
-        with tempfile.TemporaryDirectory() as path:
-            self.tokenizer.save_pretrained(path)  # type: ignore[union-attr]
-            for file in os.listdir(path):
-                if not file.startswith("."):
-                    with open(path + "/" + file, "rb") as f:
-                        data = io.BytesIO(f.read())
-                        tokenizer_payload.append((file, data))
-
-        return model_payload + tokenizer_payload  # type: ignore[return-value]
-
-
 _HOOK_TEMPLATES = {
     "init": """# custom.py autogenerated by datarobotx {timestamp:%Y-%m-%d %H:%M:%S}
 
 _HOOK_IMPLS = {{}}
 
 def init(code_dir, **kwargs):
     import pickle
@@ -605,15 +595,15 @@
 """,
     "post_process": """
 def post_process(predictions, model):
     return _HOOK_IMPLS['post_process'](predictions, model)
 """,
 }
 
-_DRUM_REQ = "datarobot-drum>=1.9.3\n"
+_DRUM_REQ = "datarobot-drum==1.10.3\ndatarobot-mlops==8.2.7\n"
 _CLOUDPICKLE_REQ = "cloudpickle>=2.0.0\n"
 
 _START_SERVER_SH_TEMPLATE = """#!/bin/sh
 echo "Starting Custom Model environment with DRUM prediction server"
 echo "Environment variables:"
 env
 echo
@@ -624,15 +614,15 @@
 exec ${CMD}
 """
 
 # Inspired by git repo: datarobot-user-models/public_dropin_environments
 _DOCKERFILE_TEMPLATE = """
 # This is the default base image for use with user models and workflows.
 # It contains a variety of common useful data-science packages and tools.
-FROM datarobot/dropin-env-base:latest
+FROM datarobot/dropin-env-base:debian11-py3.9-jre11.0.16-drum1.9.10-mlops8.2.7
 
 # Install pyenv and the desired version of python
 
 WORKDIR /opt/
 RUN apt-get update
 RUN apt-get install -y git curl wget make build-essential gdb lcov pkg-config \
       libbz2-dev libffi-dev libgdbm-dev libgdbm-compat-dev liblzma-dev \
```

## Comparing `datarobotx-0.1.5.dist-info/METADATA` & `datarobotx-0.1.6.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobotx
-Version: 0.1.5
+Version: 0.1.6
 Summary: DataRobotX is a collection of DataRobot extensions
 Home-page: https://datarobot.github.io/drx
 Author: DataRobot
 Author-email: datarobotx@datarobot.com
 License: DataRobot Tool and Utility Agreement
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
@@ -17,30 +17,31 @@
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Requires-Dist: aiohttp
-Requires-Dist: altair
+Requires-Dist: altair (<5.0.0)
 Requires-Dist: datarobot
 Requires-Dist: IPython
 Requires-Dist: ipywidgets
 Requires-Dist: names-generator
 Requires-Dist: pandas
 Requires-Dist: PyYaml
 Requires-Dist: setuptools
 Requires-Dist: tenacity
 Requires-Dist: termcolor
 Requires-Dist: tqdm
 Requires-Dist: urllib3 (<2.0.0)
 Provides-Extra: deploy
 Requires-Dist: cloudpickle ; extra == 'deploy'
-Requires-Dist: transformers ; extra == 'deploy'
 Requires-Dist: scikit-learn ; extra == 'deploy'
+Requires-Dist: transformers ; extra == 'deploy'
+Requires-Dist: torch ; extra == 'deploy'
 Provides-Extra: dev
 Requires-Dist: flake8 (==5.0.4) ; extra == 'dev'
 Requires-Dist: pylint (==2.15.0) ; extra == 'dev'
 Requires-Dist: black (==22.8.0) ; extra == 'dev'
 Requires-Dist: isort (==5.10.1) ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
 Requires-Dist: pytest-sphinx ; extra == 'dev'
@@ -64,20 +65,22 @@
 Requires-Dist: ipywidgets (==7.7.2) ; extra == 'dev'
 Requires-Dist: names-generator ; extra == 'dev'
 Requires-Dist: pandas ; extra == 'dev'
 Requires-Dist: PyYaml ; extra == 'dev'
 Requires-Dist: tenacity ; extra == 'dev'
 Requires-Dist: termcolor ; extra == 'dev'
 Requires-Dist: tqdm ; extra == 'dev'
-Requires-Dist: altair ; extra == 'dev'
+Requires-Dist: altair (<5.0.0) ; extra == 'dev'
 Requires-Dist: cloudpickle ; extra == 'dev'
 Requires-Dist: transformers ; extra == 'dev'
 Requires-Dist: scikit-learn ; extra == 'dev'
+Requires-Dist: torch ; extra == 'dev'
 Requires-Dist: pyspark ; extra == 'dev'
 Requires-Dist: openai ; extra == 'dev'
+Requires-Dist: pyarrow (==11.0.0) ; extra == 'dev'
 Requires-Dist: mypy (==1.0.0) ; extra == 'dev'
 Requires-Dist: urllib3 (<2.0.0) ; extra == 'dev'
 Requires-Dist: langchain ; (python_version >= "3.8") and extra == 'dev'
 Requires-Dist: pydantic ; (python_version >= "3.8") and extra == 'dev'
 Requires-Dist: tiktoken (==0.3.3) ; (python_version >= "3.8") and extra == 'dev'
 Provides-Extra: llm
 Requires-Dist: langchain ; extra == 'llm'
```

## Comparing `datarobotx-0.1.5.dist-info/RECORD` & `datarobotx-0.1.6.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,39 +1,39 @@
 datarobotx/__init__.py,sha256=znA3kfQXkSRBeNke-g_y-KrY4bBPDplDMl2DH_uBUFg,1566
-datarobotx/_version.py,sha256=Wd2aKbia08x43-oHKyIDRGwj7IeEAS1mmza_eA-pd9A,271
+datarobotx/_version.py,sha256=A6NFca_f375TF1bMygUCsNsbyn0-PgnCFKOfCUT2Xa4,271
 datarobotx/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/client/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/client/datasets.py,sha256=l1SoA0yl2Eqbb3pQNcoa9G_sj9MK_xObxt9pt7wvON8,9402
-datarobotx/client/deployments.py,sha256=ogPqk63Tamgb22aIaHk1IkaARoVjW6h_WrQhcyDbOAs,15368
+datarobotx/client/deployments.py,sha256=KThuu3P5eeeFix8pRItg3NOD_cQlhxGpluBBwsRW2r4,16124
 datarobotx/client/prediction_servers.py,sha256=BS6Z1fT5ev6ldiIOxKZcuELGcT2k5scZ5j8VH7q3ads,1002
 datarobotx/client/projects.py,sha256=3bVdOMN8tTNrIYcyB7_KdlTghjy02fcSEkxPKmNj5W8,25695
 datarobotx/client/status.py,sha256=ExkEDpnBbyQCemp2r4cptUMzwhM5o3F_febWq4qsmKI,2420
 datarobotx/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/common/client.py,sha256=FCfYTdvGTdneJDcFhmDzK4ZBIf1BtmzV5vYYi0z5CkI,7273
-datarobotx/common/config.py,sha256=KZfTORwtIWimYD4yihz8E37WvwQgj2QJnuK5lIpDCFs,8741
+datarobotx/common/config.py,sha256=kVkaVX9JCjxuM_rzWfjgbATOoBvDuzDf9WuMn10zadY,9358
 datarobotx/common/configurator.py,sha256=vdTL4Esk_QYyWNg31EGap7PsBvhL7QizGDVr0FEKxSo,4588
 datarobotx/common/dr_config.py,sha256=RCFE7VEMFxmSFF2Ry2NvONC4g9xh98_QChuubgaq1OI,131191
 datarobotx/common/logging.py,sha256=r9vB5UAT4yGhX9jp98rtHGr3TykFS2yDYiAJvygaF6Q,10494
 datarobotx/common/transformations.py,sha256=ZZpNBmgu1jZvfrNoVtwfLsvSvwGDE5WCTrKkfYosngA,3980
 datarobotx/common/ts_helpers.py,sha256=SF1l_WxaYgt2Z6cKH5ab5WX6ExH3c4BW0aEgrm1qMz8,15032
 datarobotx/common/types.py,sha256=HJqSsXwz5kr7TeVzNc3J5vPsBwCO3pgMCkEartnoG9o,605
-datarobotx/common/utils.py,sha256=93cGtEASW7dM_IAfw1YwKUEsZNzoJDlBQoFoGGdu9CM,26936
+datarobotx/common/utils.py,sha256=U0rZkyksp9xgadBi6stZGFBPa2F93hZFWMV9MkwPVCc,27474
 datarobotx/llm/__init__.py,sha256=3FvuXJ7JCHlVKIxlpw1tk7H7WfIco3Qe7vj6d55sj44,136
 datarobotx/llm/utils.py,sha256=VrHRbXIhfQgIidJ7b1r0zuw0tUbIp8-3yPpXOBoJ2HQ,2119
 datarobotx/llm/chains/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-datarobotx/llm/chains/data_dict.py,sha256=dF-JzOvTjd3BFdyknpiGbdBZXDG0385g7RCKJLJ8PV4,11994
+datarobotx/llm/chains/data_dict.py,sha256=Ee4761z8v5AvcbX7nykMhEHhEW-TnXr56K6BDe8q5G0,12665
 datarobotx/llm/chains/enrich.py,sha256=CEGafCnHM6DJBSQxUKLKWXLOUZWsPYM159lS5VP2aI0,9398
 datarobotx/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/models/autoanomaly.py,sha256=IkhJzilTX1NWiiiEQbVgnd9rWRHnFdeUtdv03kREDXc,2754
 datarobotx/models/autocluster.py,sha256=UBwcx1_oFrlXnae4oNx3poNybmq1bBh_MPnsPbF_46w,3332
 datarobotx/models/automl.py,sha256=K3tTFn84WVdLobzDBDvEnOtyXazrIcZ86y0hN7elSho,2465
 datarobotx/models/autopilot.py,sha256=v2WlNOM55nQuaRtmCaBsnXJqlFtsfg71EAUXKkIo7BI,11349
 datarobotx/models/autots.py,sha256=zf5UmPJ_M1fF_lkiB_oixQ4hWb196F1KofHHm3ym6Fw,11242
 datarobotx/models/colreduce.py,sha256=Y0gXfziUEKOtOTMgGwprl4AZAAxtAvYT3zATuWHBJWg,14976
-datarobotx/models/deploy.py,sha256=RmVGf1VmHonBjThh_BfL2TVpzW_2QDCDQqr1I_27DJs,24227
+datarobotx/models/deploy.py,sha256=_VKbYohhAHHEHIOcZea9I34epE_uRtlN1btVrOQdWdM,24175
 datarobotx/models/deployment.py,sha256=7Dd1Fi8G08lh2ztuMXmZZDRGb9vhnkwotQt-JX5ECCs,33947
 datarobotx/models/evaluation.py,sha256=zoIgR5myd42gK5w3D0pFICBBoskgeVF1moP1FfZ3_pA,7914
 datarobotx/models/featurediscovery.py,sha256=MvqXKQOKZcGNVF8lv5L1oh1JEF9bZNUmCG4vGZ1-_gk,16937
 datarobotx/models/intraproject.py,sha256=3B4d26qjwgq73HCkMLLh2m05Rtxe7mDhX1epmmFsWnM,5593
 datarobotx/models/model.py,sha256=owuGy1RTiRlBG2KA62LRLv6QKDj9t_0LEkaGtVhFc5Q,28420
 datarobotx/models/selfdiscovery.py,sha256=4cJ-ADXcQ1ten0ZNHVp2KvKLZ8Jb8xKIT7Dj50laPgA,9736
 datarobotx/models/share.py,sha256=DHUu-V08sDS0uLEYI35_KXkX6PHbvyJJUdOss38Bsno,4422
@@ -46,11 +46,11 @@
 datarobotx/viz/templates/drx_button.html,sha256=z2wsHdX64v0kbsleMCyo943QnEMGxi0RfHI34hyHMCg,926
 datarobotx/viz/templates/leaderboard.html,sha256=SYSaWRuNBXEzNcm0KE9TUbCo4KDHVB8wABbeQiiz3f8,1490
 datarobotx/viz/templates/leaderboard.md,sha256=QeFvLJVpLSJ1SwHF1cWdj46toXo_1ZD7OznMSG1Bx-k,414
 datarobotx/viz/templates/model_card.html,sha256=H2YKd4gNBGnaEMLCGeywx1DR6O3zvFM_ci9cxfvQ-p0,5433
 datarobotx/viz/templates/model_card.md,sha256=5Bdd7jKLEYY-hpeZ2I99C6sJwMqaeMS5jRbwLcmrCuc,48
 datarobotx/viz/templates/robot.svg,sha256=ZVruQowP1A7ri6frXGiCZnpOnY_KM8Osi66nMJAdyiM,3140
 datarobotx/viz/templates/robot_large.svg,sha256=lAbBH7yv151ngs7Y1jlMw8anqQ-PG77tzTJawLCrc3E,3135
-datarobotx-0.1.5.dist-info/METADATA,sha256=jI3j3lEZzKWF57XtSbgSawVKY0R2bcx8MZjYujUJ-hM,5330
-datarobotx-0.1.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-datarobotx-0.1.5.dist-info/top_level.txt,sha256=_lN2CPexvLnaRX18n5OTMk9KDKnkKNABD7EzuwyfUpI,11
-datarobotx-0.1.5.dist-info/RECORD,,
+datarobotx-0.1.6.dist-info/METADATA,sha256=9GlXV8OgeptV5FVSZWhTSk58vd52vKDRwYpr6RwshgQ,5478
+datarobotx-0.1.6.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+datarobotx-0.1.6.dist-info/top_level.txt,sha256=_lN2CPexvLnaRX18n5OTMk9KDKnkKNABD7EzuwyfUpI,11
+datarobotx-0.1.6.dist-info/RECORD,,
```

