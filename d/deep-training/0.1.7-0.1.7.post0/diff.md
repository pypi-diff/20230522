# Comparing `tmp/deep_training-0.1.7-py3-none-any.whl.zip` & `tmp/deep_training-0.1.7.post0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,14 +1,14 @@
-Zip file size: 346146 bytes, number of entries: 185
+Zip file size: 345967 bytes, number of entries: 185
 -rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      897 b- defN 23-May-19 07:59 deep_training/setup.py
+-rw-rw-rw-  2.0 fat      902 b- defN 23-May-22 06:49 deep_training/setup.py
 -rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
--rw-rw-rw-  2.0 fat    17851 b- defN 23-May-19 07:38 deep_training/data_helper/data_helper.py
--rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-28 06:13 deep_training/data_helper/data_module.py
+-rw-rw-rw-  2.0 fat    18126 b- defN 23-May-22 00:45 deep_training/data_helper/data_helper.py
+-rw-rw-rw-  2.0 fat     3757 b- defN 23-May-22 00:45 deep_training/data_helper/data_module.py
 -rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
 -rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
 -rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
 -rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
 -rw-rw-rw-  2.0 fat      435 b- defN 22-Dec-02 00:22 deep_training/nlp/layers/mask.py
@@ -99,17 +99,17 @@
 -rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
 -rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
 -rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA_parallel/__init__.py
 -rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
 -rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
 -rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/configuration.py
--rw-rw-rw-  2.0 fat    60510 b- defN 23-May-04 00:28 deep_training/nlp/models/chatglm/__init__.py
+-rw-rw-rw-  2.0 fat    60510 b- defN 23-May-22 06:43 deep_training/nlp/models/chatglm/__init__.py
 -rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/configuration.py
--rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-03 00:32 deep_training/nlp/models/chatglm/quantization.py
+-rw-rw-rw-  2.0 fat    15169 b- defN 23-May-22 06:39 deep_training/nlp/models/chatglm/quantization.py
 -rw-rw-rw-  2.0 fat    17037 b- defN 23-May-12 07:25 deep_training/nlp/models/chatglm/tokenization.py
 -rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-13 06:18 deep_training/nlp/models/laMDA/__init__.py
 -rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-13 06:15 deep_training/nlp/models/laMDA/configuration.py
 -rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 07:19 deep_training/nlp/models/lora/__init__.py
 -rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
 -rw-rw-rw-  2.0 fat     7095 b- defN 23-May-19 07:59 deep_training/nlp/models/lora/v1/configuration.py
 -rw-rw-rw-  2.0 fat    13688 b- defN 23-May-04 00:28 deep_training/nlp/models/lora/v1/lora_wrapper.py
@@ -146,15 +146,15 @@
 -rw-rw-rw-  2.0 fat     2499 b- defN 23-May-10 03:18 deep_training/nlp/optimizer/lion/triton.py
 -rw-rw-rw-  2.0 fat       79 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/__init__.py
 -rw-rw-rw-  2.0 fat       88 b- defN 23-May-15 00:35 deep_training/nlp/rl/ilql/__init__.py
 -rw-rw-rw-  2.0 fat     2232 b- defN 23-May-16 05:01 deep_training/nlp/rl/ilql/configuration.py
 -rw-rw-rw-  2.0 fat     3691 b- defN 23-May-15 00:35 deep_training/nlp/rl/ilql/data_define.py
 -rw-rw-rw-  2.0 fat     7800 b- defN 23-May-15 02:51 deep_training/nlp/rl/ilql/ilql_dataset.py
 -rw-rw-rw-  2.0 fat     6244 b- defN 23-May-16 07:17 deep_training/nlp/rl/ilql/ilql_module.py
--rw-rw-rw-  2.0 fat    35897 b- defN 23-May-19 05:44 deep_training/nlp/rl/ilql/ilql_trainer.py
+-rw-rw-rw-  2.0 fat    35917 b- defN 23-May-22 00:45 deep_training/nlp/rl/ilql/ilql_trainer.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/__init__.py
 -rw-rw-rw-  2.0 fat     3460 b- defN 23-May-15 00:35 deep_training/nlp/rl/ppo/configuration.py
 -rw-rw-rw-  2.0 fat     3201 b- defN 23-May-15 00:35 deep_training/nlp/rl/ppo/data_define.py
 -rw-rw-rw-  2.0 fat     3344 b- defN 23-May-15 00:35 deep_training/nlp/rl/ppo/ppo_dataset.py
 -rw-rw-rw-  2.0 fat    10103 b- defN 23-May-15 09:17 deep_training/nlp/rl/ppo/ppo_module.py
 -rw-rw-rw-  2.0 fat    47611 b- defN 23-May-19 04:52 deep_training/nlp/rl/ppo/ppo_trainer.py
 -rw-rw-rw-  2.0 fat       88 b- defN 23-May-15 00:35 deep_training/nlp/rl/rl_base/__init__.py
@@ -176,12 +176,12 @@
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
 -rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
 -rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
 -rw-rw-rw-  2.0 fat    14500 b- defN 23-May-11 00:39 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      602 b- defN 23-May-19 08:02 deep_training-0.1.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-19 08:02 deep_training-0.1.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-19 08:02 deep_training-0.1.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    17940 b- defN 23-May-19 08:02 deep_training-0.1.7.dist-info/RECORD
-185 files, 1246498 bytes uncompressed, 317090 bytes compressed:  74.6%
+-rw-rw-rw-  2.0 fat      608 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    17964 b- defN 23-May-22 06:50 deep_training-0.1.7.post0.dist-info/RECORD
+185 files, 1245563 bytes uncompressed, 316863 bytes compressed:  74.6%
```

## zipnote {}

```diff
@@ -537,20 +537,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.7.dist-info/METADATA
+Filename: deep_training-0.1.7.post0.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.7.dist-info/WHEEL
+Filename: deep_training-0.1.7.post0.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.7.dist-info/top_level.txt
+Filename: deep_training-0.1.7.post0.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.7.dist-info/RECORD
+Filename: deep_training-0.1.7.post0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,15 +1,15 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.7',
+    version='0.1.7post0',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['lightning>=2',
```

## deep_training/data_helper/data_helper.py

```diff
@@ -83,20 +83,22 @@
                     D.append(line)
         return D
 
 
 
 
 class DataHelper(DataPreprocessHelper):
-    def __init__(self,model_args: ModelArguments,
-                training_args: typing.Optional[TrainingArguments],
-                data_args: DataArguments,**kwargs):
+    def __init__(self,
+                 model_args: ModelArguments,
+                 training_args: typing.Optional[TrainingArguments],
+                 data_args: typing.Optional[DataArguments],
+                 **kwargs):
         super(DataHelper, self).__init__()
 
-        self.backend = data_args.data_backend
+
         self.data_process_fn = self.on_data_process
 
         self.train_files = []
         self.eval_files = []
         self.test_files = []
 
         self.tokenizer = None
@@ -109,25 +111,31 @@
         self.max_seq_length_dict = {}
 
 
         self._external_kwargs = kwargs
 
         self.model_args = model_args
         self.training_args = training_args
+
+        self.backend = data_args.data_backend if data_args else 'record'
         self.data_args = data_args
 
-        label2id, id2label = self.on_get_labels(data_args.label_file)
-        self.label2id = label2id
-        self.id2label = id2label
-
-        self.max_seq_length_dict['train'] = data_args.train_max_seq_length
-        self.max_seq_length_dict['eval'] = data_args.eval_max_seq_length
-        self.max_seq_length_dict['val'] = data_args.eval_max_seq_length
-        self.max_seq_length_dict['test'] = data_args.test_max_seq_length
-        self.max_seq_length_dict['predict'] = data_args.test_max_seq_length
+        if data_args:
+            label2id, id2label = self.on_get_labels(data_args.label_file)
+            self.label2id = label2id
+            self.id2label = id2label
+
+            self.max_seq_length_dict['train'] = data_args.train_max_seq_length
+            self.max_seq_length_dict['eval'] = data_args.eval_max_seq_length
+            self.max_seq_length_dict['val'] = data_args.eval_max_seq_length
+            self.max_seq_length_dict['test'] = data_args.test_max_seq_length
+            self.max_seq_length_dict['predict'] = data_args.test_max_seq_length
+        else:
+            self.label2id = None
+            self.id2label = None
 
 
     @property
     def external_kwargs(self):
         return self._external_kwargs
 
 
@@ -223,15 +231,14 @@
 
         if tokenizer_kwargs is None:
             tokenizer_kwargs = {}
 
         if config_kwargs is None:
             config_kwargs = {}
 
-
         model_args: ModelArguments = self.model_args
         training_args: TrainingArguments = self.training_args
         data_args: DataArguments = self.data_args
 
 
 
         tokenizer = load_tokenizer(tokenizer_name=tokenizer_name or model_args.tokenizer_name,
@@ -242,20 +249,20 @@
                                    use_fast_tokenizer=model_args.use_fast_tokenizer,
                                    model_revision=model_args.model_revision,
                                    use_auth_token=model_args.use_auth_token,
                                    **tokenizer_kwargs,
                                    )
         self.tokenizer = tokenizer
 
-
-        self.max_seq_length_dict['train'] = data_args.train_max_seq_length
-        self.max_seq_length_dict['eval'] = data_args.eval_max_seq_length
-        self.max_seq_length_dict['val'] = data_args.eval_max_seq_length
-        self.max_seq_length_dict['test'] = data_args.test_max_seq_length
-        self.max_seq_length_dict['predict'] = data_args.test_max_seq_length
+        if data_args is not None:
+            self.max_seq_length_dict['train'] = data_args.train_max_seq_length
+            self.max_seq_length_dict['eval'] = data_args.eval_max_seq_length
+            self.max_seq_length_dict['val'] = data_args.eval_max_seq_length
+            self.max_seq_length_dict['test'] = data_args.test_max_seq_length
+            self.max_seq_length_dict['predict'] = data_args.test_max_seq_length
 
         if with_task_params:
             task_specific_params = task_specific_params or {}
             task_params = self.on_task_specific_params()
             if task_params is not None:
                 task_specific_params.update(task_params)
 
@@ -294,15 +301,14 @@
 
         if with_labels and self.label2id is not None and hasattr(config, 'num_labels'):
             if with_print_labels:
                 print('*' * 30, 'num_labels = ', config.num_labels)
                 print(self.label2id)
                 print(self.id2label)
 
-
         if with_labels:
             return tokenizer, config, self.label2id, self.id2label
         return tokenizer, config
```

## deep_training/data_helper/data_module.py

```diff
@@ -57,22 +57,29 @@
                    return_dict=False,
                    task_specific_params=None,
                    **kwargs):
     config_kwargs = {
         "cache_dir": cache_dir,
         "revision": model_revision,
         "use_auth_token": True if use_auth_token else None,
+        "return_dict": return_dict,
+        **kwargs
+    }
+    tmp_kwargs = {
         "bos_token_id": bos_token_id,
         "pad_token_id": pad_token_id,
         "eos_token_id": eos_token_id,
         "sep_token_id": sep_token_id,
-        "return_dict": return_dict,
         "task_specific_params": task_specific_params,
-        **kwargs
     }
+    for k in list(tmp_kwargs.keys()):
+        if tmp_kwargs[k] is None:
+            tmp_kwargs.pop(k)
+    if tmp_kwargs:
+        config_kwargs.update(tmp_kwargs)
 
     if class_name is not None:
         config = class_name.from_pretrained(config_name, **config_kwargs)
     elif isinstance(config_name,PretrainedConfig):
         for k,v in config_kwargs.items():
             setattr(config_name,k,v)
         config = config_name
@@ -87,47 +94,7 @@
         raise ValueError(
             "You are instantiating a new config_gpt2 from scratch. This is not supported by this script."
             "You can do it from another script, save it, and load it from here, using --config_name."
         )
     if config_overrides is not None:
         config.update_from_string(config_overrides)
     return config
-
-
-
-
-# class DataCommonModule(LightningDataModule):
-#     def __init__(self,*args,**kwargs):
-#         super().__init__()
-#
-#
-#     def prepare_data(self):
-#         pass
-#
-#
-#     # def setup(self, stage: str):
-#     #     pass
-#
-#     # def train_dataloader(self):
-#     #     try:
-#     #         length = len(self.dataset["train"])
-#     #     except:
-#     #         length = None
-#     #     collate_fn = self.dataReaderHelper.collate_fn
-#     #     if length is None:
-#     #         return DataLoader(torch_IterableDataset(self.dataset["train"].shuffle(1024).repeat(-1)), batch_size=self.train_batch_size,collate_fn=collate_fn)
-#     #     return DataLoader(torch_Dataset(self.dataset["train"].shuffle(buffer_size=-1)), batch_size=self.train_batch_size,collate_fn=collate_fn)
-#
-#     # def val_dataloader(self):
-#     #     if self.dataset["validation"] is None:
-#     #         return super(GLUEDataModule, self).val_dataloader()
-#     #
-#     #     collate_fn = self.data_helper.collate_fn
-#     #     return DataLoader(self.dataset["validation"], batch_size=self.eval_batch_size,collate_fn=collate_fn)
-#     #
-#     # def test_dataloader(self):
-#     #     if self.dataset["test"] is None:
-#     #         return super(GLUEDataModule, self).test_dataloader()
-#     #
-#     #     collate_fn = self.data_helper.collate_fn
-#     #     return DataLoader(self.dataset["test"], batch_size=self.test_batch_size,collate_fn=collate_fn)
-
```

## deep_training/nlp/models/chatglm/quantization.py

```diff
@@ -56,15 +56,15 @@
 
     @staticmethod
     def backward(ctx, grad_output: torch.Tensor):
         inp, quant_w, scale_w = ctx.saved_tensors
         weight = extract_weight_to_half(quant_w, scale_w, ctx.weight_bit_width)
         grad_output = grad_output.contiguous().view(-1, weight.size(0))
         grad_input = grad_output.mm(weight)
-        grad_weight = grad_output.t().mm(inp)
+        grad_weight = grad_output.t().mm(inp.type(weight.dtype))
         return grad_input.view(ctx.inp_shape), grad_weight.view(ctx.weight_shape), None, None
 
 
 def compress_int4_weight(weight: torch.Tensor):  # (n, m)
     with torch.cuda.device(weight.device):
         n, m = weight.size(0), weight.size(1)
         assert m % 2 == 0
```

## deep_training/nlp/rl/ilql/ilql_trainer.py

```diff
@@ -1,13 +1,15 @@
 # coding=utf8
 # @Time    : 2023/5/3 14:19
 # @Author  : tk
 # @FileName: ilql_trainner
 import typing
 import os
+
+import lightning
 import numpy as np
 from tqdm import tqdm
 from time import time
 from collections.abc import Mapping
 from functools import partial
 from typing import Any, cast, Iterable, List, Literal, Optional, Tuple, Union, Callable
 import torch
```

## Comparing `deep_training-0.1.7.dist-info/METADATA` & `deep_training-0.1.7.post0.dist-info/METADATA`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deep-training
-Version: 0.1.7
+Version: 0.1.7.post0
 Summary: an easy training architecture
 Home-page: https://github.com/ssbuild/deep_training
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache License 2.0
 Platform: UNKNOWN
 Requires-Dist: lightning (>=2)
```

## Comparing `deep_training-0.1.7.dist-info/RECORD` & `deep_training-0.1.7.post0.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=ITNPJA05WzgBqK9gjSmooAethPA03pIij8H1505u8-4,897
+deep_training/setup.py,sha256=OS3BUJOK7TVnWjcpREUHLZBc4vIxaADvsTISSFN0khY,902
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
-deep_training/data_helper/data_helper.py,sha256=ttO-HeVFHiGJqBBLb0zCghQxwoVwNfQfUSf5BkUCO98,17851
-deep_training/data_helper/data_module.py,sha256=EmXCTU2jLnldgHubQL4lpwzmlJErSVJf7YIotbQBQJU,5041
+deep_training/data_helper/data_helper.py,sha256=afiDCuNfuHgRzDuGQwlxpmMqU2LSgInUfv9aqIrbJyo,18126
+deep_training/data_helper/data_module.py,sha256=0V38xPpgHJK7gGgffBRyobUMgV2MqqtHKL5y5PQzUaA,3757
 deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
 deep_training/nlp/layers/activate.py,sha256=0q7htFl9Az2fdUjrjv-QMUCE5oenYPVTLZ3lRemIKzA,241
 deep_training/nlp/layers/crf.py,sha256=JTihPuJuBBp83I9UZzVg0wogwwpdJrs0VKtuLPBSCDM,13271
 deep_training/nlp/layers/handshakingkernel.py,sha256=BRJZbEjKM347q8zEMEtJXxXjmqhegmQgqebhqMy4UkI,4653
 deep_training/nlp/layers/mask.py,sha256=8SB_Hl9X48-yuJMCPjLDabDXvgWvH4VPqUOSVDmePFs,435
@@ -100,15 +100,15 @@
 deep_training/nlp/models/LLaMA/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/LLaMA_parallel/__init__.py,sha256=4fOhbq0tQOTSH5e3X6XN3PnI6athUR8tsTCn4AUg94Q,19207
 deep_training/nlp/models/LLaMA_parallel/configuration.py,sha256=HNzzhIIdR9HBN9Y4Oavv6cGgIf0ExcphwsbVkltJ2ZM,5087
 deep_training/nlp/models/PaLM/__init__.py,sha256=P1qwWPUycRmZ6I48tov6janJUNpp4L-iMoVN54ykcQw,31627
 deep_training/nlp/models/PaLM/configuration.py,sha256=kIb3nj-2pQB2wyNrYHSZqr_ta1F0Cg-VbGEbnM5icPc,5890
 deep_training/nlp/models/chatglm/__init__.py,sha256=osBnXXXUoHwREHqXRX6WIEY0OQ0K1Yg7MKjQf04FNIQ,60510
 deep_training/nlp/models/chatglm/configuration.py,sha256=4w-Kbp_FJ2crIQVyu6kie9lbMSuE3U4nnjwjVPos2E8,4575
-deep_training/nlp/models/chatglm/quantization.py,sha256=sqX_poTcYNLJLDPbCwfRllDCF0enhshjX_dw7yZa604,15150
+deep_training/nlp/models/chatglm/quantization.py,sha256=8B6OHPv0EH3sTRbKrYR5MCM0PmYIlLvAMPzNRkCKAjM,15169
 deep_training/nlp/models/chatglm/tokenization.py,sha256=IMyHa8uPOgE0ia1DYp8Lx-IZ0N4TKSh1HVlA5sDdw-s,17037
 deep_training/nlp/models/laMDA/__init__.py,sha256=fvxTQQ8jfU-msPRdC8KsGlCwzM6u8-WBmayu6gE-s0E,34123
 deep_training/nlp/models/laMDA/configuration.py,sha256=8ZvPEl1C1KUGYWw7a8XcgIgl3gWH9WXa_-ZNDqz34PE,5981
 deep_training/nlp/models/lora/__init__.py,sha256=YyYmxdwz0IdyE4QXMPWhjUNmvOccqO8kJzPblfPwGJI,181
 deep_training/nlp/models/lora/v1/__init__.py,sha256=zwGdNKqudVj7c8sMWbmZ9CnnncWXuEapAucWY-VEhLs,123
 deep_training/nlp/models/lora/v1/configuration.py,sha256=0Fw6hHmu0j1DwW-CdrAcT2InCyDCMHTa4SvVFJYizY8,7095
 deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=c7X2raQW6tEN1stIIBJxoHA87mHEsDJZjcPDl_8D9g0,13688
@@ -145,15 +145,15 @@
 deep_training/nlp/optimizer/lion/triton.py,sha256=W7aZkc5SSgsiyrQFazpeqEkc_UyW0g-EZiALBpT5a8k,2499
 deep_training/nlp/rl/__init__.py,sha256=lXJsb8d-9R7DshCEdcx3iPyndlf1t5FNXiJsh1SUr0s,79
 deep_training/nlp/rl/ilql/__init__.py,sha256=tW9NHjvG7VvDbFBU9pVD1xDFONGu8-RJkKfx1lK1BIQ,88
 deep_training/nlp/rl/ilql/configuration.py,sha256=FFGppxPxHDktvMxVPl-_a9InRmVB8PqqRRKRnc1e5xc,2232
 deep_training/nlp/rl/ilql/data_define.py,sha256=B-yDx2t1gs2947Vn-g7lv6evUyQwwbtjlqsw9lYQ0No,3691
 deep_training/nlp/rl/ilql/ilql_dataset.py,sha256=lBWBv5-OwalK-qmTbS_LQoHMP3CFuvdc-nsh0sSATqQ,7800
 deep_training/nlp/rl/ilql/ilql_module.py,sha256=ZeAKFmlLKbHHF045JyiPFqYv7DKu8Yoqn3bTSdBdDLE,6244
-deep_training/nlp/rl/ilql/ilql_trainer.py,sha256=TEHwnPKpvqICiM3shJvJRSL4FhMxkjCnxhMoelWHGHo,35897
+deep_training/nlp/rl/ilql/ilql_trainer.py,sha256=SfDm6lYs6znBJ10CQaNQQn7tjZMUoGXO3aX0OiAgzxc,35917
 deep_training/nlp/rl/ppo/__init__.py,sha256=IqNQicmSmtZVbJIdNZdaQxpx0EbqvTJSUb2Bx1pRdys,55
 deep_training/nlp/rl/ppo/configuration.py,sha256=ZhbmK4LN_soY-TO2AJVrQgeoO4Lww6-2ZWQrC82RCBA,3460
 deep_training/nlp/rl/ppo/data_define.py,sha256=G1LQFZGbhlQ_eGIEWK_8_KrqZGEViUrDyuGj8SrrA9A,3201
 deep_training/nlp/rl/ppo/ppo_dataset.py,sha256=MZB9t3PizhRWBCNJuDHXUrZMVb97uwJUU0_UbFatd0A,3344
 deep_training/nlp/rl/ppo/ppo_module.py,sha256=fivwRNQKkf5Dzi5PxqlaQauGpQ_U5oJhDNkVVf80iDY,10103
 deep_training/nlp/rl/ppo/ppo_trainer.py,sha256=SO5nUGHLJLoITzNQOHv-wRIX1Zq0e5zKV3wc9-udN1I,47611
 deep_training/nlp/rl/rl_base/__init__.py,sha256=6pBQ9y-xnuMFThlwlzpT1oCVLZJG0rDUvWvFwu0ox3Y,88
@@ -175,11 +175,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=F1usofzi1lBVHeieDJ7WWdfd1d0Q7tftktwdJgczlg8,14500
-deep_training-0.1.7.dist-info/METADATA,sha256=xMlvbv_bI36zPO693ZKJMzjwoHvDj02w9mcBzKKqFys,602
-deep_training-0.1.7.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.7.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.7.dist-info/RECORD,,
+deep_training-0.1.7.post0.dist-info/METADATA,sha256=197-GY-xariYwK0ZD8RaSKxwBd9pb_W5cy8tiWQD42k,608
+deep_training-0.1.7.post0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.7.post0.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.7.post0.dist-info/RECORD,,
```

