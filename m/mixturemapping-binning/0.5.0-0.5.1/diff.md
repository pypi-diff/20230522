# Comparing `tmp/mixturemapping_binning-0.5.0-py39-none-any.whl.zip` & `tmp/mixturemapping_binning-0.5.1-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,22 +1,22 @@
-Zip file size: 35497 bytes, number of entries: 20
--rw-r--r--  2.0 unx      728 b- defN 23-May-21 20:40 mixturemapping_binning/__init__.py
--rw-r--r--  2.0 unx     3984 b- defN 23-May-21 20:40 mixturemapping_binning/binning.py
--rw-r--r--  2.0 unx     2672 b- defN 23-May-21 20:38 src/__init__.py
--rw-r--r--  2.0 unx     8897 b- defN 23-May-21 20:38 src/binning.py
--rw-r--r--  2.0 unx     9672 b- defN 23-May-21 20:38 src/distributions.py
--rw-r--r--  2.0 unx     5534 b- defN 23-May-21 20:38 src/utils.py
--rw-r--r--  2.0 unx     1416 b- defN 23-May-21 20:38 src/layers/__init__.py
--rw-r--r--  2.0 unx     2011 b- defN 23-May-21 20:38 src/layers/_mapping.py
--rw-r--r--  2.0 unx     6637 b- defN 23-May-21 20:38 src/layers/binyield.py
--rw-r--r--  2.0 unx     5354 b- defN 23-May-21 20:38 src/layers/covmatrix.py
--rw-r--r--  2.0 unx     9859 b- defN 23-May-21 20:38 src/layers/distribution.py
--rw-r--r--  2.0 unx    21470 b- defN 23-May-21 20:38 src/layers/disttrafo.py
--rw-r--r--  2.0 unx    10990 b- defN 23-May-21 20:38 src/layers/generalmapping.py
--rw-r--r--  2.0 unx    15915 b- defN 23-May-21 20:38 src/layers/gmrmapping.py
--rw-r--r--  2.0 unx     7866 b- defN 23-May-21 20:38 src/layers/linearmapping.py
--rw-r--r--  2.0 unx     8059 b- defN 23-May-21 20:38 src/layers/normalization.py
--rw-r--r--  2.0 unx     2680 b- defN 23-May-21 20:40 mixturemapping_binning-0.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-May-21 20:40 mixturemapping_binning-0.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       27 b- defN 23-May-21 20:40 mixturemapping_binning-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1644 b- defN 23-May-21 20:40 mixturemapping_binning-0.5.0.dist-info/RECORD
-20 files, 125508 bytes uncompressed, 32835 bytes compressed:  73.8%
+Zip file size: 36003 bytes, number of entries: 20
+-rw-r--r--  2.0 unx      728 b- defN 23-May-22 18:44 mixturemapping_binning/__init__.py
+-rw-r--r--  2.0 unx     3984 b- defN 23-May-22 18:44 mixturemapping_binning/binning.py
+-rw-r--r--  2.0 unx     2672 b- defN 23-May-22 18:42 src/__init__.py
+-rw-r--r--  2.0 unx     8897 b- defN 23-May-22 18:42 src/binning.py
+-rw-r--r--  2.0 unx     9700 b- defN 23-May-22 18:42 src/distributions.py
+-rw-r--r--  2.0 unx     5534 b- defN 23-May-22 18:42 src/utils.py
+-rw-r--r--  2.0 unx     1416 b- defN 23-May-22 18:42 src/layers/__init__.py
+-rw-r--r--  2.0 unx     2011 b- defN 23-May-22 18:42 src/layers/_mapping.py
+-rw-r--r--  2.0 unx     6637 b- defN 23-May-22 18:42 src/layers/binyield.py
+-rw-r--r--  2.0 unx     5328 b- defN 23-May-22 18:42 src/layers/covmatrix.py
+-rw-r--r--  2.0 unx    10625 b- defN 23-May-22 18:42 src/layers/distribution.py
+-rw-r--r--  2.0 unx    21470 b- defN 23-May-22 18:42 src/layers/disttrafo.py
+-rw-r--r--  2.0 unx    10977 b- defN 23-May-22 18:42 src/layers/generalmapping.py
+-rw-r--r--  2.0 unx    16954 b- defN 23-May-22 18:42 src/layers/gmrmapping.py
+-rw-r--r--  2.0 unx     7853 b- defN 23-May-22 18:42 src/layers/linearmapping.py
+-rw-r--r--  2.0 unx     8059 b- defN 23-May-22 18:42 src/layers/normalization.py
+-rw-r--r--  2.0 unx     2680 b- defN 23-May-22 18:44 mixturemapping_binning-0.5.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-May-22 18:44 mixturemapping_binning-0.5.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       27 b- defN 23-May-22 18:44 mixturemapping_binning-0.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1645 b- defN 23-May-22 18:44 mixturemapping_binning-0.5.1.dist-info/RECORD
+20 files, 127290 bytes uncompressed, 33341 bytes compressed:  73.8%
```

## zipnote {}

```diff
@@ -42,20 +42,20 @@
 
 Filename: src/layers/linearmapping.py
 Comment: 
 
 Filename: src/layers/normalization.py
 Comment: 
 
-Filename: mixturemapping_binning-0.5.0.dist-info/METADATA
+Filename: mixturemapping_binning-0.5.1.dist-info/METADATA
 Comment: 
 
-Filename: mixturemapping_binning-0.5.0.dist-info/WHEEL
+Filename: mixturemapping_binning-0.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: mixturemapping_binning-0.5.0.dist-info/top_level.txt
+Filename: mixturemapping_binning-0.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: mixturemapping_binning-0.5.0.dist-info/RECORD
+Filename: mixturemapping_binning-0.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## src/distributions.py

```diff
@@ -92,15 +92,15 @@
     diagMatrix = _tf.sqrt(_tf.linalg.diag_part(covMatrix))
 
     # compute a covariance matrix with full correlation 1
     n = _tf.expand_dims(diagMatrix, -1)
     maxCorr = n*_tf.linalg.matrix_transpose(n)
 
     # reduce the covariance matrix
-    corr = _tf.divide(covMatrix, maxCorr) * epsilon
+    corr = _tf.clip_by_value(_tf.divide(covMatrix, maxCorr), -epsilon, epsilon)
     # set the diagonal elements back to one
     newCorr = _tf.linalg.set_diag(corr, _tf.ones_like(diagMatrix))
 
     # compute a new covariance matrix from the max Values and the reduced off diagonal elements
     return _tf.multiply(newCorr, maxCorr)
```

## src/layers/covmatrix.py

```diff
@@ -19,16 +19,16 @@
 import tensorflow_probability as _tfp
 _tfd = _tfp.distributions
 from tensorflow.keras.layers import Layer as _Layer
 
 from ._mapping import _M
 
 # mixturemapping imports
-from mixturemapping.distributions import createCovMatrix as _createCovMatrix
-from mixturemapping.distributions import regularizeCovMatrix as _r
+from ..distributions import createCovMatrix as _createCovMatrix
+from ..distributions import regularizeCovMatrix as _r
 
 
 class TrainableCovMatrix(_Layer):
     """Generates an independently trainable covariance matrix
 
     Important symmetry properties are generated automatically because the covariance matrix
     is built up from standard deviations `TrainableCovMatrix.spread`
```

## src/layers/distribution.py

```diff
@@ -190,24 +190,32 @@
     Example::
 
         distParams = {"means": ???, "covariances": ???, "weights": ???}
         distLayer = mm.layers.Distribution(regularize_cov_epsilon=0.95)
         dist = distLayer(distParams)
 
         lossdist = mm.layers.DistributionKLLoss()({"dist": dist, "means": means, "covariances": covs, "weights": weights})
+
+    :param n_samples: Number of samples to integrate the KL divergence
+    :type n_samples: int
+    :param symmetric: Flag to symmetrize the KL divergence
+    :type symmetric: bool
+    :param fix_na: Skip samples that lead have a nan KL divergence
+    :type fix_na: bool
     """
 
-    def __init__(self, n_samples=100, symmetric=False, ** kwargs):
+    def __init__(self, n_samples=100, symmetric=False, fix_na=True, ** kwargs):
         super(DistributionKLLoss, self).__init__(**kwargs)
         self.n_samples = n_samples
         self.loss = None
         self.ideal_dist = None
         self.samples = None
         self.kl_divergence = None
         self.symmetric = symmetric
+        self.fix_na = fix_na
 
     def build(self, input_shapes):
         if not ("means" in input_shapes and "covariances" in input_shapes and "weights" in input_shapes):
             raise Exception(
                 "means, covariances and weights are needed to construct the ideal GMM!")
 
         # deduce the input shape my the means tensor
@@ -232,32 +240,37 @@
 
         :returns: distribution
         """
         x = x.copy()
 
         dist = x["dist"]
         self.ideal_dist = _tfp.layers.DistributionLambda(
-            lambda t: _c(
-                t[0], t[1], t[2], self.mix_dim, self.output_dim)
+            lambda t: _c(t[0], t[1], t[2], self.mix_dim, self.output_dim)
         )([x["means"], x["covariances"], x["weights"]])
 
         # compute sampels
         samples_ideal = self.ideal_dist.sample(self.n_samples)
         self.samples_ideal = _tf.transpose(samples_ideal, [1, 0, 2])
 
-
-     
-
         # compute the KL divergence based on the samples
         kl_A = self.ideal_dist.log_prob(samples_ideal) - dist.log_prob(samples_ideal)
 
+        if self.fix_na:
+            max_kl_A = _tf.stop_gradient(_tf.reduce_max(_tf.where(_tf.math.is_nan(kl_A), _tf.zeros_like(kl_A), kl_A)))
+            kl_A = _tf.where(_tf.math.is_nan(kl_A), _tf.ones_like(kl_A)*max_kl_A, kl_A)
+
         if self.symmetric:
             samples_pred = dist.sample(self.n_samples)
             self.samples_pred = _tf.transpose(samples_pred, [1, 0, 2])
             kl_B = dist.log_prob(samples_pred) - self.ideal_dist.log_prob(samples_pred)
+
+            if self.fix_na:
+                max_kl_B = _tf.stop_gradient(_tf.reduce_max(_tf.where(_tf.math.is_nan(kl_B), _tf.zeros_like(kl_B), kl_B)))
+                kl_B = _tf.where(_tf.math.is_nan(kl_B), _tf.ones_like(kl_B)*max_kl_B, kl_B)
+
             self.kl_divergence = _tf.reduce_mean(_tf.transpose(kl_A, [1,0]), 1) + _tf.reduce_mean(_tf.transpose(kl_B, [1,0]), 1)
             
         else:
             self.kl_divergence = _tf.reduce_mean(_tf.transpose(kl_A, [1,0]), 1)
         
 
         self.loss = self.kl_divergence
```

## src/layers/generalmapping.py

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
 # tensorflow imports
-from mixturemapping.distributions import regularizeCovMatrix as _r
+from ..distributions import regularizeCovMatrix as _r
 from ._mapping import _M
 import tensorflow as _tf
 import tensorflow_probability as _tfp
 _tfd = _tfp.distributions
 
 # internal imports
```

## src/layers/gmrmapping.py

```diff
@@ -17,14 +17,17 @@
 # tensorflow imports
 from ._mapping import _M
 import numpy as _np
 import tensorflow as _tf
 import tensorflow_probability as _tfp
 _tfd = _tfp.distributions
 
+# internal inmports
+from ..distributions import regularizeCovMatrix as _r
+
 
 class DynamicGMR(_M):
     """ A trainable mapping layer using gaussian mixtures.
 
     This layers can be used if an initial guess of a trained Gaussian mixture of the combined input and
     output coordinates is available.
 
@@ -36,25 +39,34 @@
             output_dim=2,
             input_dim=2,
             mix_size=30,
         )
 
         transformedDist = gmr({"means": inMeans, "covariances": cov, "weights": inWeight})
 
-        distLayer = mm.layers.Distribution(dtype=dataType, regularize_cov_epsilon=0.95)
+        distLayer = mm.layers.Distribution(dtype=dataType, regularize_cov_epsilon=0.98)
         dist = distLayer(transformedDist)
 
         sample_loss = distLayer.sample_loss(inTsamples)  
 
 
     Parameters
     ----------
     output_dim: int
         size of the output dimension
-    ???
+    input_dim:  int
+        size of the input dimension
+    mix_size:   int
+        size of the internally used GMM to represent the distribution
+    n_components: int
+        if set, the output GMM is limited to the most important n_components
+    regularize_cov_epsilon: float
+        if set, a maximal correlation is introduced (default: 0.98)
+    inv_rcond: float
+        singular value cutoffs for inverse (default: 1e-5)    
            
 
 
     Returns
     -------
     {
         "means" : Tensorflow Tensor
@@ -62,31 +74,42 @@
         "covariances" : Tensorflow Tensor
             Covariance matrices of the distributions
         "weights" : Tensorflow Tensor
             Weights of the mixture components
     }
     """
 
-    def __init__(self,  output_dim, input_dim, mix_size, n_components=None, **kwargs):
+    def __init__(
+            self,
+            output_dim,
+            input_dim,
+            mix_size,
+            n_components=None,
+            regularize_cov_epsilon=0.95,
+            inv_rcond = 1e-5,
+            **kwargs):
         super(DynamicGMR, self).__init__(output_dim, **kwargs)
         self.mix_size = mix_size
         self.input_dim = input_dim
         self.var_size = self.output_dim + self.input_dim
         self.output_indices = list(range(self.input_dim, self.var_size))
+        self._r = regularize_cov_epsilon
+        self._rcond = inv_rcond
 
         # compute the input indices by inversion of the output indices
         inv = _np.ones(self.var_size, dtype=bool)
         inv[self.output_indices] = False
         inv, = _np.where(inv)
         self.input_indices = inv
 
         self.n_components = n_components
         self.corr_size = int(self.var_size * (self.var_size-1) / 2)
 
 
+
     def _to_np(self, a):
         if isinstance(a, _np.ndarray):
             return a
         else:
             return _np.array(a, dtype=self.dtype)
 
 
@@ -118,15 +141,15 @@
         #     triu_indices)) if x in self._input_indices and y in self._input_indices]
         # var_input_corr_idx = [idx for idx in range(
         #     self._corr_size) if idx not in const_input_corr_idx]
 
         # set the trainable variable parts
         _tf.keras.backend.set_value(self.model_cov_stddev_i, _np.log(input_spread)[:, self.input_indices])
         _tf.keras.backend.set_value(self.model_cov_stddev_o, _np.log(input_spread)[:, self.output_indices])
-        _tf.keras.backend.set_value(self.model_cov_stddev_o_spread, _np.ones( (self.mix_size, self.output_dim))*-1.0)
+        _tf.keras.backend.set_value(self.model_cov_stddev_o_spread, _np.ones( (self.mix_size, self.output_dim))*-2.0)
         _tf.keras.backend.set_value(self.model_m_i, means[:, self.input_indices])
         _tf.keras.backend.set_value(self.model_m_o, means[:, self.output_indices])
         _tf.keras.backend.set_value(self.model_cov_corr, _np.arctanh(input_corr))
         _tf.keras.backend.set_value(self.model_w, weights)
 
 
     def fit_gmms(self, np_inputs, tf_inputs, input_gmm, output_gmm, n_samples=10):
@@ -167,15 +190,18 @@
         )
 
         sample_model =_tf.keras.Model(inputs=tf_inputs, outputs=samples_tf)
         sample_model.compile()
         samples = sample_model.predict(np_inputs)  
 
         trainModel = GaussianMixture(n_components=self.mix_size)
-        trainModel.fit(samples)
+
+        # remove all nan rows
+        samples_without_nan = samples[~_np.isnan(samples).any(axis=1)]
+        trainModel.fit(samples_without_nan)
 
         self.set_gmm_values(
             means=trainModel.means_,
             covariances=trainModel.covariances_,
             weights=trainModel.weights_
         )
 
@@ -320,20 +346,24 @@
 
         # 4. extend the array sizes because we have to compute multiple overlaps between all Gaussians
         model_m = _tf.expand_dims(_tf.expand_dims(model_m, 0), 2)
         model_w = _tf.expand_dims(_tf.expand_dims(self.model_w, 0), 2)       
         # note the cov matrix might be already different for each entry of the batch
         model_cov = _tf.expand_dims(model_cov, 2) 
 
+        # regularize the covmatrix if needed
+        if self._r:
+            model_cov = _r(model_cov, self._r)
+
 
         # 5. cut the parts from means and covariances based on the input and output indices
         s11 = _tf.stop_gradient(_tf.gather(_tf.gather(model_cov, self.input_indices, axis=3), self.input_indices, axis=4))
         s22 = _tf.gather(_tf.gather(model_cov, self.output_indices, axis=3), self.output_indices, axis=4)
 
-        s11I = _tf.linalg.pinv(s11)
+        s11I = _tf.linalg.pinv(s11, self._rcond)
         s21 = _tf.gather(_tf.gather(
             model_cov, self.output_indices, axis=3), self.input_indices, axis=4)
         s12 = _tf.linalg.matrix_transpose(s21)
 
         mu1 = _tf.gather(model_m, self.input_indices, axis=3)
         mu2 = _tf.gather(model_m, self.output_indices, axis=3)
 
@@ -401,9 +431,11 @@
         config = super().get_config().copy()
 
         config.update({
             'output_dim': self.output_dim,
             'input_dim': self.input_dim,
             'mix_size': self.mix_size,
             'n_components  ': self.n_components,
+            'regularize_cov_epsilon': self._r,
+            'inv_rcond': self._rcond
         })
         return config
```

## src/layers/linearmapping.py

```diff
@@ -17,15 +17,15 @@
 # tensorflow imports
 import tensorflow as _tf
 import tensorflow_probability as _tfp
 _tfd = _tfp.distributions
 
 # internal imports
 from ._mapping import _M
-from mixturemapping.distributions import regularizeCovMatrix as _r
+from ..distributions import regularizeCovMatrix as _r
 
 class LinearMapping(_M):
     """ A free linear mapping layer, that maps centers and distributions independently.
 
     This layers can be used if the relationship between input distributions
     and output distributions is free and linear
```

## Comparing `mixturemapping_binning-0.5.0.dist-info/METADATA` & `mixturemapping_binning-0.5.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mixturemapping-binning
-Version: 0.5.0
+Version: 0.5.1
 Summary: UNKNOWN
 Home-page: https://vk.github.io/mixturemapping-doc/
 License: UNKNOWN
 Keywords: binning mixturemapping
 Platform: UNKNOWN
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
```

## Comparing `mixturemapping_binning-0.5.0.dist-info/RECORD` & `mixturemapping_binning-0.5.1.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 mixturemapping_binning/__init__.py,sha256=chjTj6oZUpQEcIM4OLwRelEXttgxHlZdRjRFaH1fHro,728
 mixturemapping_binning/binning.py,sha256=H9amezvqpUqejl-PeZJuIzIPyKZrXmSY5oVSPdcdark,3984
 src/__init__.py,sha256=4gqCPdcjt-h-FgKb3HmAGyFv957Ola42A9ijKaWHluI,2672
 src/binning.py,sha256=nhrDtvKbJho6LV58LS_sJ_XBjnjLNyL4XadMRS3hpd0,8897
-src/distributions.py,sha256=HU_Yb7Jwwj-_11tSHi4nrpd1DEEQqg7FhHwkevgipzg,9672
+src/distributions.py,sha256=bKLdPpuv1YLd-uOh5e7LjzN9jysC0eq-vkwBDkNVaUI,9700
 src/utils.py,sha256=QgqMf2q6CmCZmOGBFlWTiZm9KVCSZRVegFWPoO06mFE,5534
 src/layers/__init__.py,sha256=k9Zql2ivbuQ8gOfCKBFQPW7t08-zgE5y8fV5UeS6fkg,1416
 src/layers/_mapping.py,sha256=dOFZwpnofPbuiZxiL7hFkWLE8xJUnVvGj4zUbtpIUoI,2011
 src/layers/binyield.py,sha256=25eilheYPF1LXtYevnRVR9UvwOI7zaQgeHa_pHkkdzU,6637
-src/layers/covmatrix.py,sha256=JGpP0SXhjyVqV4MR-i8u5V3IfHVd7_O3_0eG4sNOI4E,5354
-src/layers/distribution.py,sha256=RyhaUNsFa-VR_2OwvLmq5TzieOv9F_7BduON0GGS41g,9859
+src/layers/covmatrix.py,sha256=EtCDV9NrQJfRLz0mdZf9lPcSkq3EmG8tO2mnnWopxrE,5328
+src/layers/distribution.py,sha256=ERrb58BmLKcs_i9j6epy30MZEyHJxX3MN7Scy_s8gWs,10625
 src/layers/disttrafo.py,sha256=PIE4n3-qhtlbT1JoLbtzyOgMKnUpvPcn_gm0aykT7Kw,21470
-src/layers/generalmapping.py,sha256=noFo5y1EeW07Y8fcN1AxaFa7cwNGb3d5YZlzchxC_60,10990
-src/layers/gmrmapping.py,sha256=scoUCxGyEfFlUidmKAhw7s1f3ztAEdS0Rie4ZibsFjA,15915
-src/layers/linearmapping.py,sha256=USyTO0WUxA5rRVNnQou5T3zx_hs82QELWg0dF7A4dFA,7866
+src/layers/generalmapping.py,sha256=YiZZY1cRgY8mEkW8CSDQL_Ev7bRvnZ2hiRBvja4lS1U,10977
+src/layers/gmrmapping.py,sha256=e29Ad587iGnlaqbZPlcd7eKlVWvai99keXaeBk9BO00,16954
+src/layers/linearmapping.py,sha256=fPKgsYntXjmCI77rnzznO43s6TpgcwXVn1NYNvuiMxg,7853
 src/layers/normalization.py,sha256=ykvcHSHeDEkkhdxD5RXhXanE5FbqjTW1NZeSuk4eSmk,8059
-mixturemapping_binning-0.5.0.dist-info/METADATA,sha256=4aJb7otSB1MQ_upYGeJXzJvvqcQtcgPoSx6rkVjDLZc,2680
-mixturemapping_binning-0.5.0.dist-info/WHEEL,sha256=ijjRDmPkGsL9eKpOeSzmTjLbiyw0Dy8TY4QGa2Zy9J8,93
-mixturemapping_binning-0.5.0.dist-info/top_level.txt,sha256=YGM-yKt81JqgECuc9YI0yCcAM43MQStbqwI-kVmqypQ,27
-mixturemapping_binning-0.5.0.dist-info/RECORD,,
+mixturemapping_binning-0.5.1.dist-info/METADATA,sha256=spViAxGFmrb82NywEpARPKqbO_1HzFleUrMqKrj1Mxc,2680
+mixturemapping_binning-0.5.1.dist-info/WHEEL,sha256=ijjRDmPkGsL9eKpOeSzmTjLbiyw0Dy8TY4QGa2Zy9J8,93
+mixturemapping_binning-0.5.1.dist-info/top_level.txt,sha256=YGM-yKt81JqgECuc9YI0yCcAM43MQStbqwI-kVmqypQ,27
+mixturemapping_binning-0.5.1.dist-info/RECORD,,
```

