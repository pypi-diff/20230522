# Comparing `tmp/mixturemapping-0.5.0-py39-none-any.whl.zip` & `tmp/mixturemapping-0.5.1-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,20 @@
-Zip file size: 21073 bytes, number of entries: 18
--rw-r--r--  2.0 unx      998 b- defN 23-May-22 11:11 mixturemapping/__init__.py
--rw-r--r--  2.0 unx     3984 b- defN 23-May-22 11:11 mixturemapping/binning.py
--rw-r--r--  2.0 unx     3040 b- defN 23-May-22 11:11 mixturemapping/distributions.py
--rw-r--r--  2.0 unx     2077 b- defN 23-May-22 11:11 mixturemapping/utils.py
--rw-r--r--  2.0 unx     1385 b- defN 23-May-22 11:11 mixturemapping/layers/__init__.py
--rw-r--r--  2.0 unx     1454 b- defN 23-May-22 11:11 mixturemapping/layers/_mapping.py
--rw-r--r--  2.0 unx     2841 b- defN 23-May-22 11:11 mixturemapping/layers/binyield.py
--rw-r--r--  2.0 unx     3044 b- defN 23-May-22 11:11 mixturemapping/layers/covmatrix.py
--rw-r--r--  2.0 unx     3671 b- defN 23-May-22 11:11 mixturemapping/layers/distribution.py
--rw-r--r--  2.0 unx     5956 b- defN 23-May-22 11:11 mixturemapping/layers/disttrafo.py
--rw-r--r--  2.0 unx     2802 b- defN 23-May-22 11:11 mixturemapping/layers/generalmapping.py
--rw-r--r--  2.0 unx     6247 b- defN 23-May-22 11:11 mixturemapping/layers/gmrmapping.py
--rw-r--r--  2.0 unx     2475 b- defN 23-May-22 11:11 mixturemapping/layers/linearmapping.py
--rw-r--r--  2.0 unx     2205 b- defN 23-May-22 11:11 mixturemapping/layers/normalization.py
--rw-r--r--  2.0 unx     2625 b- defN 23-May-22 11:11 mixturemapping-0.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-May-22 11:11 mixturemapping-0.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 23-May-22 11:11 mixturemapping-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1582 b- defN 23-May-22 11:11 mixturemapping-0.5.0.dist-info/RECORD
-18 files, 46494 bytes uncompressed, 18453 bytes compressed:  60.3%
+Zip file size: 21320 bytes, number of entries: 18
+-rw-r--r--  2.0 unx      998 b- defN 23-May-22 18:44 mixturemapping/__init__.py
+-rw-r--r--  2.0 unx     3984 b- defN 23-May-22 18:44 mixturemapping/binning.py
+-rw-r--r--  2.0 unx     3066 b- defN 23-May-22 18:44 mixturemapping/distributions.py
+-rw-r--r--  2.0 unx     2077 b- defN 23-May-22 18:44 mixturemapping/utils.py
+-rw-r--r--  2.0 unx     1385 b- defN 23-May-22 18:44 mixturemapping/layers/__init__.py
+-rw-r--r--  2.0 unx     1454 b- defN 23-May-22 18:44 mixturemapping/layers/_mapping.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-May-22 18:44 mixturemapping/layers/binyield.py
+-rw-r--r--  2.0 unx     3044 b- defN 23-May-22 18:44 mixturemapping/layers/covmatrix.py
+-rw-r--r--  2.0 unx     4015 b- defN 23-May-22 18:44 mixturemapping/layers/distribution.py
+-rw-r--r--  2.0 unx     5956 b- defN 23-May-22 18:44 mixturemapping/layers/disttrafo.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-May-22 18:44 mixturemapping/layers/generalmapping.py
+-rw-r--r--  2.0 unx     6506 b- defN 23-May-22 18:44 mixturemapping/layers/gmrmapping.py
+-rw-r--r--  2.0 unx     2475 b- defN 23-May-22 18:44 mixturemapping/layers/linearmapping.py
+-rw-r--r--  2.0 unx     2205 b- defN 23-May-22 18:44 mixturemapping/layers/normalization.py
+-rw-r--r--  2.0 unx     2625 b- defN 23-May-22 18:44 mixturemapping-0.5.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-May-22 18:44 mixturemapping-0.5.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 23-May-22 18:44 mixturemapping-0.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1582 b- defN 23-May-22 18:44 mixturemapping-0.5.1.dist-info/RECORD
+18 files, 47123 bytes uncompressed, 18700 bytes compressed:  60.3%
```

## zipnote {}

```diff
@@ -36,20 +36,20 @@
 
 Filename: mixturemapping/layers/linearmapping.py
 Comment: 
 
 Filename: mixturemapping/layers/normalization.py
 Comment: 
 
-Filename: mixturemapping-0.5.0.dist-info/METADATA
+Filename: mixturemapping-0.5.1.dist-info/METADATA
 Comment: 
 
-Filename: mixturemapping-0.5.0.dist-info/WHEEL
+Filename: mixturemapping-0.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: mixturemapping-0.5.0.dist-info/top_level.txt
+Filename: mixturemapping-0.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: mixturemapping-0.5.0.dist-info/RECORD
+Filename: mixturemapping-0.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mixturemapping/distributions.py

```diff
@@ -27,15 +27,15 @@
 		for J in range(A-1):F=F-1;C.append(C[-1]+F)
 		D=_tf.matmul(_tf.reshape(B,[G,A,1],name='colVec'),_tf.reshape(B,[G,1,A],name='rowVec'),name='CovMatrix')
 		if _tf.shape(E).shape[0]==1:
 			with _tf.name_scope(I):H=[[D[:,A,B]if A==B else D[:,A,B]*E[B+C[A]]if A<B else D[:,A,B]*E[A+C[B]]for A in range(A)]for B in range(A)]
 		else:
 			with _tf.name_scope(I):H=[[D[:,A,B]if A==B else D[:,A,B]*E[:,B+C[A]]if A<B else D[:,A,B]*E[:,A+C[B]]for A in range(A)]for B in range(A)]
 		return _tf.transpose(H,name=name+'Transpose')
-def regularizeCovMatrix(covMatrix,epsilon=.999):A=covMatrix;B=_tf.sqrt(_tf.linalg.diag_part(A));C=_tf.expand_dims(B,-1);D=C*_tf.linalg.matrix_transpose(C);E=_tf.divide(A,D)*epsilon;F=_tf.linalg.set_diag(E,_tf.ones_like(B));return _tf.multiply(F,D)
+def regularizeCovMatrix(covMatrix,epsilon=.999):B=epsilon;A=covMatrix;C=_tf.sqrt(_tf.linalg.diag_part(A));D=_tf.expand_dims(C,-1);E=D*_tf.linalg.matrix_transpose(D);F=_tf.clip_by_value(_tf.divide(A,E),-B,B);G=_tf.linalg.set_diag(F,_tf.ones_like(C));return _tf.multiply(G,E)
 def createMixDistBYmeanCovWeight(meanTensor,covTensor,weightTensor,mixSize,matrixSize,name=_A):
 	with _tf.name_scope(name):
 		with _tf.name_scope(_B):A=_tf.linalg.cholesky(covTensor);B=_tfd.MultivariateNormalTriL(loc=meanTensor,scale_tril=A,name=_C);C=_tfd.Categorical(probs=weightTensor,name=_D);D=_tfp.distributions.MixtureSameFamily(mixture_distribution=C,components_distribution=B,name=_E)
 		return D
 def createMixDistribution(meanTensor,stdDevTensor,correlationTensor,weightTensor,mixSize,matrixSize,name=_A):
 	C=meanTensor;A=matrixSize
 	with _tf.name_scope(name):
```

## mixturemapping/layers/distribution.py

```diff
@@ -36,21 +36,25 @@
 class DistributionMean(_Layer):
 	def __init__(A,**B):super(DistributionMean,A).__init__(**B)
 	def call(A,x,**B):return x.mean()
 class DistributionSampleLoss(_Layer):
 	def __init__(A,**B):super(DistributionSampleLoss,A).__init__(**B);A.loss=_B
 	def call(A,x,**D):B=x['dist'];C=x['samples'];A.loss=-_tf.reduce_mean(B.log_prob(C));A.add_loss(A.loss);x.update({'loss':A.loss});return x
 class DistributionKLLoss(_Layer):
-	def __init__(A,n_samples=100,symmetric=False,**B):super(DistributionKLLoss,A).__init__(**B);A.n_samples=n_samples;A.loss=_B;A.ideal_dist=_B;A.samples=_B;A.kl_divergence=_B;A.symmetric=symmetric
+	def __init__(A,n_samples=100,symmetric=False,fix_na=True,**B):super(DistributionKLLoss,A).__init__(**B);A.n_samples=n_samples;A.loss=_B;A.ideal_dist=_B;A.samples=_B;A.kl_divergence=_B;A.symmetric=symmetric;A.fix_na=fix_na
 	def build(B,input_shapes):
 		A=input_shapes
 		if not(_A in A and _C in A and _D in A):raise Exception('means, covariances and weights are needed to construct the ideal GMM!')
 		C=A[_A];B.mix_dim=C[1];B.output_dim=C[2];super(DistributionKLLoss,B).build(A[_A])
-	def call(A,x,**G):
-		x=x.copy();B=x['dist'];A.ideal_dist=_tfp.layers.DistributionLambda(lambda t:_c(t[0],t[1],t[2],A.mix_dim,A.output_dim))([x[_A],x[_C],x[_D]]);C=A.ideal_dist.sample(A.n_samples);A.samples_ideal=_tf.transpose(C,[1,0,2]);E=A.ideal_dist.log_prob(C)-B.log_prob(C)
-		if A.symmetric:D=B.sample(A.n_samples);A.samples_pred=_tf.transpose(D,[1,0,2]);F=B.log_prob(D)-A.ideal_dist.log_prob(D);A.kl_divergence=_tf.reduce_mean(_tf.transpose(E,[1,0]),1)+_tf.reduce_mean(_tf.transpose(F,[1,0]),1)
-		else:A.kl_divergence=_tf.reduce_mean(_tf.transpose(E,[1,0]),1)
+	def call(A,x,**I):
+		x=x.copy();D=x['dist'];A.ideal_dist=_tfp.layers.DistributionLambda(lambda t:_c(t[0],t[1],t[2],A.mix_dim,A.output_dim))([x[_A],x[_C],x[_D]]);E=A.ideal_dist.sample(A.n_samples);A.samples_ideal=_tf.transpose(E,[1,0,2]);B=A.ideal_dist.log_prob(E)-D.log_prob(E)
+		if A.fix_na:G=_tf.stop_gradient(_tf.reduce_max(_tf.where(_tf.math.is_nan(B),_tf.zeros_like(B),B)));B=_tf.where(_tf.math.is_nan(B),_tf.ones_like(B)*G,B)
+		if A.symmetric:
+			F=D.sample(A.n_samples);A.samples_pred=_tf.transpose(F,[1,0,2]);C=D.log_prob(F)-A.ideal_dist.log_prob(F)
+			if A.fix_na:H=_tf.stop_gradient(_tf.reduce_max(_tf.where(_tf.math.is_nan(C),_tf.zeros_like(C),C)));C=_tf.where(_tf.math.is_nan(C),_tf.ones_like(C)*H,C)
+			A.kl_divergence=_tf.reduce_mean(_tf.transpose(B,[1,0]),1)+_tf.reduce_mean(_tf.transpose(C,[1,0]),1)
+		else:A.kl_divergence=_tf.reduce_mean(_tf.transpose(B,[1,0]),1)
 		A.loss=A.kl_divergence;A.add_loss(A.loss);x.update({'loss':A.loss});return x
 class DistributionSamples(_Layer):
 	def __init__(A,n_samples,**B):A.n_samples=n_samples;super(DistributionSamples,A).__init__(**B)
 	def call(A,x,**C):B=_tf.transpose(x.sample(A.n_samples),[1,2,0]);return B
 	def get_config(B):A=super().get_config().copy();A.update({'n_samples':B.n_samples});return A
```

## mixturemapping/layers/gmrmapping.py

```diff
@@ -16,29 +16,32 @@
 _D='weights'
 _C='means'
 _B=False
 _A=True
 from._mapping import _M
 import numpy as _np,tensorflow as _tf,tensorflow_probability as _tfp
 _tfd=_tfp.distributions
+from..distributions import regularizeCovMatrix as _r
 class DynamicGMR(_M):
-	def __init__(A,output_dim,input_dim,mix_size,n_components=None,**C):super(DynamicGMR,A).__init__(output_dim,**C);A.mix_size=mix_size;A.input_dim=input_dim;A.var_size=A.output_dim+A.input_dim;A.output_indices=list(range(A.input_dim,A.var_size));B=_np.ones(A.var_size,dtype=bool);B[A.output_indices]=_B;B,=_np.where(B);A.input_indices=B;A.n_components=n_components;A.corr_size=int(A.var_size*(A.var_size-1)/2)
+	def __init__(A,output_dim,input_dim,mix_size,n_components=None,regularize_cov_epsilon=.95,inv_rcond=1e-05,**C):super(DynamicGMR,A).__init__(output_dim,**C);A.mix_size=mix_size;A.input_dim=input_dim;A.var_size=A.output_dim+A.input_dim;A.output_indices=list(range(A.input_dim,A.var_size));A._r=regularize_cov_epsilon;A._rcond=inv_rcond;B=_np.ones(A.var_size,dtype=bool);B[A.output_indices]=_B;B,=_np.where(B);A.input_indices=B;A.n_components=n_components;A.corr_size=int(A.var_size*(A.var_size-1)/2)
 	def _to_np(A,a):
 		if isinstance(a,_np.ndarray):return a
 		else:return _np.array(a,dtype=A.dtype)
-	def set_gmm_values(A,means,covariances,weights):E=covariances;D=means;B=_np.array([_np.sqrt(_np.diagonal(A))for A in E]);C=E/_np.reshape(B,[A.mix_size,1,A.var_size])/_np.reshape(B,[A.mix_size,A.var_size,1]);F=_np.triu_indices(A.var_size,1);C=C[:,F[0],F[1]];_tf.keras.backend.set_value(A.model_cov_stddev_i,_np.log(B)[:,A.input_indices]);_tf.keras.backend.set_value(A.model_cov_stddev_o,_np.log(B)[:,A.output_indices]);_tf.keras.backend.set_value(A.model_cov_stddev_o_spread,_np.ones((A.mix_size,A.output_dim))*-1.);_tf.keras.backend.set_value(A.model_m_i,D[:,A.input_indices]);_tf.keras.backend.set_value(A.model_m_o,D[:,A.output_indices]);_tf.keras.backend.set_value(A.model_cov_corr,_np.arctanh(C));_tf.keras.backend.set_value(A.model_w,weights)
-	def fit_gmms(B,np_inputs,tf_inputs,input_gmm,output_gmm,n_samples=10):C=n_samples;from sklearn.mixture import GaussianMixture as G;from.distribution import Distribution as D,DistributionSamples as E;H=D()(input_gmm);I=E(C)(H);J=D()(output_gmm);K=E(C)(J);L=_tf.reshape(_tf.transpose(_tf.concat([I,K],axis=1),[0,2,1]),[-1,B.var_size]);F=_tf.keras.Model(inputs=tf_inputs,outputs=L);F.compile();M=F.predict(np_inputs);A=G(n_components=B.mix_size);A.fit(M);B.set_gmm_values(means=A.means_,covariances=A.covariances_,weights=A.weights_)
+	def set_gmm_values(A,means,covariances,weights):E=covariances;D=means;B=_np.array([_np.sqrt(_np.diagonal(A))for A in E]);C=E/_np.reshape(B,[A.mix_size,1,A.var_size])/_np.reshape(B,[A.mix_size,A.var_size,1]);F=_np.triu_indices(A.var_size,1);C=C[:,F[0],F[1]];_tf.keras.backend.set_value(A.model_cov_stddev_i,_np.log(B)[:,A.input_indices]);_tf.keras.backend.set_value(A.model_cov_stddev_o,_np.log(B)[:,A.output_indices]);_tf.keras.backend.set_value(A.model_cov_stddev_o_spread,_np.ones((A.mix_size,A.output_dim))*-2.);_tf.keras.backend.set_value(A.model_m_i,D[:,A.input_indices]);_tf.keras.backend.set_value(A.model_m_o,D[:,A.output_indices]);_tf.keras.backend.set_value(A.model_cov_corr,_np.arctanh(C));_tf.keras.backend.set_value(A.model_w,weights)
+	def fit_gmms(B,np_inputs,tf_inputs,input_gmm,output_gmm,n_samples=10):C=n_samples;from sklearn.mixture import GaussianMixture as H;from.distribution import Distribution as D,DistributionSamples as E;I=D()(input_gmm);J=E(C)(I);K=D()(output_gmm);L=E(C)(K);M=_tf.reshape(_tf.transpose(_tf.concat([J,L],axis=1),[0,2,1]),[-1,B.var_size]);F=_tf.keras.Model(inputs=tf_inputs,outputs=M);F.compile();G=F.predict(np_inputs);A=H(n_components=B.mix_size);N=G[~_np.isnan(G).any(axis=1)];A.fit(N);B.set_gmm_values(means=A.means_,covariances=A.covariances_,weights=A.weights_)
 	def build(A,input_shapes):
 		C=input_shapes;B='uniform'
 		if not(_C in C and _D in C):raise Exception('means and weights are needed to construct the mapping layer!')
 		A.model_m_i=A.add_weight(name='mean_var_i',shape=(A.mix_size,A.input_dim),initializer=B,trainable=_B);A.model_cov_stddev_i=A.add_weight(name='cov_stddev_var_i',shape=(A.mix_size,A.input_dim),initializer=B,trainable=_A);A.model_m_o=A.add_weight(name='mean_var_o',shape=(A.mix_size,A.output_dim),initializer=B,trainable=_A);A.model_cov_stddev_o=A.add_weight(name='cov_stddev_var_o',shape=(A.mix_size,A.output_dim),initializer=B,trainable=_A);A.model_cov_stddev_o_spread=A.add_weight(name='cov_stddev_var_o_delta',shape=(A.mix_size,A.output_dim),initializer=B,trainable=_A);A.model_cov_corr=A.add_weight(name='cov_corr_var',shape=(A.mix_size,A.corr_size),initializer=B,trainable=_A);A.model_w=A.add_weight(name='weight_var',shape=(A.mix_size,),initializer=B,trainable=_B)
 	def call(A,x,**p):
-		b='baseCovMatrix';a='rowVec';Z='colVec';Y='covariances';M=x[_C];c=x[Y];d=x[_D];C=_tf.shape(M)[0];H=_tf.concat([A.model_m_i,A.model_m_o],axis=-1)
-		def e():B=_tf.concat([A.model_cov_stddev_i,A.model_cov_stddev_o],axis=-1);B=_tf.exp(B);return _tf.tile(_tf.expand_dims(_tf.matmul(_tf.reshape(B,[A.mix_size,A.var_size,1],name=Z),_tf.reshape(B,[A.mix_size,1,A.var_size],name=a),name=b),0),[C,1,1,1])
-		def f():D=_tfd.Normal(A.model_cov_stddev_o,_tf.exp(A.model_cov_stddev_o_spread)).sample(C);E=_tf.tile(_tf.expand_dims(A.model_cov_stddev_i,0),[C,1,1]);B=_tf.concat([E,D],axis=-1);B=_tf.exp(B);return _tf.matmul(_tf.reshape(B,[C,A.mix_size,A.var_size,1],name=Z),_tf.reshape(B,[C,A.mix_size,1,A.var_size],name=a),name=b)
+		b='baseCovMatrix';a='rowVec';Z='colVec';Y='covariances';M=x[_C];c=x[Y];d=x[_D];D=_tf.shape(M)[0];H=_tf.concat([A.model_m_i,A.model_m_o],axis=-1)
+		def e():B=_tf.concat([A.model_cov_stddev_i,A.model_cov_stddev_o],axis=-1);B=_tf.exp(B);return _tf.tile(_tf.expand_dims(_tf.matmul(_tf.reshape(B,[A.mix_size,A.var_size,1],name=Z),_tf.reshape(B,[A.mix_size,1,A.var_size],name=a),name=b),0),[D,1,1,1])
+		def f():C=_tfd.Normal(A.model_cov_stddev_o,_tf.exp(A.model_cov_stddev_o_spread)).sample(D);E=_tf.tile(_tf.expand_dims(A.model_cov_stddev_i,0),[D,1,1]);B=_tf.concat([E,C],axis=-1);B=_tf.exp(B);return _tf.matmul(_tf.reshape(B,[D,A.mix_size,A.var_size,1],name=Z),_tf.reshape(B,[D,A.mix_size,1,A.var_size],name=a),name=b)
 		N=_tf.cond(A.sampling,f,e);S=_tf.expand_dims(_tf.tanh(A.model_cov_corr),0);O=A.var_size-1;I=[-1]
 		for q in range(A.var_size-1):O=O-1;I.append(I[-1]+O)
 		with _tf.name_scope('TransposedCorrelation'):g=[[N[:,:,A,B]if A==B else N[:,:,A,B]*S[:,:,B+I[A]]if A<B else N[:,:,A,B]*S[:,:,A+I[B]]for A in range(A.var_size)]for B in range(A.var_size)]
-		D=_tf.transpose(g,[2,3,0,1],name='TransposeBack');H=_tf.expand_dims(_tf.expand_dims(H,0),2);h=_tf.expand_dims(_tf.expand_dims(A.model_w,0),2);D=_tf.expand_dims(D,2);T=_tf.stop_gradient(_tf.gather(_tf.gather(D,A.input_indices,axis=3),A.input_indices,axis=4));i=_tf.gather(_tf.gather(D,A.output_indices,axis=3),A.output_indices,axis=4);J=_tf.linalg.pinv(T);K=_tf.gather(_tf.gather(D,A.output_indices,axis=3),A.input_indices,axis=4);U=_tf.linalg.matrix_transpose(K);V=_tf.gather(H,A.input_indices,axis=3);j=_tf.gather(H,A.output_indices,axis=3);k=_tfd.MultivariateNormalTriL(loc=V,scale_tril=_tf.linalg.cholesky(T));W=_tf.shape(M);P=W[0];L=W[1];X=_tf.expand_dims(M,1);l=_tf.expand_dims(c,1);m=_tf.expand_dims(d,1);E=j+_tf.linalg.matmul(_tf.linalg.matmul(K,J),_tf.expand_dims(X-V,4))[:,:,:,:,0];n=_tf.linalg.matmul(_tf.linalg.matmul(_tf.linalg.matmul(_tf.linalg.matmul(K,J),l),J),U);F=i-_tf.matmul(_tf.matmul(K,J),U)+n;G=h*k.prob(X);Q=_tf.reduce_sum(G,axis=1,keepdims=_A);G=G/Q;G*=m;E=_tf.reshape(E,shape=[P,L*A.mix_size,A.output_dim]);F=_tf.reshape(F,shape=[P,L*A.mix_size,A.output_dim,A.output_dim]);B=_tf.reshape(G,shape=[P,L*A.mix_size])
-		if A.n_components:o=_tf.minimum(A.n_components,L*A.mix_size);R=_tf.math.top_k(B,k=o);B=R.values;Q=_tf.reduce_sum(B,axis=1,keepdims=_A);B=B/Q;E=_tf.gather(E,R.indices,axis=1,batch_dims=1);F=_tf.gather(F,R.indices,axis=1,batch_dims=1)
-		return{_C:E,Y:F,_D:B}
-	def get_config(A):B=super().get_config().copy();B.update({'output_dim':A.output_dim,'input_dim':A.input_dim,'mix_size':A.mix_size,'n_components  ':A.n_components});return B
+		B=_tf.transpose(g,[2,3,0,1],name='TransposeBack');H=_tf.expand_dims(_tf.expand_dims(H,0),2);h=_tf.expand_dims(_tf.expand_dims(A.model_w,0),2);B=_tf.expand_dims(B,2)
+		if A._r:B=_r(B,A._r)
+		T=_tf.stop_gradient(_tf.gather(_tf.gather(B,A.input_indices,axis=3),A.input_indices,axis=4));i=_tf.gather(_tf.gather(B,A.output_indices,axis=3),A.output_indices,axis=4);J=_tf.linalg.pinv(T,A._rcond);K=_tf.gather(_tf.gather(B,A.output_indices,axis=3),A.input_indices,axis=4);U=_tf.linalg.matrix_transpose(K);V=_tf.gather(H,A.input_indices,axis=3);j=_tf.gather(H,A.output_indices,axis=3);k=_tfd.MultivariateNormalTriL(loc=V,scale_tril=_tf.linalg.cholesky(T));W=_tf.shape(M);P=W[0];L=W[1];X=_tf.expand_dims(M,1);l=_tf.expand_dims(c,1);m=_tf.expand_dims(d,1);E=j+_tf.linalg.matmul(_tf.linalg.matmul(K,J),_tf.expand_dims(X-V,4))[:,:,:,:,0];n=_tf.linalg.matmul(_tf.linalg.matmul(_tf.linalg.matmul(_tf.linalg.matmul(K,J),l),J),U);F=i-_tf.matmul(_tf.matmul(K,J),U)+n;G=h*k.prob(X);Q=_tf.reduce_sum(G,axis=1,keepdims=_A);G=G/Q;G*=m;E=_tf.reshape(E,shape=[P,L*A.mix_size,A.output_dim]);F=_tf.reshape(F,shape=[P,L*A.mix_size,A.output_dim,A.output_dim]);C=_tf.reshape(G,shape=[P,L*A.mix_size])
+		if A.n_components:o=_tf.minimum(A.n_components,L*A.mix_size);R=_tf.math.top_k(C,k=o);C=R.values;Q=_tf.reduce_sum(C,axis=1,keepdims=_A);C=C/Q;E=_tf.gather(E,R.indices,axis=1,batch_dims=1);F=_tf.gather(F,R.indices,axis=1,batch_dims=1)
+		return{_C:E,Y:F,_D:C}
+	def get_config(A):B=super().get_config().copy();B.update({'output_dim':A.output_dim,'input_dim':A.input_dim,'mix_size':A.mix_size,'n_components  ':A.n_components,'regularize_cov_epsilon':A._r,'inv_rcond':A._rcond});return B
```

## Comparing `mixturemapping-0.5.0.dist-info/METADATA` & `mixturemapping-0.5.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mixturemapping
-Version: 0.5.0
+Version: 0.5.1
 Summary: UNKNOWN
 Home-page: https://vk.github.io/mixturemapping-doc/
 License: UNKNOWN
 Keywords: tensorflow distributions gaussian mixtures
 Platform: UNKNOWN
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python
```

## Comparing `mixturemapping-0.5.0.dist-info/RECORD` & `mixturemapping-0.5.1.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 mixturemapping/__init__.py,sha256=Pc2jdsv_ScDynuYETcEYZnND5AxG_FE2o5qyce2evgs,998
 mixturemapping/binning.py,sha256=H9amezvqpUqejl-PeZJuIzIPyKZrXmSY5oVSPdcdark,3984
-mixturemapping/distributions.py,sha256=b_X2J6TcFRA33V0O0VxODB_WUaSSZCLEYmOA_daW4Wo,3040
+mixturemapping/distributions.py,sha256=hgroFVFO6em5HH5GWzmZYa14a1rrG0u_yKjTOd1auZc,3066
 mixturemapping/utils.py,sha256=M2rYcP2pAhw6-s9BZiCdEWlJRP2nqpYUk3JrcWF1s-8,2077
 mixturemapping/layers/__init__.py,sha256=IZgHPul_1b19wvraT5tOBBeCvWvCyAw9ce0yrUEISg4,1385
 mixturemapping/layers/_mapping.py,sha256=Vzqw9x7ITj14YZt4ZSQbhfyiq7aSQmWyCh5zQQnRiio,1454
 mixturemapping/layers/binyield.py,sha256=XLEnk5U1pQ_4c-P1b0iwXO2YasjwlzRq4bxe9Lbv82I,2841
 mixturemapping/layers/covmatrix.py,sha256=nC9Dy1TcHmV6yN57aBPfvcF-CpDuXSLzGylaa0J3-yA,3044
-mixturemapping/layers/distribution.py,sha256=PWJOIQSdl2dsKv7MeXzSMrXAypf7z54qCb6VOTHn614,3671
+mixturemapping/layers/distribution.py,sha256=H1xMitTgp8ShJrcMrqC2yIhd1v9nlU7th5eMXsYKxLw,4015
 mixturemapping/layers/disttrafo.py,sha256=pXZvXBtgzIp_CJf2Jfwd7zcqNyys3e7XbAeFpW2XQg4,5956
 mixturemapping/layers/generalmapping.py,sha256=qVbWMQ8-IQ7m4IHHloRFJdlyFekqfXP4FwcL4LR4L-8,2802
-mixturemapping/layers/gmrmapping.py,sha256=CT2mdLMZllkIv9_h7mhHVMrPtRqSKuCjhxJxehcIGg4,6247
+mixturemapping/layers/gmrmapping.py,sha256=08KhekyDHEXUUuYjs3nuMZGiA9r7UsXQu1di_mDKbLY,6506
 mixturemapping/layers/linearmapping.py,sha256=hpzsbVloEqjl1wxshmnGKN1eFg40hdXj_Cf3QlJgB-A,2475
 mixturemapping/layers/normalization.py,sha256=w6mybEvZUF55d1yBQXxeJmEW5fST3xi3E8fuwjgnXAo,2205
-mixturemapping-0.5.0.dist-info/METADATA,sha256=Vbv7ccjLuHsEe4U3kUZ62ZagFGH7UdsZFpm4qbmxK3M,2625
-mixturemapping-0.5.0.dist-info/WHEEL,sha256=ijjRDmPkGsL9eKpOeSzmTjLbiyw0Dy8TY4QGa2Zy9J8,93
-mixturemapping-0.5.0.dist-info/top_level.txt,sha256=-JhRdJENTVXY2H6LFXYeiMioedZlaZbhtIrE5suMLnI,15
-mixturemapping-0.5.0.dist-info/RECORD,,
+mixturemapping-0.5.1.dist-info/METADATA,sha256=HvMliA_R3LKzm2AFz8pzXUA31frRid8qXIssgstO-qE,2625
+mixturemapping-0.5.1.dist-info/WHEEL,sha256=ijjRDmPkGsL9eKpOeSzmTjLbiyw0Dy8TY4QGa2Zy9J8,93
+mixturemapping-0.5.1.dist-info/top_level.txt,sha256=-JhRdJENTVXY2H6LFXYeiMioedZlaZbhtIrE5suMLnI,15
+mixturemapping-0.5.1.dist-info/RECORD,,
```

