# Comparing `tmp/modelscope-1.5.2.tar.gz` & `tmp/modelscope-1.6.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/modelscope-1.5.2.tar", last modified: Sat Apr 22 02:47:25 2023, max compression
+gzip compressed data, was "dist/modelscope-1.6.0.tar", last modified: Mon May 22 03:10:13 2023, max compression
```

## Comparing `modelscope-1.5.2.tar` & `modelscope-1.6.0.tar`

### file list

```diff
@@ -1,2787 +1,2766 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/
--rw-r--r--   0 runner    (1001) docker     (122)       57 2023-04-22 02:47:24.000000 modelscope-1.5.2/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (122)    16425 2023-04-22 02:47:25.000000 modelscope-1.5.2/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)    15442 2023-04-22 02:47:24.000000 modelscope-1.5.2/README.md
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/
--rw-r--r--   0 runner    (1001) docker     (122)      156 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/cli/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      404 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      829 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/cli.py
--rw-r--r--   0 runner    (1001) docker     (122)     1230 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/download.py
--rw-r--r--   0 runner    (1001) docker     (122)     6240 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/modelcard.py
--rw-r--r--   0 runner    (1001) docker     (122)     4369 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/cli/plugins.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/configs/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/configs/examples/
--rw-r--r--   0 runner    (1001) docker     (122)       36 2023-04-22 02:46:12.000000 modelscope-1.5.2/modelscope/configs/examples/configuration.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/exporters/
--rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2667 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      732 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/exporters/cv/
--rw-r--r--   0 runner    (1001) docker     (122)      869 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2593 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/cv/cartoon_translation_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3619 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/cv/face_detection_scrfd_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     1245 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/cv/object_detection_damoyolo_exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/exporters/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7341 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/nlp/csanmt_for_translation_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     4579 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/nlp/model_for_token_classification_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     3409 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     2316 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)     4809 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/tf_model_exporter.py
--rw-r--r--   0 runner    (1001) docker     (122)    15145 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/exporters/torch_model_exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/fileio/
--rw-r--r--   0 runner    (1001) docker     (122)      122 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/file.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/fileio/format/
--rw-r--r--   0 runner    (1001) docker     (122)      143 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/format/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      454 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/format/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1050 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/format/json.py
--rw-r--r--   0 runner    (1001) docker     (122)    13853 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/format/jsonplus.py
--rw-r--r--   0 runner    (1001) docker     (122)      669 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/format/yaml.py
--rw-r--r--   0 runner    (1001) docker     (122)     4254 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/fileio/io.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/hub/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    40149 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/api.py
--rw-r--r--   0 runner    (1001) docker     (122)     3649 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/check_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1395 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/constants.py
--rw-r--r--   0 runner    (1001) docker     (122)    11358 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/deploy.py
--rw-r--r--   0 runner    (1001) docker     (122)     3728 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/errors.py
--rw-r--r--   0 runner    (1001) docker     (122)    10284 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/file_download.py
--rw-r--r--   0 runner    (1001) docker     (122)     8958 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/git.py
--rw-r--r--   0 runner    (1001) docker     (122)     4157 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/push_to_hub.py
--rw-r--r--   0 runner    (1001) docker     (122)    12489 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/repository.py
--rw-r--r--   0 runner    (1001) docker     (122)     6535 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/snapshot_download.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/hub/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9916 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/utils/caching.py
--rw-r--r--   0 runner    (1001) docker     (122)     2978 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/hub/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    54236 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metainfo.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/metrics/
--rw-r--r--   0 runner    (1001) docker     (122)     3829 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/accuracy_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     7530 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/action_detection_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (122)     1925 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/audio_noise_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1454 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1726 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/bleu_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3585 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/metrics/ciderD/
--rwxr-xr-x   0 runner    (1001) docker     (122)       21 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/ciderD/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1926 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/ciderD/ciderD.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8931 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/ciderD/ciderD_scorer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8664 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_color_enhance_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)    13387 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_colorization_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)    10011 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_denoise_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_inpainting_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)    13318 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_instance_segmentation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1739 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_portrait_enhancement_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_quality_assessment_degradation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1632 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/image_quality_assessment_mos_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2231 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/inbatch_recall_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/loss_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3002 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/map_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2032 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/movie_scene_segmentation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/ned_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2310 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/ocr_recognition_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     2082 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/ppl_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     1163 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/prediction_saving_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     4453 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/referring_video_object_segmentation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3112 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/sequence_classification_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/text_generation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/text_ranking_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     5765 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/token_classification_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     5435 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_frame_interpolation_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     8655 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_stabilization_metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3092 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_summarization_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7030 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/matlab_functions.py
--rw-r--r--   0 runner    (1001) docker     (122)     4771 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/metric_util.py
--rw-r--r--   0 runner    (1001) docker     (122)     8932 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/niqe.py
--rw-r--r--   0 runner    (1001) docker     (122)     1710 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/
--rw-r--r--   0 runner    (1001) docker     (122)      519 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/aec/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1434 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/activations.py
--rw-r--r--   0 runner    (1001) docker     (122)     2700 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/affine_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)     5630 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/deep_fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1322 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/layer_base.py
--rw-r--r--   0 runner    (1001) docker     (122)    14384 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/layers/uni_deep_fsmn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/aec/network/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14438 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/network/loss.py
--rw-r--r--   0 runner    (1001) docker     (122)     9382 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/network/modulation_loss.py
--rw-r--r--   0 runner    (1001) docker     (122)    15396 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/aec/network/se_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/ans/
--rw-r--r--   0 runner    (1001) docker     (122)      515 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6475 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/complex_nn.py
--rw-r--r--   0 runner    (1001) docker     (122)     3694 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/conv_stft.py
--rw-r--r--   0 runner    (1001) docker     (122)     2513 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/denoise_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/frcrn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/ans/layers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1468 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/layers/activations.py
--rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/layers/affine_transform.py
--rw-r--r--   0 runner    (1001) docker     (122)      661 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/layers/layer_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5284 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/layers/uni_deep_fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1038 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/se_module_complex.py
--rw-r--r--   0 runner    (1001) docker     (122)     9833 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/ans/unet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/asr/
--rw-r--r--   0 runner    (1001) docker     (122)      585 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/asr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/asr/generic_automatic_speech_recognition.py
--rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/itn/
--rw-r--r--   0 runner    (1001) docker     (122)      557 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/itn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1462 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/itn/generic_inverse_text_processing.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/kws/
--rw-r--r--   0 runner    (1001) docker     (122)      735 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/kws/farfield/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/farfield/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14349 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/farfield/fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     7462 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)     2806 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/farfield/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     2608 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/farfield/model_def.py
--rw-r--r--   0 runner    (1001) docker     (122)      908 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/generic_key_word_spotting.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/kws/nearfield/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/nearfield/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3300 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/nearfield/cmvn.py
--rw-r--r--   0 runner    (1001) docker     (122)    17496 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/nearfield/fsmn.py
--rw-r--r--   0 runner    (1001) docker     (122)     6079 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/kws/nearfield/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/punc/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/punc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1466 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/punc/generic_punctuation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/separation/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/separation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2240 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/separation/layer_norm.py
--rw-r--r--   0 runner    (1001) docker     (122)    13963 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/separation/mossformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9142 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/separation/mossformer_block.py
--rw-r--r--   0 runner    (1001) docker     (122)     9004 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/separation/mossformer_conv_module.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/sv/
--rw-r--r--   0 runner    (1001) docker     (122)     6544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/sv/DTDNN.py
--rw-r--r--   0 runner    (1001) docker     (122)     8425 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/sv/DTDNN_layers.py
--rw-r--r--   0 runner    (1001) docker     (122)      498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/sv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14531 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/sv/ecapa_tdnn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/sv/generic_speaker_verification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/audio/tts/
--rw-r--r--   0 runner    (1001) docker     (122)      474 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/tts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11845 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/tts/sambert_hifi.py
--rw-r--r--   0 runner    (1001) docker     (122)    26394 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/audio/tts/voice.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/base/
--rw-r--r--   0 runner    (1001) docker     (122)      303 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/base/base_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     6994 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/base/base_model.py
--rw-r--r--   0 runner    (1001) docker     (122)      605 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/base/base_torch_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     5679 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/base/base_torch_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     3647 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/
--rw-r--r--   0 runner    (1001) docker     (122)     1872 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      490 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3806 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/
--rw-r--r--   0 runner    (1001) docker     (122)      113 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/
--rw-r--r--   0 runner    (1001) docker     (122)      211 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/
--rw-r--r--   0 runner    (1001) docker     (122)      145 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6729 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      493 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7307 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/action_detection_onnx.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9364 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_detection/modules/resnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/action_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      709 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4304 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_recognition/models.py
--rw-r--r--   0 runner    (1001) docker     (122)    10276 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_recognition/s3dg.py
--rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_recognition/tada_convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)    45442 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/animal_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      558 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/animal_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14141 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/animal_recognition/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     3955 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/animal_recognition/splat.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/bad_image_detecting/
--rw-r--r--   0 runner    (1001) docker     (122)      496 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/bad_image_detecting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2589 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      502 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12660 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py
--rw-r--r--   0 runner    (1001) docker     (122)     8776 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/w48.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      601 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/
--rw-r--r--   0 runner    (1001) docker     (122)       51 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8957 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py
--rw-r--r--   0 runner    (1001) docker     (122)     8196 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/
--rw-r--r--   0 runner    (1001) docker     (122)       98 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11552 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    12870 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/block.py
--rw-r--r--   0 runner    (1001) docker     (122)     8080 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py
--rw-r--r--   0 runner    (1001) docker     (122)     2516 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7719 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/
--rw-r--r--   0 runner    (1001) docker     (122)     1216 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/LK/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/LK/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/LK/lk.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      713 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/config.py
--rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/face_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     5161 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/face_landmark.py
--rw-r--r--   0 runner    (1001) docker     (122)     4563 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/facer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9075 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/loss.py
--rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/model_tf.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6072 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py
--rw-r--r--   0 runner    (1001) docker     (122)     8519 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py
--rw-r--r--   0 runner    (1001) docker     (122)     4404 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/network.py
--rw-r--r--   0 runner    (1001) docker     (122)     5265 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cartoon/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/
--rw-r--r--   0 runner    (1001) docker     (122)      647 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3876 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/c3d.py
--rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py
--rw-r--r--   0 runner    (1001) docker     (122)     8949 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      414 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13516 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/annotator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4960 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      504 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    10152 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py
--rw-r--r--   0 runner    (1001) docker     (122)     2746 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5829 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py
--rw-r--r--   0 runner    (1001) docker     (122)     8045 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    15438 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py
--rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9896 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py
--rw-r--r--   0 runner    (1001) docker     (122)    25158 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12595 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
--rw-r--r--   0 runner    (1001) docker     (122)     3894 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
--rw-r--r--   0 runner    (1001) docker     (122)     9024 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     8031 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
--rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/controlnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/crowd_counting/
--rw-r--r--   0 runner    (1001) docker     (122)      491 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/crowd_counting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1100 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/crowd_counting/cc_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    22878 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
--rw-r--r--   0 runner    (1001) docker     (122)      872 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/easycv_base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      500 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      631 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_2d_keypoints/face_2d_keypoints_align.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_attribute_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      490 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_attribute_recognition/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_attribute_recognition/fair_face/
--rw-r--r--   0 runner    (1001) docker     (122)      115 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2559 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      930 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/detectors.py
--rw-r--r--   0 runner    (1001) docker     (122)     4675 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/mogface.py
--rw-r--r--   0 runner    (1001) docker     (122)     5578 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/mogprednet.py
--rw-r--r--   0 runner    (1001) docker     (122)     6264 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/resnet.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     7072 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/
--rw-r--r--   0 runner    (1001) docker     (122)       97 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7061 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5415 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py
--rw-r--r--   0 runner    (1001) docker     (122)     5232 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3444 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3578 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     5132 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py
--rw-r--r--   0 runner    (1001) docker     (122)     4210 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/facer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4832 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/models/
--rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/models/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4766 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/models/net.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4590 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/models/retinaface.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/
--rw-r--r--   0 runner    (1001) docker     (122)      174 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      955 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/damofd_detect.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/
--rwxr-xr-x   0 runner    (1001) docker     (122)      192 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/
--rw-r--r--   0 runner    (1001) docker     (122)      325 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2945 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/
--rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      276 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/
--rwxr-xr-x   0 runner    (1001) docker     (122)      471 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11707 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
--rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
--rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    30670 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5543 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/
--rwxr-xr-x   0 runner    (1001) docker     (122)      289 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/
--rwxr-xr-x   0 runner    (1001) docker     (122)      361 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4202 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     3590 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
--rw-r--r--   0 runner    (1001) docker     (122)    16146 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/
--rwxr-xr-x   0 runner    (1001) docker     (122)      270 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    46999 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/
--rwxr-xr-x   0 runner    (1001) docker     (122)      295 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11062 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6092 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
--rw-r--r--   0 runner    (1001) docker     (122)     6395 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5970 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py
--rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
--rw-r--r--   0 runner    (1001) docker     (122)      960 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/
--rw-r--r--   0 runner    (1001) docker     (122)       90 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1477 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4126 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2058 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      571 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py
--rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     3719 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py
--rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py
--rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/
--rw-r--r--   0 runner    (1001) docker     (122)      540 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/efficient/
--rw-r--r--   0 runner    (1001) docker     (122)      327 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/efficient/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15911 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/efficient/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    20077 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/efficient/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/emotion_infer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/emotion_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/face_alignment/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/face_alignment/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2844 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/face_alignment/face.py
--rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_emotion/face_alignment/face_align.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      475 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/op/
--rwxr-xr-x   0 runner    (1001) docker     (122)       89 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/op/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6497 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/op/conv2d_gradfix.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3104 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/op/fused_act.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5922 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/op/upfirdn2d.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    19998 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_generation/stylegan2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4109 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/det_infer.py
--rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/ghost_pan.py
--rw-r--r--   0 runner    (1001) docker     (122)    16846 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1833 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     5791 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py
--rw-r--r--   0 runner    (1001) docker     (122)     8874 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/align_face.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/
--rwxr-xr-x   0 runner    (1001) docker     (122)      473 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/
--rwxr-xr-x   0 runner    (1001) docker     (122)     1080 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1977 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8551 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4744 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    31337 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/bfm.py
--rw-r--r--   0 runner    (1001) docker     (122)     1428 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2583 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5124 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    30403 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facerecon_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    13171 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)    21235 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/networks.py
--rw-r--r--   0 runner    (1001) docker     (122)     8821 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
--rw-r--r--   0 runner    (1001) docker     (122)      277 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/opt.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    31957 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py
--rw-r--r--   0 runner    (1001) docker     (122)     5908 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py
--rw-r--r--   0 runner    (1001) docker     (122)      779 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    13410 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/renderer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)    31247 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/face_reconstruction/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      484 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/
--rw-r--r--   0 runner    (1001) docker     (122)      121 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
--rw-r--r--   0 runner    (1001) docker     (122)     3277 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/vgg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/
--rw-r--r--   0 runner    (1001) docker     (122)      478 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/flc/
--rw-r--r--   0 runner    (1001) docker     (122)      115 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
--rw-r--r--   0 runner    (1001) docker     (122)     4932 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/hand_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/hand_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      602 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/hand_2d_keypoints/hand_2d_keypoints.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/hand_static/
--rw-r--r--   0 runner    (1001) docker     (122)      502 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/hand_static/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2480 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/hand_static/hand_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    10891 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/hand_static/networks.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/
--rw-r--r--   0 runner    (1001) docker     (122)     5936 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/Reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/
--rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/Embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/PixToMesh.py
--rw-r--r--   0 runner    (1001) docker     (122)    11301 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/Res_backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     2418 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/Surface_head.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2716 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/detectors.py
--rw-r--r--   0 runner    (1001) docker     (122)     1982 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/geometry.py
--rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/human_segmenter.py
--rw-r--r--   0 runner    (1001) docker     (122)    11865 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/networks.py
--rw-r--r--   0 runner    (1001) docker     (122)     6111 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_reconstruction/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/human_wholebody_keypoint/
--rw-r--r--   0 runner    (1001) docker     (122)      530 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_wholebody_keypoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      636 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      573 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2859 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    19111 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/bnext.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/
--rw-r--r--   0 runner    (1001) docker     (122)      500 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3920 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
--rw-r--r--   0 runner    (1001) docker     (122)     6541 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    13616 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/person_info.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12027 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py
--rw-r--r--   0 runner    (1001) docker     (122)     5673 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py
--rw-r--r--   0 runner    (1001) docker     (122)    15760 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/slim_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      598 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      107 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    20295 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/backbones/beit_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)    17261 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/backbones/nextvit.py
--rw-r--r--   0 runner    (1001) docker     (122)     2185 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/mmcls_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1554 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/resnet50_cc.py
--rw-r--r--   0 runner    (1001) docker     (122)     5255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_classification/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/
--rw-r--r--   0 runner    (1001) docker     (122)      704 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/adaint/
--rw-r--r--   0 runner    (1001) docker     (122)       44 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/adaint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14338 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/adaint/adaint.py
--rw-r--r--   0 runner    (1001) docker     (122)     3753 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/csrnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/deeplpf/
--rw-r--r--   0 runner    (1001) docker     (122)       66 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/deeplpf/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2575 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py
--rw-r--r--   0 runner    (1001) docker     (122)    31764 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     2821 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_color_enhance/image_color_enhance.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/
--rw-r--r--   0 runner    (1001) docker     (122)      640 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/
--rw-r--r--   0 runner    (1001) docker     (122)      122 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9385 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py
--rw-r--r--   0 runner    (1001) docker     (122)     6437 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py
--rw-r--r--   0 runner    (1001) docker     (122)     9576 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/loss.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6857 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     7796 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     6465 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/unet/
--rw-r--r--   0 runner    (1001) docker     (122)      129 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/unet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10574 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/unet/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)    10647 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_colorization/unet/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_debanding/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_debanding/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_debanding/rrdb/
--rw-r--r--   0 runner    (1001) docker     (122)       53 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_debanding/rrdb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3005 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_deblur/
--rw-r--r--   0 runner    (1001) docker     (122)      510 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_deblur/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4241 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4329 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12474 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py
--rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/
--rw-r--r--   0 runner    (1001) docker     (122)      448 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6827 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7064 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py
--rw-r--r--   0 runner    (1001) docker     (122)    10779 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py
--rw-r--r--   0 runner    (1001) docker     (122)     1220 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py
--rw-r--r--   0 runner    (1001) docker     (122)     1732 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     4770 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    23599 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py
--rw-r--r--   0 runner    (1001) docker     (122)     4803 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     3595 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py
--rw-r--r--   0 runner    (1001) docker     (122)      583 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     2617 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py
--rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/
--rw-r--r--   0 runner    (1001) docker     (122)      514 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet/
--rw-r--r--   0 runner    (1001) docker     (122)     6664 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1627 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet/arch_util.py
--rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6757 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py
--rw-r--r--   0 runner    (1001) docker     (122)    18233 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py
--rw-r--r--   0 runner    (1001) docker     (122)    10106 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    26006 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    13560 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1600 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/newcrfs_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2606 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1485 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     2819 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     2506 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     8545 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_driving_perception/
--rw-r--r--   0 runner    (1001) docker     (122)      999 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_driving_perception/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_driving_perception/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     7475 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_driving_perception/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3124 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py
--rw-r--r--   0 runner    (1001) docker     (122)    21432 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/
--rw-r--r--   0 runner    (1001) docker     (122)      242 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6497 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py
--rw-r--r--   0 runner    (1001) docker     (122)     3094 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py
--rw-r--r--   0 runner    (1001) docker     (122)     5912 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facelib/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facelib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10514 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facelib/align_trans.py
--rw-r--r--   0 runner    (1001) docker     (122)     5955 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py
--rw-r--r--   0 runner    (1001) docker     (122)     9077 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/image_face_fusion.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3045 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/aad_layer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8555 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     9366 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/bfm.py
--rw-r--r--   0 runner    (1001) docker     (122)    12449 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/dense_motion.py
--rw-r--r--   0 runner    (1001) docker     (122)    20604 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/facerecon_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7433 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/model_irse.py
--rw-r--r--   0 runner    (1001) docker     (122)     7790 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/ops.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/
--rw-r--r--   0 runner    (1001) docker     (122)      575 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      525 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11736 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/
--rw-r--r--   0 runner    (1001) docker     (122)      640 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8257 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     8361 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    15097 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5633 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_human_parsing/parsing_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/
--rw-r--r--   0 runner    (1001) docker     (122)      475 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2690 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7605 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/default.py
--rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ade20k/
--rw-r--r--   0 runner    (1001) docker     (122)       81 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12202 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ade20k/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     5348 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     7509 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/adversarial.py
--rw-r--r--   0 runner    (1001) docker     (122)     1564 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/feature_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)    19052 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ffc.py
--rw-r--r--   0 runner    (1001) docker     (122)    11842 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/inception.py
--rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/perceptual.py
--rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
--rw-r--r--   0 runner    (1001) docker     (122)    13350 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_inpainting/refinement.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      984 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      574 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    27174 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9305 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      101 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3992 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/
--rw-r--r--   0 runner    (1001) docker     (122)      600 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10072 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    15407 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    18188 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     7274 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
--rw-r--r--   0 runner    (1001) docker     (122)     2720 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     6712 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1429 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    11454 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
--rw-r--r--   0 runner    (1001) docker     (122)     1549 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7904 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/
--rw-r--r--   0 runner    (1001) docker     (122)      553 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/config/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6991 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/config/default.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/
--rw-r--r--   0 runner    (1001) docker     (122)      171 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      588 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7244 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     3784 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/
--rw-r--r--   0 runner    (1001) docker     (122)      239 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     3012 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     9677 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10531 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     3048 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py
--rw-r--r--   0 runner    (1001) docker     (122)     2132 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)     2735 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/quadtree_attention_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      344 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_matching/utils/misc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      521 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    18181 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9864 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     9404 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    21735 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/module.py
--rw-r--r--   0 runner    (1001) docker     (122)     3367 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_paintbyexample/
--rw-r--r--   0 runner    (1001) docker     (122)      507 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_paintbyexample/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_paintbyexample/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      571 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1694 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
--rw-r--r--   0 runner    (1001) docker     (122)      644 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8563 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/align_faces.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/eqface/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1836 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
--rw-r--r--   0 runner    (1001) docker     (122)     4710 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    22869 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/gpen.py
--rw-r--r--   0 runner    (1001) docker     (122)     7114 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4336 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py
--rw-r--r--   0 runner    (1001) docker     (122)     3286 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)     3155 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     7313 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/
--rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4845 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4676 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_probing_model/
--rw-r--r--   0 runner    (1001) docker     (122)      533 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_probing_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10076 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_probing_model/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     3319 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_probing_model/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5206 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_probing_model/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/
--rw-r--r--   0 runner    (1001) docker     (122)      584 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5064 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4654 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2600 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py
--rw-r--r--   0 runner    (1001) docker     (122)     5163 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/maniqa.py
--rw-r--r--   0 runner    (1001) docker     (122)    23230 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/swin.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      272 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16222 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     1060 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/heads/
--rw-r--r--   0 runner    (1001) docker     (122)       36 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      529 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2651 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_reid_person/
--rw-r--r--   0 runner    (1001) docker     (122)      467 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_reid_person/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5444 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_reid_person/pass_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    14108 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_reid_person/transreid_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_restoration/
--rw-r--r--   0 runner    (1001) docker     (122)      527 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_restoration/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_restoration/demoire_models/
--rw-r--r--   0 runner    (1001) docker     (122)       69 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_restoration/demoire_models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10243 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_restoration/demoire_models/nets.py
--rw-r--r--   0 runner    (1001) docker     (122)     2640 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_restoration/image_restoration_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      712 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7896 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
--rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
--rw-r--r--   0 runner    (1001) docker     (122)     5155 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
--rw-r--r--   0 runner    (1001) docker     (122)     2273 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3208 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/
--rw-r--r--   0 runner    (1001) docker     (122)      111 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1516 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2122 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py
--rw-r--r--   0 runner    (1001) docker     (122)      618 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/segformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2569 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/
--rw-r--r--   0 runner    (1001) docker     (122)      303 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/
--rw-r--r--   0 runner    (1001) docker     (122)      290 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      249 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17484 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/
--rw-r--r--   0 runner    (1001) docker     (122)      196 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18241 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py
--rw-r--r--   0 runner    (1001) docker     (122)     6379 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      251 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    26514 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/
--rw-r--r--   0 runner    (1001) docker     (122)      253 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12370 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py
--rw-r--r--   0 runner    (1001) docker     (122)    11715 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      368 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      398 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)     1938 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py
--rw-r--r--   0 runner    (1001) docker     (122)     1788 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/
--rw-r--r--   0 runner    (1001) docker     (122)      650 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9312 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/
--rw-r--r--   0 runner    (1001) docker     (122)     3740 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21278 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py
--rw-r--r--   0 runner    (1001) docker     (122)    18029 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py
--rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/unet.py
--rw-r--r--   0 runner    (1001) docker     (122)    11210 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/skychange.py
--rw-r--r--   0 runner    (1001) docker     (122)     7874 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_skychange/skychange_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      120 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/data/
--rw-r--r--   0 runner    (1001) docker     (122)      561 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/data/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    10476 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/
--rw-r--r--   0 runner    (1001) docker     (122)      603 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/clip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/
--rw-r--r--   0 runner    (1001) docker     (122)      561 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    24469 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/losses.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/data/
--rw-r--r--   0 runner    (1001) docker     (122)      127 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/data/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    10527 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/model_translation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/models/
--rw-r--r--   0 runner    (1001) docker     (122)      161 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/models/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/models/clip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/
--rw-r--r--   0 runner    (1001) docker     (122)      384 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21828 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/apps.py
--rw-r--r--   0 runner    (1001) docker     (122)    36549 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/degradation.py
--rw-r--r--   0 runner    (1001) docker     (122)    24615 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/metrics.py
--rw-r--r--   0 runner    (1001) docker     (122)     6736 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/random_color.py
--rw-r--r--   0 runner    (1001) docker     (122)     2745 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/random_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     3777 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/svd.py
--rw-r--r--   0 runner    (1001) docker     (122)     6728 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      486 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)      136 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4099 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py
--rw-r--r--   0 runner    (1001) docker     (122)     6931 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py
--rw-r--r--   0 runner    (1001) docker     (122)     3251 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py
--rw-r--r--   0 runner    (1001) docker     (122)    14887 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/
--rw-r--r--   0 runner    (1001) docker     (122)       86 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5518 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py
--rw-r--r--   0 runner    (1001) docker     (122)     3406 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py
--rw-r--r--   0 runner    (1001) docker     (122)     4808 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1834 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/panovit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/
--rwxr-xr-x   0 runner    (1001) docker     (122)      542 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6441 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/summarizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/
--rwxr-xr-x   0 runner    (1001) docker     (122)      508 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1900 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     7275 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/models.py
--rwxr-xr-x   0 runner    (1001) docker     (122)      826 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2690 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/
--rw-r--r--   0 runner    (1001) docker     (122)      639 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2151 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/cfg_sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)    26783 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    13594 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/mdm.py
--rw-r--r--   0 runner    (1001) docker     (122)     5412 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/respace.py
--rw-r--r--   0 runner    (1001) docker     (122)     3914 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/rotation2xyz.py
--rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/smpl.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      615 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/get_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     8465 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      181 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      798 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4291 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py
--rw-r--r--   0 runner    (1001) docker     (122)    10325 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     5057 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/trn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/
--rw-r--r--   0 runner    (1001) docker     (122)      602 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/dataloader/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/dataloader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18782 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    20567 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     7297 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     7876 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13317 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/nerf.py
--rw-r--r--   0 runner    (1001) docker     (122)     1512 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/segmenter.py
--rw-r--r--   0 runner    (1001) docker     (122)     5720 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      606 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      620 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/dino.py
--rw-r--r--   0 runner    (1001) docker     (122)     3432 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/
--rw-r--r--   0 runner    (1001) docker     (122)      321 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      211 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21306 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      278 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2220 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    11529 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/necks/
--rw-r--r--   0 runner    (1001) docker     (122)      213 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9021 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      426 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      375 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8542 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      239 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17739 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      306 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21926 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py
--rw-r--r--   0 runner    (1001) docker     (122)      832 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection/yolox_pai.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/
--rw-r--r--   0 runner    (1001) docker     (122)      467 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/
--rw-r--r--   0 runner    (1001) docker     (122)       86 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2657 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/depe_detect.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/
--rw-r--r--   0 runner    (1001) docker     (122)      605 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/
--rw-r--r--   0 runner    (1001) docker     (122)      299 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6670 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/
--rw-r--r--   0 runner    (1001) docker     (122)      282 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/
--rw-r--r--   0 runner    (1001) docker     (122)      276 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1064 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py
--rw-r--r--   0 runner    (1001) docker     (122)     2041 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      294 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3095 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/
--rw-r--r--   0 runner    (1001) docker     (122)      522 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7936 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py
--rw-r--r--   0 runner    (1001) docker     (122)     8689 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12874 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/
--rw-r--r--   0 runner    (1001) docker     (122)      282 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7442 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    59197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/
--rw-r--r--   0 runner    (1001) docker     (122)      255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9533 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/
--rw-r--r--   0 runner    (1001) docker     (122)      256 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9032 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      562 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18126 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5175 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py
--rw-r--r--   0 runner    (1001) docker     (122)    11059 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/result_vis.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      473 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2870 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18335 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/modules/dbnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     8217 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py
--rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     7972 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_detection/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      477 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5848 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)      517 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/convnextvit.py
--rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/crnn.py
--rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/vitstr.py
--rw-r--r--   0 runner    (1001) docker     (122)     3603 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/ocr_recognition/preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/open_vocabulary_detection_vild/
--rw-r--r--   0 runner    (1001) docker     (122)      501 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/open_vocabulary_detection_vild/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14249 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/open_vocabulary_detection_vild/vild.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      511 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)      102 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5077 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/equi.py
--rw-r--r--   0 runner    (1001) docker     (122)     7503 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/layers.py
--rw-r--r--   0 runner    (1001) docker     (122)     7839 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
--rw-r--r--   0 runner    (1001) docker     (122)    14421 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9175 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
--rw-r--r--   0 runner    (1001) docker     (122)     3962 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/util.py
--rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/pedestrian_attribute_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pedestrian_attribute_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3468 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pedestrian_attribute_recognition/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      495 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16147 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
--rw-r--r--   0 runner    (1001) docker     (122)    11898 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1797 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/
--rw-r--r--   0 runner    (1001) docker     (122)      548 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    20974 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/item_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/item_embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     4340 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/item_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/product_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      528 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7978 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_segmentation/net.py
--rw-r--r--   0 runner    (1001) docker     (122)     2194 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/product_segmentation/seg_infer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      514 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5832 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      318 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8504 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     9206 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py
--rw-r--r--   0 runner    (1001) docker     (122)     7299 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py
--rw-r--r--   0 runner    (1001) docker     (122)     7935 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (122)     6242 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py
--rw-r--r--   0 runner    (1001) docker     (122)    16610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2091 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py
--rw-r--r--   0 runner    (1001) docker     (122)     5629 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py
--rw-r--r--   0 runner    (1001) docker     (122)     5180 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py
--rw-r--r--   0 runner    (1001) docker     (122)    27941 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/robust_image_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      501 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/robust_image_classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/robust_image_classification/easyrobust_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      497 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/
--rw-r--r--   0 runner    (1001) docker     (122)      213 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)     6558 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py
--rw-r--r--   0 runner    (1001) docker     (122)      256 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6525 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/modules.py
--rw-r--r--   0 runner    (1001) docker     (122)     3047 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/senet.py
--rw-r--r--   0 runner    (1001) docker     (122)    12007 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/u2net.py
--rw-r--r--   0 runner    (1001) docker     (122)     3524 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/models/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2704 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/salient_detection/salient_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      464 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2203 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     4385 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/head_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)    32989 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/models.py
--rw-r--r--   0 runner    (1001) docker     (122)     9319 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/neck_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     5632 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/shop_seg_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4095 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/shop_seg_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6699 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/shop_segmentation/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/detection_model/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/detection_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/detection_model/detection_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/inpainting_model/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/inpainting_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5759 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py
--rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10898 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/box_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3911 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/net.py
--rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/network.py
--rw-r--r--   0 runner    (1001) docker     (122)     5003 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/predict_single.py
--rw-r--r--   0 runner    (1001) docker     (122)     1035 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/prior_box.py
--rw-r--r--   0 runner    (1001) docker     (122)     2110 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/utils.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3890 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/unet_deploy.py
--rw-r--r--   0 runner    (1001) docker     (122)     9917 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1169 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/skin_retouching/weights_init.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/
--rw-r--r--   0 runner    (1001) docker     (122)      526 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/data/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2094 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/data/data_augment.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/
--rw-r--r--   0 runner    (1001) docker     (122)      165 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      274 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/base_exp.py
--rw-r--r--   0 runner    (1001) docker     (122)      392 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/build.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/default/
--rw-r--r--   0 runner    (1001) docker     (122)      137 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/default/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py
--rw-r--r--   0 runner    (1001) docker     (122)     1845 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/yolox_base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/
--rw-r--r--   0 runner    (1001) docker     (122)      238 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/darknet.py
--rw-r--r--   0 runner    (1001) docker     (122)    10911 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/network_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     1314 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/streamyolo.py
--rw-r--r--   0 runner    (1001) docker     (122)     5669 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/tal_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/realtime_video_detector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      270 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3681 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/utils/boxes.py
--rw-r--r--   0 runner    (1001) docker     (122)      209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/stream_yolo/utils/format.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/super_resolution/
--rw-r--r--   0 runner    (1001) docker     (122)      555 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/super_resolution/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7652 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/super_resolution/arch_util.py
--rw-r--r--   0 runner    (1001) docker     (122)    10508 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/super_resolution/ecb.py
--rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/super_resolution/ecbsr_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/super_resolution/rrdbnet_arch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/
--rw-r--r--   0 runner    (1001) docker     (122)      462 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15842 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/lineless_table_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     3360 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/model_lore.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12614 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/modules/lore_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)    15090 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/table_recognition/modules/lore_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)      777 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7587 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     4143 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6145 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_net.py
--rw-r--r--   0 runner    (1001) docker     (122)    18951 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_vit.py
--rw-r--r--   0 runner    (1001) docker     (122)    16418 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5165 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/
--rw-r--r--   0 runner    (1001) docker     (122)      594 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    43681 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/basic_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/global_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/master_net.py
--rw-r--r--   0 runner    (1001) docker     (122)      766 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/model_zoo.py
--rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/plain_net_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     7405 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)    14983 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py
--rw-r--r--   0 runner    (1001) docker     (122)     8709 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py
--rw-r--r--   0 runner    (1001) docker     (122)     6923 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      699 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/apis/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/apis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2067 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py
--rw-r--r--   0 runner    (1001) docker     (122)     4000 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3460 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py
--rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py
--rw-r--r--   0 runner    (1001) docker     (122)     2345 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py
--rw-r--r--   0 runner    (1001) docker     (122)     5911 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py
--rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/
--rw-r--r--   0 runner    (1001) docker     (122)      148 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/
--rw-r--r--   0 runner    (1001) docker     (122)      621 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3701 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9356 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py
--rw-r--r--   0 runner    (1001) docker     (122)     7465 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14261 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    10559 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    13101 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    18626 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py
--rw-r--r--   0 runner    (1001) docker     (122)     7526 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py
--rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      561 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/
--rw-r--r--   0 runner    (1001) docker     (122)      409 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12602 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py
--rw-r--r--   0 runner    (1001) docker     (122)    19162 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5588 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py
--rw-r--r--   0 runner    (1001) docker     (122)    12284 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/
--rw-r--r--   0 runner    (1001) docker     (122)      421 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7347 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py
--rw-r--r--   0 runner    (1001) docker     (122)    24842 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py
--rw-r--r--   0 runner    (1001) docker     (122)     3499 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/detectors/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/detectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2809 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py
--rw-r--r--   0 runner    (1001) docker     (122)     2627 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)     2774 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      189 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11594 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py
--rw-r--r--   0 runner    (1001) docker     (122)     5838 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1164 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (122)     6400 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/detector.py
--rw-r--r--   0 runner    (1001) docker     (122)      627 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py
--rw-r--r--   0 runner    (1001) docker     (122)      643 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/tinynas_detector.py
--rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/tinynas_detection/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/
--rw-r--r--   0 runner    (1001) docker     (122)     2896 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py
--rw-r--r--   0 runner    (1001) docker     (122)      494 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      773 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/deinterlace_arch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3159 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/archs.py
--rw-r--r--   0 runner    (1001) docker     (122)     1441 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py
--rw-r--r--   0 runner    (1001) docker     (122)     2972 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/enh.py
--rw-r--r--   0 runner    (1001) docker     (122)     3572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/fre.py
--rw-r--r--   0 runner    (1001) docker     (122)     3716 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/configs/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/configs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12753 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/configs/default_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     6731 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/dro_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5494 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/camera.py
--rw-r--r--   0 runner    (1001) docker     (122)     2285 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3737 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/pose.py
--rw-r--r--   0 runner    (1001) docker     (122)     3685 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5277 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    10098 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     7204 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py
--rw-r--r--   0 runner    (1001) docker     (122)     4387 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10532 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8902 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     1607 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py
--rw-r--r--   0 runner    (1001) docker     (122)     1915 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/optim/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15781 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py
--rw-r--r--   0 runner    (1001) docker     (122)     8039 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/optim/update.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6815 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/augmentations.py
--rw-r--r--   0 runner    (1001) docker     (122)    10248 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/config.py
--rw-r--r--   0 runner    (1001) docker     (122)    15071 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/depth.py
--rw-r--r--   0 runner    (1001) docker     (122)     1153 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/horovod.py
--rw-r--r--   0 runner    (1001) docker     (122)    10965 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     9407 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/image_gt.py
--rw-r--r--   0 runner    (1001) docker     (122)     6499 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/load.py
--rw-r--r--   0 runner    (1001) docker     (122)     2726 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1348 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/types.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/
--rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py
--rw-r--r--   0 runner    (1001) docker     (122)     3334 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py
--rw-r--r--   0 runner    (1001) docker     (122)      458 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3212 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py
--rw-r--r--   0 runner    (1001) docker     (122)     9228 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5346 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py
--rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/update.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/
--rw-r--r--   0 runner    (1001) docker     (122)    13745 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py
--rw-r--r--   0 runner    (1001) docker     (122)     4049 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3823 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py
--rw-r--r--   0 runner    (1001) docker     (122)    16050 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py
--rw-r--r--   0 runner    (1001) docker     (122)    36415 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3317 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py
--rw-r--r--   0 runner    (1001) docker     (122)     2907 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/utils/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/
--rw-r--r--   0 runner    (1001) docker     (122)      558 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1324 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/
--rw-r--r--   0 runner    (1001) docker     (122)       36 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10465 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     2662 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     5506 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/effv2.py
--rw-r--r--   0 runner    (1001) docker     (122)     2904 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/lraspp.py
--rw-r--r--   0 runner    (1001) docker     (122)     2234 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/matting.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_inpainting/
--rw-r--r--   0 runner    (1001) docker     (122)      524 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_inpainting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11056 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_inpainting/inpainting.py
--rw-r--r--   0 runner    (1001) docker     (122)    13727 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_inpainting/inpainting_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      582 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18236 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    21551 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    16955 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    20983 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4136 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/neck/
--rw-r--r--   0 runner    (1001) docker     (122)      525 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/neck/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11733 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/track/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/track/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    26843 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    10771 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py
--rw-r--r--   0 runner    (1001) docker     (122)     4163 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    18059 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/video_knet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/
--rw-r--r--   0 runner    (1001) docker     (122)      541 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2970 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     2420 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/decode.py
--rw-r--r--   0 runner    (1001) docker     (122)     1890 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     4921 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/yolo.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1084 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py
--rw-r--r--   0 runner    (1001) docker     (122)     3129 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py
--rw-r--r--   0 runner    (1001) docker     (122)    14998 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2230 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     9570 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py
--rw-r--r--   0 runner    (1001) docker     (122)     7296 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2888 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      497 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      818 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/aggregate.py
--rw-r--r--   0 runner    (1001) docker     (122)     3582 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/cbam.py
--rw-r--r--   0 runner    (1001) docker     (122)     1979 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/eval_network.py
--rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/inference_core.py
--rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py
--rw-r--r--   0 runner    (1001) docker     (122)     7037 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/mod_resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)      974 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    15309 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/modules.py
--rw-r--r--   0 runner    (1001) docker     (122)     5593 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/network.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      477 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/backbone/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/backbone/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10040 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)    26628 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/
--rw-r--r--   0 runner    (1001) docker     (122)      515 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8349 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    20677 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py
--rw-r--r--   0 runner    (1001) docker     (122)    28653 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     4081 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py
--rw-r--r--   0 runner    (1001) docker     (122)     6696 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     8627 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     5885 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/neck/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/neck/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/track/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/track/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8508 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py
--rw-r--r--   0 runner    (1001) docker     (122)    17155 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     5440 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/visualizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/config/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1024 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/config/ostrack.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py
--rw-r--r--   0 runner    (1001) docker     (122)     5004 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)     4805 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1234 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3380 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     3393 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py
--rw-r--r--   0 runner    (1001) docker     (122)      653 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12278 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py
--rw-r--r--   0 runner    (1001) docker     (122)      943 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/tracker/
--rw-r--r--   0 runner    (1001) docker     (122)      114 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/tracker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5507 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py
--rw-r--r--   0 runner    (1001) docker     (122)     7123 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/utils/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/
--rw-r--r--   0 runner    (1001) docker     (122)    14702 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py
--rw-r--r--   0 runner    (1001) docker     (122)     4715 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/MotionPro.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3290 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py
--rw-r--r--   0 runner    (1001) docker     (122)     9214 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5275 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py
--rw-r--r--   0 runner    (1001) docker     (122)     5526 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py
--rw-r--r--   0 runner    (1001) docker     (122)     3922 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/Smoother.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1447 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/config.py
--rw-r--r--   0 runner    (1001) docker     (122)     7292 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py
--rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/
--rw-r--r--   0 runner    (1001) docker     (122)     4684 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py
--rw-r--r--   0 runner    (1001) docker     (122)    14212 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/MedianFilter.py
--rw-r--r--   0 runner    (1001) docker     (122)    22544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2893 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2817 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/WarpUtils.py
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14474 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/image_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4977 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/math_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/
--rw-r--r--   0 runner    (1001) docker     (122)      487 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7097 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5774 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py
--rw-r--r--   0 runner    (1001) docker     (122)     4848 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py
--rw-r--r--   0 runner    (1001) docker     (122)     9990 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py
--rw-r--r--   0 runner    (1001) docker     (122)     3905 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4959 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/base_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/kts/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/kts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/kts/cpd_auto.py
--rw-r--r--   0 runner    (1001) docker     (122)     3238 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py
--rw-r--r--   0 runner    (1001) docker     (122)    13098 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/pgl_sum.py
--rw-r--r--   0 runner    (1001) docker     (122)     8812 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_summarization/summarizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/
--rw-r--r--   0 runner    (1001) docker     (122)      689 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14118 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/basicvsr_net.py
--rw-r--r--   0 runner    (1001) docker     (122)     4727 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     3636 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py
--rw-r--r--   0 runner    (1001) docker     (122)     3637 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/vidt/
--rw-r--r--   0 runner    (1001) docker     (122)      464 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vidt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    40074 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vidt/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    25588 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vidt/deformable_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7436 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vidt/fpn_fusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    16697 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vidt/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     3651 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vidt/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/virual_tryon/
--rw-r--r--   0 runner    (1001) docker     (122)      464 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/virual_tryon/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17690 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/virual_tryon/sdafnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/
--rw-r--r--   0 runner    (1001) docker     (122)      502 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16102 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)      797 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     1786 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     9340 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/petl.py
--rw-r--r--   0 runner    (1001) docker     (122)     5021 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py
--rw-r--r--   0 runner    (1001) docker     (122)    28715 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5040 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py
--rw-r--r--   0 runner    (1001) docker     (122)     5614 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/vision_middleware/
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_middleware/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5749 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_middleware/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    28089 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_middleware/head.py
--rw-r--r--   0 runner    (1001) docker     (122)     6494 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_middleware/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vision_middleware/vim.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/
--rw-r--r--   0 runner    (1001) docker     (122)      919 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12238 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5188 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/basic_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    13520 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5299 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/model_se.py
--rw-r--r--   0 runner    (1001) docker     (122)     5177 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/cv/vop_retrieval/tokenization_clip.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/
--rw-r--r--   0 runner    (1001) docker     (122)     1957 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/clip/
--rw-r--r--   0 runner    (1001) docker     (122)       97 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14496 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/clip/bert_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3898 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/clip/configuration_bert.py
--rw-r--r--   0 runner    (1001) docker     (122)    22351 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/clip/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    20998 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/clip/modeling_bert.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      140 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    26652 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    14427 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    39910 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/structbert.py
--rw-r--r--   0 runner    (1001) docker     (122)    11507 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)    11834 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/unet_generator.py
--rw-r--r--   0 runner    (1001) docker     (122)     8566 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
--rw-r--r--   0 runner    (1001) docker     (122)    12361 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
--rw-r--r--   0 runner    (1001) docker     (122)    44602 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/dpm_solver_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/efficient_diffusion_tuning/
--rw-r--r--   0 runner    (1001) docker     (122)      540 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10302 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/gemm/
--rw-r--r--   0 runner    (1001) docker     (122)      139 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/gemm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21694 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/gemm/gemm_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     3718 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/gemm/gemm_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/gemm/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      548 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    36265 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     2969 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/respace.py
--rw-r--r--   0 runner    (1001) docker     (122)     1365 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/script.py
--rw-r--r--   0 runner    (1001) docker     (122)    36898 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/unet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/mgeo/
--rw-r--r--   0 runner    (1001) docker     (122)     1444 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mgeo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)   101942 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mgeo/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     8651 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mgeo/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     3268 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mgeo/text_ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)    10331 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mgeo/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/
--rw-r--r--   0 runner    (1001) docker     (122)      141 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/dataloaders/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/dataloaders/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3789 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/
--rw-r--r--   0 runner    (1001) docker     (122)      162 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     1442 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py
--rw-r--r--   0 runner    (1001) docker     (122)    21257 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/modeling.py
--rw-r--r--   0 runner    (1001) docker     (122)    18790 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/module_clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     3577 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/module_cross.py
--rw-r--r--   0 runner    (1001) docker     (122)     5581 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/tokenization_clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     4116 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/until_module.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/
--rw-r--r--   0 runner    (1001) docker     (122)      739 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/clip/
--rw-r--r--   0 runner    (1001) docker     (122)       86 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16605 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/clip/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     6030 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/configuration_mplug.py
--rwxr-xr-x   0 runner    (1001) docker     (122)   120753 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/modeling_mplug.py
--rw-r--r--   0 runner    (1001) docker     (122)    34049 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/mvit.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    21332 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug/predictor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/mplug_for_all_tasks.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      150 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10208 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)    11503 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
--rw-r--r--   0 runner    (1001) docker     (122)    28436 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    13487 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/prior.py
--rw-r--r--   0 runner    (1001) docker     (122)     6898 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)    16675 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     6565 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/
--rw-r--r--   0 runner    (1001) docker     (122)      306 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/adaptor/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/adaptor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18720 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/configuration_mmspeech.py
--rw-r--r--   0 runner    (1001) docker     (122)    16552 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/configuration_ofa.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1785 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    21049 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/multihead_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     5658 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py
--rw-r--r--   0 runner    (1001) docker     (122)    43439 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/search.py
--rw-r--r--   0 runner    (1001) docker     (122)    41976 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/sequence_generator.py
--rw-r--r--   0 runner    (1001) docker     (122)    16690 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py
--rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    44296 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/modeling_mmspeech.py
--rw-r--r--   0 runner    (1001) docker     (122)   100933 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/modeling_ofa.py
--rw-r--r--   0 runner    (1001) docker     (122)    13226 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/resnet.py
--rw-r--r--   0 runner    (1001) docker     (122)    15028 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/tokenization_ofa.py
--rw-r--r--   0 runner    (1001) docker     (122)     8004 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/utils/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      676 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/utils/constant.py
--rw-r--r--   0 runner    (1001) docker     (122)     1877 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     8435 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa/vit.py
--rw-r--r--   0 runner    (1001) docker     (122)    24779 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa_for_all_tasks.py
--rw-r--r--   0 runner    (1001) docker     (122)    12983 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/rleg/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/rleg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/rleg/model.py
--rw-r--r--   0 runner    (1001) docker     (122)     3410 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/rleg/rleg.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/
--rw-r--r--   0 runner    (1001) docker     (122)      678 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9838 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/blocks.py
--rw-r--r--   0 runner    (1001) docker     (122)    11896 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/clip.py
--rw-r--r--   0 runner    (1001) docker     (122)     6165 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    22584 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/swin_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     1597 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/soonet/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/team/
--rw-r--r--   0 runner    (1001) docker     (122)      140 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/team/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5181 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/team/team_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    11502 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/team/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18477 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     9264 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)     9504 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    38498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/unet_sd.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/
--rw-r--r--   0 runner    (1001) docker     (122)       93 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11339 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py
--rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)    16687 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/model.py
--rw-r--r--   0 runner    (1001) docker     (122)    47348 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py
--rw-r--r--   0 runner    (1001) docker     (122)    20956 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/processing.py
--rw-r--r--   0 runner    (1001) docker     (122)     4680 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)     7333 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/multi_modal/vldoc/transformer_local.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/T5/
--rw-r--r--   0 runner    (1001) docker     (122)      599 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/T5/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    67374 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/T5/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7541 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/T5/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    21467 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/T5/text2text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     6368 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/bart/
--rw-r--r--   0 runner    (1001) docker     (122)      112 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bart/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3058 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bart/text_error_correction.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/bert/
--rw-r--r--   0 runner    (1001) docker     (122)     1519 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    40458 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7308 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4113 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/document_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (122)      512 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     6074 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/sentence_embedding.py
--rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/siamese_uie.py
--rw-r--r--   0 runner    (1001) docker     (122)     1478 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     1138 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/text_ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)     2140 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/token_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     6904 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bert/word_alignment.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/bloom/
--rw-r--r--   0 runner    (1001) docker     (122)      472 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bloom/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      505 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/bloom/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/canmt/
--rw-r--r--   0 runner    (1001) docker     (122)      102 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/canmt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    52235 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/canmt/canmt_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     2940 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/canmt/canmt_translation.py
--rw-r--r--   0 runner    (1001) docker     (122)    35688 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/canmt/sequence_generator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/
--rwxr-xr-x   0 runner    (1001) docker     (122)      730 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    35473 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/codegeex.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3995 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4008 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     9997 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/inference.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6111 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/codegeex/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/csanmt/
--rw-r--r--   0 runner    (1001) docker     (122)       96 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/csanmt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    61868 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/csanmt/translation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/
--rw-r--r--   0 runner    (1001) docker     (122)     1771 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    47998 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     6771 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    10364 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)    21421 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)    10698 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/deberta_v2/tokenization_fast.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/dgds/
--rw-r--r--   0 runner    (1001) docker     (122)      939 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/dgds/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7092 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/dgds/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     1656 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
--rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
--rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/fid_T5/
--rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/fid_T5/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8108 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/fid_T5/text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/fid_plug/
--rw-r--r--   0 runner    (1001) docker     (122)     1181 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/fid_plug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    45082 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/fid_plug/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5045 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/fid_plug/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     6709 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/fid_plug/text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/
--rw-r--r--   0 runner    (1001) docker     (122)      540 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/generation/
--rw-r--r--   0 runner    (1001) docker     (122)       87 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/generation/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    10112 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/generation/strategies.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5152 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/kernels/
--rw-r--r--   0 runner    (1001) docker     (122)     3263 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/kernels/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/
--rw-r--r--   0 runner    (1001) docker     (122)     2635 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/functional.py
--rw-r--r--   0 runner    (1001) docker     (122)     4510 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/layers.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    13504 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/glm_130b/text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/gpt2/
--rw-r--r--   0 runner    (1001) docker     (122)      470 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt2/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/
--rw-r--r--   0 runner    (1001) docker     (122)      852 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15811 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     8971 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    51600 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/distributed_gpt3.py
--rw-r--r--   0 runner    (1001) docker     (122)     2847 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2586 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt3/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/
--rw-r--r--   0 runner    (1001) docker     (122)      874 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13129 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5198 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/checkpointing.py
--rw-r--r--   0 runner    (1001) docker     (122)     4930 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    49316 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/experts.py
--rw-r--r--   0 runner    (1001) docker     (122)     3504 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/layer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2553 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/mappings.py
--rw-r--r--   0 runner    (1001) docker     (122)    23756 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py
--rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2717 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2277 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_moe/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_neo/
--rw-r--r--   0 runner    (1001) docker     (122)      474 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_neo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      518 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/gpt_neo/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/heads/
--rw-r--r--   0 runner    (1001) docker     (122)      661 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    25248 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/crf_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     6947 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/fill_mask_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     5145 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/infromation_extraction_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2056 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/text_classification_head.py
--rw-r--r--   0 runner    (1001) docker     (122)      964 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/text_generation_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2029 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/text_ranking_head.py
--rw-r--r--   0 runner    (1001) docker     (122)     2596 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/token_classification_head.py
--rw-r--r--   0 runner    (1001) docker     (122)      948 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/heads/torch_pretrain_head.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/hf_transformers/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/hf_transformers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4897 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/hf_transformers/backbone.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/llama/
--rw-r--r--   0 runner    (1001) docker     (122)      866 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    29197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     4695 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    11349 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py
--rw-r--r--   0 runner    (1001) docker     (122)     7212 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)    10333 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/llama/tokenization_fast.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/lstm/
--rw-r--r--   0 runner    (1001) docker     (122)      610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/lstm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1312 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/lstm/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/lstm/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/megatron_bert/
--rw-r--r--   0 runner    (1001) docker     (122)      688 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/megatron_bert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    39864 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/megatron_bert/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     6599 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/megatron_bert/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    12452 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/megatron_bert/fill_mask.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/
--rw-r--r--   0 runner    (1001) docker     (122)      572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    28115 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/arguments.py
--rw-r--r--   0 runner    (1001) docker     (122)    28361 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/blocklm_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    17995 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/configure_data.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/
--rw-r--r--   0 runner    (1001) docker     (122)    13656 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    20350 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/corpora.py
--rw-r--r--   0 runner    (1001) docker     (122)    45773 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/datasets.py
--rw-r--r--   0 runner    (1001) docker     (122)     3342 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/extraction.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     8459 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/file_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/lazy_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     7221 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/samplers.py
--rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    53304 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/tokenization.py
--rw-r--r--   0 runner    (1001) docker     (122)    14551 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    15773 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/wordpiece.py
--rw-r--r--   0 runner    (1001) docker     (122)    21669 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/generation_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    16247 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/mglm_for_text_summarization.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/
--rwxr-xr-x   0 runner    (1001) docker     (122)      984 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     5443 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/distributed.py
--rw-r--r--   0 runner    (1001) docker     (122)     9967 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/downstream.py
--rw-r--r--   0 runner    (1001) docker     (122)    70249 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/modeling_bert.py
--rw-r--r--   0 runner    (1001) docker     (122)     8743 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/modeling_glm.py
--rw-r--r--   0 runner    (1001) docker     (122)     2531 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/prompt.py
--rw-r--r--   0 runner    (1001) docker     (122)    48886 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/model/transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/process_grid.py
--rw-r--r--   0 runner    (1001) docker     (122)      203 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/run_test.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/test/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/test/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      954 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/test/test_block.py
--rw-r--r--   0 runner    (1001) docker     (122)      708 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/test/test_rel_shift.py
--rw-r--r--   0 runner    (1001) docker     (122)    17815 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/train_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    19029 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/mglm/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/palm_v2/
--rw-r--r--   0 runner    (1001) docker     (122)     1268 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/palm_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5532 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/palm_v2/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    29453 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/palm_v2/dureader_eval.py
--rw-r--r--   0 runner    (1001) docker     (122)    55450 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/palm_v2/text_generation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/peer/
--rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/peer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    55780 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/peer/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    11716 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/peer/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/peer/sas_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/peer/text_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/plug/
--rwxr-xr-x   0 runner    (1001) docker     (122)     3321 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug/AnnealingLR.py
--rw-r--r--   0 runner    (1001) docker     (122)      660 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    41209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)    11797 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    10983 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug/distributed_plug.py
--rw-r--r--   0 runner    (1001) docker     (122)     8427 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug/generator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/plug_mental/
--rw-r--r--   0 runner    (1001) docker     (122)     1359 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug_mental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7671 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug_mental/adv_utils.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    47486 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug_mental/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7956 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug_mental/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    10881 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/plug_mental/text_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/
--rw-r--r--   0 runner    (1001) docker     (122)     1490 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    37918 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     6366 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4176 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/document_segmentation.py
--rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     7147 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/ponet/tokenization.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)      987 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1208 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     3832 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/dialog_intent_prediction.py
--rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/dialog_modeling.py
--rw-r--r--   0 runner    (1001) docker     (122)    16648 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/dialog_state_tracking.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/
--rw-r--r--   0 runner    (1001) docker     (122)      421 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10281 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/gen_unified_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10757 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/generator.py
--rw-r--r--   0 runner    (1001) docker     (122)     7505 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/intent_unified_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/model_base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/tokenization_space.py
--rw-r--r--   0 runner    (1001) docker     (122)    11941 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/model/unified_transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2398 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/embedder.py
--rw-r--r--   0 runner    (1001) docker     (122)      956 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/feedforward.py
--rw-r--r--   0 runner    (1001) docker     (122)     1721 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/functions.py
--rw-r--r--   0 runner    (1001) docker     (122)     3397 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/multihead_attention.py
--rw-r--r--   0 runner    (1001) docker     (122)     1922 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space/modules/transformer_block.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_cn/
--rw-r--r--   0 runner    (1001) docker     (122)      529 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_cn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    44438 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_cn/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     5194 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_cn/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    30117 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_cn/table_question_answering.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_en/
--rw-r--r--   0 runner    (1001) docker     (122)      492 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_en/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/space_T_en/text_to_sql.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/
--rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7673 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/adv_utils.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    40848 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7686 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)    27203 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/faq_question_answering.py
--rw-r--r--   0 runner    (1001) docker     (122)    12581 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)    11862 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)    11074 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/structbert/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/
--rw-r--r--   0 runner    (1001) docker     (122)     1453 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5323 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/feature_extraction.py
--rw-r--r--   0 runner    (1001) docker     (122)     2698 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)      766 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/information_extraction.py
--rw-r--r--   0 runner    (1001) docker     (122)    28991 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/task_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     8678 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/text_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2092 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/text_ranking.py
--rw-r--r--   0 runner    (1001) docker     (122)     5116 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/task_models/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/unite/
--rw-r--r--   0 runner    (1001) docker     (122)      622 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/unite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      412 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/unite/configuration_unite.py
--rw-r--r--   0 runner    (1001) docker     (122)    17494 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/unite/modeling_unite.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/use/
--rw-r--r--   0 runner    (1001) docker     (122)      545 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/use/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5133 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/use/transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5795 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/use/user_satisfaction_estimation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/veco/
--rw-r--r--   0 runner    (1001) docker     (122)     1479 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/veco/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4031 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/veco/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     1192 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/veco/configuration.py
--rw-r--r--   0 runner    (1001) docker     (122)     4278 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/veco/fill_mask.py
--rw-r--r--   0 runner    (1001) docker     (122)     6643 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/veco/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     4129 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/veco/token_classification.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/
--rw-r--r--   0 runner    (1001) docker     (122)     1156 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    42932 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/backbone.py
--rw-r--r--   0 runner    (1001) docker     (122)     7697 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/configuration.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/science/
--rw-r--r--   0 runner    (1001) docker     (122)      491 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/science/unifold/
--rw-r--r--   0 runner    (1001) docker     (122)       46 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    24057 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/
--rw-r--r--   0 runner    (1001) docker     (122)      634 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    49139 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/data_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    19694 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/msa_pairing.py
--rw-r--r--   0 runner    (1001) docker     (122)     8941 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/process.py
--rw-r--r--   0 runner    (1001) docker     (122)    14792 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/process_multimer.py
--rw-r--r--   0 runner    (1001) docker     (122)    11422 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/protein.py
--rw-r--r--   0 runner    (1001) docker     (122)    42023 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/residue_constants.py
--rw-r--r--   0 runner    (1001) docker     (122)     4628 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/data/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    19250 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/model.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/
--rw-r--r--   0 runner    (1001) docker     (122)      187 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17066 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/alphafold.py
--rw-r--r--   0 runner    (1001) docker     (122)    12323 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/attentions.py
--rw-r--r--   0 runner    (1001) docker     (122)     5403 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/auxillary_heads.py
--rw-r--r--   0 runner    (1001) docker     (122)    10676 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/common.py
--rw-r--r--   0 runner    (1001) docker     (122)     5814 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/confidence.py
--rw-r--r--   0 runner    (1001) docker     (122)     8771 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/embedders.py
--rw-r--r--   0 runner    (1001) docker     (122)    11078 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/evoformer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6820 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/featurization.py
--rw-r--r--   0 runner    (1001) docker     (122)    18025 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/frame.py
--rw-r--r--   0 runner    (1001) docker     (122)    18476 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/structure_module.py
--rw-r--r--   0 runner    (1001) docker     (122)     9771 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/template.py
--rw-r--r--   0 runner    (1001) docker     (122)     5623 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/modules/triangle_multiplication.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/
--rw-r--r--   0 runner    (1001) docker     (122)       46 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17940 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/mmcif.py
--rw-r--r--   0 runner    (1001) docker     (122)     3128 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/msa_identifiers.py
--rw-r--r--   0 runner    (1001) docker     (122)    23503 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/parsers.py
--rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    45466 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/templates.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/
--rw-r--r--   0 runner    (1001) docker     (122)      639 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6218 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hhblits.py
--rw-r--r--   0 runner    (1001) docker     (122)     3991 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hhsearch.py
--rw-r--r--   0 runner    (1001) docker     (122)     5098 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hmmbuild.py
--rw-r--r--   0 runner    (1001) docker     (122)     5013 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hmmsearch.py
--rw-r--r--   0 runner    (1001) docker     (122)     8662 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/jackhmmer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3752 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/kalign.py
--rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/models/science/unifold/msa/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/
--rw-r--r--   0 runner    (1001) docker     (122)       84 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/audio/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      346 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/audio/asr_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/auth/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/auth/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1326 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/auth/auth_config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/context/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3578 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/context/dataset_context_config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/data_files/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/data_files/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5110 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/data_files/data_files_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/data_loader/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/data_loader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11545 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/data_loader/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     5767 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/data_loader/data_loader_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/
--rw-r--r--   0 runner    (1001) docker     (122)      125 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/
--rw-r--r--   0 runner    (1001) docker     (122)     4817 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/
--rw-r--r--   0 runner    (1001) docker     (122)      730 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1737 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     8512 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     7723 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    11689 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/
--rw-r--r--   0 runner    (1001) docker     (122)      541 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1264 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      791 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/
--rw-r--r--   0 runner    (1001) docker     (122)      134 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4708 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py
--rw-r--r--   0 runner    (1001) docker     (122)      945 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/
--rw-r--r--   0 runner    (1001) docker     (122)      177 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3815 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py
--rw-r--r--   0 runner    (1001) docker     (122)    16180 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/
--rw-r--r--   0 runner    (1001) docker     (122)      934 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/
--rw-r--r--   0 runner    (1001) docker     (122)      544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/
--rw-r--r--   0 runner    (1001) docker     (122)      312 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2622 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
--rw-r--r--   0 runner    (1001) docker     (122)     4858 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/
--rw-r--r--   0 runner    (1001) docker     (122)      196 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1086 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
--rw-r--r--   0 runner    (1001) docker     (122)     2509 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/
--rw-r--r--   0 runner    (1001) docker     (122)      512 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1459 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2141 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/
--rw-r--r--   0 runner    (1001) docker     (122)      556 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/
--rw-r--r--   0 runner    (1001) docker     (122)      488 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1393 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/
--rw-r--r--   0 runner    (1001) docker     (122)      539 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/
--rw-r--r--   0 runner    (1001) docker     (122)      529 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
--rw-r--r--   0 runner    (1001) docker     (122)    12152 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    12574 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/
--rw-r--r--   0 runner    (1001) docker     (122)      577 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1629 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/
--rw-r--r--   0 runner    (1001) docker     (122)      615 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1487 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/
--rw-r--r--   0 runner    (1001) docker     (122)      583 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1792 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     4849 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     6225 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      559 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6171 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     4013 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      536 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4294 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/
--rw-r--r--   0 runner    (1001) docker     (122)      161 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1589 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
--rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     5437 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/
--rw-r--r--   0 runner    (1001) docker     (122)       40 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7547 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (122)     3416 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/
--rw-r--r--   0 runner    (1001) docker     (122)      309 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3172 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py
--rw-r--r--   0 runner    (1001) docker     (122)      971 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     5818 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py
--rw-r--r--   0 runner    (1001) docker     (122)     2000 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py
--rw-r--r--   0 runner    (1001) docker     (122)      707 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py
--rw-r--r--   0 runner    (1001) docker     (122)     4912 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py
--rw-r--r--   0 runner    (1001) docker     (122)     2297 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2017 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      599 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16767 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     8725 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/
--rw-r--r--   0 runner    (1001) docker     (122)      545 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1475 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1984 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2684 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     5114 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     1635 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2664 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/
--rw-r--r--   0 runner    (1001) docker     (122)      573 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1390 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/
--rw-r--r--   0 runner    (1001) docker     (122)      543 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      809 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2624 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/
--rw-r--r--   0 runner    (1001) docker     (122)      553 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2370 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     7104 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/dataset_cls/dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/download/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/download/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16700 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/download/dataset_builder.py
--rw-r--r--   0 runner    (1001) docker     (122)      609 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/download/download_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     2487 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/download/download_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/meta/
--rw-r--r--   0 runner    (1001) docker     (122)       50 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/meta/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1772 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/meta/data_meta_config.py
--rw-r--r--   0 runner    (1001) docker     (122)     8625 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/meta/data_meta_manager.py
--rw-r--r--   0 runner    (1001) docker     (122)    36975 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/ms_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/
--rw-r--r--   0 runner    (1001) docker     (122)     1072 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      408 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/gopro_image_deblurring_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      401 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      404 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/sidd_image_denoising.py
--rw-r--r--   0 runner    (1001) docker     (122)      410 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/torch_base_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)      404 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/task_datasets/video_summarization_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/msdatasets/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/utils/dataset_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/utils/delete_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     6124 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/utils/oss_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/msdatasets/utils/upload_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/ailut/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/csrc/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/csrc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6065 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/csrc/ailut_transform.cpp
--rw-r--r--   0 runner    (1001) docker     (122)    27053 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/csrc/ailut_transform_cpu.cpp
--rw-r--r--   0 runner    (1001) docker     (122)    31866 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/Ailut/csrc/ailut_transform_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (122)      105 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4314 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/ailut/pyinterfaces.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/
--rw-r--r--   0 runner    (1001) docker     (122)      106 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/functions/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2925 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/functions/quadtree_attention.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/modules/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13956 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/modules/quadtree_attention.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1425 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/score_computation.cpp
--rw-r--r--   0 runner    (1001) docker     (122)     1008 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/score_computation.h
--rw-r--r--   0 runner    (1001) docker     (122)     6033 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/score_computation_kernal.cu
--rw-r--r--   0 runner    (1001) docker     (122)      612 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/utils.h
--rw-r--r--   0 runner    (1001) docker     (122)     2369 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/value_aggregation.cpp
--rw-r--r--   0 runner    (1001) docker     (122)      815 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/value_aggregation.h
--rw-r--r--   0 runner    (1001) docker     (122)     3600 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/ops/quadtree_attention/src/value_aggregation_kernel.cu
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/outputs/
--rw-r--r--   0 runner    (1001) docker     (122)      132 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/outputs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      908 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/outputs/cv_outputs.py
--rw-r--r--   0 runner    (1001) docker     (122)    19638 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/outputs/nlp_outputs.py
--rw-r--r--   0 runner    (1001) docker     (122)    42453 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/outputs/outputs.py
--rw-r--r--   0 runner    (1001) docker     (122)     9630 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipeline_inputs.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/
--rw-r--r--   0 runner    (1001) docker     (122)      242 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/audio/
--rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7481 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/ans_dfsmn_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5016 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/ans_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    27456 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/asr_inference_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4357 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/inverse_text_processing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3830 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/kws_farfield_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7276 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/kws_kwsbp_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5555 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/linear_aec_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7916 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/lm_infer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6329 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/punctuation_processing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2560 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/separation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    10993 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/speaker_diarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4118 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/speaker_verification_light_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    10369 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/speaker_verification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/text_to_speech_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    11785 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/timestamp_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9544 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/audio/voice_activity_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    22330 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     7279 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/cv/
--rw-r--r--   0 runner    (1001) docker     (122)    17487 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2545 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/action_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4852 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/action_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/animal_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2576 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/arc_face_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2614 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/bad_image_detecting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    14219 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4740 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/card_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5307 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2528 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/content_check_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5055 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/controllable_image_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/crowd_counting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5136 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/
--rw-r--r--   0 runner    (1001) docker     (122)      959 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     2241 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8202 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2491 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1599 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1531 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_emotion_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2894 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_image_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3312 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_liveness_ir_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3573 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_liveness_xc_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7748 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_processing_base_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3872 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_quality_assessment_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3528 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3493 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2878 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_ood_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    19633 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/face_reconstruction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2400 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2681 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4030 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/general_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2055 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/hand_2d_keypoints_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/hand_static_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2982 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/human_reconstruction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1399 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_body_reshaping_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5251 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_cartoon_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5857 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3296 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_color_enhance_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4441 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_colorization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2648 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_debanding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4624 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_deblur_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3517 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4160 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_denoise_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1943 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1852 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4134 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_driving_perception_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2317 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_face_fusion_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4866 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_human_parsing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5854 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_inpainting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3903 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5929 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_matching_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_matting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2796 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2770 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6113 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_paintbyexample_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4807 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8567 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3549 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2835 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2801 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2026 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_reid_person_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1702 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_restoration_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1644 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_salient_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3207 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2237 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_skychange_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2839 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_style_transfer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3138 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_super_resolution_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9718 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_to_image_generate_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    12737 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/image_to_image_translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1883 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     9872 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4570 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/license_plate_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4182 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/live_category_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2757 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/mask_face_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2787 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3670 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1857 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/mog_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4844 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/motion_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2628 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1953 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3328 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6018 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/object_detection_3d_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    11033 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_recognition_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/
--rw-r--r--   0 runner    (1001) docker     (122)      966 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      720 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    19821 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_dla34.py
--rw-r--r--   0 runner    (1001) docker     (122)     8768 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
--rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
--rw-r--r--   0 runner    (1001) docker     (122)    15421 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_vlpt.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/
--rw-r--r--   0 runner    (1001) docker     (122)      550 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
--rw-r--r--   0 runner    (1001) docker     (122)     1560 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
--rw-r--r--   0 runner    (1001) docker     (122)    38278 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ops.py
--rw-r--r--   0 runner    (1001) docker     (122)    19004 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py
--rw-r--r--   0 runner    (1001) docker     (122)    11371 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/resnet_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    11168 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/table_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     7971 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9781 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1457 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1451 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/product_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2020 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9237 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/retina_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1760 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/shop_segmentation_pipleline.py
--rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/skin_retouching_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4446 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/table_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/tbs_detection_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/cv/tbs_detection_utils/
--rw-r--r--   0 runner    (1001) docker     (122)       10 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/tbs_detection_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15542 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/tbs_detection_utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1841 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3246 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/tinynas_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3244 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/tinynas_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1888 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    13776 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_category_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6097 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_colorization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7757 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_deinterlace_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_depth_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    24271 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3635 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_human_matting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1719 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_inpainting_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9652 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3294 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4724 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_object_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4683 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3436 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4647 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_stabilization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4289 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7078 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/video_super_resolution_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7564 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/vidt_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5240 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/virtual_try_on_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3986 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/vision_middleware_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4762 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/vop_retrieval_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5787 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/
--rw-r--r--   0 runner    (1001) docker     (122)     2927 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2366 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/asr_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/
--rw-r--r--   0 runner    (1001) docker     (122)      636 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2013 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/
--rw-r--r--   0 runner    (1001) docker     (122)      718 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11691 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2880 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/
--rw-r--r--   0 runner    (1001) docker     (122)      585 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15631 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
--rw-r--r--   0 runner    (1001) docker     (122)    18987 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2311 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2931 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1093 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9352 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/gridvlp_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2634 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/image_captioning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1612 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7869 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     8561 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1848 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/sudoku_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/text2sql_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3160 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2150 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/video_captioning_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1329 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1781 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2396 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     6798 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6939 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3696 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/canmt_translation_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2202 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     2885 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2464 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2308 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/dialog_modeling_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7550 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2218 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1851 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4425 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/distributed_plug_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2641 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    25886 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5367 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9648 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/document_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/extractive_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3348 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/faq_question_answering_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2437 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3145 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/feature_extraction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    10173 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/fid_dialogue_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/fill_mask_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (122)      999 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2383 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/information_extraction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6363 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/interactive_translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     9928 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/language_identification_pipline.py
--rw-r--r--   0 runner    (1001) docker     (122)     1776 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3071 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/sentence_embedding_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    14210 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/siamese_uie_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/summarization_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)    16202 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/table_question_answering_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6797 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/text_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3490 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/text_error_correction_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     7622 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/text_generation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2764 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/text_ranking_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     6286 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/token_classification_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4312 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/translation_evaluation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5586 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/translation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2339 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/word_alignment_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     4523 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/word_segmentation_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     5762 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/pipelines/science/
--rw-r--r--   0 runner    (1001) docker     (122)      538 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/science/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/science/protein_structure_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (122)     3492 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/pipelines/util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/
--rw-r--r--   0 runner    (1001) docker     (122)     5610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12580 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/asr.py
--rw-r--r--   0 runner    (1001) docker     (122)     8680 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/audio.py
--rw-r--r--   0 runner    (1001) docker     (122)    15312 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/base.py
--rw-r--r--   0 runner    (1001) docker     (122)      812 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/common.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/cv/
--rw-r--r--   0 runner    (1001) docker     (122)     1896 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7446 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/action_detection_mapper.py
--rw-r--r--   0 runner    (1001) docker     (122)     1126 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     7789 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/controllable_image_generation.py
--rw-r--r--   0 runner    (1001) docker     (122)    20909 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/cv2_transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    12826 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/image_classification_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/image_quality_assessment_man.py
--rw-r--r--   0 runner    (1001) docker     (122)     2113 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/image_quality_assessment_mos.py
--rw-r--r--   0 runner    (1001) docker     (122)     2790 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/image_restoration_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2625 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/mmcls_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3050 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/timer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3606 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/util.py
--rw-r--r--   0 runner    (1001) docker     (122)     1501 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/video_stabilization.py
--rw-r--r--   0 runner    (1001) docker     (122)     8579 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/cv/video_super_resolution.py
--rw-r--r--   0 runner    (1001) docker     (122)    13406 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/image.py
--rw-r--r--   0 runner    (1001) docker     (122)     4884 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/kws.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/movie_scene_segmentation/
--rw-r--r--   0 runner    (1001) docker     (122)      483 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/movie_scene_segmentation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10809 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/movie_scene_segmentation/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)    24768 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/multi_modal.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      762 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3929 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/canmt_translation.py
--rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4102 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4391 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4086 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    11004 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     6844 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    12086 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/fill_mask_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     8199 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1168 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4023 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1424 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)     1219 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1908 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/args.py
--rw-r--r--   0 runner    (1001) docker     (122)     1555 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/batch.py
--rw-r--r--   0 runner    (1001) docker     (122)     3629 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (122)     2701 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     5457 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    56779 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/dst_processors.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/
--rw-r--r--   0 runner    (1001) docker     (122)      592 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    33884 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/gen_field.py
--rw-r--r--   0 runner    (1001) docker     (122)    42465 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/intent_field.py
--rw-r--r--   0 runner    (1001) docker     (122)     1147 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/lazy_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     1935 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/preprocess.py
--rw-r--r--   0 runner    (1001) docker     (122)     1610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/sampler.py
--rw-r--r--   0 runner    (1001) docker     (122)     2084 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/tensorlistdataset.py
--rw-r--r--   0 runner    (1001) docker     (122)    24803 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space/tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/
--rw-r--r--   0 runner    (1001) docker     (122)      654 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4936 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/database.py
--rw-r--r--   0 runner    (1001) docker     (122)    15867 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py
--rw-r--r--   0 runner    (1001) docker     (122)     4888 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py
--rw-r--r--   0 runner    (1001) docker     (122)     4175 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/
--rw-r--r--   0 runner    (1001) docker     (122)      846 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/
--rw-r--r--   0 runner    (1001) docker     (122)      818 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21521 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12500 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/parse.py
--rw-r--r--   0 runner    (1001) docker     (122)     1380 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     2111 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/text_classification_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/text_clean.py
--rw-r--r--   0 runner    (1001) docker     (122)     2298 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/text_error_correction.py
--rw-r--r--   0 runner    (1001) docker     (122)    13896 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/text_generation_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3834 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/text_ranking_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)    19473 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/token_classification_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1640 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     1238 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     4482 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/transformers_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3566 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     3622 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/word_alignment_preprocessor.py
--rw-r--r--   0 runner    (1001) docker     (122)     2584 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/
--rw-r--r--   0 runner    (1001) docker     (122)      762 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4839 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/asr.py
--rw-r--r--   0 runner    (1001) docker     (122)    11909 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4172 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/image_captioning.py
--rw-r--r--   0 runner    (1001) docker     (122)     6461 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/image_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     5740 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/ocr_recognition.py
--rw-r--r--   0 runner    (1001) docker     (122)     4603 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/sudoku.py
--rw-r--r--   0 runner    (1001) docker     (122)     5210 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/summarization.py
--rw-r--r--   0 runner    (1001) docker     (122)    16449 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/text2sql.py
--rw-r--r--   0 runner    (1001) docker     (122)     5368 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/text_classification.py
--rw-r--r--   0 runner    (1001) docker     (122)     2164 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/text_to_image_synthesis.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3221 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/audio_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py
--rw-r--r--   0 runner    (1001) docker     (122)     6809 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/collate.py
--rw-r--r--   0 runner    (1001) docker     (122)      660 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/constant.py
--rw-r--r--   0 runner    (1001) docker     (122)     3421 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/get_tables.py
--rw-r--r--   0 runner    (1001) docker     (122)     1302 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/random_help.py
--rw-r--r--   0 runner    (1001) docker     (122)     5459 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/text2phone.py
--rw-r--r--   0 runner    (1001) docker     (122)    19116 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/transforms.py
--rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/utils/vision_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)     7664 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/visual_entailment.py
--rw-r--r--   0 runner    (1001) docker     (122)     8484 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/visual_grounding.py
--rw-r--r--   0 runner    (1001) docker     (122)     6796 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/ofa/visual_question_answering.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/preprocessors/science/
--rw-r--r--   0 runner    (1001) docker     (122)      478 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/science/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21698 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/science/uni_fold.py
--rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/tts.py
--rw-r--r--   0 runner    (1001) docker     (122)    13168 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/preprocessors/video.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/tools/
--rw-r--r--   0 runner    (1001) docker     (122)       49 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      772 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tools/eval.py
--rw-r--r--   0 runner    (1001) docker     (122)     5362 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tools/speech_tts_autolabel.py
--rw-r--r--   0 runner    (1001) docker     (122)      607 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tools/train.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/
--rw-r--r--   0 runner    (1001) docker     (122)     1595 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/audio/
--rw-r--r--   0 runner    (1001) docker     (122)      826 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1873 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/ans_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6969 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/asr_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    12816 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_farfield_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    22065 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_nearfield_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/
--rw-r--r--   0 runner    (1001) docker     (122)     1768 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22114 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/batch_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    11499 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/det_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     7022 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/file_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2854 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/kws_utils/runtime_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    21962 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/separation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10858 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/audio/tts_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     1808 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/cv/
--rw-r--r--   0 runner    (1001) docker     (122)     1974 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7494 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/action_detection_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      668 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/card_detection_scrfd_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10044 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/cartoon_translation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6737 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/face_detection_scrfd_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20051 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/image_classifition_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    11655 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    22965 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/image_detection_damoyolo_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4405 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/image_inpainting_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      816 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/image_instance_segmentation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     5520 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/image_portrait_enhancement_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      680 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/movie_scene_segmentation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20149 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/nerf_recon_acc_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    16911 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/ocr_detection_db_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/ocr_recognition_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4602 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/cv/vision_efficient_tuning_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3229 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/default_config.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/easycv/
--rw-r--r--   0 runner    (1001) docker     (122)      487 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/easycv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8237 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/easycv/trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/easycv/utils/
--rw-r--r--   0 runner    (1001) docker     (122)      523 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/easycv/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1029 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/easycv/utils/hooks.py
--rw-r--r--   0 runner    (1001) docker     (122)     1855 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/easycv/utils/metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     3415 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/easycv/utils/register_util.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/hooks/
--rw-r--r--   0 runner    (1001) docker     (122)     1650 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      296 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)    30111 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/checkpoint_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)      764 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/hooks/compression/
--rw-r--r--   0 runner    (1001) docker     (122)      610 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/compression/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4847 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/compression/sparsity_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     6915 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/compression/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1383 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/ddp_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     6843 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/deepspeed_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     4020 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/early_stop_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     3062 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/evaluation_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     8101 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/hook.py
--rw-r--r--   0 runner    (1001) docker     (122)      731 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/iter_timer_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/hooks/logger/
--rw-r--r--   0 runner    (1001) docker     (122)      718 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/logger/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4498 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/logger/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     4507 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/logger/tensorboard_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     7443 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/logger/text_logger_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     5146 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/lr_scheduler_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/megatron_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/hooks/optimizer/
--rw-r--r--   0 runner    (1001) docker     (122)      743 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     3585 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/optimizer/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     3301 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py
--rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/hooks/priority.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/
--rw-r--r--   0 runner    (1001) docker     (122)      699 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2040 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/warmup/
--rw-r--r--   0 runner    (1001) docker     (122)      614 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/warmup/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2547 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/warmup/base.py
--rw-r--r--   0 runner    (1001) docker     (122)     2934 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/lrscheduler/warmup/warmup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/
--rw-r--r--   0 runner    (1001) docker     (122)      682 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/clip/
--rw-r--r--   0 runner    (1001) docker     (122)       89 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/clip/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9742 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/clip/clip_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4388 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/efficient_diffusion_tuning/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/efficient_diffusion_tuning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2874 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8086 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/mplug/
--rw-r--r--   0 runner    (1001) docker     (122)       91 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/mplug/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1642 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/mplug/mplug_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/ofa/
--rw-r--r--   0 runner    (1001) docker     (122)       87 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/ofa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10186 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/ofa/ofa_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    17647 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/team/
--rw-r--r--   0 runner    (1001) docker     (122)       95 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/team/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5429 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/team/team_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2443 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/multi_modal/team/team_trainer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)     1179 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14266 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/csanmt_translation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     9999 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    24284 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    12769 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/faq_question_answering_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3064 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/gpt3_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2097 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/gpt_moe_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8171 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/plug_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/sentence_embedding_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     8378 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/sequence_classification_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    17110 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/siamese_uie_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5324 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/dialog_intent_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/dialog_modeling_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    36354 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/eval.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/metrics/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/metrics/metrics_tracker.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/trainer/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/trainer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    30567 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/trainer/gen_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    29570 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/space/trainer/intent_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    20453 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/table_question_answering_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)      991 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/text_generation_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7514 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp/text_ranking_trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7129 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/nlp_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/optimizer/
--rw-r--r--   0 runner    (1001) docker     (122)      223 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1753 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/optimizer/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)     7081 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/parallel/
--rw-r--r--   0 runner    (1001) docker     (122)       80 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      681 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/parallel/builder.py
--rw-r--r--   0 runner    (1001) docker     (122)      754 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/parallel/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    58132 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/trainer.py
--rw-r--r--   0 runner    (1001) docker     (122)    28082 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/training_args.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/trainers/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11090 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/utils/inference.py
--rw-r--r--   0 runner    (1001) docker     (122)     1294 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/trainers/utils/log_buffer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/tuners/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tuners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    38904 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tuners/control_sd_lora.py
--rw-r--r--   0 runner    (1001) docker     (122)    24058 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tuners/lora.py
--rw-r--r--   0 runner    (1001) docker     (122)     8840 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/tuners/sd_lora.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)   521233 2023-04-22 02:47:24.000000 modelscope-1.5.2/modelscope/utils/ast_index_file.py
--rw-r--r--   0 runner    (1001) docker     (122)    28886 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/ast_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/audio/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12122 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/audio/audio_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/audio/tts_exceptions.py
--rw-r--r--   0 runner    (1001) docker     (122)    26452 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (122)     2573 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/chinese_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    27433 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/config.py
--rw-r--r--   0 runner    (1001) docker     (122)     1027 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/config_ds.py
--rw-r--r--   0 runner    (1001) docker     (122)    18411 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/constant.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/cv/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/cv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    23195 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/cv/image_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/cv/motion_utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/cv/motion_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/cv/motion_utils/motion_process.py
--rw-r--r--   0 runner    (1001) docker     (122)     3847 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/cv/motion_utils/plot_script.py
--rw-r--r--   0 runner    (1001) docker     (122)     4290 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/cv/motion_utils/rotation_conversions.py
--rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/data_collators.py
--rw-r--r--   0 runner    (1001) docker     (122)     1279 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     9437 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/demo_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3744 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/device.py
--rw-r--r--   0 runner    (1001) docker     (122)     6147 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/error.py
--rw-r--r--   0 runner    (1001) docker     (122)     1128 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/file_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5939 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/hub.py
--rw-r--r--   0 runner    (1001) docker     (122)    15752 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/import_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      478 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/json_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (122)     7561 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/megatron_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2335 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/metric.py
--rw-r--r--   0 runner    (1001) docker     (122)     5099 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/model_tag.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/nlp/
--rw-r--r--   0 runner    (1001) docker     (122)      499 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4424 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/distributed.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     4526 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/load_checkpoint.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/nlp/space/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1909 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/args.py
--rw-r--r--   0 runner    (1001) docker     (122)    11818 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/clean_dataset.py
--rw-r--r--   0 runner    (1001) docker     (122)     1439 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/criterions.py
--rw-r--r--   0 runner    (1001) docker     (122)    11252 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/db_ops.py
--rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/ontology.py
--rw-r--r--   0 runner    (1001) docker     (122)      197 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/scores.py
--rw-r--r--   0 runner    (1001) docker     (122)     6356 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1054 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space/utils_dst.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope/utils/nlp/space_T_en/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space_T_en/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      859 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/space_T_en/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2554 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/nlp/utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    35005 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/plugins.py
--rw-r--r--   0 runner    (1001) docker     (122)     7833 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/registry.py
--rw-r--r--   0 runner    (1001) docker     (122)    30201 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/regress_test_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     6049 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/service_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/task_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1593 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/tensor_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/timer.py
--rw-r--r--   0 runner    (1001) docker     (122)    10966 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/torch_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      597 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/trie.py
--rw-r--r--   0 runner    (1001) docker     (122)     1696 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/type_assert.py
--rw-r--r--   0 runner    (1001) docker     (122)      296 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/typing.py
--rw-r--r--   0 runner    (1001) docker     (122)      783 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/utils/url_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      272 2023-04-22 02:46:13.000000 modelscope-1.5.2/modelscope/version.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)    16425 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)   132706 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)       59 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (122)     4717 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)       11 2023-04-22 02:47:25.000000 modelscope-1.5.2/modelscope.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (122)       38 2023-04-22 02:47:25.000000 modelscope-1.5.2/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/
+-rw-r--r--   0 runner    (1001) docker     (122)       57 2023-05-22 03:10:12.000000 modelscope-1.6.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (122)    19082 2023-05-22 03:10:13.000000 modelscope-1.6.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    15726 2023-05-22 03:10:12.000000 modelscope-1.6.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/
+-rw-r--r--   0 runner    (1001) docker     (122)     3655 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/cli/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      829 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/cli.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1230 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/download.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6240 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/modelcard.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4369 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/cli/plugins.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/configs/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/configs/examples/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-05-22 03:09:28.000000 modelscope-1.6.0/modelscope/configs/examples/configuration.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/exporters/
+-rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/exporters/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2272 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/audio/ans_dfsmn_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2667 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      732 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/exporters/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)      869 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2593 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/cv/cartoon_translation_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3619 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/cv/face_detection_scrfd_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1245 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/exporters/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7341 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4579 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/nlp/model_for_token_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3409 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2316 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4809 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/tf_model_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15145 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/exporters/torch_model_exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/fileio/
+-rw-r--r--   0 runner    (1001) docker     (122)      122 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/file.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/fileio/format/
+-rw-r--r--   0 runner    (1001) docker     (122)      143 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/format/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      454 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/format/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1050 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/format/json.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13853 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/format/jsonplus.py
+-rw-r--r--   0 runner    (1001) docker     (122)      669 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/format/yaml.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4254 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/fileio/io.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/hub/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    40951 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/api.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3649 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/check_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1395 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/constants.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11358 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/deploy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4072 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/errors.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10249 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/file_download.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8958 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/git.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5386 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/push_to_hub.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12489 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/repository.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6535 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/snapshot_download.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/hub/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9916 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/utils/caching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2978 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/hub/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    54353 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)     3978 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/accuracy_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7530 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/action_detection_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1925 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/audio_noise_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1454 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1726 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/bleu_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/metrics/ciderD/
+-rwxr-xr-x   0 runner    (1001) docker     (122)       21 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/ciderD/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1926 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/ciderD/ciderD.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8931 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/ciderD/ciderD_scorer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8664 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_color_enhance_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13387 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_colorization_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10011 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_denoise_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_inpainting_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13318 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_instance_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1739 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_portrait_enhancement_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2536 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_quality_assessment_degradation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1632 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/image_quality_assessment_mos_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2231 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/inbatch_recall_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1358 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/loss_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3002 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/map_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2032 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/movie_scene_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/ned_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2310 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/ocr_recognition_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2082 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/ppl_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1163 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/prediction_saving_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4453 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/referring_video_object_segmentation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3112 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/sequence_classification_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/text_generation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/text_ranking_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5765 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/token_classification_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5703 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/translation_evaluation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5435 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_frame_interpolation_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8655 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_stabilization_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3092 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_summarization_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7030 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4771 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/metric_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8932 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/niqe.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1710 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      519 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/aec/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1434 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/activations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2700 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/affine_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5630 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/deep_fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1322 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/layer_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14384 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/aec/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14438 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/network/loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9382 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/network/modulation_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15396 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/aec/network/se_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/ans/
+-rw-r--r--   0 runner    (1001) docker     (122)      515 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/complex_nn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3688 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/conv_stft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2513 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/denoise_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10241 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/frcrn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/ans/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1468 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/layers/activations.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/layers/affine_transform.py
+-rw-r--r--   0 runner    (1001) docker     (122)      661 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/layers/layer_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5284 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1038 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/se_module_complex.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9833 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/ans/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/asr/
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/asr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/asr/generic_automatic_speech_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/itn/
+-rw-r--r--   0 runner    (1001) docker     (122)      557 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/itn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1462 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/itn/generic_inverse_text_processing.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/kws/
+-rw-r--r--   0 runner    (1001) docker     (122)      735 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14349 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7462 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7861 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/fsmn_sele_v3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3521 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2608 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/farfield/model_def.py
+-rw-r--r--   0 runner    (1001) docker     (122)      908 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/generic_key_word_spotting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/kws/nearfield/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/nearfield/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3300 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/nearfield/cmvn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17496 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/nearfield/fsmn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6079 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/kws/nearfield/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/punc/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/punc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1466 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/punc/generic_punctuation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/separation/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/separation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2240 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/separation/layer_norm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13963 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/separation/mossformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9142 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/separation/mossformer_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9004 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/separation/mossformer_conv_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/sv/
+-rw-r--r--   0 runner    (1001) docker     (122)     6905 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/DTDNN.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8425 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/DTDNN_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11515 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/ERes2Net.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14531 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/ecapa_tdnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)      904 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/fusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/generic_speaker_verification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3630 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/pooling_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16895 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/rdino.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11975 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/sv/speaker_change_locator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/audio/tts/
+-rw-r--r--   0 runner    (1001) docker     (122)      474 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/tts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11845 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/tts/sambert_hifi.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26390 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/audio/tts/voice.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/base/
+-rw-r--r--   0 runner    (1001) docker     (122)      303 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/base/base_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6994 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/base/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      605 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/base/base_torch_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5679 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/base/base_torch_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3647 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1812 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      490 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3806 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/
+-rw-r--r--   0 runner    (1001) docker     (122)      113 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/
+-rw-r--r--   0 runner    (1001) docker     (122)      211 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6414 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/
+-rw-r--r--   0 runner    (1001) docker     (122)      145 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6729 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      493 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7307 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/action_detection_onnx.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9364 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12132 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_detection/modules/resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/action_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      709 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4304 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_recognition/models.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10276 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_recognition/s3dg.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_recognition/tada_convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45442 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/animal_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/animal_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14141 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/animal_recognition/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/animal_recognition/splat.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/bad_image_detecting/
+-rw-r--r--   0 runner    (1001) docker     (122)      496 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/bad_image_detecting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2589 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12660 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8773 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/w48.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/
+-rw-r--r--   0 runner    (1001) docker     (122)      601 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/
+-rw-r--r--   0 runner    (1001) docker     (122)       51 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8957 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8196 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/
+-rw-r--r--   0 runner    (1001) docker     (122)       98 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11552 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12870 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8080 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2516 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7719 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/
+-rw-r--r--   0 runner    (1001) docker     (122)     1216 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/LK/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/LK/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/LK/lk.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      713 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/face_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5158 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/face_landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4563 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/facer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9075 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/model_tf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6072 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8519 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4404 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5265 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cartoon/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/
+-rw-r--r--   0 runner    (1001) docker     (122)      647 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3876 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/c3d.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8949 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      414 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13516 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4960 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      504 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10152 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2746 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5829 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8045 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15438 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9896 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25158 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12595 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3894 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9024 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8031 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/controlnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/crowd_counting/
+-rw-r--r--   0 runner    (1001) docker     (122)      491 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/crowd_counting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1100 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/crowd_counting/cc_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22875 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_attribute_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      490 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_attribute_recognition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_attribute_recognition/fair_face/
+-rw-r--r--   0 runner    (1001) docker     (122)      115 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2559 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      930 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/detectors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4675 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/mogface.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5578 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6264 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/resnet.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7072 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/
+-rw-r--r--   0 runner    (1001) docker     (122)       97 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7061 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5415 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5232 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3444 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3578 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5129 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4210 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4832 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/models/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4766 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/models/net.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4590 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/
+-rw-r--r--   0 runner    (1001) docker     (122)      174 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      192 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/
+-rw-r--r--   0 runner    (1001) docker     (122)      325 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2945 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      292 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3401 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      276 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      471 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11707 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4169 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    30670 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5543 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      289 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      361 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4202 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3590 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16146 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      270 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    46999 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      295 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11062 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6092 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6395 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5970 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py
+-rw-r--r--   0 runner    (1001) docker     (122)      960 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/
+-rw-r--r--   0 runner    (1001) docker     (122)       90 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1477 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4126 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2058 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      571 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3719 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2845 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4621 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1497 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/efficient/
+-rw-r--r--   0 runner    (1001) docker     (122)      327 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/efficient/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15911 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/efficient/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20077 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/efficient/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2007 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/emotion_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/emotion_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/face_alignment/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/face_alignment/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2844 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/face_alignment/face.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_emotion/face_alignment/face_align.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/op/
+-rwxr-xr-x   0 runner    (1001) docker     (122)       89 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/op/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6497 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3104 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/op/fused_act.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5922 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/op/upfirdn2d.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    19998 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_generation/stylegan2.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4109 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/det_infer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12584 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16846 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1833 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5791 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8874 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/align_face.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      473 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1080 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1977 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8551 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4744 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7186 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31337 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/bfm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1428 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2583 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6561 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5124 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30403 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21235 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8821 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py
+-rw-r--r--   0 runner    (1001) docker     (122)      277 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/opt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31957 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5908 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)      779 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13410 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/renderer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31247 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/face_reconstruction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      484 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/
+-rw-r--r--   0 runner    (1001) docker     (122)      121 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2414 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3277 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/flc/
+-rw-r--r--   0 runner    (1001) docker     (122)      115 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4932 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/hand_static/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/hand_static/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2480 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/hand_static/hand_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10891 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/hand_static/networks.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/
+-rw-r--r--   0 runner    (1001) docker     (122)     5936 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/Reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/
+-rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/Embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4542 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11301 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2418 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/Surface_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2716 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/detectors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1982 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/geometry.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11865 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/networks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6105 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/human_reconstruction/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2859 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19111 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/bnext.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/
+-rw-r--r--   0 runner    (1001) docker     (122)      500 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3920 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6541 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13616 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/person_info.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12027 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5673 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1311 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15760 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/slim_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      598 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      107 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20295 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/backbones/beit_v2.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17261 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/backbones/nextvit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2185 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/mmcls_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1554 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/resnet50_cc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_classification/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/
+-rw-r--r--   0 runner    (1001) docker     (122)      704 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/adaint/
+-rw-r--r--   0 runner    (1001) docker     (122)       44 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/adaint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14338 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/adaint/adaint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3753 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/csrnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/deeplpf/
+-rw-r--r--   0 runner    (1001) docker     (122)       66 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/deeplpf/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2575 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31764 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2821 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_color_enhance/image_color_enhance.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/
+-rw-r--r--   0 runner    (1001) docker     (122)      122 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9385 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6437 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9576 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/loss.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6857 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7796 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6465 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6510 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/unet/
+-rw-r--r--   0 runner    (1001) docker     (122)      129 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/unet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10574 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/unet/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10647 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_colorization/unet/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_debanding/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_debanding/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_debanding/rrdb/
+-rw-r--r--   0 runner    (1001) docker     (122)       53 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_debanding/rrdb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3005 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_deblur/
+-rw-r--r--   0 runner    (1001) docker     (122)      510 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_deblur/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4241 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4329 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12474 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3141 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      448 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6827 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7064 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10779 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1220 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1732 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4770 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23599 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4803 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3595 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py
+-rw-r--r--   0 runner    (1001) docker     (122)      583 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2617 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11013 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/
+-rw-r--r--   0 runner    (1001) docker     (122)      514 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet/
+-rw-r--r--   0 runner    (1001) docker     (122)     6664 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1627 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet/arch_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6757 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18233 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10106 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26006 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13560 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1600 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2606 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1485 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2819 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2506 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8545 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_driving_perception/
+-rw-r--r--   0 runner    (1001) docker     (122)      999 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_driving_perception/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_driving_perception/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_driving_perception/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3124 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21432 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/
+-rw-r--r--   0 runner    (1001) docker     (122)      242 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6497 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3094 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5912 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facelib/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facelib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10514 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9077 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/image_face_fusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3045 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/aad_layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8555 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9366 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/bfm.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12449 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/dense_motion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20604 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7433 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/model_irse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7790 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/ops.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/
+-rw-r--r--   0 runner    (1001) docker     (122)      575 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      525 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11736 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/
+-rw-r--r--   0 runner    (1001) docker     (122)      640 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8257 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8361 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15097 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5633 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_human_parsing/parsing_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2690 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7605 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/default.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ade20k/
+-rw-r--r--   0 runner    (1001) docker     (122)       81 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12202 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5348 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7509 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/adversarial.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1564 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/feature_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19052 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ffc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11842 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/inception.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/perceptual.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13350 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_inpainting/refinement.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)     1065 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      664 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3826 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27174 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9305 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      101 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3992 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/fastinst/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/fastinst/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14867 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6266 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8692 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/fastinst_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/
+-rw-r--r--   0 runner    (1001) docker     (122)      600 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10072 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15407 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18188 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7274 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2720 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6712 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1429 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11454 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1549 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7886 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/config/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6991 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/config/default.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/
+-rw-r--r--   0 runner    (1001) docker     (122)      171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      588 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7244 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3784 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3012 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2994 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9677 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10531 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3048 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2132 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2735 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/quadtree_attention_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      344 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_matching/utils/misc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      521 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8546 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18175 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9861 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9404 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21735 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3367 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_paintbyexample/
+-rw-r--r--   0 runner    (1001) docker     (122)      507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_paintbyexample/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_paintbyexample/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_panoptic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      513 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_panoptic_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1694 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8563 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/align_faces.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/eqface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1836 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4710 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    22869 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/gpen.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7114 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4336 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3286 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3155 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7313 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/
+-rwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4845 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4676 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4120 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_probing_model/
+-rw-r--r--   0 runner    (1001) docker     (122)      533 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_probing_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10076 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_probing_model/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3319 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_probing_model/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5206 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_probing_model/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/
+-rw-r--r--   0 runner    (1001) docker     (122)      584 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5064 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4654 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2600 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5163 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/maniqa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23230 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/swin.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      272 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16222 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1060 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2651 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_reid_person/
+-rw-r--r--   0 runner    (1001) docker     (122)      467 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_reid_person/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5444 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_reid_person/pass_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14108 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_reid_person/transreid_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_restoration/
+-rw-r--r--   0 runner    (1001) docker     (122)      527 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_restoration/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_restoration/demoire_models/
+-rw-r--r--   0 runner    (1001) docker     (122)       69 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_restoration/demoire_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10243 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_restoration/demoire_models/nets.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2640 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_restoration/image_restoration_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      712 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7896 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5155 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2273 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3208 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1516 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2122 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/
+-rw-r--r--   0 runner    (1001) docker     (122)      303 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      290 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      249 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17484 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/
+-rw-r--r--   0 runner    (1001) docker     (122)      196 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18241 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6379 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      251 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10785 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26514 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/
+-rw-r--r--   0 runner    (1001) docker     (122)      253 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12370 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11715 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      368 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      398 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1938 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1788 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/
+-rw-r--r--   0 runner    (1001) docker     (122)      650 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9312 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/
+-rw-r--r--   0 runner    (1001) docker     (122)     3740 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21275 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18023 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/unet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11210 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/skychange.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7874 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_skychange/skychange_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      120 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/data/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10476 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      603 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24469 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/losses.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      127 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3309 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/data/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10527 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/model_translation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12670 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12594 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/models/clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)      384 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21828 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/apps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36549 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/degradation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24615 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1320 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/losses.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6736 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/random_color.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2745 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3777 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/svd.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6728 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      486 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)      136 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4099 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6931 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2510 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3251 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14887 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5518 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3406 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4808 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1834 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/panovit.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      542 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6441 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/summarizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      508 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1900 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     7275 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      826 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2690 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2151 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26783 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13594 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/mdm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5412 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/respace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3914 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/smpl.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/get_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8465 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      181 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      798 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4291 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10325 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5057 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/
+-rw-r--r--   0 runner    (1001) docker     (122)      602 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/dataloader/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/dataloader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18782 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20567 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7297 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7876 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13317 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/nerf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1509 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5720 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      606 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3432 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/
+-rw-r--r--   0 runner    (1001) docker     (122)      321 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      211 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21306 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      278 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2220 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11529 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      213 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9021 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      426 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      375 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8542 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      239 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17739 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21926 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1204 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/
+-rw-r--r--   0 runner    (1001) docker     (122)      467 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2657 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/
+-rw-r--r--   0 runner    (1001) docker     (122)      605 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/
+-rw-r--r--   0 runner    (1001) docker     (122)      299 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6670 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/
+-rw-r--r--   0 runner    (1001) docker     (122)      282 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4476 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/
+-rw-r--r--   0 runner    (1001) docker     (122)      276 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1064 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2041 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      294 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3095 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      522 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7936 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8689 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12874 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      282 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7442 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    59197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/
+-rw-r--r--   0 runner    (1001) docker     (122)      255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9533 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9032 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      562 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18126 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5175 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11047 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/result_vis.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      473 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2870 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18335 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/modules/dbnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8217 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2647 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7972 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      477 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6164 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)      517 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/convnextvit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3451 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/crnn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/vitstr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3603 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/ocr_recognition/preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/open_vocabulary_detection_vild/
+-rw-r--r--   0 runner    (1001) docker     (122)      501 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/open_vocabulary_detection_vild/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14225 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      511 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5077 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7500 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7839 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14421 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9175 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3962 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3307 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/pedestrian_attribute_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pedestrian_attribute_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3468 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pedestrian_attribute_recognition/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      495 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16147 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11898 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1797 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18618 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/
+-rw-r--r--   0 runner    (1001) docker     (122)      548 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20974 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/item_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4340 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/item_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/product_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      528 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7978 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_segmentation/net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2194 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/product_segmentation/seg_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      514 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5832 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      318 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8504 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9206 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7299 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7935 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6242 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16610 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2091 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5629 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5180 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27941 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/robust_image_classification/
+-rw-r--r--   0 runner    (1001) docker     (122)      501 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/robust_image_classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/robust_image_classification/easyrobust_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      497 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      213 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)     6558 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py
+-rw-r--r--   0 runner    (1001) docker     (122)      256 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6525 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3047 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/senet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12007 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/u2net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3524 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/models/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2704 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/salient_detection/salient_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2203 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4385 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/head_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32989 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/models.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9319 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/neck_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5632 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/shop_seg_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4095 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/shop_seg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6699 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/shop_segmentation/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/detection_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/detection_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2532 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/inpainting_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/inpainting_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5759 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3233 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10898 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3911 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4799 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5003 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1035 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2110 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3890 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/unet_deploy.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9917 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1169 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/skin_retouching/weights_init.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/
+-rw-r--r--   0 runner    (1001) docker     (122)      526 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/data/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2094 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/data/data_augment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/
+-rw-r--r--   0 runner    (1001) docker     (122)      165 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      274 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/base_exp.py
+-rw-r--r--   0 runner    (1001) docker     (122)      392 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/build.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/default/
+-rw-r--r--   0 runner    (1001) docker     (122)      137 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/default/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1845 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/yolox_base.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      238 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6274 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/darknet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10911 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6156 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/network_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1314 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/streamyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5669 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/tal_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4352 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/realtime_video_detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      270 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3681 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/utils/boxes.py
+-rw-r--r--   0 runner    (1001) docker     (122)      209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/stream_yolo/utils/format.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      555 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7652 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/super_resolution/arch_util.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10508 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/super_resolution/ecb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3525 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/super_resolution/ecbsr_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/super_resolution/rrdbnet_arch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/
+-rw-r--r--   0 runner    (1001) docker     (122)      462 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15842 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/lineless_table_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3360 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/model_lore.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12614 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/modules/lore_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15090 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/table_recognition/modules/lore_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)      777 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7587 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4143 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6145 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18951 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16418 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5165 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/
+-rw-r--r--   0 runner    (1001) docker     (122)      594 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    43681 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/basic_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2174 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/global_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2902 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/master_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)      766 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/model_zoo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7405 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14983 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8709 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6923 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      699 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/apis/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2067 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4000 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3460 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8123 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2345 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5911 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2842 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/
+-rw-r--r--   0 runner    (1001) docker     (122)      148 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (122)      621 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3701 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9356 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7465 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14261 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10559 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13101 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    18626 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7526 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      561 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      409 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12602 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19162 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5588 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12284 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/
+-rw-r--r--   0 runner    (1001) docker     (122)      421 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7347 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24842 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3499 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/detectors/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/detectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2809 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2627 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2774 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      189 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11594 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5838 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1164 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6400 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)      627 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py
+-rw-r--r--   0 runner    (1001) docker     (122)      643 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/tinynas_detector.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/tinynas_detection/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/
+-rw-r--r--   0 runner    (1001) docker     (122)     2896 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py
+-rw-r--r--   0 runner    (1001) docker     (122)      494 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      773 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3159 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/archs.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1441 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2972 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/enh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/fre.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3716 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/configs/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/configs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12753 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/configs/default_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6731 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/dro_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5494 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/camera.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2285 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3737 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/pose.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3685 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5277 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2356 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10098 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7204 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4387 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10532 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8902 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1607 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1915 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/optim/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15781 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8039 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6815 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10248 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15068 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/depth.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1153 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/horovod.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10965 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9407 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6499 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/load.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2726 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1348 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/types.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/
+-rw-r--r--   0 runner    (1001) docker     (122)     1894 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3334 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      458 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3212 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9228 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5346 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/
+-rw-r--r--   0 runner    (1001) docker     (122)    13745 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4049 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3823 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16050 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36415 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3314 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2907 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/
+-rw-r--r--   0 runner    (1001) docker     (122)      558 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1324 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/
+-rw-r--r--   0 runner    (1001) docker     (122)       36 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10465 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2662 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5506 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/effv2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2904 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/lraspp.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2234 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/matting.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      524 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11056 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_inpainting/inpainting.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13727 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_inpainting/inpainting_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      582 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18236 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21551 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20983 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4136 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/neck/
+-rw-r--r--   0 runner    (1001) docker     (122)      525 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11733 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/track/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/track/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26843 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10771 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4163 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18059 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/video_knet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)      541 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2970 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2420 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/decode.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1890 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4921 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1084 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3117 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14995 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2230 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9570 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7296 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2888 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      497 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/aggregate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3582 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/cbam.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1979 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/eval_network.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/inference_core.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7037 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/mod_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)      974 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15309 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/modules.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5593 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/network.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      477 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/backbone/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/backbone/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10040 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26628 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/
+-rw-r--r--   0 runner    (1001) docker     (122)      515 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8349 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20677 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28653 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4081 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6696 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8627 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5885 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/neck/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/neck/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/track/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/track/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8508 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17155 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5440 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/config/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1024 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5004 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4805 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1234 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3380 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3393 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)      653 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12278 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py
+-rw-r--r--   0 runner    (1001) docker     (122)      943 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4473 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/tracker/
+-rw-r--r--   0 runner    (1001) docker     (122)      114 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/tracker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7123 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/
+-rw-r--r--   0 runner    (1001) docker     (122)    14702 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4715 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3290 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9214 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5275 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5526 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3922 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/Smoother.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1447 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7292 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7981 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3452 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     4684 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14212 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22544 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2893 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2817 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14474 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/image_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4977 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/math_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/
+-rw-r--r--   0 runner    (1001) docker     (122)      487 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7097 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5774 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4848 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9990 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3905 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/
+-rw-r--r--   0 runner    (1001) docker     (122)      536 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4959 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/base_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/kts/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/kts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1221 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/kts/cpd_auto.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3238 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13098 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/pgl_sum.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8812 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_summarization/summarizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      689 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14118 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/basicvsr_net.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4727 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3636 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3637 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/vidt/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vidt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    40074 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vidt/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25588 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vidt/deformable_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7436 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vidt/fpn_fusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16697 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vidt/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3651 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vidt/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/virual_tryon/
+-rw-r--r--   0 runner    (1001) docker     (122)      464 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/virual_tryon/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17690 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/virual_tryon/sdafnet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)      502 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16102 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)      797 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1786 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9340 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/petl.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5021 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28715 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5040 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5614 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/vision_middleware/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_middleware/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5749 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_middleware/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28089 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_middleware/head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6494 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_middleware/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vision_middleware/vim.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/
+-rw-r--r--   0 runner    (1001) docker     (122)      919 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12238 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5188 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/basic_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13520 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5299 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/model_se.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5177 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/cv/vop_retrieval/tokenization_clip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)     2182 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       97 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14496 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip/bert_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3898 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip/configuration_bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22351 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20998 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip/modeling_bert.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip_interrogator/
+-rw-r--r--   0 runner    (1001) docker     (122)       37 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip_interrogator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24293 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/clip_interrogator/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      140 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26652 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14427 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39910 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/structbert.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11834 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/unet_generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8566 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12361 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44602 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/dpm_solver_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/efficient_diffusion_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10302 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/gemm/
+-rw-r--r--   0 runner    (1001) docker     (122)      139 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/gemm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21694 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/gemm/gemm_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3718 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/gemm/gemm_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/gemm/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      548 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36265 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2969 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/respace.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1365 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/script.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36898 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mgeo/
+-rw-r--r--   0 runner    (1001) docker     (122)     1444 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mgeo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)   101942 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mgeo/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8651 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mgeo/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3268 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mgeo/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10331 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mgeo/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/
+-rw-r--r--   0 runner    (1001) docker     (122)      141 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/dataloaders/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/dataloaders/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3789 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/
+-rw-r--r--   0 runner    (1001) docker     (122)      162 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9790 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1442 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21257 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/modeling.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18790 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/module_clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3577 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/module_cross.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5581 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4116 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/until_module.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/
+-rw-r--r--   0 runner    (1001) docker     (122)      739 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       86 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16605 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/clip/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6030 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/configuration_mplug.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)   120753 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/modeling_mplug.py
+-rw-r--r--   0 runner    (1001) docker     (122)    34049 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/mvit.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    21332 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5536 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug_for_all_tasks.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug_owl/
+-rw-r--r--   0 runner    (1001) docker     (122)      835 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug_owl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10293 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug_owl/configuration_mplug_owl.py
+-rw-r--r--   0 runner    (1001) docker     (122)    64194 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/mplug_owl/modeling_mplug_owl.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      150 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10208 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11503 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28436 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13487 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6898 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16675 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6565 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)      306 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18720 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16552 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/configuration_ofa.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1785 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21049 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5658 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)    43439 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/search.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41976 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16690 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44296 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py
+-rw-r--r--   0 runner    (1001) docker     (122)   100933 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/modeling_ofa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13226 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15028 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/tokenization_ofa.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8004 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      676 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/utils/constant.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1877 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8435 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa/vit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24779 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa_for_all_tasks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12983 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/rleg/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/rleg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4865 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/rleg/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3410 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/rleg/rleg.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/
+-rw-r--r--   0 runner    (1001) docker     (122)      678 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9838 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/blocks.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11896 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/clip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6165 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22584 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/swin_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4885 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1597 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/soonet/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/team/
+-rw-r--r--   0 runner    (1001) docker     (122)      140 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/team/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5181 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/team/team_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11502 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/team/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18477 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9264 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9504 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38498 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/unet_sd.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/
+-rw-r--r--   0 runner    (1001) docker     (122)       93 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11339 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16687 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    47348 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20956 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/processing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4680 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7333 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/multi_modal/vldoc/transformer_local.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/T5/
+-rw-r--r--   0 runner    (1001) docker     (122)      599 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/T5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    67374 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/T5/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7541 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/T5/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21467 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/T5/text2text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6368 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/bart/
+-rw-r--r--   0 runner    (1001) docker     (122)      112 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bart/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3058 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bart/text_error_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/bert/
+-rw-r--r--   0 runner    (1001) docker     (122)     1519 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    40458 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7308 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4113 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/document_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)      512 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6074 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/sentence_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7153 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/siamese_uie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1478 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1138 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2140 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/token_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6904 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bert/word_alignment.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/bloom/
+-rw-r--r--   0 runner    (1001) docker     (122)      472 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bloom/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      505 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/bloom/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/canmt/
+-rw-r--r--   0 runner    (1001) docker     (122)      102 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/canmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    52235 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/canmt/canmt_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2940 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/canmt/canmt_translation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35688 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/canmt/sequence_generator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      730 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    35473 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/codegeex.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3995 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4008 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9997 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/inference.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6111 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/codegeex/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/csanmt/
+-rw-r--r--   0 runner    (1001) docker     (122)       96 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/csanmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    61868 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/csanmt/translation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1771 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    47998 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6771 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10364 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21421 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10698 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/deberta_v2/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/dgds/
+-rw-r--r--   0 runner    (1001) docker     (122)      939 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/dgds/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7092 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/dgds/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1656 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/fid_T5/
+-rw-r--r--   0 runner    (1001) docker     (122)     1082 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/fid_T5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8108 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/fid_T5/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/fid_plug/
+-rw-r--r--   0 runner    (1001) docker     (122)     1181 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/fid_plug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45082 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/fid_plug/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5045 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/fid_plug/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6709 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/fid_plug/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/
+-rw-r--r--   0 runner    (1001) docker     (122)      540 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/generation/
+-rw-r--r--   0 runner    (1001) docker     (122)       87 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/generation/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10112 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/generation/strategies.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5152 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/kernels/
+-rw-r--r--   0 runner    (1001) docker     (122)     3263 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/kernels/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/
+-rw-r--r--   0 runner    (1001) docker     (122)     2635 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/functional.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4510 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/layers.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13504 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/glm_130b/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/gpt2/
+-rw-r--r--   0 runner    (1001) docker     (122)      470 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      498 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt2/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/
+-rw-r--r--   0 runner    (1001) docker     (122)      852 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15811 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8971 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    51600 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/distributed_gpt3.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2847 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2586 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt3/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/
+-rw-r--r--   0 runner    (1001) docker     (122)      874 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13129 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5198 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4930 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49316 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3504 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/layer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2553 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/mappings.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23756 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4110 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2717 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2277 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_moe/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_neo/
+-rw-r--r--   0 runner    (1001) docker     (122)      474 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_neo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      518 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/gpt_neo/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/heads/
+-rw-r--r--   0 runner    (1001) docker     (122)      661 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25248 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/crf_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6947 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/fill_mask_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5145 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/infromation_extraction_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2056 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/text_classification_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      964 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/text_generation_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2029 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/text_ranking_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2596 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/token_classification_head.py
+-rw-r--r--   0 runner    (1001) docker     (122)      948 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/heads/torch_pretrain_head.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/hf_transformers/
+-rw-r--r--   0 runner    (1001) docker     (122)      488 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/hf_transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4897 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/hf_transformers/backbone.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/llama/
+-rw-r--r--   0 runner    (1001) docker     (122)      866 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    29197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4695 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11349 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7212 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10333 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4720 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/llama/tokenization_fast.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/lstm/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/lstm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1312 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/lstm/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2187 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/lstm/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/megatron_bert/
+-rw-r--r--   0 runner    (1001) docker     (122)      688 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/megatron_bert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39864 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/megatron_bert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6599 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/megatron_bert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12452 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/megatron_bert/fill_mask.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/
+-rw-r--r--   0 runner    (1001) docker     (122)      572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    28115 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/arguments.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28162 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/blocklm_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17995 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/configure_data.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)    13656 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    20350 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/corpora.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45765 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3342 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/extraction.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     8459 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9797 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7221 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/samplers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4661 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    53304 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/tokenization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14551 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15773 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/wordpiece.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21669 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/generation_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16247 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/
+-rwxr-xr-x   0 runner    (1001) docker     (122)      984 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     5443 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9967 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/downstream.py
+-rw-r--r--   0 runner    (1001) docker     (122)    70249 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/modeling_bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8743 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/modeling_glm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2531 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48886 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/model/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/process_grid.py
+-rw-r--r--   0 runner    (1001) docker     (122)      203 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/run_test.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/test/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/test/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      950 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/test/test_block.py
+-rw-r--r--   0 runner    (1001) docker     (122)      704 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/test/test_rel_shift.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17815 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/train_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19029 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/mglm/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/palm_v2/
+-rw-r--r--   0 runner    (1001) docker     (122)     1268 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/palm_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5532 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/palm_v2/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29453 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/palm_v2/dureader_eval.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55450 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/palm_v2/text_generation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/peer/
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/peer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    55780 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/peer/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11716 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/peer/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6166 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/peer/sas_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4933 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/peer/text_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/plug/
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3321 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug/AnnealingLR.py
+-rw-r--r--   0 runner    (1001) docker     (122)      660 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11797 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10983 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug/distributed_plug.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8427 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug/generator.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/plug_mental/
+-rw-r--r--   0 runner    (1001) docker     (122)     1359 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug_mental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7671 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug_mental/adv_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    47486 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug_mental/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7956 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug_mental/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10881 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/plug_mental/text_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/
+-rw-r--r--   0 runner    (1001) docker     (122)     1490 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    37918 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6366 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4176 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/document_segmentation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11011 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7147 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/ponet/tokenization.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)      987 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1208 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3832 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/dialog_intent_prediction.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4354 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/dialog_modeling.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16648 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/dialog_state_tracking.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/
+-rw-r--r--   0 runner    (1001) docker     (122)      421 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10281 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/gen_unified_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10757 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/generator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7505 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/intent_unified_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3032 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/model_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1189 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/tokenization_space.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11941 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/model/unified_transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2398 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/embedder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      956 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/feedforward.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1721 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/functions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3397 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/multihead_attention.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1922 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space/modules/transformer_block.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_cn/
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_cn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44438 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_cn/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5194 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_cn/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30117 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_cn/table_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)      492 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3715 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/space_T_en/text_to_sql.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/
+-rw-r--r--   0 runner    (1001) docker     (122)     1674 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7673 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/adv_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    40848 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7686 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27203 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/faq_question_answering.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12581 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11862 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11074 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/structbert/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/
+-rw-r--r--   0 runner    (1001) docker     (122)     1453 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5323 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/feature_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2698 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)      766 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/information_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28991 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/task_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8678 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/text_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2092 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/text_ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5116 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/task_models/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/unite/
+-rw-r--r--   0 runner    (1001) docker     (122)      626 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/unite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      409 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/unite/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18320 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/unite/translation_evaluation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/use/
+-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/use/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5133 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/use/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5795 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/use/user_satisfaction_estimation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/veco/
+-rw-r--r--   0 runner    (1001) docker     (122)     1479 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/veco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4031 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/veco/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1192 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/veco/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4278 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/veco/fill_mask.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6643 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/veco/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4129 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/veco/token_classification.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/
+-rw-r--r--   0 runner    (1001) docker     (122)     1156 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42932 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/backbone.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7697 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/configuration.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      491 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/science/unifold/
+-rw-r--r--   0 runner    (1001) docker     (122)       46 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24057 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/
+-rw-r--r--   0 runner    (1001) docker     (122)      634 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49139 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/data_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19691 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/msa_pairing.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8941 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/process.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14792 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/process_multimer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11422 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/protein.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42023 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/residue_constants.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4628 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/data/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19250 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/model.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)      187 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17066 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/alphafold.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12323 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/attentions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5403 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/auxillary_heads.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10676 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5814 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/confidence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8771 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/embedders.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11078 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/evoformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6820 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/featurization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18025 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/frame.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18476 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/structure_module.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9771 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/template.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5623 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/modules/triangle_multiplication.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/
+-rw-r--r--   0 runner    (1001) docker     (122)       46 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17940 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/mmcif.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3128 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/msa_identifiers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23503 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/parsers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45468 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/templates.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/
+-rw-r--r--   0 runner    (1001) docker     (122)      639 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6218 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hhblits.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3991 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hhsearch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5098 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hmmbuild.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5013 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hmmsearch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8662 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/jackhmmer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3752 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/kalign.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2892 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/models/science/unifold/msa/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/
+-rw-r--r--   0 runner    (1001) docker     (122)       84 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      346 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/audio/asr_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/auth/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/auth/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1326 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/auth/auth_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/context/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3369 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/context/dataset_context_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/data_files/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/data_files/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5110 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/data_files/data_files_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/data_loader/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/data_loader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12704 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/data_loader/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5767 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/data_loader/data_loader_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/
+-rw-r--r--   0 runner    (1001) docker     (122)      111 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)     4111 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      730 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1737 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8512 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7723 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11689 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/
+-rw-r--r--   0 runner    (1001) docker     (122)      541 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1264 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      791 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/
+-rw-r--r--   0 runner    (1001) docker     (122)      134 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4708 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)      945 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)      177 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3815 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16180 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (122)      934 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/
+-rw-r--r--   0 runner    (1001) docker     (122)      544 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/
+-rw-r--r--   0 runner    (1001) docker     (122)      312 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2622 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4858 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1469 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/
+-rw-r--r--   0 runner    (1001) docker     (122)      196 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1086 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2509 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1511 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2141 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/
+-rw-r--r--   0 runner    (1001) docker     (122)      539 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/
+-rw-r--r--   0 runner    (1001) docker     (122)      529 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2968 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12152 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12574 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/
+-rw-r--r--   0 runner    (1001) docker     (122)      577 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1091 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1629 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/
+-rw-r--r--   0 runner    (1001) docker     (122)      615 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1487 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/
+-rw-r--r--   0 runner    (1001) docker     (122)      583 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4849 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6225 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      559 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4013 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/
+-rw-r--r--   0 runner    (1001) docker     (122)      161 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1589 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4945 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5437 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/
+-rw-r--r--   0 runner    (1001) docker     (122)       40 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7547 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3416 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/
+-rw-r--r--   0 runner    (1001) docker     (122)      309 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3172 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      971 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5818 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2000 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3512 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)      707 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4912 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2393 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2017 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      599 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16767 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8725 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/
+-rw-r--r--   0 runner    (1001) docker     (122)      545 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1984 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2684 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5114 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1635 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2664 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/
+-rw-r--r--   0 runner    (1001) docker     (122)      573 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1390 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/
+-rw-r--r--   0 runner    (1001) docker     (122)      543 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      809 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2624 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/
+-rw-r--r--   0 runner    (1001) docker     (122)      553 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2370 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10687 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/dataset_cls/dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/download/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/download/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16684 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/download/dataset_builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      609 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/download/download_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2487 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/download/download_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/meta/
+-rw-r--r--   0 runner    (1001) docker     (122)       50 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/meta/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1772 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/meta/data_meta_config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8634 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/meta/data_meta_manager.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36466 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/ms_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/
+-rw-r--r--   0 runner    (1001) docker     (122)     1072 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      408 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/gopro_image_deblurring_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      401 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/sidd_image_denoising.py
+-rw-r--r--   0 runner    (1001) docker     (122)      410 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/torch_base_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)      404 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/task_datasets/video_summarization_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/msdatasets/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7610 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/utils/dataset_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1026 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/utils/delete_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5540 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/utils/maxcompute_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6124 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/utils/oss_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2496 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/msdatasets/utils/upload_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/ailut/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/ailut/Ailut/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/ailut/Ailut/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/ailut/Ailut/csrc/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/ailut/Ailut/csrc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      105 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/ailut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4314 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/ailut/pyinterfaces.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/
+-rw-r--r--   0 runner    (1001) docker     (122)      106 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/functions/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2925 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/modules/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13956 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/src/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/ops/quadtree_attention/src/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/outputs/
+-rw-r--r--   0 runner    (1001) docker     (122)      132 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/outputs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      908 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/outputs/cv_outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19855 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/outputs/nlp_outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    51098 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/outputs/outputs.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11135 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipeline_inputs.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/
+-rw-r--r--   0 runner    (1001) docker     (122)      150 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)     1567 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7481 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5016 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/ans_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23877 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/asr_inference_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3189 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4357 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3830 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/kws_farfield_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7276 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5555 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/linear_aec_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8056 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/lm_infer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6469 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/punctuation_processing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2560 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/separation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4042 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/speaker_change_locating_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11089 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/speaker_diarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4078 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/speaker_verification_eres2net_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4118 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10499 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/speaker_verification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4072 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/speaker_verification_rdino_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/text_to_speech_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11925 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/timestamp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9684 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22849 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7279 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)    16252 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2545 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/action_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4852 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/action_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4024 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/animal_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2576 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2614 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14219 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4740 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/card_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5307 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2528 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/content_check_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5055 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5942 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/crowd_counting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5136 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1965 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3980 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1531 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_emotion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2894 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_image_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3312 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3573 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7748 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_processing_base_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3872 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3528 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3493 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2878 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2498 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19630 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/face_reconstruction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2400 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2681 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4442 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/fast_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4030 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/general_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/hand_static_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2982 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/human_reconstruction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1399 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2919 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5251 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_cartoon_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5857 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3296 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_color_enhance_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4441 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2648 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_debanding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4624 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_deblur_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3517 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4160 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_denoise_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1943 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1854 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4134 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_driving_perception_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2317 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_face_fusion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4866 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_human_parsing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5854 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_inpainting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3903 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5929 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_matching_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2607 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_matting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2796 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2770 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6113 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8567 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3549 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2835 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2801 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2026 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_reid_person_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1702 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_restoration_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1644 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_salient_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3207 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2237 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_skychange_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2839 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4615 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_style_transfer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3138 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9718 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12737 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1883 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9872 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4570 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/license_plate_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4182 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5276 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/live_category_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2757 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2787 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3670 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1857 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/mog_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4844 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/motion_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2628 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1953 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3328 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6018 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/object_detection_3d_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11033 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_recognition_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)      966 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      720 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19821 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_dla34.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8768 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5286 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15421 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/
+-rw-r--r--   0 runner    (1001) docker     (122)      550 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11644 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1560 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38278 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19004 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11371 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11168 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/table_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7971 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3029 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9781 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4199 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1457 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1451 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/product_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2020 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9239 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/retina_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1760 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/shop_segmentation_pipleline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12129 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/skin_retouching_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4446 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/table_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4899 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/tbs_detection_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/cv/tbs_detection_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)       10 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/tbs_detection_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15542 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/tbs_detection_utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1841 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3246 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/tinynas_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3244 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/tinynas_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1888 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13776 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_category_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6097 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_colorization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7757 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_deinterlace_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1565 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24271 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3635 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_human_matting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1719 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_inpainting_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9652 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3294 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4724 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4683 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3436 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4647 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_stabilization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4289 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7078 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/video_super_resolution_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7564 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/vidt_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5240 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/virtual_try_on_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3986 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2180 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/vision_middleware_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4762 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/vop_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5787 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)     3071 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2366 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/asr_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/
+-rw-r--r--   0 runner    (1001) docker     (122)      636 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2013 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (122)      718 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11691 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2880 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/
+-rw-r--r--   0 runner    (1001) docker     (122)      585 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15631 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18987 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2311 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2931 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1093 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9352 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3219 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1612 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7869 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1648 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3325 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8561 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1848 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/sudoku_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1059 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1789 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/text2sql_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3556 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2150 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1329 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1956 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1780 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1781 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2396 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     6798 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6939 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3696 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/canmt_translation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2202 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2885 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2577 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2308 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7663 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2218 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1851 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4425 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/distributed_plug_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2754 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25997 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5480 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9623 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/document_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6259 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3348 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2437 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3258 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/feature_extraction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4655 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/fill_mask_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)      999 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2383 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/information_extraction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6363 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/interactive_translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9928 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/language_identification_pipline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1776 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2955 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3184 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14334 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/siamese_uie_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2566 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/summarization_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16315 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/table_question_answering_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6787 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/text_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3490 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/text_error_correction_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7735 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/text_generation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2877 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/text_ranking_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6399 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/token_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4372 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5586 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/translation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2339 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4613 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2713 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/word_alignment_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4523 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/word_segmentation_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5875 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2816 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/pipeline_template.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/pipelines/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      538 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/science/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8255 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/science/protein_structure_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3492 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/pipelines/util.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/
+-rw-r--r--   0 runner    (1001) docker     (122)     5792 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10063 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/asr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8680 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/audio.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15312 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)      812 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6982 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/common.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1896 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7446 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/action_detection_mapper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1126 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7789 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/controllable_image_generation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20909 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/cv2_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12826 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/image_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1250 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/image_quality_assessment_man.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2113 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/image_quality_assessment_mos.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2790 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/image_restoration_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2625 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/mmcls_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3050 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/timer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3606 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/util.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1501 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/video_stabilization.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8579 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/cv/video_super_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13406 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/image.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4884 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/kws.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/movie_scene_segmentation/
+-rw-r--r--   0 runner    (1001) docker     (122)      483 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/movie_scene_segmentation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10809 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/movie_scene_segmentation/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30147 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/multi_modal.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     5966 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3929 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/canmt_translation.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2572 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4102 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4391 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4086 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11004 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6844 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3304 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12086 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8199 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1168 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4023 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1424 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)     1219 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1908 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/args.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1555 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/batch.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3629 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2701 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5457 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    56779 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/dst_processors.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)      592 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33884 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/gen_field.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42465 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/intent_field.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1147 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/lazy_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1935 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1610 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2084 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/tensorlistdataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24803 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space/tokenizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/
+-rw-r--r--   0 runner    (1001) docker     (122)      654 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4936 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15867 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4888 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4175 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)      846 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4902 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/
+-rw-r--r--   0 runner    (1001) docker     (122)      818 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21521 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12500 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1380 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2111 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6422 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/text_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1641 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/text_clean.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2298 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/text_error_correction.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13896 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/text_generation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3834 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19475 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/token_classification_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1640 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1238 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4482 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/transformers_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7304 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/translation_evaluation_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3622 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2584 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)      762 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4839 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/asr.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11909 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4172 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/image_captioning.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6461 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/image_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5740 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/ocr_recognition.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4603 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/sudoku.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5210 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/summarization.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16449 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/text2sql.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5368 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/text_classification.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2164 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/text_to_image_synthesis.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3221 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/audio_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8936 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6809 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/collate.py
+-rw-r--r--   0 runner    (1001) docker     (122)      660 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/constant.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3421 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/get_tables.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1302 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/random_help.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5459 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/text2phone.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19116 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9581 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/utils/vision_helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7664 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/visual_entailment.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8484 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/visual_grounding.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6796 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/ofa/visual_question_answering.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/preprocessors/science/
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/science/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21698 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/science/uni_fold.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/tts.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13168 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/preprocessors/video.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/tools/
+-rw-r--r--   0 runner    (1001) docker     (122)       49 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      772 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tools/eval.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5362 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tools/speech_tts_autolabel.py
+-rw-r--r--   0 runner    (1001) docker     (122)      607 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tools/train.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/
+-rw-r--r--   0 runner    (1001) docker     (122)     1799 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)      826 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1873 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/ans_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6969 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/asr_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12816 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_farfield_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22065 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_nearfield_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)     1768 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22114 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/batch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11499 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/det_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7022 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4509 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2854 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/kws_utils/runtime_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21962 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/separation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10858 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/audio/tts_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4389 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1808 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5553 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cli_argument_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)     1974 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7494 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/action_detection_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      668 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/card_detection_scrfd_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10044 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/cartoon_translation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6737 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/face_detection_scrfd_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20323 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/image_classifition_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11655 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22965 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4405 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/image_inpainting_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      816 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/image_instance_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5520 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      680 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20149 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/nerf_recon_acc_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16911 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/ocr_detection_db_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/ocr_recognition_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2248 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4602 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2594 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/default_config.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/hooks/
+-rw-r--r--   0 runner    (1001) docker     (122)     2064 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      296 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/hooks/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (122)      116 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18438 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/checkpoint/checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10797 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/checkpoint/checkpoint_processor.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5541 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/checkpoint/load_checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)      764 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/hooks/compression/
+-rw-r--r--   0 runner    (1001) docker     (122)      610 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/compression/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4812 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/compression/sparsity_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6915 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/compression/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/hooks/distributed/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/distributed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1458 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/distributed/ddp_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8200 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/distributed/deepspeed_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6905 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/distributed/megatron_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4398 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/early_stop_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3825 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/evaluation_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)      731 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/iter_timer_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/hooks/logger/
+-rw-r--r--   0 runner    (1001) docker     (122)      718 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4498 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/logger/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4507 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/logger/tensorboard_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7443 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/logger/text_logger_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6402 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/lr_scheduler_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/hooks/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (122)      743 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3139 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3786 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/optimizer/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3729 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1707 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/hooks/priority.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/
+-rw-r--r--   0 runner    (1001) docker     (122)      699 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2040 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/
+-rw-r--r--   0 runner    (1001) docker     (122)      614 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2547 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2934 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/warmup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/
+-rw-r--r--   0 runner    (1001) docker     (122)      682 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/clip/
+-rw-r--r--   0 runner    (1001) docker     (122)       89 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9702 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/clip/clip_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4388 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2874 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8086 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/mplug/
+-rw-r--r--   0 runner    (1001) docker     (122)       91 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/mplug/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1642 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/ofa/
+-rw-r--r--   0 runner    (1001) docker     (122)       87 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/ofa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10186 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17647 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/team/
+-rw-r--r--   0 runner    (1001) docker     (122)       95 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/team/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5429 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/team/team_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2443 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/multi_modal/team/team_trainer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)     1332 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14266 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/csanmt_translation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9999 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24284 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8159 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12769 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/faq_question_answering_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3064 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/gpt3_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2097 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/gpt_moe_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8171 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/plug_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3869 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/sentence_embedding_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8378 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/sequence_classification_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17110 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/siamese_uie_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5324 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/dialog_intent_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4229 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36354 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/eval.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/metrics/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2447 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/trainer/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30567 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/trainer/gen_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    29570 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/space/trainer/intent_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20453 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/table_question_answering_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)      991 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/text_generation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7514 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/text_ranking_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15326 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp/translation_evaluation_trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7129 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/nlp_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (122)      223 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1753 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/optimizer/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7081 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/parallel/
+-rw-r--r--   0 runner    (1001) docker     (122)       80 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      681 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/parallel/builder.py
+-rw-r--r--   0 runner    (1001) docker     (122)      754 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/parallel/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    59078 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/trainer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17574 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/training_args.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/trainers/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11090 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/utils/inference.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1294 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/trainers/utils/log_buffer.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/tuners/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tuners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    38904 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tuners/control_sd_lora.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24058 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tuners/lora.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8840 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/tuners/sd_lora.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)   518673 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope/utils/ast_index_file.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28920 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/ast_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11795 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/audio/audio_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2428 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/audio/tts_exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26430 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2573 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/chinese_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27433 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/config.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1027 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/config_ds.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18753 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/constant.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/cv/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/cv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23195 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/cv/image_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/cv/motion_utils/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/cv/motion_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2307 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/cv/motion_utils/motion_process.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3847 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/cv/motion_utils/plot_script.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4290 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/cv/motion_utils/rotation_conversions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3153 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/data_collators.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1279 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3744 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/device.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6339 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/error.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1128 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/file_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5939 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/hub.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15823 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/import_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25538 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/input_output.py
+-rw-r--r--   0 runner    (1001) docker     (122)      478 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/json_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2789 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7612 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/megatron_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2335 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5099 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/model_tag.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/nlp/
+-rw-r--r--   0 runner    (1001) docker     (122)      499 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4424 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/distributed.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     4526 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/load_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/nlp/space/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1909 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/args.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11818 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/clean_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1439 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/criterions.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11252 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/db_ops.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6174 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/ontology.py
+-rw-r--r--   0 runner    (1001) docker     (122)      197 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/scores.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6356 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1054 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space/utils_dst.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:13.000000 modelscope-1.6.0/modelscope/utils/nlp/space_T_en/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space_T_en/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      859 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/space_T_en/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2554 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/nlp/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    39508 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/plugins.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7833 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/registry.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30195 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/regress_test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6127 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/service_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2500 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/task_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1593 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/tensor_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12532 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1209 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/timer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10966 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/torch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      597 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/trie.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1696 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/type_assert.py
+-rw-r--r--   0 runner    (1001) docker     (122)      296 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/typing.py
+-rw-r--r--   0 runner    (1001) docker     (122)      783 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/utils/url_utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)      272 2023-05-22 03:09:29.000000 modelscope-1.6.0/modelscope/version.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    19082 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)   131512 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       59 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (122)     4738 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       11 2023-05-22 03:10:12.000000 modelscope-1.6.0/modelscope.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-05-22 03:10:13.000000 modelscope-1.6.0/setup.cfg
```

### Comparing `modelscope-1.5.2/PKG-INFO` & `modelscope-1.6.0/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -1,37 +1,7 @@
-Metadata-Version: 2.1
-Name: modelscope
-Version: 1.5.2
-Summary: UNKNOWN
-Home-page: https://github.com/modelscope/modelscope
-Author: Alibaba ModelScope team
-Author-email: modelscope@list.alibaba-inc.com
-License: Apache License 2.0
-Keywords: python,nlp,science,cv,speech,multi-modal
-Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Description-Content-Type: text/markdown
-Provides-Extra: audio
-Provides-Extra: cv
-Provides-Extra: multi-modal
-Provides-Extra: nlp
-Provides-Extra: science
-Provides-Extra: audio_asr
-Provides-Extra: audio_kws
-Provides-Extra: audio_signal
-Provides-Extra: audio_tts
-Provides-Extra: all
-
 
 <p align="center">
     <br>
     <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
     <br>
 <p>
 
@@ -135,14 +105,18 @@
 
 * [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
 
 * [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
 
 * [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
 
+* [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)
+
+* [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)
+
 * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
 
 * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
 
 
 
 AI for Science:
@@ -311,9 +285,7 @@
 * [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
 * [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
 * [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
 
 # License
 
 This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
-
-
```

#### html2text {}

```diff
@@ -1,20 +1,7 @@
-Metadata-Version: 2.1 Name: modelscope Version: 1.5.2 Summary: UNKNOWN Home-
-page: https://github.com/modelscope/modelscope Author: Alibaba ModelScope team
-Author-email: modelscope@list.alibaba-inc.com License: Apache License 2.0
-Keywords: python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta Classifier: License :: OSI Approved
-:: Apache Software License Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3 Classifier: Programming
-Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
-Language :: Python :: 3.10 Description-Content-Type: text/markdown Provides-
-Extra: audio Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp
-Provides-Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws
-Provides-Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
@@ -95,107 +82,110 @@
 cv_resnest101_general_recognition) Audio: * [speech_paraformer-large_asr_nat-
 zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/
 speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch) *
 [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/
 speech_sambert-hifigan_tts_zh-cn_16k) * [speech_charctc_kws_phone-xiaoyun]
 (https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun) *
 [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/
-u2pp_conformer-asr-cn-16k-online) * [speech_frcrn_ans_cirm_16k](https://
-modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k) *
-[speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/
-speech_dfsmn_aec_psm_16k) AI for Science: * [uni-fold-monomer](https://
-modelscope.cn/models/DPTech/uni-fold-monomer/summary) * [uni-fold-multimer]
-(https://modelscope.cn/models/DPTech/uni-fold-multimer/summary) **Note:** Most
-models on ModelScope are public and can be downloaded without account
-registration on modelscope website([www.modelscope.cn](www.modelscope.cn)),
-please refer to instructions for [model download](https://modelscope.cn/docs/
-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api
-provided by modelscope library or git. # QuickTour We provide unified interface
-for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for
-different tasks. For any given task with any type of input (image, text, audio,
-video...), inference pipeline can be implemented with only a few lines of code,
-which will automatically load the underlying model to get inference result, as
-is exemplified below: ```python >>> from modelscope.pipelines import pipeline
->>> word_segmentation = pipeline('word-segmentation',model='damo/
-nlp_structbert_word-segmentation_chinese-base') >>> word_segmentation
-('') {'output': '  
-   '} ``` Given an image, portrait matting (aka.
-background-removal) can be accomplished with the following code snippet: !
-[image](data/resource/portrait_input.png) ```python >>> import cv2 >>> from
-modelscope.pipelines import pipeline >>> portrait_matting = pipeline('portrait-
-matting') >>> result = portrait_matting('https://modelscope.oss-cn-
-beijing.aliyuncs.com/test/images/image_matting.png') >>> cv2.imwrite
-('result.png', result['output_img']) ``` The output image with the background
-removed is: ![image](data/resource/portrait_output.png) Fine-tuning and
-evaluation can also be done with a few more lines of code to set up training
-dataset and trainer, with the heavy-lifting work of training and evaluation a
-model encapsulated in the implementation of `traner.train()` and
-`trainer.evaluate()` interfaces. For example, the gpt3 base model (1.3B) can be
-fine-tuned with the chinese-poetry dataset, resulting in a model that can be
-used for chinese-poetry generation. ```python >>> from modelscope.metainfo
-import Trainers >>> from modelscope.msdatasets import MsDataset >>> from
-modelscope.trainers import build_trainer >>> train_dataset = MsDataset.load
-('chinese-poetry-collection', split='train'). remap_columns({'text1':
-'src_txt'}) >>> eval_dataset = MsDataset.load('chinese-poetry-collection',
-split='test').remap_columns({'text1': 'src_txt'}) >>> max_epochs = 10 >>>
-tmp_dir = './gpt3_poetry' >>> kwargs = dict( model='damo/nlp_gpt3_text-
-generation_1.3B', train_dataset=train_dataset, eval_dataset=eval_dataset,
-max_epochs=max_epochs, work_dir=tmp_dir) >>> trainer = build_trainer
-(name=Trainers.gpt3_trainer, default_args=kwargs) >>> trainer.train() ``` # Why
-should I use ModelScope library 1. A unified and concise user interface is
-abstracted for different tasks and different models. Model inferences and
-training can be implemented by as few as 3 and 10 lines of code, respectively.
-It is convenient for users to explore models in different fields in the
-ModelScope community. All models integrated into ModelScope are ready to use,
-which makes it easy to get started with AI, in both educational and industrial
-settings. 2. ModelScope offers a model-centric development and application
-experience. It streamlines the support for model training, inference, export
-and deployment, and facilitates users to build their own MLOps based on the
-ModelScope ecosystem. 3. For the model inference and training process, a
-modular design is put in place, and a wealth of functional module
-implementations are provided, which is convenient for users to customize their
-own model inference, training and other processes. 4. For distributed model
-training, especially for large models, it provides rich training strategy
-support, including data parallel, model parallel, hybrid parallel and so on. #
-Installation ## Docker ModelScope Library currently supports popular deep
-learning framework for model training and inference, including PyTorch,
-TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch
-1.8+, Tensorflow1.15 or Tensorflow2.0+. To allow out-of-box usage for all the
-models on ModelScope, official docker images are provided for all releases.
-Based on the docker image, developers can skip all environment installation and
-configuration and use it directly. Currently, the latest version of the CPU
-image and GPU image can be obtained from: CPU docker image ```shell
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-
-torch1.11.0-tf1.15.5-1.3.0 ``` GPU docker image ```shell registry.cn-
-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-
-torch1.11.0-tf1.15.5-1.3.0 ``` ## Setup Local Python Environment One can also
-set up local ModelScope environment using pip and conda. We suggest [anaconda]
-(https://docs.anaconda.com/anaconda/install/) for creating local python
-environment: ```shell conda create -n modelscope python=3.7 conda activate
-modelscope ``` PyTorch or TensorFlow can be installed separately according to
-each model's requirements. * Install pytorch [doc](https://pytorch.org/get-
-started/locally/) * Install tensorflow [doc](https://www.tensorflow.org/
-install/pip) After installing the necessary machine-learning framework, you can
-install modelscope library as follows: If you only want to play around with the
-modelscope framework, of trying out model/dataset download, you can install the
-core modelscope components: ```shell pip install modelscope ``` If you want to
-use multi-modal models: ```shell pip install modelscope[multi-modal] ``` If you
-want to use nlp models: ```shell pip install modelscope[nlp] -f https://
+u2pp_conformer-asr-cn-16k-online) * [speech_fsmn_vad_zh-cn-16k-common-pytorch]
+(https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/
+summary) * [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://
+modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/
+summary) * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/
+speech_frcrn_ans_cirm_16k) * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/
+models/damo/speech_dfsmn_aec_psm_16k) AI for Science: * [uni-fold-monomer]
+(https://modelscope.cn/models/DPTech/uni-fold-monomer/summary) * [uni-fold-
+multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+**Note:** Most models on ModelScope are public and can be downloaded without
+account registration on modelscope website([www.modelscope.cn]
+(www.modelscope.cn)), please refer to instructions for [model download](https:/
+/modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for
+dowloading models with api provided by modelscope library or git. # QuickTour
+We provide unified interface for inference using `pipeline`, fine-tuning and
+evaluation using `Trainer` for different tasks. For any given task with any
+type of input (image, text, audio, video...), inference pipeline can be
+implemented with only a few lines of code, which will automatically load the
+underlying model to get inference result, as is exemplified below: ```python
+>>> from modelscope.pipelines import pipeline >>> word_segmentation = pipeline
+('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-
+base') >>> word_segmentation('')
+{'output': '      '} ``` Given an image,
+portrait matting (aka. background-removal) can be accomplished with the
+following code snippet: ![image](data/resource/portrait_input.png) ```python
+>>> import cv2 >>> from modelscope.pipelines import pipeline >>>
+portrait_matting = pipeline('portrait-matting') >>> result = portrait_matting
+('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/
+image_matting.png') >>> cv2.imwrite('result.png', result['output_img']) ``` The
+output image with the background removed is: ![image](data/resource/
+portrait_output.png) Fine-tuning and evaluation can also be done with a few
+more lines of code to set up training dataset and trainer, with the heavy-
+lifting work of training and evaluation a model encapsulated in the
+implementation of `traner.train()` and `trainer.evaluate()` interfaces. For
+example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry
+dataset, resulting in a model that can be used for chinese-poetry generation.
+```python >>> from modelscope.metainfo import Trainers >>> from
+modelscope.msdatasets import MsDataset >>> from modelscope.trainers import
+build_trainer >>> train_dataset = MsDataset.load('chinese-poetry-collection',
+split='train'). remap_columns({'text1': 'src_txt'}) >>> eval_dataset =
+MsDataset.load('chinese-poetry-collection', split='test').remap_columns(
+{'text1': 'src_txt'}) >>> max_epochs = 10 >>> tmp_dir = './gpt3_poetry' >>>
+kwargs = dict( model='damo/nlp_gpt3_text-generation_1.3B',
+train_dataset=train_dataset, eval_dataset=eval_dataset, max_epochs=max_epochs,
+work_dir=tmp_dir) >>> trainer = build_trainer(name=Trainers.gpt3_trainer,
+default_args=kwargs) >>> trainer.train() ``` # Why should I use ModelScope
+library 1. A unified and concise user interface is abstracted for different
+tasks and different models. Model inferences and training can be implemented by
+as few as 3 and 10 lines of code, respectively. It is convenient for users to
+explore models in different fields in the ModelScope community. All models
+integrated into ModelScope are ready to use, which makes it easy to get started
+with AI, in both educational and industrial settings. 2. ModelScope offers a
+model-centric development and application experience. It streamlines the
+support for model training, inference, export and deployment, and facilitates
+users to build their own MLOps based on the ModelScope ecosystem. 3. For the
+model inference and training process, a modular design is put in place, and a
+wealth of functional module implementations are provided, which is convenient
+for users to customize their own model inference, training and other processes.
+4. For distributed model training, especially for large models, it provides
+rich training strategy support, including data parallel, model parallel, hybrid
+parallel and so on. # Installation ## Docker ModelScope Library currently
+supports popular deep learning framework for model training and inference,
+including PyTorch, TensorFlow and ONNX. All releases are tested and run on
+Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+. To allow out-of-
+box usage for all the models on ModelScope, official docker images are provided
+for all releases. Based on the docker image, developers can skip all
+environment installation and configuration and use it directly. Currently, the
+latest version of the CPU image and GPU image can be obtained from: CPU docker
+image ```shell registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:
+ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0 ``` GPU docker image ```shell
+registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-
+cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0 ``` ## Setup Local Python
+Environment One can also set up local ModelScope environment using pip and
+conda. We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for
+creating local python environment: ```shell conda create -n modelscope
+python=3.7 conda activate modelscope ``` PyTorch or TensorFlow can be installed
+separately according to each model's requirements. * Install pytorch [doc]
+(https://pytorch.org/get-started/locally/) * Install tensorflow [doc](https://
+www.tensorflow.org/install/pip) After installing the necessary machine-learning
+framework, you can install modelscope library as follows: If you only want to
+play around with the modelscope framework, of trying out model/dataset
+download, you can install the core modelscope components: ```shell pip install
+modelscope ``` If you want to use multi-modal models: ```shell pip install
+modelscope[multi-modal] ``` If you want to use nlp models: ```shell pip install
+modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/
+repo.html ``` If you want to use cv models: ```shell pip install modelscope[cv]
+-f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html ``` If you
+want to use audio models: ```shell pip install modelscope[audio] -f https://
 modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html ``` If you want to
-use cv models: ```shell pip install modelscope[cv] -f https://modelscope.oss-
-cn-beijing.aliyuncs.com/releases/repo.html ``` If you want to use audio models:
-```shell pip install modelscope[audio] -f https://modelscope.oss-cn-
-beijing.aliyuncs.com/releases/repo.html ``` If you want to use science models:
-```shell pip install modelscope[science] -f https://modelscope.oss-cn-
-beijing.aliyuncs.com/releases/repo.html ``` `Notes`: 1. Currently, some audio-
-task models only support python3.7, tensorflow1.15.4 Linux environments. Most
-other models can be installed and used on Windows and Mac (x86). 2. Some models
-in the audio field use the third-party library SoundFile for wav file
-processing. On the Linux system, users need to manually install libsndfile of
-SoundFile([doc link](https://github.com/bastibe/python-
+use science models: ```shell pip install modelscope[science] -f https://
+modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html ``` `Notes`: 1.
+Currently, some audio-task models only support python3.7, tensorflow1.15.4
+Linux environments. Most other models can be installed and used on Windows and
+Mac (x86). 2. Some models in the audio field use the third-party library
+SoundFile for wav file processing. On the Linux system, users need to manually
+install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-
 soundfile#installation)). On Windows and MacOS, it will be installed
 automatically without user operation. For example, on Ubuntu, you can use
 following commands: ```shell sudo apt-get update sudo apt-get install
 libsndfile1 ``` 3. Some models in computer vision need mmcv-full, you can refer
 to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation),
 a minimal installation is as follows: ```shell pip uninstall mmcv # if you have
 installed mmcv, uninstall it pip install -U openmim mim install mmcv-full ``` #
```

### Comparing `modelscope-1.5.2/modelscope/cli/cli.py` & `modelscope-1.6.0/modelscope/cli/cli.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/cli/download.py` & `modelscope-1.6.0/modelscope/cli/download.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/cli/modelcard.py` & `modelscope-1.6.0/modelscope/cli/modelcard.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/cli/pipeline.py` & `modelscope-1.6.0/modelscope/cli/pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/cli/plugins.py` & `modelscope-1.6.0/modelscope/cli/plugins.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/__init__.py` & `modelscope-1.6.0/modelscope/exporters/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/base.py` & `modelscope-1.6.0/modelscope/exporters/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/builder.py` & `modelscope-1.6.0/modelscope/exporters/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/cv/__init__.py` & `modelscope-1.6.0/modelscope/exporters/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/cv/cartoon_translation_exporter.py` & `modelscope-1.6.0/modelscope/exporters/cv/cartoon_translation_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/cv/face_detection_scrfd_exporter.py` & `modelscope-1.6.0/modelscope/exporters/cv/face_detection_scrfd_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/cv/object_detection_damoyolo_exporter.py` & `modelscope-1.6.0/modelscope/exporters/cv/object_detection_damoyolo_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/nlp/__init__.py` & `modelscope-1.6.0/modelscope/exporters/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/nlp/csanmt_for_translation_exporter.py` & `modelscope-1.6.0/modelscope/exporters/nlp/csanmt_for_translation_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/nlp/model_for_token_classification_exporter.py` & `modelscope-1.6.0/modelscope/exporters/nlp/model_for_token_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py` & `modelscope-1.6.0/modelscope/exporters/nlp/sbert_for_sequence_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py` & `modelscope-1.6.0/modelscope/exporters/nlp/sbert_for_zero_shot_classification_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/tf_model_exporter.py` & `modelscope-1.6.0/modelscope/exporters/tf_model_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/exporters/torch_model_exporter.py` & `modelscope-1.6.0/modelscope/exporters/torch_model_exporter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/fileio/file.py` & `modelscope-1.6.0/modelscope/fileio/file.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/fileio/format/json.py` & `modelscope-1.6.0/modelscope/fileio/format/json.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/fileio/format/jsonplus.py` & `modelscope-1.6.0/modelscope/fileio/format/jsonplus.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/fileio/format/yaml.py` & `modelscope-1.6.0/modelscope/fileio/format/yaml.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/fileio/io.py` & `modelscope-1.6.0/modelscope/fileio/io.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/api.py` & `modelscope-1.6.0/modelscope/hub/api.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,28 +2,28 @@
 # yapf: disable
 
 import datetime
 import functools
 import os
 import pickle
 import platform
+import re
 import shutil
 import tempfile
 import uuid
 from collections import defaultdict
 from http import HTTPStatus
 from http.cookiejar import CookieJar
 from os.path import expanduser
 from typing import Dict, List, Optional, Tuple, Union
 
 import requests
 from requests import Session
 from requests.adapters import HTTPAdapter, Retry
 
-from modelscope import __version__
 from modelscope.hub.constants import (API_HTTP_CLIENT_TIMEOUT,
                                       API_RESPONSE_FIELD_DATA,
                                       API_RESPONSE_FIELD_EMAIL,
                                       API_RESPONSE_FIELD_GIT_ACCESS_TOKEN,
                                       API_RESPONSE_FIELD_MESSAGE,
                                       API_RESPONSE_FIELD_USERNAME,
                                       DEFAULT_CREDENTIALS_PATH,
@@ -157,14 +157,15 @@
         body = {
             'Path': owner_or_group,
             'Name': name,
             'ChineseName': chinese_name,
             'Visibility': visibility,  # server check
             'License': license,
             'OriginalModelId': original_model_id,
+            'TrainId': os.environ.get('MODELSCOPE_TRAIN_ID', ''),
         }
         r = self.session.post(
             path, json=body, cookies=cookies, headers=self.headers)
         handle_http_post_error(r, path, body)
         raise_on_error(r.json())
         model_repo_url = f'{get_endpoint()}/{model_id}'
         return model_repo_url
@@ -233,16 +234,18 @@
     def push_model(self,
                    model_id: str,
                    model_dir: str,
                    visibility: Optional[int] = ModelVisibility.PUBLIC,
                    license: Optional[str] = Licenses.APACHE_V2,
                    chinese_name: Optional[str] = None,
                    commit_message: Optional[str] = 'upload model',
+                   tag: Optional[str] = None,
                    revision: Optional[str] = DEFAULT_REPOSITORY_REVISION,
-                   original_model_id: Optional[str] = None):
+                   original_model_id: Optional[str] = None,
+                   ignore_file_pattern: Optional[Union[List[str], str]] = None):
         """Upload model from a given directory to given repository. A valid model directory
         must contain a configuration.json file.
 
         This function upload the files in given directory to given repository. If the
         given repository is not exists in remote, it will automatically create it with
         given visibility, license and chinese_name parameters. If the revision is also
         not exists in remote repository, it will create a new branch for it.
@@ -265,18 +268,21 @@
                 in ModelScope, this function will create a new model with this license
                 and this parameter is required. You can ignore this parameter if you
                 make sure the model's existence.
             chinese_name(`str`, *optional*, defaults to `None`):
                 chinese name of the new created model.
             commit_message(`str`, *optional*, defaults to `None`):
                 commit message of the push request.
+            tag(`str`, *optional*, defaults to `None`):
+                The tag on this commit
             revision (`str`, *optional*, default to DEFAULT_MODEL_REVISION):
                 which branch to push. If the branch is not exists, It will create a new
                 branch and push to it.
             original_model_id (str, optional): The base model id which this model is trained from
+            ignore_file_pattern (`Union[List[str], str]`, optional): The file pattern to ignore uploading
 
         Raises:
             InvalidParameter: Parameter invalid.
             NotLoginException: Not login
             ValueError: No configuration.json
             Exception: Create failed.
         """
@@ -289,14 +295,18 @@
         cfg_file = os.path.join(model_dir, ModelFile.CONFIGURATION)
         if not os.path.exists(cfg_file):
             raise ValueError(f'{model_dir} must contain a configuration.json.')
         cookies = ModelScopeConfig.get_cookies()
         if cookies is None:
             raise NotLoginException('Must login before upload!')
         files_to_save = os.listdir(model_dir)
+        if ignore_file_pattern is None:
+            ignore_file_pattern = []
+        if isinstance(ignore_file_pattern, str):
+            ignore_file_pattern = [ignore_file_pattern]
         try:
             self.get_model(model_id=model_id)
         except Exception:
             if visibility is None or license is None:
                 raise InvalidParameter(
                     'visibility and license cannot be empty if want to create new repo'
                 )
@@ -322,27 +332,31 @@
                     src = os.path.join(tmp_dir, f)
                     if os.path.isfile(src):
                         os.remove(src)
                     else:
                         shutil.rmtree(src, ignore_errors=True)
             for f in files_to_save:
                 if f[0] != '.':
+                    if any([re.search(pattern, f) is not None for pattern in ignore_file_pattern]):
+                        continue
                     src = os.path.join(model_dir, f)
                     if os.path.isdir(src):
                         shutil.copytree(src, os.path.join(tmp_dir, f))
                     else:
                         shutil.copy(src, tmp_dir)
             if not commit_message:
                 date = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
                 commit_message = '[automsg] push model %s to hub at %s' % (
                     model_id, date)
             repo.push(
                 commit_message=commit_message,
                 local_branch=revision,
                 remote_branch=revision)
+            if tag is not None:
+                repo.tag_and_push(tag, tag)
         except Exception:
             raise
         finally:
             shutil.rmtree(tmp_dir, ignore_errors=True)
 
     def list_models(self,
                     owner_or_group: str,
@@ -924,14 +938,15 @@
         env = 'custom'
         if MODELSCOPE_CLOUD_ENVIRONMENT in os.environ:
             env = os.environ[MODELSCOPE_CLOUD_ENVIRONMENT]
         user_name = 'unknown'
         if MODELSCOPE_CLOUD_USERNAME in os.environ:
             user_name = os.environ[MODELSCOPE_CLOUD_USERNAME]
 
+        from modelscope import __version__
         ua = 'modelscope/%s; python/%s; session_id/%s; platform/%s; processor/%s; env/%s; user/%s' % (
             __version__,
             platform.python_version(),
             ModelScopeConfig.get_user_session_id(),
             platform.platform(),
             platform.processor(),
             env,
```

### Comparing `modelscope-1.5.2/modelscope/hub/check_model.py` & `modelscope-1.6.0/modelscope/hub/check_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/constants.py` & `modelscope-1.6.0/modelscope/hub/constants.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/deploy.py` & `modelscope-1.6.0/modelscope/hub/deploy.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/errors.py` & `modelscope-1.6.0/modelscope/hub/errors.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 from http import HTTPStatus
 
+import requests
 from requests.exceptions import HTTPError
 
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
@@ -53,33 +54,43 @@
 
     Returns:
        bool: `True` if success otherwise `False`.
     """
     return rsp['Code'] == HTTPStatus.OK and rsp['Success']
 
 
+def _decode_response_error(response: requests.Response):
+    if 'application/json' in response.headers.get('content-type', ''):
+        message = response.json()
+    else:
+        message = response.content.decode('utf-8')
+    return message
+
+
 def handle_http_post_error(response, url, request_body):
     try:
         response.raise_for_status()
     except HTTPError as error:
         logger.error('Request %s with body: %s exception' %
                      (url, request_body))
-        logger.error('Response details: %s' % response.content)
+        message = _decode_response_error(response)
+        logger.error('Response details: %s' % message)
         raise error
 
 
 def handle_http_response(response, logger, cookies, model_id):
     try:
         response.raise_for_status()
     except HTTPError as error:
         if cookies is None:  # code in [403] and
             logger.error(
                 f'Authentication token does not exist, failed to access model {model_id} which may not exist or may be \
                 private. Please login first.')
-        logger.error('Response details: %s' % response.content)
+        message = _decode_response_error(response)
+        logger.error('Response details: %s' % message)
         raise error
 
 
 def raise_on_error(rsp):
     """If response error, raise exception
 
     Args:
```

### Comparing `modelscope-1.5.2/modelscope/hub/file_download.py` & `modelscope-1.6.0/modelscope/hub/file_download.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 from pathlib import Path
 from typing import Dict, Optional, Union
 
 import requests
 from requests.adapters import Retry
 from tqdm import tqdm
 
-from modelscope import __version__
 from modelscope.hub.api import HubApi, ModelScopeConfig
 from modelscope.hub.constants import (API_FILE_DOWNLOAD_CHUNK_SIZE,
                                       API_FILE_DOWNLOAD_RETRY_TIMES,
                                       API_FILE_DOWNLOAD_TIMEOUT, FILE_HASH)
 from modelscope.utils.constant import DEFAULT_MODEL_REVISION
 from modelscope.utils.logger import get_logger
 from .errors import FileDownloadError, NotExistError
```

### Comparing `modelscope-1.5.2/modelscope/hub/git.py` & `modelscope-1.6.0/modelscope/hub/git.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/push_to_hub.py` & `modelscope-1.6.0/modelscope/hub/push_to_hub.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,39 +1,45 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import concurrent.futures
 import os
 
 from modelscope.hub.api import HubApi
-from modelscope.hub.constants import Licenses, ModelVisibility
-from modelscope.hub.errors import NotExistError
+from modelscope.hub.constants import ModelVisibility
+from modelscope.utils.constant import DEFAULT_REPOSITORY_REVISION
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 _executor = concurrent.futures.ProcessPoolExecutor(max_workers=8)
 
 
 def _api_push_to_hub(repo_name,
                      output_dir,
                      token,
                      private=True,
                      commit_message='',
-                     source_repo=''):
+                     tag=None,
+                     source_repo='',
+                     ignore_file_pattern=None,
+                     revision=DEFAULT_REPOSITORY_REVISION):
     try:
         api = HubApi()
         api.login(token)
         api.push_model(
             repo_name,
             output_dir,
             visibility=ModelVisibility.PUBLIC
             if not private else ModelVisibility.PRIVATE,
             chinese_name=repo_name,
             commit_message=commit_message,
-            original_model_id=source_repo)
+            tag=tag,
+            original_model_id=source_repo,
+            ignore_file_pattern=ignore_file_pattern,
+            revision=revision)
         commit_message = commit_message or 'No commit message'
         logger.info(
             f'Successfully upload the model to {repo_name} with message: {commit_message}'
         )
         return True
     except Exception as e:
         logger.error(
@@ -44,66 +50,84 @@
 
 def push_to_hub(repo_name,
                 output_dir,
                 token=None,
                 private=True,
                 retry=3,
                 commit_message='',
-                source_repo=''):
+                tag=None,
+                source_repo='',
+                ignore_file_pattern=None,
+                revision=DEFAULT_REPOSITORY_REVISION):
     """
     Args:
         repo_name: The repo name for the modelhub repo
         output_dir: The local output_dir for the checkpoint
         token: The user api token, function will check the `MODELSCOPE_API_TOKEN` variable if this argument is None
         private: If is a private repo, default True
         retry: Retry times if something error in uploading, default 3
         commit_message: The commit message
+        tag: The tag of this commit
         source_repo: The source repo (model id) which this model comes from
-
+        ignore_file_pattern: The file pattern to be ignored in uploading.
+        revision: The branch to commit to
     Returns:
         The boolean value to represent whether the model is uploaded.
     """
     if token is None:
         token = os.environ.get('MODELSCOPE_API_TOKEN')
+    if ignore_file_pattern is None:
+        ignore_file_pattern = os.environ.get('UPLOAD_IGNORE_FILE_PATTERN')
+    assert repo_name is not None
     assert token is not None, 'Either pass in a token or to set `MODELSCOPE_API_TOKEN` in the environment variables.'
     assert os.path.isdir(output_dir)
     assert 'configuration.json' in os.listdir(output_dir) or 'configuration.yaml' in os.listdir(output_dir) \
            or 'configuration.yml' in os.listdir(output_dir)
 
     logger.info(
         f'Uploading {output_dir} to {repo_name} with message {commit_message}')
     for i in range(retry):
         if _api_push_to_hub(repo_name, output_dir, token, private,
-                            commit_message, source_repo):
+                            commit_message, tag, source_repo,
+                            ignore_file_pattern, revision):
             return True
     return False
 
 
 def push_to_hub_async(repo_name,
                       output_dir,
                       token=None,
                       private=True,
                       commit_message='',
-                      source_repo=''):
+                      tag=None,
+                      source_repo='',
+                      ignore_file_pattern=None,
+                      revision=DEFAULT_REPOSITORY_REVISION):
     """
     Args:
         repo_name: The repo name for the modelhub repo
         output_dir: The local output_dir for the checkpoint
         token: The user api token, function will check the `MODELSCOPE_API_TOKEN` variable if this argument is None
         private: If is a private repo, default True
         commit_message: The commit message
+        tag: The tag of this commit
         source_repo: The source repo (model id) which this model comes from
-
+        ignore_file_pattern: The file pattern to be ignored in uploading
+        revision: The branch to commit to
     Returns:
         A handler to check the result and the status
     """
     if token is None:
         token = os.environ.get('MODELSCOPE_API_TOKEN')
+    if ignore_file_pattern is None:
+        ignore_file_pattern = os.environ.get('UPLOAD_IGNORE_FILE_PATTERN')
+    assert repo_name is not None
     assert token is not None, 'Either pass in a token or to set `MODELSCOPE_API_TOKEN` in the environment variables.'
     assert os.path.isdir(output_dir)
     assert 'configuration.json' in os.listdir(output_dir) or 'configuration.yaml' in os.listdir(output_dir) \
            or 'configuration.yml' in os.listdir(output_dir)
 
     logger.info(
         f'Uploading {output_dir} to {repo_name} with message {commit_message}')
     return _executor.submit(_api_push_to_hub, repo_name, output_dir, token,
-                            private, commit_message, source_repo)
+                            private, commit_message, tag, source_repo,
+                            ignore_file_pattern, revision)
```

### Comparing `modelscope-1.5.2/modelscope/hub/repository.py` & `modelscope-1.6.0/modelscope/hub/repository.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/snapshot_download.py` & `modelscope-1.6.0/modelscope/hub/snapshot_download.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/utils/caching.py` & `modelscope-1.6.0/modelscope/hub/utils/caching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/hub/utils/utils.py` & `modelscope-1.6.0/modelscope/hub/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metainfo.py` & `modelscope-1.6.0/modelscope/metainfo.py`

 * *Files 0% similar despite different names*

```diff
@@ -112,23 +112,17 @@
     m2fp = 'm2fp'
     nerf_recon_acc = 'nerf-recon-acc'
     bts_depth_estimation = 'bts-depth-estimation'
     vision_efficient_tuning = 'vision-efficient-tuning'
     bad_image_detecting = 'bad-image-detecting'
     controllable_image_generation = 'controllable-image-generation'
     longshortnet = 'longshortnet'
+    fastinst = 'fastinst'
     pedestrian_attribute_recognition = 'pedestrian-attribute-recognition'
 
-    # EasyCV models
-    yolox = 'YOLOX'
-    segformer = 'Segformer'
-    hand_2d_keypoints = 'HRNet-Hand2D-Keypoints'
-    image_object_detection_auto = 'image-object-detection-auto'
-    dino = 'DINO'
-
     # nlp models
     bert = 'bert'
     palm = 'palm-v2'
     structbert = 'structbert'
     deberta_v2 = 'deberta_v2'
     veco = 'veco'
     translation = 'csanmt-translation'
@@ -173,24 +167,28 @@
     llama = 'llama'
 
     # audio models
     sambert_hifigan = 'sambert-hifigan'
     speech_frcrn_ans_cirm_16k = 'speech_frcrn_ans_cirm_16k'
     speech_dfsmn_ans = 'speech_dfsmn_ans'
     speech_dfsmn_kws_char_farfield = 'speech_dfsmn_kws_char_farfield'
+    speech_dfsmn_kws_char_farfield_iot = 'speech_dfsmn_kws_char_farfield_iot'
     speech_kws_fsmn_char_ctc_nearfield = 'speech_kws_fsmn_char_ctc_nearfield'
     speech_mossformer_separation_temporal_8k = 'speech_mossformer_separation_temporal_8k'
     kws_kwsbp = 'kws-kwsbp'
     generic_asr = 'generic-asr'
     wenet_asr = 'wenet-asr'
     generic_itn = 'generic-itn'
     generic_punc = 'generic-punc'
     generic_sv = 'generic-sv'
     ecapa_tdnn_sv = 'ecapa-tdnn-sv'
     campplus_sv = 'cam++-sv'
+    eres2net_sv = 'eres2net-sv'
+    scl_sd = 'scl-sd'
+    rdino_tdnn_sv = 'rdino_ecapa-tdnn-sv'
     generic_lm = 'generic-lm'
 
     # multi-modal models
     ofa = 'ofa'
     clip = 'clip-multi-modal-embedding'
     gemm = 'gemm-generative-multi-modal'
     rleg = 'rleg-generative-multi-modal'
@@ -201,14 +199,16 @@
     team = 'team-multi-modal-similarity'
     video_clip = 'video-clip-multi-modal-embedding'
     mgeo = 'mgeo'
     vldoc = 'vldoc'
     hitea = 'hitea'
     soonet = 'soonet'
     efficient_diffusion_tuning = 'efficient-diffusion-tuning'
+    mplug_owl = 'mplug-owl'
+    clip_interrogator = 'clip-interrogator'
 
     # science models
     unifold = 'unifold'
     unifold_symmetry = 'unifold-symmetry'
 
 
 class TaskModels(object):
@@ -251,14 +251,15 @@
         Holds the standard pipline name to use for identifying different pipeline.
     This should be used to register pipelines.
 
         For pipeline which support different models and implements the common function, we
     should use task name for this pipeline.
         For pipeline which suuport only one model, we should use ${Model}-${Task} as its name.
     """
+    pipeline_template = 'pipeline-template'
     # vision tasks
     portrait_matting = 'unet-image-matting'
     universal_matting = 'unet-universal-matting'
     image_denoise = 'nafnet-image-denoise'
     image_deblur = 'nafnet-image-deblur'
     person_image_cartoon = 'unet-person-image-cartoon'
     ocr_detection = 'resnet18-ocr-detection'
@@ -273,16 +274,14 @@
     body_2d_keypoints = 'hrnetv2w32_body-2d-keypoints_image'
     body_3d_keypoints = 'canonical_body-3d-keypoints_video'
     hand_2d_keypoints = 'hrnetv2w18_hand-2d-keypoints_image'
     human_detection = 'resnet18-human-detection'
     tbs_detection = 'tbs-detection'
     object_detection = 'vit-object-detection'
     abnormal_object_detection = 'abnormal-object-detection'
-    easycv_detection = 'easycv-detection'
-    easycv_segmentation = 'easycv-segmentation'
     face_2d_keypoints = 'mobilenet_face-2d-keypoints_alignment'
     salient_detection = 'u2net-salient-detection'
     salient_boudary_detection = 'res2net-salient-detection'
     camouflaged_detection = 'res2net-camouflaged-detection'
     image_demoire = 'uhdm-image-demoireing'
     image_classification = 'image-classification'
     face_detection = 'resnet-face-detection-scrfd10gkps'
@@ -343,15 +342,14 @@
     tinynas_detection = 'tinynas-detection'
     crowd_counting = 'hrnet-crowd-counting'
     action_detection = 'ResNetC3D-action-detection'
     video_single_object_tracking = 'ostrack-vitb-video-single-object-tracking'
     video_single_object_tracking_procontext = 'procontext-vitb-video-single-object-tracking'
     video_multi_object_tracking = 'video-multi-object-tracking'
     image_panoptic_segmentation = 'image-panoptic-segmentation'
-    image_panoptic_segmentation_easycv = 'image-panoptic-segmentation-easycv'
     video_summarization = 'googlenet_pgl_video_summarization'
     language_guided_video_summarization = 'clip-it-video-summarization'
     image_semantic_segmentation = 'image-semantic-segmentation'
     image_depth_estimation = 'image-depth-estimation'
     indoor_layout_estimation = 'indoor-layout-estimation'
     video_depth_estimation = 'video-depth-estimation'
     panorama_depth_estimation = 'panorama-depth-estimation'
@@ -398,15 +396,15 @@
     motion_generattion = 'mdm-motion-generation'
     mobile_image_super_resolution = 'mobile-image-super-resolution'
     image_human_parsing = 'm2fp-image-human-parsing'
     object_detection_3d_depe = 'object-detection-3d-depe'
     nerf_recon_acc = 'nerf-recon-acc'
     bad_image_detecting = 'bad-image-detecting'
     controllable_image_generation = 'controllable-image-generation'
-
+    fast_instance_segmentation = 'fast-instance-segmentation'
     image_quality_assessment_mos = 'image-quality-assessment-mos'
     image_quality_assessment_man = 'image-quality-assessment-man'
     image_quality_assessment_degradation = 'image-quality-assessment-degradation'
     vision_efficient_tuning = 'vision-efficient-tuning'
     image_bts_depth_estimation = 'image-bts-depth-estimation'
     pedestrian_attribute_recognition = 'resnet50_pedestrian-attribute-recognition_image'
 
@@ -481,14 +479,17 @@
     asr_wenet_inference = 'asr-wenet-inference'
     itn_inference = 'itn-inference'
     punc_inference = 'punc-inference'
     sv_inference = 'sv-inference'
     speaker_diarization_inference = 'speaker-diarization-inference'
     vad_inference = 'vad-inference'
     speaker_verification = 'speaker-verification'
+    speaker_verification_rdino = 'speaker-verification-rdino'
+    speaker_verification_eres2net = 'speaker-verification-eres2net'
+    speaker_change_locating = 'speaker-change-locating'
     lm_inference = 'language-score-prediction'
     speech_timestamp_inference = 'speech-timestamp-inference'
 
     # multi-modal tasks
     image_captioning = 'image-captioning'
     multi_modal_embedding = 'multi-modal-embedding'
     generative_multi_modal_embedding = 'generative-multi-modal-embedding'
@@ -510,14 +511,15 @@
     document_vl_embedding = 'document-vl-embedding'
     chinese_stable_diffusion = 'chinese-stable-diffusion'
     text_to_video_synthesis = 'latent-text-to-video-synthesis'  # latent-text-to-video-synthesis
     gridvlp_multi_modal_classification = 'gridvlp-multi-modal-classification'
     gridvlp_multi_modal_embedding = 'gridvlp-multi-modal-embedding'
     soonet_video_temporal_grounding = 'soonet-video-temporal-grounding'
     efficient_diffusion_tuning = 'efficient-diffusion-tuning'
+    multimodal_dialogue = 'multimodal-dialogue'
 
     # science tasks
     protein_structure = 'unifold-protein-structure'
 
 
 DEFAULT_MODEL_FOR_PIPELINE = {
     # TaskName: (pipeline_module_name, model_repo)
@@ -877,14 +879,15 @@
     faq_question_answering_trainer = 'faq-question-answering-trainer'
     gpt_moe_trainer = 'nlp-gpt-moe-trainer'
     table_question_answering_trainer = 'table-question-answering-trainer'
     document_grounded_dialog_generate_trainer = 'document-grounded-dialog-generate-trainer'
     document_grounded_dialog_rerank_trainer = 'document-grounded-dialog-rerank-trainer'
     document_grounded_dialog_retrieval_trainer = 'document-grounded-dialog-retrieval-trainer'
     siamese_uie_trainer = 'siamese-uie-trainer'
+    translation_evaluation_trainer = 'translation-evaluation-trainer'
 
 
 class MultiModalTrainers(object):
     clip_multi_modal_embedding = 'clip-multi-modal-embedding'
     ofa = 'ofa'
     mplug = 'mplug'
     mgeo_ranking_trainer = 'mgeo-ranking-trainer'
@@ -907,15 +910,14 @@
     This should be used to register trainers.
 
         For a general Trainer, you can use EpochBasedTrainer.
         For a model specific Trainer, you can use ${ModelName}-${Task}-trainer.
     """
 
     default = 'trainer'
-    easycv = 'easycv'
     tinynas_damoyolo = 'tinynas-damoyolo'
 
     @staticmethod
     def get_trainer_domain(attribute_or_value):
         if attribute_or_value in vars(
                 CVTrainers) or attribute_or_value in vars(CVTrainers).values():
             return Fields.cv
@@ -929,16 +931,14 @@
             return Fields.audio
         elif attribute_or_value in vars(
                 MultiModalTrainers) or attribute_or_value in vars(
                     MultiModalTrainers).values():
             return Fields.multi_modal
         elif attribute_or_value == Trainers.default:
             return Trainers.default
-        elif attribute_or_value == Trainers.easycv:
-            return Trainers.easycv
         else:
             return 'unknown'
 
 
 class Preprocessors(object):
     """ Names for different preprocessor.
 
@@ -1030,14 +1030,16 @@
     ofa_tasks_preprocessor = 'ofa-tasks-preprocessor'
     clip_preprocessor = 'clip-preprocessor'
     mplug_tasks_preprocessor = 'mplug-tasks-preprocessor'
     mgeo_ranking = 'mgeo-ranking'
     vldoc_preprocessor = 'vldoc-preprocessor'
     hitea_tasks_preprocessor = 'hitea-tasks-preprocessor'
     diffusion_image_generation_preprocessor = 'diffusion-image-generation-preprocessor'
+    mplug_owl_preprocessor = 'mplug-owl-preprocessor'
+    image_captioning_clip_interrogator_preprocessor = 'image-captioning-clip-interrogator-preprocessor'
 
     # science preprocessor
     unifold_preprocessor = 'unifold-preprocessor'
 
 
 class Metrics(object):
     """ Names for different metrics.
@@ -1094,14 +1096,16 @@
     # metirc for image-quality-assessment-degradation task
     image_quality_assessment_degradation_metric = 'image-quality-assessment-degradation-metric'
     # metric for text-ranking task
     text_ranking_metric = 'text-ranking-metric'
     # metric for image-colorization task
     image_colorization_metric = 'image-colorization-metric'
     ocr_recognition_metric = 'ocr-recognition-metric'
+    # metric for translation evaluation
+    translation_evaluation_metric = 'translation-evaluation-metric'
 
 
 class Optimizers(object):
     """ Names for different OPTIMIZER.
 
         Holds the standard optimizer name to use for identifying different optimizer.
         This should be used to register optimizer.
@@ -1161,19 +1165,11 @@
     ConstantWarmup = 'ConstantWarmup'
     ExponentialWarmup = 'ExponentialWarmup'
 
 
 class CustomDatasets(object):
     """ Names for different datasets.
     """
-    ClsDataset = 'ClsDataset'
-    Face2dKeypointsDataset = 'FaceKeypointDataset'
-    HandCocoWholeBodyDataset = 'HandCocoWholeBodyDataset'
-    HumanWholeBodyKeypointDataset = 'WholeBodyCocoTopDownDataset'
-    SegDataset = 'SegDataset'
-    DetDataset = 'DetDataset'
-    DetImagesMixDataset = 'DetImagesMixDataset'
-    PanopticDataset = 'PanopticDataset'
     PairedDataset = 'PairedDataset'
     SiddDataset = 'SiddDataset'
     GoproDataset = 'GoproDataset'
     RedsDataset = 'RedsDataset'
```

### Comparing `modelscope-1.5.2/modelscope/metrics/__init__.py` & `modelscope-1.6.0/modelscope/metrics/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,14 +27,15 @@
     from .ppl_metric import PplMetric
     from .image_quality_assessment_degradation_metric import ImageQualityAssessmentDegradationMetric
     from .image_quality_assessment_mos_metric import ImageQualityAssessmentMosMetric
     from .text_ranking_metric import TextRankingMetric
     from .loss_metric import LossMetric
     from .image_colorization_metric import ImageColorizationMetric
     from .ocr_recognition_metric import OCRRecognitionMetric
+    from .translation_evaluation_metric import TranslationEvaluationMetric
 else:
     _import_structure = {
         'audio_noise_metric': ['AudioNoiseMetric'],
         'base': ['Metric'],
         'builder': ['METRICS', 'build_metric', 'task_default_metrics'],
         'image_color_enhance_metric': ['ImageColorEnhanceMetric'],
         'image_denoise_metric': ['ImageDenoiseMetric'],
@@ -58,15 +59,16 @@
         'image_quality_assessment_degradation_metric':
         ['ImageQualityAssessmentDegradationMetric'],
         'image_quality_assessment_mos_metric':
         ['ImageQualityAssessmentMosMetric'],
         'text_ranking_metric': ['TextRankingMetric'],
         'loss_metric': ['LossMetric'],
         'image_colorization_metric': ['ImageColorizationMetric'],
-        'ocr_recognition_metric': ['OCRRecognitionMetric']
+        'ocr_recognition_metric': ['OCRRecognitionMetric'],
+        'translation_evaluation_metric': ['TranslationEvaluationMetric']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/metrics/accuracy_metric.py` & `modelscope-1.6.0/modelscope/metrics/accuracy_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/action_detection_evaluator.py` & `modelscope-1.6.0/modelscope/metrics/action_detection_evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/audio_noise_metric.py` & `modelscope-1.6.0/modelscope/metrics/audio_noise_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/base.py` & `modelscope-1.6.0/modelscope/metrics/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/bleu_metric.py` & `modelscope-1.6.0/modelscope/metrics/bleu_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/builder.py` & `modelscope-1.6.0/modelscope/metrics/builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,14 +38,15 @@
     PLCC = 'plcc'
     SRCC = 'srcc'
     RMSE = 'rmse'
     MRR = 'mrr'
     NDCG = 'ndcg'
     AR = 'AR'
     Colorfulness = 'colorfulness'
+    Kendall_Tau_Correlation = 'kendall_tau_correlation'
 
 
 task_default_metrics = {
     Tasks.image_segmentation: [Metrics.image_ins_seg_coco_metric],
     Tasks.sentence_similarity: [Metrics.seq_cls_metric],
     Tasks.nli: [Metrics.seq_cls_metric],
     Tasks.sentiment_classification: [Metrics.seq_cls_metric],
@@ -72,14 +73,15 @@
     Tasks.image_quality_assessment_degradation:
     [Metrics.image_quality_assessment_degradation_metric],
     Tasks.image_quality_assessment_mos:
     [Metrics.image_quality_assessment_mos_metric],
     Tasks.bad_image_detecting: [Metrics.accuracy],
     Tasks.ocr_recognition: [Metrics.ocr_recognition_metric],
     Tasks.efficient_diffusion_tuning: [Metrics.loss_metric],
+    Tasks.translation_evaluation: [Metrics.translation_evaluation_metric]
 }
 
 
 def build_metric(metric_cfg: Union[str, Dict],
                  field: str = default_group,
                  default_args: dict = None):
     """ Build metric given metric_name and field.
```

### Comparing `modelscope-1.5.2/modelscope/metrics/ciderD/ciderD.py` & `modelscope-1.6.0/modelscope/metrics/ciderD/ciderD.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/ciderD/ciderD_scorer.py` & `modelscope-1.6.0/modelscope/metrics/ciderD/ciderD_scorer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_color_enhance_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_color_enhance_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_colorization_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_colorization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_denoise_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_denoise_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_inpainting_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_inpainting_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_instance_segmentation_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_instance_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_portrait_enhancement_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_portrait_enhancement_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_quality_assessment_degradation_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_quality_assessment_degradation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/image_quality_assessment_mos_metric.py` & `modelscope-1.6.0/modelscope/metrics/image_quality_assessment_mos_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/inbatch_recall_metric.py` & `modelscope-1.6.0/modelscope/metrics/inbatch_recall_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/loss_metric.py` & `modelscope-1.6.0/modelscope/metrics/loss_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/map_metric.py` & `modelscope-1.6.0/modelscope/metrics/map_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/movie_scene_segmentation_metric.py` & `modelscope-1.6.0/modelscope/metrics/movie_scene_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/ned_metric.py` & `modelscope-1.6.0/modelscope/metrics/ned_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/ocr_recognition_metric.py` & `modelscope-1.6.0/modelscope/metrics/ocr_recognition_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/ppl_metric.py` & `modelscope-1.6.0/modelscope/metrics/ppl_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/prediction_saving_wrapper.py` & `modelscope-1.6.0/modelscope/metrics/prediction_saving_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/referring_video_object_segmentation_metric.py` & `modelscope-1.6.0/modelscope/metrics/referring_video_object_segmentation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/sequence_classification_metric.py` & `modelscope-1.6.0/modelscope/metrics/sequence_classification_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/text_generation_metric.py` & `modelscope-1.6.0/modelscope/metrics/text_generation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/text_ranking_metric.py` & `modelscope-1.6.0/modelscope/metrics/text_ranking_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/token_classification_metric.py` & `modelscope-1.6.0/modelscope/metrics/token_classification_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_frame_interpolation_metric.py` & `modelscope-1.6.0/modelscope/metrics/video_frame_interpolation_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_stabilization_metric.py` & `modelscope-1.6.0/modelscope/metrics/video_stabilization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_summarization_metric.py` & `modelscope-1.6.0/modelscope/metrics/video_summarization_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/matlab_functions.py` & `modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/matlab_functions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/metric_util.py` & `modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/metric_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/niqe.py` & `modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/niqe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py` & `modelscope-1.6.0/modelscope/metrics/video_super_resolution_metric/video_super_resolution_metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/__init__.py` & `modelscope-1.6.0/modelscope/models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/layers/activations.py` & `modelscope-1.6.0/modelscope/models/audio/aec/layers/activations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/layers/affine_transform.py` & `modelscope-1.6.0/modelscope/models/audio/aec/layers/affine_transform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/layers/deep_fsmn.py` & `modelscope-1.6.0/modelscope/models/audio/aec/layers/deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/layers/layer_base.py` & `modelscope-1.6.0/modelscope/models/audio/aec/layers/layer_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/layers/uni_deep_fsmn.py` & `modelscope-1.6.0/modelscope/models/audio/aec/layers/uni_deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/network/loss.py` & `modelscope-1.6.0/modelscope/models/audio/aec/network/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/network/modulation_loss.py` & `modelscope-1.6.0/modelscope/models/audio/aec/network/modulation_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/aec/network/se_net.py` & `modelscope-1.6.0/modelscope/models/audio/aec/network/se_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/__init__.py` & `modelscope-1.6.0/modelscope/models/audio/ans/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/complex_nn.py` & `modelscope-1.6.0/modelscope/models/audio/ans/complex_nn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/conv_stft.py` & `modelscope-1.6.0/modelscope/models/audio/ans/conv_stft.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,15 +35,15 @@
                  fft_len=None,
                  win_type='hamming',
                  feature_type='real',
                  fix=True):
         super(ConvSTFT, self).__init__()
 
         if fft_len is None:
-            self.fft_len = np.int(2**np.ceil(np.log2(win_len)))
+            self.fft_len = int(2**np.ceil(np.log2(win_len)))
         else:
             self.fft_len = fft_len
 
         kernel, _ = init_kernels(win_len, win_inc, self.fft_len, win_type)
         self.weight = nn.Parameter(kernel, requires_grad=(not fix))
         self.feature_type = feature_type
         self.stride = win_inc
@@ -74,15 +74,15 @@
                  win_inc,
                  fft_len=None,
                  win_type='hamming',
                  feature_type='real',
                  fix=True):
         super(ConviSTFT, self).__init__()
         if fft_len is None:
-            self.fft_len = np.int(2**np.ceil(np.log2(win_len)))
+            self.fft_len = int(2**np.ceil(np.log2(win_len)))
         else:
             self.fft_len = fft_len
         kernel, window = init_kernels(
             win_len, win_inc, self.fft_len, win_type, invers=True)
         self.weight = nn.Parameter(kernel, requires_grad=(not fix))
         self.feature_type = feature_type
         self.win_type = win_type
```

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/denoise_net.py` & `modelscope-1.6.0/modelscope/models/audio/ans/denoise_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/frcrn.py` & `modelscope-1.6.0/modelscope/models/audio/ans/frcrn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/layers/activations.py` & `modelscope-1.6.0/modelscope/models/audio/ans/layers/activations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/layers/affine_transform.py` & `modelscope-1.6.0/modelscope/models/audio/ans/layers/affine_transform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/layers/layer_base.py` & `modelscope-1.6.0/modelscope/models/audio/ans/layers/layer_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/layers/uni_deep_fsmn.py` & `modelscope-1.6.0/modelscope/models/audio/ans/layers/uni_deep_fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/se_module_complex.py` & `modelscope-1.6.0/modelscope/models/audio/ans/se_module_complex.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/ans/unet.py` & `modelscope-1.6.0/modelscope/models/audio/ans/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/asr/__init__.py` & `modelscope-1.6.0/modelscope/models/audio/asr/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py` & `modelscope-1.6.0/modelscope/models/audio/asr/wenet_automatic_speech_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/itn/__init__.py` & `modelscope-1.6.0/modelscope/models/audio/itn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/itn/generic_inverse_text_processing.py` & `modelscope-1.6.0/modelscope/models/audio/itn/generic_inverse_text_processing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/__init__.py` & `modelscope-1.6.0/modelscope/models/audio/kws/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/farfield/fsmn.py` & `modelscope-1.6.0/modelscope/models/audio/kws/farfield/fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py` & `modelscope-1.6.0/modelscope/models/audio/kws/farfield/fsmn_sele_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/farfield/model.py` & `modelscope-1.6.0/modelscope/models/audio/kws/farfield/model.py`

 * *Files 18% similar despite different names*

```diff
@@ -7,21 +7,23 @@
 from modelscope.metainfo import Models
 from modelscope.models import TorchModel
 from modelscope.models.base import Tensor
 from modelscope.models.builder import MODELS
 from modelscope.utils.audio.audio_utils import update_conf
 from modelscope.utils.constant import Tasks
 from .fsmn_sele_v2 import FSMNSeleNetV2
+from .fsmn_sele_v3 import FSMNSeleNetV3
 
 
 @MODELS.register_module(
     Tasks.keyword_spotting, module_name=Models.speech_dfsmn_kws_char_farfield)
 class FSMNSeleNetV2Decorator(TorchModel):
     r""" A decorator of FSMNSeleNetV2 for integrating into modelscope framework """
 
+    MODEL_CLASS = FSMNSeleNetV2
     MODEL_TXT = 'model.txt'
     SC_CONFIG = 'sound_connect.conf'
 
     def __init__(self,
                  model_dir: str,
                  training: Optional[bool] = False,
                  *args,
@@ -29,33 +31,33 @@
         """initialize the dfsmn model from the `model_dir` path.
 
         Args:
             model_dir (str): the model path.
         """
         super().__init__(model_dir, *args, **kwargs)
         if training:
-            self.model = FSMNSeleNetV2(*args, **kwargs)
+            self.model = self.MODEL_CLASS(*args, **kwargs)
         else:
             sc_config_file = os.path.join(model_dir, self.SC_CONFIG)
             model_txt_file = os.path.join(model_dir, self.MODEL_TXT)
             self.tmp_dir = tempfile.TemporaryDirectory()
             new_config_file = os.path.join(self.tmp_dir.name, self.SC_CONFIG)
 
             self._sc = None
             if os.path.exists(model_txt_file):
-                conf_dict = dict(mode=56542, kws_model=model_txt_file)
+                conf_dict = dict(kws_model=model_txt_file)
                 update_conf(sc_config_file, new_config_file, conf_dict)
                 import py_sound_connect
                 self._sc = py_sound_connect.SoundConnect(new_config_file)
                 self.size_in = self._sc.bytesPerBlockIn()
                 self.size_out = self._sc.bytesPerBlockOut()
             else:
                 raise Exception(
-                    f'Invalid model directory! Failed to load model file: {model_txt_file}.'
-                )
+                    f'Invalid model directory! Failed to load model file:'
+                    f' {model_txt_file}.')
 
     def __del__(self):
         if hasattr(self, 'tmp_dir'):
             self.tmp_dir.cleanup()
 
     def forward(self, input: Dict[str, Tensor]) -> Dict[str, Tensor]:
         return self.model.forward(input)
@@ -69,7 +71,28 @@
                 self._sc.kwsKeyword(self._sc.kwsSpottedKeywordIndex()),
                 'offset': self._sc.kwsKeywordOffset(),
                 'channel': self._sc.kwsBestChannel(),
                 'length': self._sc.kwsKeywordLength(),
                 'confidence': self._sc.kwsConfidence()
             }
         return result
+
+
+@MODELS.register_module(
+    Tasks.keyword_spotting,
+    module_name=Models.speech_dfsmn_kws_char_farfield_iot)
+class FSMNSeleNetV3Decorator(FSMNSeleNetV2Decorator):
+    r""" A decorator of FSMNSeleNetV3 for integrating into modelscope framework """
+
+    MODEL_CLASS = FSMNSeleNetV3
+
+    def __init__(self,
+                 model_dir: str,
+                 training: Optional[bool] = False,
+                 *args,
+                 **kwargs):
+        """initialize the dfsmn model from the `model_dir` path.
+
+        Args:
+            model_dir (str): the model path.
+        """
+        super().__init__(model_dir, training, *args, **kwargs)
```

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/farfield/model_def.py` & `modelscope-1.6.0/modelscope/models/audio/kws/farfield/model_def.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/generic_key_word_spotting.py` & `modelscope-1.6.0/modelscope/models/audio/kws/generic_key_word_spotting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/nearfield/cmvn.py` & `modelscope-1.6.0/modelscope/models/audio/kws/nearfield/cmvn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/nearfield/fsmn.py` & `modelscope-1.6.0/modelscope/models/audio/kws/nearfield/fsmn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/kws/nearfield/model.py` & `modelscope-1.6.0/modelscope/models/audio/kws/nearfield/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/punc/generic_punctuation.py` & `modelscope-1.6.0/modelscope/models/audio/punc/generic_punctuation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/separation/layer_norm.py` & `modelscope-1.6.0/modelscope/models/audio/separation/layer_norm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/separation/mossformer.py` & `modelscope-1.6.0/modelscope/models/audio/separation/mossformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/separation/mossformer_block.py` & `modelscope-1.6.0/modelscope/models/audio/separation/mossformer_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/separation/mossformer_conv_module.py` & `modelscope-1.6.0/modelscope/models/audio/separation/mossformer_conv_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/sv/DTDNN.py` & `modelscope-1.6.0/modelscope/models/audio/sv/DTDNN.py`

 * *Files 4% similar despite different names*

```diff
@@ -72,19 +72,21 @@
     def __init__(self,
                  feat_dim=80,
                  embedding_size=512,
                  growth_rate=32,
                  bn_size=4,
                  init_channels=128,
                  config_str='batchnorm-relu',
-                 memory_efficient=True):
+                 memory_efficient=True,
+                 output_level='segment'):
         super(CAMPPlus, self).__init__()
 
         self.head = FCM(feat_dim=feat_dim)
         channels = self.head.out_channels
+        self.output_level = output_level
 
         self.xvector = nn.Sequential(
             OrderedDict([
                 ('tdnn',
                  TDNNLayer(
                      channels,
                      init_channels,
@@ -114,29 +116,35 @@
                     channels, channels // 2, bias=False,
                     config_str=config_str))
             channels //= 2
 
         self.xvector.add_module('out_nonlinear',
                                 get_nonlinear(config_str, channels))
 
-        self.xvector.add_module('stats', StatsPool())
-        self.xvector.add_module(
-            'dense',
-            DenseLayer(channels * 2, embedding_size, config_str='batchnorm_'))
+        if self.output_level == 'segment':
+            self.xvector.add_module('stats', StatsPool())
+            self.xvector.add_module(
+                'dense',
+                DenseLayer(
+                    channels * 2, embedding_size, config_str='batchnorm_'))
+        else:
+            assert self.output_level == 'frame', '`output_level` should be set to \'segment\' or \'frame\'. '
 
         for m in self.modules():
             if isinstance(m, (nn.Conv1d, nn.Linear)):
                 nn.init.kaiming_normal_(m.weight.data)
                 if m.bias is not None:
                     nn.init.zeros_(m.bias)
 
     def forward(self, x):
         x = x.permute(0, 2, 1)  # (B,T,F) => (B,F,T)
         x = self.head(x)
         x = self.xvector(x)
+        if self.output_level == 'frame':
+            x = x.transpose(1, 2)
         return x
 
 
 @MODELS.register_module(
     Tasks.speaker_verification, module_name=Models.campplus_sv)
 class SpeakerVerificationCAMPPlus(TorchModel):
     r"""A fast and efficient speaker embedding model, using a 2-dimensional convolution residual network as the head
```

### Comparing `modelscope-1.5.2/modelscope/models/audio/sv/DTDNN_layers.py` & `modelscope-1.6.0/modelscope/models/audio/sv/DTDNN_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/sv/ecapa_tdnn.py` & `modelscope-1.6.0/modelscope/models/audio/sv/ecapa_tdnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/sv/generic_speaker_verification.py` & `modelscope-1.6.0/modelscope/models/audio/sv/generic_speaker_verification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/tts/sambert_hifi.py` & `modelscope-1.6.0/modelscope/models/audio/tts/sambert_hifi.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/audio/tts/voice.py` & `modelscope-1.6.0/modelscope/models/audio/tts/voice.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,19 +13,17 @@
 from kantts.datasets.dataset import get_am_datasets, get_voc_datasets
 from kantts.models import model_builder
 from kantts.train.loss import criterion_builder
 from kantts.train.trainer import GAN_Trainer, Sambert_Trainer, distributed_init
 from kantts.utils.ling_unit.ling_unit import KanTtsLinguisticUnit
 from torch.utils.data import DataLoader
 
-from modelscope import __version__
 from modelscope.utils.audio.audio_utils import TtsCustomParams
 from modelscope.utils.audio.tts_exceptions import (
     TtsModelConfigurationException, TtsModelNotExistsException)
-from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 def count_parameters(model):
     return sum(p.numel() for p in model.parameters() if p.requires_grad)
@@ -390,14 +388,15 @@
         if train_steps > 0:
             train_max_steps = train_steps + from_steps
             config['train_max_steps'] = train_max_steps
 
         logger.info(f'TRAINING steps: {train_max_steps}')
         config['create_time'] = time.strftime('%Y-%m-%d %H:%M:%S',
                                               time.localtime())
+        from modelscope import __version__
         config['modelscope_version'] = __version__
 
         with open(os.path.join(stage_dir, 'config.yaml'), 'w') as f:
             yaml.dump(config, f, Dumper=yaml.Dumper, default_flow_style=None)
 
         for key, value in config.items():
             logger.info(f'{key} = {value}')
@@ -554,14 +553,15 @@
             train_max_steps = train_steps
             config['train_max_steps'] = train_max_steps
 
         logger.info(f'TRAINING steps: {train_max_steps}')
         logger.info(f'resume from: {resume_from}')
         config['create_time'] = time.strftime('%Y-%m-%d %H:%M:%S',
                                               time.localtime())
+        from modelscope import __version__
         config['modelscope_version'] = __version__
 
         with open(os.path.join(stage_dir, 'config.yaml'), 'w') as f:
             yaml.dump(config, f, Dumper=yaml.Dumper, default_flow_style=None)
 
         for key, value in config.items():
             logger.info(f'{key} = {value}')
```

### Comparing `modelscope-1.5.2/modelscope/models/base/base_head.py` & `modelscope-1.6.0/modelscope/models/base/base_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/base/base_model.py` & `modelscope-1.6.0/modelscope/models/base/base_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/base/base_torch_head.py` & `modelscope-1.6.0/modelscope/models/base/base_torch_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/base/base_torch_model.py` & `modelscope-1.6.0/modelscope/models/base/base_torch_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/builder.py` & `modelscope-1.6.0/modelscope/models/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 # yapf: disable
 from . import (action_recognition, animal_recognition, bad_image_detecting,
                body_2d_keypoints, body_3d_keypoints, cartoon,
                cmdssl_video_embedding, controllable_image_generation,
-               crowd_counting, face_2d_keypoints, face_detection,
-               face_generation, face_reconstruction, human_reconstruction,
-               human_wholebody_keypoint, image_classification,
+               crowd_counting, face_detection, face_generation,
+               face_reconstruction, human_reconstruction, image_classification,
                image_color_enhance, image_colorization, image_defrcn_fewshot,
                image_denoise, image_inpainting, image_instance_segmentation,
                image_matching, image_mvs_depth_estimation,
                image_panoptic_segmentation, image_portrait_enhancement,
                image_probing_model, image_quality_assessment_degradation,
                image_quality_assessment_man, image_quality_assessment_mos,
                image_reid_person, image_restoration,
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_model.py` & `modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py` & `modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py` & `modelscope-1.6.0/modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_detection/action_detection_onnx.py` & `modelscope-1.6.0/modelscope/models/cv/action_detection/action_detection_onnx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py` & `modelscope-1.6.0/modelscope/models/cv/action_detection/modules/action_detection_pytorch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_detection/modules/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/action_detection/modules/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_recognition/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/action_recognition/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_recognition/models.py` & `modelscope-1.6.0/modelscope/models/cv/action_recognition/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_recognition/s3dg.py` & `modelscope-1.6.0/modelscope/models/cv/action_recognition/s3dg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_recognition/tada_convnext.py` & `modelscope-1.6.0/modelscope/models/cv/action_recognition/tada_convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/action_recognition/temporal_patch_shift_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/animal_recognition/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/animal_recognition/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/animal_recognition/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/animal_recognition/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/animal_recognition/splat.py` & `modelscope-1.6.0/modelscope/models/cv/animal_recognition/splat.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py` & `modelscope-1.6.0/modelscope/models/cv/bad_image_detecting/bad_image_detecting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py` & `modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/hrnet_basic_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py` & `modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/hrnet_v2.py`

 * *Files 0% similar despite different names*

```diff
@@ -68,15 +68,15 @@
             for i in range(len(num_channels))
         ]
         self.transition3 = self._make_transition_layer(pre_stage_channels,
                                                        num_channels)
         self.stage4, pre_stage_channels = self._make_stage(
             self.stage4_cfg, num_channels, multi_scale_output=True)
         """final four layers"""
-        last_inp_channels = np.int(np.sum(pre_stage_channels))
+        last_inp_channels = int(np.sum(pre_stage_channels))
         self.final_layer = nn.Sequential(
             nn.Conv2d(
                 in_channels=last_inp_channels,
                 out_channels=last_inp_channels,
                 kernel_size=1,
                 stride=1,
                 padding=0),
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_2d_keypoints/w48.py` & `modelscope-1.6.0/modelscope/models/cv/body_2d_keypoints/w48.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/block.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/directed_graph.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py` & `modelscope-1.6.0/modelscope/models/cv/body_3d_keypoints/hdformer/skeleton.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/LK/lk.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/LK/lk.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/config.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/face_detector.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/face_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/face_landmark.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/face_landmark.py`

 * *Files 0% similar despite different names*

```diff
@@ -77,15 +77,15 @@
         center = [(bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2]
 
         bbox[0] = center[0] - one_edge // 2
         bbox[1] = center[1] - one_edge // 2
         bbox[2] = center[0] + one_edge // 2
         bbox[3] = center[1] + one_edge // 2
 
-        bbox = bbox.astype(np.int)
+        bbox = bbox.astype(int)
         crop_image = bimg[bbox[1]:bbox[3], bbox[0]:bbox[2], :]
         h, w, _ = crop_image.shape
         crop_image = cv2.resize(
             crop_image,
             (cfg.KEYPOINTS.input_shape[1], cfg.KEYPOINTS.input_shape[0]))
         crop_image = crop_image.astype(np.float32)
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/facelib/facer.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/facelib/facer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/loss.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/model_tf.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/model_tf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/network.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cartoon/utils.py` & `modelscope-1.6.0/modelscope/models/cv/cartoon/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/c3d.py` & `modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/c3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py` & `modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/resnet2p1d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py` & `modelscope-1.6.0/modelscope/models/cv/cmdssl_video_embedding/resnet3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/annotator.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/annotator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/api.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/midas/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/midas/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/mlsd/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/controllable_image_generation/controlnet.py` & `modelscope-1.6.0/modelscope/models/cv/controllable_image_generation/controlnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/crowd_counting/cc_model.py` & `modelscope-1.6.0/modelscope/models/cv/crowd_counting/cc_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py` & `modelscope-1.6.0/modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py`

 * *Files 0% similar despite different names*

```diff
@@ -352,15 +352,15 @@
             num_channels[i] * block.expansion
             for i in range(len(num_channels))
         ]
         self.transition2 = self._make_transition_layer(pre_stage_channels,
                                                        num_channels)
         self.stage3, pre_stage_channels = self._make_stage(
             self.stage3_cfg, num_channels)
-        last_inp_channels = np.int(np.sum(pre_stage_channels)) + 256
+        last_inp_channels = int(np.sum(pre_stage_channels)) + 256
         self.redc_layer = nn.Sequential(
             nn.Conv2d(
                 in_channels=last_inp_channels,
                 out_channels=128,
                 kernel_size=3,
                 stride=1,
                 padding=1),
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py` & `modelscope-1.6.0/modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/detectors.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/detectors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/mogface.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/mogface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/mogprednet.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/mogprednet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mogface/models/utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mogface/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/detector.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/first_stage.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/mtcnn/models/get_nets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/LK/lk.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/face_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/face_landmark.py`

 * *Files 1% similar despite different names*

```diff
@@ -78,15 +78,15 @@
         center = [(bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2]
 
         bbox[0] = center[0] - one_edge // 2
         bbox[1] = center[1] - one_edge // 2
         bbox[2] = center[0] + one_edge // 2
         bbox[3] = center[1] + one_edge // 2
 
-        bbox = bbox.astype(np.int)
+        bbox = bbox.astype(int)
         crop_image = bimg[bbox[1]:bbox[3], bbox[0]:bbox[2], :]
         h, w, _ = crop_image.shape
         crop_image = cv2.resize(crop_image,
                                 (self.kp_shape[1], self.kp_shape[0]))
         crop_image = crop_image.astype(np.float32)
 
         keypoints, state = self.simple_run(crop_image)
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/peppa_pig_face/facer.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/peppa_pig_face/facer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/detection.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/models/net.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/models/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/models/retinaface.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/models/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/retinaface/utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/damofd_detect.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/damofd_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/preprocessor.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/scrfd_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/scrfd/tinymog_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/detection.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/face_detection/ulfd_slim/vision/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/efficient/model.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/efficient/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/efficient/utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/efficient/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/emotion_infer.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/emotion_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/emotion_model.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/emotion_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/face_alignment/face.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/face_alignment/face.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_emotion/face_alignment/face_align.py` & `modelscope-1.6.0/modelscope/models/cv/face_emotion/face_alignment/face_align.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_generation/op/conv2d_gradfix.py` & `modelscope-1.6.0/modelscope/models/cv/face_generation/op/conv2d_gradfix.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_generation/op/fused_act.py` & `modelscope-1.6.0/modelscope/models/cv/face_generation/op/fused_act.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_generation/op/upfirdn2d.py` & `modelscope-1.6.0/modelscope/models/cv/face_generation/op/upfirdn2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_generation/stylegan2.py` & `modelscope-1.6.0/modelscope/models/cv/face_generation/stylegan2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/det_infer.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/det_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/ghost_pan.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/ghost_pan.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/nanodet_plus_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/one_stage_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/shufflenetv2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_human_hand_detection/utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_human_hand_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/align_face.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/align_face.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/common.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/backbone/model_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py` & `modelscope-1.6.0/modelscope/models/cv/face_recognition/torchkit/rts_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/bfm.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/bfm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/de_retouching_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/facerecon_model.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/facerecon_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/losses.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/networks.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/nv_diffrast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/renderer.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/renderer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/models/unet.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/models/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/face_reconstruction/utils.py` & `modelscope-1.6.0/modelscope/models/cv/face_reconstruction/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py` & `modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/facial_expression_recognition/fer/vgg.py` & `modelscope-1.6.0/modelscope/models/cv/facial_expression_recognition/fer/vgg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py` & `modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py` & `modelscope-1.6.0/modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/hand_static/hand_model.py` & `modelscope-1.6.0/modelscope/models/cv/hand_static/hand_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/hand_static/networks.py` & `modelscope-1.6.0/modelscope/models/cv/hand_static/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/Reconstruction.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/Reconstruction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/Embedding.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/Embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/PixToMesh.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/PixToMesh.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/Res_backbone.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/Res_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/Surface_head.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/Surface_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/detectors.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/detectors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/geometry.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/geometry.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/human_segmenter.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/human_segmenter.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,15 +27,15 @@
         print('human_segmenter init done')
 
     def image_preprocess(self, img):
         if len(img.shape) == 2:
             img = np.dstack((img, img, img))
         elif img.shape[2] == 4:
             img = img[:, :, :3]
-        img = img.astype(np.float)
+        img = img.astype(float)
         return img
 
     def run(self, img):
         image_feed = self.image_preprocess(img)
         output_img_value, logits_value = self.sess.run(
             [self.output_node, self.logits_node],
             feed_dict={self.image_node: image_feed})
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/models/networks.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/models/networks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_reconstruction/utils.py` & `modelscope-1.6.0/modelscope/models/cv/human_reconstruction/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -65,16 +65,16 @@
 def eval_grid(coords,
               eval_func,
               init_resolution=64,
               threshold=0.01,
               num_samples=512 * 512 * 512):
     resolution = coords.shape[1:4]
     sdf = np.zeros(resolution)
-    dirty = np.ones(resolution, dtype=np.bool)
-    grid_mask = np.zeros(resolution, dtype=np.bool)
+    dirty = np.ones(resolution, dtype=bool)
+    grid_mask = np.zeros(resolution, dtype=bool)
     reso = resolution[0] // init_resolution
 
     while reso > 0:
         grid_mask[0:resolution[0]:reso, 0:resolution[1]:reso,
                   0:resolution[2]:reso] = True
         test_mask = np.logical_and(grid_mask, dirty)
         points = coords[:, test_mask]
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/human_wholebody_keypoint/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,18 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .human_wholebody_keypoint import HumanWholeBodyKeypoint
-
+    from .image_inpainting_dataset import ImageInpaintingDataset
 else:
     _import_structure = {
-        'human_wholebody_keypoint': ['HumanWholeBodyKeypoint']
+        'image_inpainting_dataset': ['ImageInpaintingDataset'],
     }
-
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
         _import_structure,
         module_spec=__spec__,
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_binary_quant_classification/bnext.py` & `modelscope-1.6.0/modelscope/models/cv/image_binary_quant_classification/bnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/image_body_reshaping.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/person_info.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/person_info.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/body.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/pose_estimator/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_body_reshaping/slim_utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_body_reshaping/slim_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_classification/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_classification/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_classification/backbones/beit_v2.py` & `modelscope-1.6.0/modelscope/models/cv/image_classification/backbones/beit_v2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_classification/backbones/nextvit.py` & `modelscope-1.6.0/modelscope/models/cv/image_classification/backbones/nextvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_classification/mmcls_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_classification/mmcls_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_classification/resnet50_cc.py` & `modelscope-1.6.0/modelscope/models/cv/image_classification/resnet50_cc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_classification/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_classification/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_color_enhance/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_color_enhance/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_color_enhance/adaint/adaint.py` & `modelscope-1.6.0/modelscope/models/cv/image_color_enhance/adaint/adaint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_color_enhance/csrnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_color_enhance/csrnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py` & `modelscope-1.6.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_color_enhance/deeplpf/deeplpfnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_color_enhance/image_color_enhance.py` & `modelscope-1.6.0/modelscope/models/cv/image_color_enhance/image_color_enhance.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/loss.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/transformer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/ddcolor/utils/vgg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/unet/unet.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/unet/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_colorization/unet/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_colorization/unet/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py` & `modelscope-1.6.0/modelscope/models/cv/image_debanding/rrdb/rrdb_image_debanding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py` & `modelscope-1.6.0/modelscope/models/cv/image_deblur/nafnet_for_image_deblur.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/calibration_layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/defrcn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/fast_rcnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/gdl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/models/roi_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/coco_register.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/register_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/requirements_check.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py` & `modelscope-1.6.0/modelscope/models/cv/image_defrcn_fewshot/utils/voc_register.py`

 * *Files 1% similar despite different names*

```diff
@@ -159,15 +159,15 @@
         seed = int(name.split('_seed')[-1])
         split_dir = os.path.join(split_dir, 'seed{}'.format(seed))
         for cls in classnames:
             with PathManager.open(
                     os.path.join(split_dir,
                                  'box_{}shot_{}_train.txt'.format(shot,
                                                                   cls))) as f:
-                fileids_ = np.loadtxt(f, dtype=np.str).tolist()
+                fileids_ = np.loadtxt(f, dtype=np.str_).tolist()
                 if isinstance(fileids_, str):
                     fileids_ = [fileids_]
                 fileids_ = [
                     fid.split('/')[-1].split('.jpg')[0] for fid in fileids_
                 ]
                 fileids[cls] = fileids_
 
@@ -215,15 +215,15 @@
             if len(dicts_) > int(shot):
                 dicts_ = np.random.choice(dicts_, int(shot), replace=False)
             dicts.extend(dicts_)
     else:
         with PathManager.open(
                 os.path.join(root, dirname, 'ImageSets', 'Main',
                              split + '.txt')) as f:
-            fileids = np.loadtxt(f, dtype=np.str)
+            fileids = np.loadtxt(f, dtype=np.str_)
 
         for fileid in fileids:
             anno_file = os.path.join(root, dirname, 'Annotations',
                                      fileid + '.xml')
             jpeg_file = os.path.join(root, dirname, 'JPEGImages',
                                      fileid + '.jpg')
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_denoise/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_denoise/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py` & `modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet/NAFNet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet/arch_util.py` & `modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet/arch_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py` & `modelscope-1.6.0/modelscope/models/cv/image_denoise/nafnet_for_image_denoise.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_depth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/newcrf_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/networks/uper_crf_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation/newcrfs_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation/newcrfs_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/bts_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_depth_estimation_bts/networks/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_driving_perception/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_driving_perception/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_driving_perception/image_driving_percetion_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_driving_perception/preprocessor.py` & `modelscope-1.6.0/modelscope/models/cv/image_driving_perception/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_driving_perception/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_driving_perception/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/gan_wrap.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/fused_act.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facegan/op/upfirdn2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facelib/align_trans.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facelib/align_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/facelib/matlab_cp2tform.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/image_face_fusion.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/image_face_fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/aad_layer.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/aad_layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/aei_flow_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/bfm.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/bfm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/dense_motion.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/dense_motion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/facerecon_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/facerecon_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/model_irse.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_face_fusion/network/ops.py` & `modelscope-1.6.0/modelscope/models/cv/image_face_fusion/network/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/backbone/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/backbone/deeplab_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp/m2fp_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/m2fp_net.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/m2fp_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_human_parsing/parsing_utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_human_parsing/parsing_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/base.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/default.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/default.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ade20k/base.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ade20k/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/adversarial.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/adversarial.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/feature_matching.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/feature_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/ffc.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/ffc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/inception.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/inception.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/perceptual.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/perceptual.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/modules/pix2pixhd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_inpainting/refinement.py` & `modelscope-1.6.0/modelscope/models/cv/image_inpainting/refinement.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,18 +4,20 @@
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
     from .cascade_mask_rcnn_swin import CascadeMaskRCNNSwin
     from .maskdino_swin import MaskDINOSwin
     from .model import CascadeMaskRCNNSwinModel
     from .maskdino_model import MaskDINOSwinModel
+    from .fastinst_model import FastInst
     from .postprocess_utils import get_img_ins_seg_result, get_maskdino_ins_seg_result
 else:
     _import_structure = {
         'cascade_mask_rcnn_swin': ['CascadeMaskRCNNSwin'],
+        'fastinst_model': ['FastInst'],
         'maskdino_swin': ['MaskDINOSwin'],
         'model': ['CascadeMaskRCNNSwinModel'],
         'maskdino_model': ['MaskDINOSwinModel'],
         'postprocess_utils':
         ['get_img_ins_seg_result', 'get_maskdino_ins_seg_result'],
     }
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .swin_transformer import SwinTransformer
-    from .swin_transformer import D2SwinTransformer
+    from .table_question_answering_preprocessor import TableQuestionAnsweringPreprocessor
+    from .fields import MultiWOZBPETextField, IntentBPETextField
 
 else:
     _import_structure = {
-        'swin_transformer': ['SwinTransformer', 'D2SwinTransformer'],
+        'table_question_answering_preprocessor':
+        ['TableQuestionAnsweringPreprocessor'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/datasets/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/maskdino_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/postprocess_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -104,24 +104,24 @@
         OutputKeys.MASKS: [],
         OutputKeys.LABELS: [],
         OutputKeys.SCORES: []
     }
     for seg_result in img_seg_result:
 
         box = [
-            np.int(seg_result[0]),
-            np.int(seg_result[1]),
-            np.int(seg_result[2]),
-            np.int(seg_result[3])
+            int(seg_result[0]),
+            int(seg_result[1]),
+            int(seg_result[2]),
+            int(seg_result[3])
         ]
-        score = np.float(seg_result[4])
+        score = float(seg_result[4])
         category = seg_result[5]
 
         mask = np.array(seg_result[6], order='F', dtype='uint8')
-        mask = mask.astype(np.float)
+        mask = mask.astype(float)
 
         results_dict[OutputKeys.BOXES].append(box)
         results_dict[OutputKeys.MASKS].append(mask)
         results_dict[OutputKeys.SCORES].append(score)
         results_dict[OutputKeys.LABELS].append(category)
 
     return results_dict
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/config/default.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/config/default.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_matching/quadtree_attention_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_matching/quadtree_attention_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/cas_mvsnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/casmvs_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py`

 * *Files 1% similar despite different names*

```diff
@@ -378,15 +378,15 @@
         for p3d_id in images[i + 1].point3D_ids:
             if p3d_id == -1:
                 continue
             transformed = np.matmul(extrinsic[i + 1], [
                 points3d[p3d_id].xyz[0], points3d[p3d_id].xyz[1],
                 points3d[p3d_id].xyz[2], 1
             ])
-            zs.append(np.asscalar(transformed[2]))
+            zs.append(transformed[2].item())
         zs_sorted = sorted(zs)
         # relaxed depth range
         max_ratio = 0.1
         min_ratio = 0.03
         num_max = max(5, int(len(zs) * max_ratio))
         num_min = max(1, int(len(zs) * min_ratio))
         depth_min = 1.0 * sum(zs_sorted[:num_min]) / len(zs_sorted[:num_min])
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/depth_filter.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,15 +36,15 @@
 # read a binary mask
 def read_mask(filename):
     return read_img(filename) > 0.5
 
 
 # save a binary mask
 def save_mask(filename, mask):
-    assert mask.dtype == np.bool
+    assert mask.dtype == bool
     mask = mask.astype(np.uint8) * 255
     Image.fromarray(mask).save(filename)
 
 
 # read a pair file, [(ref_view1, [src_view1-1, ...]), (ref_view2, [src_view2-1, ...]), ...]
 def read_pair_file(filename):
     data = []
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/module.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_mvs_depth_estimation/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_mvs_depth_estimation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_paintbyexample/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_paintbyexample/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_panoptic_segmentation/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
     from .panseg_model import SwinLPanopticSegmentation
-    from .r50_panseg_model import R50PanopticSegmentation
 
 else:
     _import_structure = {
         'panseg_model': ['SwinLPanopticSegmentation'],
     }
 
     import sys
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_panoptic_segmentation/panseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/tinynas_detector.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-
-from easycv.models.segmentation import Mask2Former
+# The DAMO-YOLO implementation is also open-sourced by the authors at https://github.com/tinyvision/damo-yolo.
 
 from modelscope.metainfo import Models
 from modelscope.models.builder import MODELS
-from modelscope.models.cv.easycv_base import EasyCVBaseModel
 from modelscope.utils.constant import Tasks
+from .detector import SingleStageDetector
 
 
 @MODELS.register_module(
-    group_key=Tasks.image_segmentation,
-    module_name=Models.r50_panoptic_segmentation)
-class R50PanopticSegmentation(EasyCVBaseModel, Mask2Former):
+    Tasks.image_object_detection, module_name=Models.tinynas_detection)
+class TinynasDetector(SingleStageDetector):
 
-    def __init__(self, model_dir=None, *args, **kwargs):
-        EasyCVBaseModel.__init__(self, model_dir, args, kwargs)
-        Mask2Former.__init__(self, *args, **kwargs)
+    def __init__(self, model_dir, *args, **kwargs):
+        self.config_name = 'airdet_s.py'
+        super(TinynasDetector, self).__init__(model_dir, *args, **kwargs)
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/align_faces.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/align_faces.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/gpen.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/gpen.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/helpers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/losses.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/losses/model_irse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_portrait_enhancement/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_probing_model/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_probing_model/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_probing_model/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/image_probing_model/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_probing_model/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_probing_model/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_probing_model/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_probing_model/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/degradation_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/image_quality_assessment_man.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/maniqa.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/maniqa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_man/swin.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_man/swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/heads/simple_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py` & `modelscope-1.6.0/modelscope/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_reid_person/pass_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_reid_person/pass_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_reid_person/transreid_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_reid_person/transreid_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_restoration/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_restoration/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_restoration/demoire_models/nets.py` & `modelscope-1.6.0/modelscope/models/cv/image_restoration/demoire_models/nets.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_restoration/image_restoration_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_restoration/image_restoration_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,15 +56,15 @@
         semantic_result = Inputs[0]
 
         ids = np.unique(semantic_result)[::-1]
         legal_indices = ids != self.model.num_classes  # for VOID label
         ids = ids[legal_indices]
 
         segms = (semantic_result[None] == ids[:, None, None])
-        masks = [it.astype(np.int) for it in segms]
+        masks = [it.astype(int) for it in segms]
         labels_txt = np.array(self.CLASSES)[ids].tolist()
 
         results = {
             OutputKeys.MASKS: masks,
             OutputKeys.LABELS: labels_txt,
             OutputKeys.SCORES: [0.999 for _ in range(len(labels_txt))]
         }
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py` & `modelscope-1.6.0/modelscope/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/preprocessor.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/BlockModules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_backnone.py`

 * *Files 0% similar despite different names*

```diff
@@ -454,15 +454,15 @@
             for i in range(len(num_channels))
         ]
         self.transition3 = self._make_transition_layer(pre_stage_channels,
                                                        num_channels)
         self.stage4, pre_stage_channels = self._make_stage(
             self.stage4_cfg, num_channels, multi_scale_output=True)
 
-        self.backbone_last_inp_channels = np.int(np.sum(pre_stage_channels))
+        self.backbone_last_inp_channels = int(np.sum(pre_stage_channels))
 
     def _make_transition_layer(self, num_channels_pre_layer,
                                num_channels_cur_layer):
         num_branches_cur = len(num_channels_cur_layer)
         num_branches_pre = len(num_channels_pre_layer)
 
         transition_layers = []
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py`

 * *Files 0% similar despite different names*

```diff
@@ -255,15 +255,15 @@
         self.is_contain_aspp = True if 'aspp' in kwargs else False
 
         if self.architecture == 'hrnet_super_ocr':
             self.is_ocr_first = False
             num_channels = [64, last_inp_channels]
             self.stage_super, super_stage_channels = self._make_stage(
                 self.super_dict, num_channels)
-            last_inp_channels = np.int(np.sum(super_stage_channels))
+            last_inp_channels = int(np.sum(super_stage_channels))
 
             if self.is_contain_aspp:
                 aspp_param = kwargs['aspp']
                 self.aspp_layer = ASPP(
                     inplanes=last_inp_channels,
                     outplanes=aspp_param['outplanes'],
                     dilations=aspp_param['dilations'],
@@ -368,15 +368,15 @@
                 scale=scale,
                 dropout=dropout_rate,
             )
 
             num_channels = [64, ocr_mid_channels]
             self.stage_super, super_stage_channels = self._make_stage(
                 self.super_dict, num_channels)
-            last_inp_channels = np.int(np.sum(super_stage_channels))
+            last_inp_channels = int(np.sum(super_stage_channels))
 
             self.cls_head = nn.Sequential(
                 nn.Conv2d(
                     last_inp_channels,
                     last_inp_channels,
                     kernel_size=1,
                     stride=1,
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/ptsemseg/unet.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/ptsemseg/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/skychange.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/skychange.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_skychange/skychange_model.py` & `modelscope-1.6.0/modelscope/models/cv/image_skychange/skychange_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/data/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/data/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/data/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/data/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/model.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/autoencoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/models/clip.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/models/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/diffusion.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_generation/ops/losses.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_generation/ops/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/data/transforms.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/data/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/model_translation.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/model_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/models/autoencoder.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/models/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/models/clip.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/models/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/apps.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/apps.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/degradation.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/degradation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/diffusion.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/losses.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/losses.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/metrics.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/metrics.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/random_color.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/random_color.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/random_mask.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/random_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/svd.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/svd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/image_to_image_translation/ops/utils.py` & `modelscope-1.6.0/modelscope/models/cv/image_to_image_translation/ops/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/fourier.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/panostretch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/misc/post_proc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/modality/layout.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/panovit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/networks/utils.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/networks/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/indoor_layout_estimation/panovit.py` & `modelscope-1.6.0/modelscope/models/cv/indoor_layout_estimation/panovit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/summarizer.py` & `modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/summarizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py` & `modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/models.py` & `modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py` & `modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py` & `modelscope-1.6.0/modelscope/models/cv/language_guided_video_summarization/transformer/sub_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/model.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/cfg_sampler.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/cfg_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/mdm.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/mdm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/respace.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/respace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/rotation2xyz.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/rotation2xyz.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/motion_generation/modules/smpl.py` & `modelscope-1.6.0/modelscope/models/cv/motion_generation/modules/smpl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/get_model.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/get_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/model.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/head.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/save_op.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/shot_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/movie_scene_segmentation/utils/trn.py` & `modelscope-1.6.0/modelscope/models/cv/movie_scene_segmentation/utils/trn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/nerf_preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/nerf_recon_acc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/nerf.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/nerf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/segmenter.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/segmenter.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 
     def image_preprocess(self, img):
         if len(img.shape) == 2:
             img = np.dstack((img, img, img))
         elif img.shape[2] == 4:
             img = img[:, :, :3]
         img = img[:, :, ::-1]
-        img = img.astype(np.float)
+        img = img.astype(float)
         return img
 
     def run_mask(self, img):
         image_feed = self.image_preprocess(img)
         output_img_value, logits_value = self.sess.run(
             [self.output_node, self.logits_node],
             feed_dict={self.image_node: image_feed})
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/nerf_recon_acc/network/utils.py` & `modelscope-1.6.0/modelscope/models/cv/nerf_recon_acc/network/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_model.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/necks/fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/utils/checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/depe_detect.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/depe_detect.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/object_detection_3d/depe/result_vis.py` & `modelscope-1.6.0/modelscope/models/cv/object_detection_3d/depe/result_vis.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,15 +26,15 @@
     colors = np.array(
         [[max_lumi, 0, max_lumi], [max_lumi, 0, 0], [max_lumi, max_lumi, 0],
          [0, max_lumi, 0], [0, max_lumi, max_lumi], [0, 0, max_lumi]],
         dtype=np.float32)
     if gray == 1:
         return tuple(colors[-1].tolist())
     num_rank = len(colors) - 1
-    rank = np.floor(gray * num_rank).astype(np.int)
+    rank = np.floor(gray * num_rank).astype(int)
     diff = (gray - rank / num_rank) * num_rank
     tmp = colors[rank + 1] - colors[rank]
     return tuple((colors[rank] + tmp * diff).tolist())
 
 
 def lidar2img(points_lidar, camrera_info):
     points_lidar_homogeneous = \
@@ -132,30 +132,30 @@
             corners_global = np.concatenate(
                 [corners_global,
                  np.ones([corners_global.shape[0], 1])],
                 axis=1)
             l2g = get_lidar2global(infos)
             corners_lidar = corners_global @ np.linalg.inv(l2g).T
             corners_lidar = corners_lidar[:, :3]
-        pred_flag = np.ones((corners_lidar.shape[0] // 8, ), dtype=np.bool)
+        pred_flag = np.ones((corners_lidar.shape[0] // 8, ), dtype=bool)
         scores = [
             pred_res[rid]['detection_score'] for rid in range(len(pred_res))
         ]
         if draw_gt:
             gt_boxes = infos['gt_boxes']
             gt_boxes[:, -1] = gt_boxes[:, -1] + np.pi / 2
             width = gt_boxes[:, 4].copy()
             gt_boxes[:, 4] = gt_boxes[:, 3]
             gt_boxes[:, 3] = width
             corners_lidar_gt = \
                 LB(infos['gt_boxes'],
                    origin=(0.5, 0.5, 0.5)).corners.numpy().reshape(-1, 3)
             corners_lidar = np.concatenate([corners_lidar, corners_lidar_gt],
                                            axis=0)
-            gt_flag = np.ones((corners_lidar_gt.shape[0] // 8), dtype=np.bool)
+            gt_flag = np.ones((corners_lidar_gt.shape[0] // 8), dtype=bool)
             pred_flag = np.concatenate(
                 [pred_flag, np.logical_not(gt_flag)], axis=0)
             scores = scores + [0 for _ in range(infos['gt_boxes'].shape[0])]
         scores = np.array(scores, dtype=np.float32)
         sort_ids = np.argsort(scores)
 
         # image view
@@ -165,15 +165,15 @@
             # draw instances
             corners_img, valid = lidar2img(corners_lidar, infos['cams'][view])
             valid = np.logical_and(
                 valid,
                 check_point_in_img(corners_img, img.shape[0], img.shape[1]))
             valid = valid.reshape(
                 -1, 8)  # valid means: d>0 and visible in current view
-            corners_img = corners_img.reshape(-1, 8, 2).astype(np.int)
+            corners_img = corners_img.reshape(-1, 8, 2).astype(int)
             for aid in range(valid.shape[0]):
                 if scores[aid] < vis_thred and pred_flag[aid]:
                     continue
                 for index in draw_boxes_indexes_img_view:
                     if valid[aid, index[0]] and valid[aid, index[1]]:
                         cv2.line(
                             img,
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_detection/model.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_detection/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_detection/modules/dbnet.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_detection/modules/dbnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_detection/modules/seg_detector_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_detection/preprocessor.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_detection/preprocessor.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,22 +33,24 @@
 
     def __call__(self, inputs):
         """process the raw input data
         Args:
             inputs:
                 - A string containing an HTTP link pointing to an image
                 - A string containing a local path to an image
-                - An image loaded in PIL or opencv directly
+                - An image loaded in PIL(PIL.Image.Image) or opencv(np.ndarray) directly, 3 channels RGB
         Returns:
             outputs: the preprocessed image
         """
         if isinstance(inputs, str):
             img = np.array(load_image(inputs))
         elif isinstance(inputs, PIL.Image.Image):
             img = np.array(inputs)
+        elif isinstance(inputs, np.ndarray):
+            img = inputs
         else:
             raise TypeError(
                 f'inputs should be either str, PIL.Image, np.array, but got {type(inputs)}'
             )
 
         img = img[:, :, ::-1]
         height, width, _ = img.shape
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_detection/utils.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/model.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/model.py`

 * *Files 3% similar despite different names*

```diff
@@ -86,16 +86,23 @@
         elif cfgs.model.recognizer == 'CRNN':
             self.recognizer = CRNN()
         else:
             raise TypeError(
                 f'recognizer should be either ConvNextViT, CRNN, but got {cfgs.model.recognizer}'
             )
         if model_path != '':
-            self.recognizer.load_state_dict(
-                torch.load(model_path, map_location='cpu'))
+            params_pretrained = torch.load(model_path, map_location='cpu')
+            model_dict = self.recognizer.state_dict()
+            # remove prefix for finetuned models
+            check_point = {
+                k.replace('recognizer.', ''): v
+                for k, v in params_pretrained.items()
+            }
+            model_dict.update(check_point)
+            self.recognizer.load_state_dict(model_dict)
 
         dict_path = os.path.join(model_dir, ModelFile.VOCAB_FILE)
         self.labelMapping = dict()
         self.charMapping = dict()
         with open(dict_path, 'r', encoding='utf-8') as f:
             lines = f.readlines()
             cnt = 1
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/convnext.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/convnextvit.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/convnextvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/crnn.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/crnn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/timm_tinyc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/modules/vitstr.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/modules/vitstr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/ocr_recognition/preprocessor.py` & `modelscope-1.6.0/modelscope/models/cv/ocr_recognition/preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/open_vocabulary_detection_vild/vild.py` & `modelscope-1.6.0/modelscope/models/cv/open_vocabulary_detection_vild/vild.py`

 * *Files 0% similar despite different names*

```diff
@@ -172,16 +172,15 @@
                      - rescaled_detection_boxes[:, 0]) * (
                          rescaled_detection_boxes[:, 3]
                          - rescaled_detection_boxes[:, 1])
 
         # Filter out invalid rois (nmsed rois)
         valid_indices = np.where(
             np.logical_and(
-                np.isin(
-                    np.arange(len(roi_scores), dtype=np.int), nmsed_indices),
+                np.isin(np.arange(len(roi_scores), dtype=int), nmsed_indices),
                 np.logical_and(
                     np.logical_not(np.all(roi_boxes == 0., axis=-1)),
                     np.logical_and(roi_scores >= min_rpn_score_thresh,
                                    box_sizes > min_box_area))))[0]
         # print('number of valid indices', len(valid_indices))
 
         # detection_roi_scores = roi_scores[valid_indices][:max_boxes_to_return,
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/equi.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/equi.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/layers.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/layers.py`

 * *Files 0% similar despite different names*

```diff
@@ -68,15 +68,15 @@
         0F 1R 2B 3L 4U 5D
         '''
         tp = np.roll(
             np.arange(4).repeat(self.equ_w // 4)[None, :].repeat(
                 self.equ_h, 0), 3 * self.equ_w // 8, 1)
 
         # Prepare ceil mask
-        mask = np.zeros((self.equ_h, self.equ_w // 4), np.bool)
+        mask = np.zeros((self.equ_h, self.equ_w // 4), bool)
         idx = np.linspace(-np.pi, np.pi, self.equ_w // 4) / 4
         idx = self.equ_h // 2 - np.round(
             np.arctan(np.cos(idx)) * self.equ_h / np.pi).astype(int)
         for i, j in enumerate(idx):
             mask[:j, i] = 1
         mask = np.roll(np.concatenate([mask] * 4, 1), 3 * self.equ_w // 8, 1)
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/mobilenet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/unifuse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/networks/util.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/networks/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py` & `modelscope-1.6.0/modelscope/models/cv/panorama_depth_estimation/unifuse_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/pedestrian_attribute_recognition/model.py` & `modelscope-1.6.0/modelscope/models/cv/pedestrian_attribute_recognition/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py` & `modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py` & `modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py` & `modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/rcp_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py` & `modelscope-1.6.0/modelscope/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/item_detection.py` & `modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/item_detection.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/item_embedding.py` & `modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/item_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_retrieval_embedding/item_model.py` & `modelscope-1.6.0/modelscope/models/cv/product_retrieval_embedding/item_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/product_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_segmentation/net.py` & `modelscope-1.6.0/modelscope/models/cv/product_segmentation/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/product_segmentation/seg_infer.py` & `modelscope-1.6.0/modelscope/models/cv/product_segmentation/seg_infer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/model.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/criterion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/matcher.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/misc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/mttr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/postprocessing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/referring_video_object_segmentation/utils/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/robust_image_classification/easyrobust_model.py` & `modelscope-1.6.0/modelscope/models/cv/robust_image_classification/easyrobust_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py` & `modelscope-1.6.0/modelscope/models/cv/salient_detection/models/backbone/Res2Net_v1b.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/salient_detection/models/modules.py` & `modelscope-1.6.0/modelscope/models/cv/salient_detection/models/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/salient_detection/models/senet.py` & `modelscope-1.6.0/modelscope/models/cv/salient_detection/models/senet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/salient_detection/models/u2net.py` & `modelscope-1.6.0/modelscope/models/cv/salient_detection/models/u2net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/salient_detection/models/utils.py` & `modelscope-1.6.0/modelscope/models/cv/salient_detection/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/salient_detection/salient_model.py` & `modelscope-1.6.0/modelscope/models/cv/salient_detection/salient_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/common.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/head_fpn.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/head_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/models.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/models.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/neck_fpn.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/neck_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/shop_seg_base.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/shop_seg_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/shop_seg_model.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/shop_seg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/shop_segmentation/utils.py` & `modelscope-1.6.0/modelscope/models/cv/shop_segmentation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/detection_model/detection_module.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/detection_model/detection_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/detection_model/detection_unet_in.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/inpainting_model/gconv.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/inpainting_model/inpainting_unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/box_utils.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/box_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/net.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/network.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/predict_single.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/predict_single.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/prior_box.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/prior_box.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/retinaface/utils.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/retinaface/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/unet_deploy.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/unet_deploy.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/utils.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/skin_retouching/weights_init.py` & `modelscope-1.6.0/modelscope/models/cv/skin_retouching/weights_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/data/data_augment.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/data/data_augment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/default/streamyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/exp/yolox_base.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/exp/yolox_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/darknet.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/darknet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/dfp_pafpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/network_blocks.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/network_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/streamyolo.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/streamyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/models/tal_head.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/models/tal_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/realtime_video_detector.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/realtime_video_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/stream_yolo/utils/boxes.py` & `modelscope-1.6.0/modelscope/models/cv/stream_yolo/utils/boxes.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/super_resolution/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/super_resolution/arch_util.py` & `modelscope-1.6.0/modelscope/models/cv/super_resolution/arch_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/super_resolution/ecb.py` & `modelscope-1.6.0/modelscope/models/cv/super_resolution/ecb.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/super_resolution/ecbsr_model.py` & `modelscope-1.6.0/modelscope/models/cv/super_resolution/ecbsr_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/super_resolution/rrdbnet_arch.py` & `modelscope-1.6.0/modelscope/models/cv/super_resolution/rrdbnet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/table_recognition/lineless_table_process.py` & `modelscope-1.6.0/modelscope/models/cv/table_recognition/lineless_table_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/table_recognition/model_lore.py` & `modelscope-1.6.0/modelscope/models/cv/table_recognition/model_lore.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/table_recognition/modules/lore_detector.py` & `modelscope-1.6.0/modelscope/models/cv/table_recognition/modules/lore_detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/table_recognition/modules/lore_processor.py` & `modelscope-1.6.0/modelscope/models/cv/table_recognition/modules/lore_processor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/clip.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_base.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_model.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_net.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/lseg_vit.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/lseg_vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/model.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py` & `modelscope-1.6.0/modelscope/models/cv/text_driven_segmentation/simple_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/basic_blocks.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/basic_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/global_utils.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/global_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/master_net.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/master_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/model_zoo.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/model_zoo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/plain_net_utils.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/plain_net_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_blocks.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_res_idwexkx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_res_k1kxk1.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_classfication/super_res_kxkx.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_evaluater.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/apis/detector_inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/base_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/core/weight_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/detectors/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/bounding_box.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/boxlist_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/structures/image_list.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/boxes.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/damo/utils/scheduler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/detector.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/detector.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/tinynas_damoyolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/tinynas_detection/utils.py` & `modelscope-1.6.0/modelscope/models/cv/tinynas_detection/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/UNet_for_video_deinterlace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/deinterlace_arch.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/deinterlace_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/archs.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/archs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/deep_fourier_upsampling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/enh.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/enh.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/fre.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/fre.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_deinterlace/models/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_deinterlace/models/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/configs/default_config.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/configs/default_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/dro_model.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/dro_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/camera.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/camera.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/camera_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/pose.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/pose.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/geometry/pose_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/model_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/model_utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/model_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/sfm_model_mf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/models/sup_model_mf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/optim/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/networks/optim/update.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/networks/optim/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/augmentations.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/augmentations.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/config.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/depth.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/depth.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,15 +25,15 @@
         Depth map (invalid pixels are 0)
     """
     if file.endswith('npz'):
         return np.load(file)['depth']
     elif file.endswith('png'):
         depth_png = np.array(load_image(file), dtype=int)
         assert (np.max(depth_png) > 255), 'Wrong .png depth file'
-        return depth_png.astype(np.float) / 256.
+        return depth_png.astype(float) / 256.
     else:
         raise NotImplementedError('Depth extension not supported.')
 
 
 def write_depth(filename, depth, intrinsics=None):
     """
     Write a depth map to file, and optionally its corresponding intrinsics.
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/horovod.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/horovod.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/image.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/image_gt.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/image_gt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/load.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/load.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/misc.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/misc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_depth_estimation/utils/types.py` & `modelscope-1.6.0/modelscope/models/cv/video_depth_estimation/utils/types.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/VFINet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/corr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/flow_model/update.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/flow_model/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/UNet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/flow_reversal.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/interp_model/transformer_layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/utils/scene_change_detection.py`

 * *Files 1% similar despite different names*

```diff
@@ -81,15 +81,15 @@
     # compare pix diff
     ori_img = img0_gray
     ref_img = img1_gray[:, :, y_, x_]
 
     img_diff = ori_img.float() - ref_img.float()
     img_diff = torch.abs(img_diff)
 
-    kernel = np.ones([8, 8], np.float) / 64
+    kernel = np.ones([8, 8], float) / 64
     kernel = torch.FloatTensor(kernel).to(device).unsqueeze(0).unsqueeze(0)
     diff = F.conv2d(img_diff, kernel, padding=4)
 
     diff = diff[0, 0, y_grid, x_grid]
 
     index = (distance > thres) * (diff > 5)
     if index.sum().float() / distance.numel() > 0.5:
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_frame_interpolation/utils/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_frame_interpolation/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/model.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/decoder.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/deep_guided_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/effv2.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/effv2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/lraspp.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/lraspp.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_human_matting/models/matting.py` & `modelscope-1.6.0/modelscope/models/cv/video_human_matting/models/matting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_inpainting/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_inpainting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_inpainting/inpainting.py` & `modelscope-1.6.0/modelscope/models/cv/video_inpainting/inpainting.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_inpainting/inpainting_model.py` & `modelscope-1.6.0/modelscope/models/cv/video_inpainting/inpainting_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/head/kernel_updator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/neck/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/neck/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/track/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_instance_segmentation/video_knet.py` & `modelscope-1.6.0/modelscope/models/cv/video_instance_segmentation/video_knet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/common.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/decode.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/decode.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/model.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/models/yolo.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/models/yolo.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/basetrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/matching.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,15 +23,15 @@
     unmatched_a = np.where(x < 0)[0]
     unmatched_b = np.where(y < 0)[0]
     matches = np.asarray(matches)
     return matches, unmatched_a, unmatched_b
 
 
 def ious(atlbrs, btlbrs):
-    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float)
+    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=float)
     if ious.size == 0:
         return ious
 
     ious = bbox_iou(atlbrs, btlbrs, True).numpy()
 
     return ious
 
@@ -56,21 +56,21 @@
         tracks: list[STrack]
         detections: list[BaseTrack]
         metric: str
     Returns:
         cost_matrix: np.ndarray
     """
 
-    cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float)
+    cost_matrix = np.zeros((len(tracks), len(detections)), dtype=float)
     if cost_matrix.size == 0:
         return cost_matrix
     det_features = np.asarray([track.curr_feat for track in detections],
-                              dtype=np.float)
+                              dtype=float)
     track_features = np.asarray([track.smooth_feat for track in tracks],
-                                dtype=np.float)
+                                dtype=float)
     cost_matrix = np.maximum(0.0, cdist(track_features, det_features, metric))
     return cost_matrix
 
 
 def fuse_motion(kf,
                 cost_matrix,
                 tracks,
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/tracker/multitracker.py`

 * *Files 0% similar despite different names*

```diff
@@ -24,15 +24,15 @@
 
 class STrack(BaseTrack):
     shared_kalman = KalmanFilter()
 
     def __init__(self, tlwh, score, temp_feat, buffer_size=30):
 
         # wait activate
-        self._tlwh = np.asarray(tlwh, dtype=np.float)
+        self._tlwh = np.asarray(tlwh, dtype=float)
         self.kalman_filter = None
         self.mean, self.covariance = None, None
         self.is_activated = False
 
         self.score = score
         self.tracklet_len = 0
```

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/image.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/kalman_filter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py` & `modelscope-1.6.0/modelscope/models/cv/video_multi_object_tracking/utils/visualization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/aggregate.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/aggregate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/cbam.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/cbam.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/eval_network.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/eval_network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/inference_core.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/inference_core.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/inference_memory_bank.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/mod_resnet.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/mod_resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/model.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/modules.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/modules.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_object_segmentation/network.py` & `modelscope-1.6.0/modelscope/models/cv/video_object_segmentation/network.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_update_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/kernel_updator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/mask.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/head/track_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/neck/fpn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/video_k_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_panoptic_segmentation/visualizer.py` & `modelscope-1.6.0/modelscope/models/cv/video_panoptic_segmentation/visualizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/config/ostrack.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/config/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/attn_blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/head.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/procontext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/models/procontext/vit_ce.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/tracker/ostrack.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/tracker/procontext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_single_object_tracking/utils/utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_single_object_tracking/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/DUT_raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/MotionPro.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/MotionPro.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/corr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/extractor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/raft.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/RAFT/update.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/Smoother.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/Smoother.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/config.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/rf_det_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUT/rf_det_so.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/DUTRAFTStabilizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/IterativeSmooth.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/MedianFilter.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/MedianFilter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/ProjectionUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/RAFTUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/WarpUtils.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/WarpUtils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/image_utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/image_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_stabilization/utils/math_utils.py` & `modelscope-1.6.0/modelscope/models/cv/video_stabilization/utils/math_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py` & `modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py` & `modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/longshortnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py` & `modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py` & `modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py` & `modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py` & `modelscope-1.6.0/modelscope/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_summarization/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_summarization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_summarization/base_model.py` & `modelscope-1.6.0/modelscope/models/cv/video_summarization/base_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_summarization/kts/cpd_auto.py` & `modelscope-1.6.0/modelscope/models/cv/video_summarization/kts/cpd_auto.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py` & `modelscope-1.6.0/modelscope/models/cv/video_summarization/kts/cpd_nonlin.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_summarization/pgl_sum.py` & `modelscope-1.6.0/modelscope/models/cv/video_summarization/pgl_sum.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_summarization/summarizer.py` & `modelscope-1.6.0/modelscope/models/cv/video_summarization/summarizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_super_resolution/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/video_super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_super_resolution/basicvsr_net.py` & `modelscope-1.6.0/modelscope/models/cv/video_super_resolution/basicvsr_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_super_resolution/common.py` & `modelscope-1.6.0/modelscope/models/cv/video_super_resolution/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py` & `modelscope-1.6.0/modelscope/models/cv/video_super_resolution/msrresnet_lite_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py` & `modelscope-1.6.0/modelscope/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py` & `modelscope-1.6.0/modelscope/models/cv/video_super_resolution/real_basicvsr_net.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vidt/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/vidt/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vidt/deformable_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/vidt/deformable_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vidt/fpn_fusion.py` & `modelscope-1.6.0/modelscope/models/cv/vidt/fpn_fusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vidt/head.py` & `modelscope-1.6.0/modelscope/models/cv/vidt/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vidt/model.py` & `modelscope-1.6.0/modelscope/models/cv/vidt/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/virual_tryon/sdafnet.py` & `modelscope-1.6.0/modelscope/models/cv/virual_tryon/sdafnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/head.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/model.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/petl.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/petl.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/timm_helpers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/timm_vision_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/timm_weight_init.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py` & `modelscope-1.6.0/modelscope/models/cv/vision_efficient_tuning/vision_efficient_tuning.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_middleware/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/vision_middleware/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_middleware/head.py` & `modelscope-1.6.0/modelscope/models/cv/vision_middleware/head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_middleware/model.py` & `modelscope-1.6.0/modelscope/models/cv/vision_middleware/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vision_middleware/vim.py` & `modelscope-1.6.0/modelscope/models/cv/vision_middleware/vim.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vop_retrieval/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/vop_retrieval/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vop_retrieval/backbone.py` & `modelscope-1.6.0/modelscope/models/cv/vop_retrieval/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vop_retrieval/basic_utils.py` & `modelscope-1.6.0/modelscope/models/cv/vop_retrieval/basic_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vop_retrieval/model.py` & `modelscope-1.6.0/modelscope/models/cv/vop_retrieval/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vop_retrieval/model_se.py` & `modelscope-1.6.0/modelscope/models/cv/vop_retrieval/model_se.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/cv/vop_retrieval/tokenization_clip.py` & `modelscope-1.6.0/modelscope/models/cv/vop_retrieval/tokenization_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -16,14 +16,16 @@
     from .ofa_for_text_to_image_synthesis_model import \
         OfaForTextToImageSynthesis
     from .multi_stage_diffusion import \
         MultiStageDiffusionForTextToImageSynthesis
     from .vldoc import VLDocForDocVLEmbedding
     from .video_synthesis import TextToVideoSynthesis
     from .efficient_diffusion_tuning import EfficientStableDiffusion
+    from .mplug_owl import MplugOwlForConditionalGeneration
+    from .clip_interrogator import CLIP_Interrogator
 
 else:
     _import_structure = {
         'clip': ['CLIPForMultiModalEmbedding'],
         'diffusion': ['DiffusionForTextToImageSynthesis'],
         'gemm': ['GEMMForMultiModalEmbedding'],
         'rleg': ['RLEGForMultiModalEmbedding'],
@@ -33,15 +35,17 @@
         'ofa_for_all_tasks': ['OfaForAllTasks'],
         'ofa_for_text_to_image_synthesis_model':
         ['OfaForTextToImageSynthesis'],
         'multi_stage_diffusion':
         ['MultiStageDiffusionForTextToImageSynthesis'],
         'vldoc': ['VLDocForDocVLEmbedding'],
         'video_synthesis': ['TextToVideoSynthesis'],
-        'efficient_diffusion_tuning': ['EfficientStableDiffusion']
+        'efficient_diffusion_tuning': ['EfficientStableDiffusion'],
+        'mplug_owl': ['MplugOwlForConditionalGeneration'],
+        'clip_interrogator': ['CLIP_Interrogator'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/clip/bert_tokenizer.py` & `modelscope-1.6.0/modelscope/models/multi_modal/clip/bert_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/clip/configuration_bert.py` & `modelscope-1.6.0/modelscope/models/multi_modal/clip/configuration_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/clip/model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/clip/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/clip/modeling_bert.py` & `modelscope-1.6.0/modelscope/models/multi_modal/clip/modeling_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/diffusion.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/structbert.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/structbert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/tokenizer.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/unet_generator.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/unet_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py` & `modelscope-1.6.0/modelscope/models/multi_modal/diffusion/unet_upsampler_256.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/dpm_solver_pytorch.py` & `modelscope-1.6.0/modelscope/models/multi_modal/dpm_solver_pytorch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/efficient_diffusion_tuning/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py` & `modelscope-1.6.0/modelscope/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/gemm/gemm_base.py` & `modelscope-1.6.0/modelscope/models/multi_modal/gemm/gemm_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/gemm/gemm_model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/gemm/gemm_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/gemm/tokenizer.py` & `modelscope-1.6.0/modelscope/models/multi_modal/gemm/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py` & `modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/respace.py` & `modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/respace.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/script.py` & `modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/script.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/guided_diffusion/unet.py` & `modelscope-1.6.0/modelscope/models/multi_modal/guided_diffusion/unet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mgeo/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mgeo/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mgeo/backbone.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mgeo/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mgeo/text_classification.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mgeo/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mgeo/text_ranking.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mgeo/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mgeo/token_classification.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mgeo/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/dataloaders/rawvideo_util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py`

 * *Files 0% similar despite different names*

```diff
@@ -124,21 +124,21 @@
 
     def _get_rawvideo_dec(self,
                           video_path,
                           rawVideoExtractor,
                           local_transform,
                           s=None,
                           e=None):
-        video_mask = np.zeros(self.max_frames, dtype=np.long)
+        video_mask = np.zeros(self.max_frames, dtype=int)
         max_video_length = 0
 
         # T x 3 x H x W
         video = np.zeros((self.max_frames, 3, rawVideoExtractor.size,
                           rawVideoExtractor.size),
-                         dtype=np.float)
+                         dtype=float)
 
         if s is None:
             start_time, end_time = None, None
         else:
             start_time = int(s)
             end_time = int(e)
             start_time = start_time if start_time >= 0. else 0.
```

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/dynamic_inverted_softmax.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/modeling.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/modeling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/module_clip.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/module_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/module_cross.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/module_cross.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/tokenization_clip.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/tokenization_clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mmr/models/until_module.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mmr/models/until_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug/clip/clip.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug/clip/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug/configuration_mplug.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug/configuration_mplug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug/modeling_mplug.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug/modeling_mplug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug/mvit.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug/mvit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug/predictor.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug/predictor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/mplug_for_all_tasks.py` & `modelscope-1.6.0/modelscope/models/multi_modal/mplug_for_all_tasks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/clip.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/decoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/prior.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/prior.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/upsampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py` & `modelscope-1.6.0/modelscope/models/multi_modal/multi_stage_diffusion/xglm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/configuration_mmspeech.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/configuration_mmspeech.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/configuration_ofa.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/configuration_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/incremental_decoding_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/multihead_attention.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/multihead_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/ngram_repeat_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/search.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/search.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/sequence_generator.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/sequence_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/token_generation_constraints.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/generate/utils.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/generate/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/modeling_mmspeech.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/modeling_mmspeech.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/modeling_ofa.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/modeling_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/resnet.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/resnet.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/tokenization_ofa.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/tokenization_ofa.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/tokenization_ofa_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/utils/constant.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/utils/utils.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa/vit.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa/vit.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa_for_all_tasks.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa_for_all_tasks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/rleg/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/rleg/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/rleg/model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/rleg/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/rleg/rleg.py` & `modelscope-1.6.0/modelscope/models/multi_modal/rleg/rleg.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/blocks.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/blocks.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/clip.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/clip.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/swin_transformer.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/swin_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/tokenizer.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/soonet/utils.py` & `modelscope-1.6.0/modelscope/models/multi_modal/soonet/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/team/team_model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/team/team_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/team/utils.py` & `modelscope-1.6.0/modelscope/models/multi_modal/team/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/__init__.py` & `modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/autoencoder.py` & `modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/autoencoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/diffusion.py` & `modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/video_synthesis/unet_sd.py` & `modelscope-1.6.0/modelscope/models/multi_modal/video_synthesis/unet_sd.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/conv_fpn_trans.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/convnext.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/model.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/modeling_layout_roberta.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/processing.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/processing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/tokenization.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/multi_modal/vldoc/transformer_local.py` & `modelscope-1.6.0/modelscope/models/multi_modal/vldoc/transformer_local.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/T5/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/T5/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/T5/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/T5/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/T5/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/T5/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/T5/text2text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/T5/text2text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bart/text_error_correction.py` & `modelscope-1.6.0/modelscope/models/nlp/bart/text_error_correction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/document_segmentation.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/document_segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/sentence_embedding.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/sentence_embedding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/siamese_uie.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/siamese_uie.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/text_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/text_ranking.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/token_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/bert/word_alignment.py` & `modelscope-1.6.0/modelscope/models/nlp/bert/word_alignment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/canmt/canmt_model.py` & `modelscope-1.6.0/modelscope/models/nlp/canmt/canmt_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/canmt/canmt_translation.py` & `modelscope-1.6.0/modelscope/models/nlp/canmt/canmt_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/canmt/sequence_generator.py` & `modelscope-1.6.0/modelscope/models/nlp/canmt/sequence_generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/codegeex/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/codegeex/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/codegeex/codegeex.py` & `modelscope-1.6.0/modelscope/models/nlp/codegeex/codegeex.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/codegeex/codegeex_for_code_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py` & `modelscope-1.6.0/modelscope/models/nlp/codegeex/codegeex_for_code_translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/codegeex/inference.py` & `modelscope-1.6.0/modelscope/models/nlp/codegeex/inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/codegeex/tokenizer.py` & `modelscope-1.6.0/modelscope/models/nlp/codegeex/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/csanmt/translation.py` & `modelscope-1.6.0/modelscope/models/nlp/csanmt/translation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/deberta_v2/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/deberta_v2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/deberta_v2/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/deberta_v2/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/deberta_v2/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/deberta_v2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/deberta_v2/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/deberta_v2/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/deberta_v2/tokenization.py` & `modelscope-1.6.0/modelscope/models/nlp/deberta_v2/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/deberta_v2/tokenization_fast.py` & `modelscope-1.6.0/modelscope/models/nlp/deberta_v2/tokenization_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/dgds/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/dgds/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/dgds/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/dgds/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py` & `modelscope-1.6.0/modelscope/models/nlp/dgds/document_grounded_dialog_generate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py` & `modelscope-1.6.0/modelscope/models/nlp/dgds/document_grounded_dialog_rerank.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py` & `modelscope-1.6.0/modelscope/models/nlp/dgds/document_grounded_dialog_retrieval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/fid_T5/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/fid_T5/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/fid_T5/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/fid_T5/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/fid_plug/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/fid_plug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/fid_plug/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/fid_plug/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/fid_plug/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/fid_plug/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/fid_plug/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/fid_plug/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/generation/strategies.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/generation/strategies.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/initialize.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/initialize.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/kernels/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/kernels/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/functional.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/functional.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/quantization/layers.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/quantization/layers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/glm_130b/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/glm_130b/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt3/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt3/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt3/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt3/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt3/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt3/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt3/distributed_gpt3.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt3/distributed_gpt3.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt3/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt3/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt3/tokenizer.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt3/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/checkpointing.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/checkpointing.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/distributed_gpt_moe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/experts.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/experts.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/layer.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/layer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/mappings.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/mappings.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/sharded_moe.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/moe/utils.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/moe/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_moe/tokenizer.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_moe/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/gpt_neo/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/gpt_neo/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/crf_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/crf_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/fill_mask_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/fill_mask_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/infromation_extraction_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/infromation_extraction_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/text_classification_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/text_classification_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/text_generation_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/text_generation_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/text_ranking_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/text_ranking_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/token_classification_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/token_classification_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/heads/torch_pretrain_head.py` & `modelscope-1.6.0/modelscope/models/nlp/heads/torch_pretrain_head.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/hf_transformers/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/hf_transformers/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/convert_llama_weights_to_hf.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/tokenization.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/llama/tokenization_fast.py` & `modelscope-1.6.0/modelscope/models/nlp/llama/tokenization_fast.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/lstm/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/lstm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/lstm/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/lstm/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/lstm/token_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/lstm/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/megatron_bert/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/megatron_bert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/megatron_bert/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/megatron_bert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/megatron_bert/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/megatron_bert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/megatron_bert/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/megatron_bert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/arguments.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/arguments.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/blocklm_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/blocklm_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -208,18 +208,18 @@
     def make_masked_data(self,
                          tokens,
                          loss_masks,
                          attention_mask,
                          block_spans,
                          rng,
                          task='bert'):
-        position_ids = np.arange(len(tokens), dtype=np.long)
+        position_ids = np.arange(len(tokens), dtype=int)
         targets = copy.deepcopy(tokens)
         mask_id = self.tokenizer.get_command('MASK').Id
-        mlm_masks = np.zeros(len(tokens), dtype=np.long)
+        mlm_masks = np.zeros(len(tokens), dtype=int)
         for start, end in block_spans:
             for idx in range(start, end):
                 tokens[idx] = mask_id
             mlm_masks[start:end] = 1
         loss_masks = loss_masks * mlm_masks
         return tokens, targets, loss_masks, position_ids
 
@@ -227,15 +227,15 @@
                         tokens,
                         loss_masks,
                         attention_mask,
                         block_spans,
                         rng,
                         task='bert'):
         text_length = len(tokens)
-        position_ids = np.ones(len(tokens), dtype=np.long)
+        position_ids = np.ones(len(tokens), dtype=int)
         for start, end in block_spans:
             position_ids[start + 1:end] = 0
         position_ids = np.cumsum(position_ids) - 1
         if self.random_position and position_ids[-1] < self.max_seq_length - 1:
             position_bias = self.max_seq_length - position_ids[-1]
             position_bias = rng.randrange(0, position_bias)
             position_ids = position_ids + position_bias
@@ -266,15 +266,15 @@
                 target_position_ids.append(target_position_id)
                 target_position_ids.append([target_position_id[0]])
             else:
                 target_position_ids.append([self.max_seq_length] *  # noqa
                                            (end - start + 1))
             if self.block_position_encoding:
                 target_block_position_ids.append(
-                    np.arange(1, end - start + 2, dtype=np.long))
+                    np.arange(1, end - start + 2, dtype=int))
             else:
                 target_block_position_ids.append([1] * (end - start + 1))
         block_spans.sort(key=lambda x: x[0])
         source_tokens, source_position_ids, local_spans = [], [], []
         last, current_length = 0, 0
         for start, end, idx in block_spans:
             if task == 'generation':
@@ -303,15 +303,15 @@
                 target_tokens).tolist():
             print('Found EOS in target', self.tokenizer.DecodeIds(tokens))
             raise RuntimeError
         if self.encoder_decoder:
             target_tokens = target_tokens + [
                 self.tokenizer.get_command('eop').Id
             ]
-            loss_masks = np.ones(len(target_tokens), dtype=np.long)
+            loss_masks = np.ones(len(target_tokens), dtype=int)
             return source_tokens, target_tokens, loss_masks
         else:
             tokens = np.concatenate(source_tokens + target_tokens)
             if task == 'bert' and self.context_mask_ratio > 0:
                 mask_candidates = set()
                 for start, end in local_spans:
                     if start != 0:
@@ -322,20 +322,20 @@
                         mask_candidates.update(range(local_start, end))
                 mask_pos = rng.sample(
                     mask_candidates,
                     int(self.context_mask_ratio * text_length))
                 for pos in mask_pos:
                     tokens[pos] = self.tokenizer.get_command('dBLOCK').Id
             targets = np.concatenate(source_tokens + targets)
-            loss_masks = np.ones(len(tokens), dtype=np.long)
+            loss_masks = np.ones(len(tokens), dtype=int)
             loss_masks[:source_length] = 0
             position_ids = np.concatenate(source_position_ids
                                           + target_position_ids)
             block_position_ids = np.concatenate(
-                [np.zeros(source_length, dtype=np.long)]
+                [np.zeros(source_length, dtype=int)]
                 + target_block_position_ids)
             position_ids = np.stack([position_ids, block_position_ids], axis=0)
             if attention_mask is not None:
                 return tokens, targets, loss_masks, position_ids
             else:
                 return tokens, targets, loss_masks, position_ids, source_length
 
@@ -535,30 +535,29 @@
                         self.generation_mask,
                         self.tokenizer.get_command('sop').Id
                     ], target_tokens[:-1]))
                     targets = np.concatenate(
                         (source_tokens, [self.generation_mask], target_tokens))
                     loss_masks = np.concatenate(
                         (np.zeros(len(source_tokens) + 1,
-                                  dtype=np.long), target_masks))
+                                  dtype=int), target_masks))
                     token_batch.append(tokens)
                     target_batch.append(targets)
                     loss_mask_batch.append(loss_masks)
                     position_ids = np.arange(
-                        len(source_tokens) + len(target_tokens) + 1,
-                        dtype=np.long)
+                        len(source_tokens) + len(target_tokens) + 1, dtype=int)
                     position_ids[len(source_tokens) + 1:] = len(source_tokens)
                     if self.block_position_encoding:
                         block_position_ids = np.concatenate(
-                            (np.zeros(len(source_tokens), dtype=np.long),
-                             np.arange(len(target_tokens) + 1, dtype=np.long)))
+                            (np.zeros(len(source_tokens), dtype=int),
+                             np.arange(len(target_tokens) + 1, dtype=int)))
                     else:
                         block_position_ids = np.concatenate(
-                            (np.zeros(len(source_tokens) + 1, dtype=np.long),
-                             np.ones(len(target_tokens) + 1, dtype=np.long)))
+                            (np.zeros(len(source_tokens) + 1, dtype=int),
+                             np.ones(len(target_tokens) + 1, dtype=int)))
                     position_id_batch.append(
                         np.stack([position_ids, block_position_ids], axis=0))
                 else:
                     tokens, targets, loss_masks, position_ids = self.generate_blank_data(
                         sample, [generation_length],
                         attention_mask[-1],
                         rng,
@@ -593,31 +592,29 @@
     def pad_batch(token_batch, target_batch, loss_mask_batch,
                   position_id_batch):
         seq_lengths = list(map(len, token_batch))
         if seq_lengths.count(seq_lengths[0]) != len(seq_lengths):
             max_length = max(seq_lengths)
             token_batch = [
                 np.concatenate(
-                    (tokens, np.zeros(max_length - len(tokens),
-                                      dtype=np.long)))
+                    (tokens, np.zeros(max_length - len(tokens), dtype=int)))
                 for tokens in token_batch
             ]
             target_batch = [
                 np.concatenate(
-                    (targets,
-                     np.zeros(max_length - len(targets), dtype=np.long)))
+                    (targets, np.zeros(max_length - len(targets), dtype=int)))
                 for targets in target_batch
             ]
             loss_mask_batch = [
                 np.concatenate(
                     (loss_masks,
-                     np.zeros(max_length - len(loss_masks), dtype=np.long)))
+                     np.zeros(max_length - len(loss_masks), dtype=int)))
                 for loss_masks in loss_mask_batch
             ]
             position_id_batch = [
-                np.concatenate((position_ids,
-                                np.zeros(
-                                    (2, max_length - position_ids.shape[1]),
-                                    dtype=np.long)),
-                               axis=1) for position_ids in position_id_batch
+                np.concatenate(
+                    (position_ids,
+                     np.zeros(
+                         (2, max_length - position_ids.shape[1]), dtype=int)),
+                    axis=1) for position_ids in position_id_batch
             ]
         return token_batch, target_batch, loss_mask_batch, position_id_batch
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/configure_data.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/configure_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/corpora.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/corpora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/datasets.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/datasets.py`

 * *Files 0% similar despite different names*

```diff
@@ -579,16 +579,16 @@
             'loss_mask': np.array(loss_mask),
             'attention_mask': np.array(attention_mask)
         }
 
     def getidx(self, idx):
         tokens, targets, loss_masks = [], [], []
         attention_mask = np.concatenate(
-            (np.zeros((self.max_seq_len, self.mem_len), dtype=np.long),
-             np.ones((self.max_seq_len, self.max_seq_len), dtype=np.long)),
+            (np.zeros((self.max_seq_len, self.mem_len), dtype=int),
+             np.ones((self.max_seq_len, self.max_seq_len), dtype=int)),
             axis=1)
         sample_idx = bisect_right(self.indices, idx * self.max_seq_len)
         last_end = 0 if sample_idx == 0 else self.indices[sample_idx - 1]
         token_offset = idx * self.max_seq_len - last_end
         if token_offset != 0:
             history = min(self.mem_len, token_offset)
             attention_mask[:,
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/extraction.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/file_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/lazy_loader.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/lazy_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/samplers.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/samplers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/sp_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/tokenization.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/tokenization_gpt2.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/data_utils/wordpiece.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/data_utils/wordpiece.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/generation_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/generation_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/mglm_for_text_summarization.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/mglm_for_text_summarization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/distributed.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/downstream.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/downstream.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/modeling_bert.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/modeling_bert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/modeling_glm.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/modeling_glm.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/prompt.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/prompt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/model/transformer.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/model/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/process_grid.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/process_grid.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/test/test_block.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/test/test_block.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,13 +24,13 @@
     args.eod_token = 0
 
     strategy = ConstructBlockStrategy(
         args, None, bert_ratio=0.4, max_seq_length=128)
     counts = np.array([0] * 10)
     for _ in range(10000):
         spans = strategy.sample_span_in_document(
-            np.array([1, 2, 3, 0, 4, 5, 6, 7, 9, 0], dtype=np.long), [1, 1],
+            np.array([1, 2, 3, 0, 4, 5, 6, 7, 9, 0], dtype=int), [1, 1],
             random.Random())
         for start, end in spans:
             counts[start:end] += 1
 
     print(counts)
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/test/test_rel_shift.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/test/test_rel_shift.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,15 @@
     lr_scheduler = AnnealingLR(
         optimizer,
         start_lr=0.00015,
         warmup_iter=3000,
         num_iters=300000,
         decay_style='cosine',
         decay_ratio=0.1)
-    steps = np.arange(0, 400000, 10, dtype=np.long)
+    steps = np.arange(0, 400000, 10, dtype=int)
     rates = []
     for step in steps:
         lr_scheduler.num_iters = step
         rates.append(lr_scheduler.get_lr())
     print(rates)
     plt.plot(steps, rates)
     plt.savefig('lr.pdf', format='pdf')
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/train_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/train_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/mglm/utils.py` & `modelscope-1.6.0/modelscope/models/nlp/mglm/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/palm_v2/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/palm_v2/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/palm_v2/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/palm_v2/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/palm_v2/dureader_eval.py` & `modelscope-1.6.0/modelscope/models/nlp/palm_v2/dureader_eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/palm_v2/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/palm_v2/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/peer/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/peer/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/peer/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/peer/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/peer/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/peer/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/peer/sas_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/peer/sas_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/peer/text_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/peer/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug/AnnealingLR.py` & `modelscope-1.6.0/modelscope/models/nlp/plug/AnnealingLR.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/plug/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/plug/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/plug/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug/distributed_plug.py` & `modelscope-1.6.0/modelscope/models/nlp/plug/distributed_plug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug/generator.py` & `modelscope-1.6.0/modelscope/models/nlp/plug/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug_mental/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/plug_mental/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug_mental/adv_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/plug_mental/adv_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug_mental/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/plug_mental/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug_mental/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/plug_mental/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/plug_mental/text_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/plug_mental/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/ponet/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/ponet/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/ponet/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/ponet/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/ponet/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/ponet/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/ponet/document_segmentation.py` & `modelscope-1.6.0/modelscope/models/nlp/ponet/document_segmentation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/ponet/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/ponet/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/ponet/tokenization.py` & `modelscope-1.6.0/modelscope/models/nlp/ponet/tokenization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/space/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/space/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/dialog_intent_prediction.py` & `modelscope-1.6.0/modelscope/models/nlp/space/dialog_intent_prediction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/dialog_modeling.py` & `modelscope-1.6.0/modelscope/models/nlp/space/dialog_modeling.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/dialog_state_tracking.py` & `modelscope-1.6.0/modelscope/models/nlp/space/dialog_state_tracking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/model/gen_unified_transformer.py` & `modelscope-1.6.0/modelscope/models/nlp/space/model/gen_unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/model/generator.py` & `modelscope-1.6.0/modelscope/models/nlp/space/model/generator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/model/intent_unified_transformer.py` & `modelscope-1.6.0/modelscope/models/nlp/space/model/intent_unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/model/model_base.py` & `modelscope-1.6.0/modelscope/models/nlp/space/model/model_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/model/tokenization_space.py` & `modelscope-1.6.0/modelscope/models/nlp/space/model/tokenization_space.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/model/unified_transformer.py` & `modelscope-1.6.0/modelscope/models/nlp/space/model/unified_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/modules/embedder.py` & `modelscope-1.6.0/modelscope/models/nlp/space/modules/embedder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/modules/feedforward.py` & `modelscope-1.6.0/modelscope/models/nlp/space/modules/feedforward.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/modules/functions.py` & `modelscope-1.6.0/modelscope/models/nlp/space/modules/functions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/modules/multihead_attention.py` & `modelscope-1.6.0/modelscope/models/nlp/space/modules/multihead_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space/modules/transformer_block.py` & `modelscope-1.6.0/modelscope/models/nlp/space/modules/transformer_block.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space_T_cn/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/space_T_cn/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space_T_cn/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/space_T_cn/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space_T_cn/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/space_T_cn/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space_T_cn/table_question_answering.py` & `modelscope-1.6.0/modelscope/models/nlp/space_T_cn/table_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/space_T_en/text_to_sql.py` & `modelscope-1.6.0/modelscope/models/nlp/space_T_en/text_to_sql.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/adv_utils.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/adv_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/faq_question_answering.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/faq_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/text_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/structbert/token_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/structbert/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/feature_extraction.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/feature_extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/information_extraction.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/information_extraction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/task_model.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/task_model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/text_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/text_generation.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/text_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/text_ranking.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/text_ranking.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/task_models/token_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/task_models/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/unite/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/use/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,17 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .configuration_unite import UniTEConfig
-    from .modeling_unite import UniTEForTranslationEvaluation
+    from .user_satisfaction_estimation import UserSatisfactionEstimation
 else:
     _import_structure = {
-        'configuration_unite': ['UniTEConfig'],
-        'modeling_unite': ['UniTEForTranslationEvaluation'],
+        'user_satisfaction_estimation': ['UserSatisfactionEstimation']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/unite/modeling_unite.py` & `modelscope-1.6.0/modelscope/models/nlp/unite/translation_evaluation.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,14 +16,16 @@
 from torch.nn.utils.rnn import pad_sequence
 from transformers import XLMRobertaConfig, XLMRobertaModel
 from transformers.activations import ACT2FN
 
 from modelscope.metainfo import Models
 from modelscope.models.base import TorchModel
 from modelscope.models.builder import MODELS
+from modelscope.models.nlp.unite.configuration import InputFormat
+from modelscope.outputs.nlp_outputs import TranslationEvaluationOutput
 from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['UniTEForTranslationEvaluation']
 
@@ -67,16 +69,24 @@
 
     def forward(
         self,
         tensors: List[torch.Tensor],  # pylint: disable=arguments-differ
         mask: torch.Tensor = None,
     ) -> torch.Tensor:
         tensors = torch.cat(list(x.unsqueeze(dim=0) for x in tensors), dim=0)
-        normed_weights = softmax(
-            self.scalar_parameters, dim=0).view(-1, 1, 1, 1)
+
+        if self.training and self.dropout:
+            normed_weights = softmax(
+                torch.where(self.dropout_mask.uniform_() > self.dropout,
+                            self.scalar_parameters, self.dropout_fill),
+                dim=-1)
+        else:
+            normed_weights = softmax(self.scalar_parameters, dim=-1)
+
+        normed_weights = normed_weights.view(-1, 1, 1, 1)
 
         mask_float = mask.float()
         weighted_sum = (normed_weights
                         * _layer_norm_all(tensors, mask_float)).sum(dim=0)
         weighted_sum = weighted_sum[:, 0, :]
 
         return self.gamma * weighted_sum
@@ -93,26 +103,26 @@
         final_activation: Optional[str] = None,
         dropout: float = 0.1,
     ) -> None:
         """
         Feed Forward Neural Network.
 
         Args:
-        in_dim (:obj:`int`):
-            Number of input features.
-        out_dim (:obj:`int`, defaults to 1):
-            Number of output features. Default is 1 -- a single scalar.
-        hidden_sizes (:obj:`List[int]`, defaults to `[3072, 768]`):
-            List with hidden layer sizes.
-        activations (:obj:`str`, defaults to `Sigmoid`):
-            Name of the activation function to be used in the hidden layers.
-        final_activation (:obj:`str`, Optional, defaults to `None`):
-            Name of the final activation function if any.
-        dropout (:obj:`float`, defaults to 0.1):
-            Dropout ratio to be used in the hidden layers.
+            in_dim (:obj:`int`):
+                Number of input features.
+            out_dim (:obj:`int`, defaults to 1):
+                Number of output features. Default is 1 -- a single scalar.
+            hidden_sizes (:obj:`List[int]`, defaults to `[3072, 768]`):
+                List with hidden layer sizes.
+            activations (:obj:`str`, defaults to `Sigmoid`):
+                Name of the activation function to be used in the hidden layers.
+            final_activation (:obj:`str`, Optional, defaults to `None`):
+                Name of the final activation function if any.
+            dropout (:obj:`float`, defaults to 0.1):
+                Dropout ratio to be used in the hidden layers.
         """
         super().__init__()
         modules = []
         modules.append(Linear(in_dim, hidden_sizes[0]))
         modules.append(self.build_activation(activations))
         modules.append(Dropout(dropout))
 
@@ -262,139 +272,155 @@
             hidden_sizes=self.mlp_hidden_sizes,
             activations=self.mlp_act,
             final_activation=self.mlp_final_act,
             dropout=self.mlp_dropout)
 
         return
 
-    def forward(self, input_sentences: List[torch.Tensor]):
-        input_ids = self.combine_input_sentences(input_sentences)
+    def forward(self,
+                input_ids: torch.Tensor,
+                input_format: Optional[List[InputFormat]] = None,
+                score: Optional[torch.Tensor] = None,
+                **kwargs) -> TranslationEvaluationOutput:
         attention_mask = input_ids.ne(self.pad_token_id).long()
         outputs = self.encoder(
             input_ids=input_ids,
             attention_mask=attention_mask,
             output_hidden_states=True,
             return_dict=True)
         mix_states = self.layerwise_attention(outputs['hidden_states'],
                                               attention_mask)
-        pred = self.estimator(mix_states)
-        return pred.squeeze(dim=-1)
-
-    def load_checkpoint(self, path: str, device: torch.device):
-        state_dict = torch.load(path, map_location=device)
-        self.load_state_dict(state_dict)
+        pred = self.estimator(mix_states).squeeze(dim=-1)
+        output = TranslationEvaluationOutput(
+            score=pred.cpu().tolist(), input_format=input_format)
+
+        if score is not None:
+            loss = (pred - score).pow(2).mean()
+            output['loss'] = loss
+
+        return output
+
+    def load_checkpoint(self, path: str, device: torch.device, plm_only: bool):
+        if plm_only:
+            self.encoder = self.encoder.from_pretrained(path).to(device)
+            self.encoder.pooler = None
+        else:
+            state_dict = torch.load(path, map_location=device)
+            self.load_state_dict(state_dict)
         logger.info('Loading checkpoint parameters from %s' % path)
         return
 
-    def combine_input_sentences(self, input_sent_groups: List[torch.Tensor]):
-        for input_sent_group in input_sent_groups[1:]:
-            input_sent_group[:, 0] = self.eos_token_id
 
-        if len(input_sent_groups) == 3:
-            cutted_sents = self.cut_long_sequences3(input_sent_groups)
-        else:
-            cutted_sents = self.cut_long_sequences2(input_sent_groups)
-        return cutted_sents
-
-    @staticmethod
-    def cut_long_sequences2(all_input_concat: List[List[torch.Tensor]],
+def combine_input_sentences(all_input_concat: List[List[torch.Tensor]],
                             maximum_length: int = 512,
-                            pad_idx: int = 1):
-        all_input_concat = list(zip(*all_input_concat))
-        collected_tuples = list()
-        for tensor_tuple in all_input_concat:
-            all_lens = tuple(len(x) for x in tensor_tuple)
-
-            if sum(all_lens) > maximum_length:
-                lengths = dict(enumerate(all_lens))
-                lengths_sorted_idxes = list(x[0] for x in sorted(
-                    lengths.items(), key=lambda d: d[1], reverse=True))
-
-                offset = ceil((sum(lengths.values()) - maximum_length) / 2)
-
-                if min(all_lens) > (maximum_length
-                                    // 2) and min(all_lens) > offset:
-                    lengths = dict((k, v - offset) for k, v in lengths.items())
-                else:
-                    lengths[lengths_sorted_idxes[
-                        0]] = maximum_length - lengths[lengths_sorted_idxes[1]]
-
-                new_lens = list(lengths[k]
-                                for k in range(0, len(tensor_tuple)))
-                new_tensor_tuple = tuple(
-                    x[:y] for x, y in zip(tensor_tuple, new_lens))
-                for x, y in zip(new_tensor_tuple, tensor_tuple):
-                    x[-1] = y[-1]
-                collected_tuples.append(new_tensor_tuple)
+                            pad_idx: int = 1,
+                            eos_idx: int = 2):
+    for group in all_input_concat[1:]:
+        group[:, 0] = eos_idx
+
+    if len(all_input_concat) == 3:
+        return cut_long_sequences3(all_input_concat, maximum_length, pad_idx)
+    else:
+        return cut_long_sequences2(all_input_concat, maximum_length, pad_idx)
+
+
+def cut_long_sequences2(all_input_concat: List[List[torch.Tensor]],
+                        maximum_length: int = 512,
+                        pad_idx: int = 1):
+    all_input_concat = list(zip(*all_input_concat))
+    collected_tuples = list()
+    for tensor_tuple in all_input_concat:
+        tensor_tuple = tuple(
+            x.masked_select(x.ne(pad_idx)) for x in tensor_tuple)
+        all_lens = tuple(len(x) for x in tensor_tuple)
+
+        if sum(all_lens) > maximum_length:
+            lengths = dict(enumerate(all_lens))
+            lengths_sorted_idxes = list(x[0] for x in sorted(
+                lengths.items(), key=lambda d: d[1], reverse=True))
+
+            offset = ceil((sum(lengths.values()) - maximum_length) / 2)
+
+            if min(all_lens) > (maximum_length
+                                // 2) and min(all_lens) > offset:
+                lengths = dict((k, v - offset) for k, v in lengths.items())
             else:
-                collected_tuples.append(tensor_tuple)
-
-        concat_tensor = list(torch.cat(x, dim=0) for x in collected_tuples)
-        all_input_concat_padded = pad_sequence(
-            concat_tensor, batch_first=True, padding_value=pad_idx)
+                lengths[lengths_sorted_idxes[0]] = maximum_length - lengths[
+                    lengths_sorted_idxes[1]]
 
-        return all_input_concat_padded
+            new_lens = list(lengths[k] for k in range(0, len(tensor_tuple)))
+            new_tensor_tuple = tuple(x[:y]
+                                     for x, y in zip(tensor_tuple, new_lens))
+            for x, y in zip(new_tensor_tuple, tensor_tuple):
+                x[-1] = y[-1]
+            collected_tuples.append(new_tensor_tuple)
+        else:
+            collected_tuples.append(tensor_tuple)
 
-    @staticmethod
-    def cut_long_sequences3(all_input_concat: List[List[torch.Tensor]],
-                            maximum_length: int = 512,
-                            pad_idx: int = 1):
-        all_input_concat = list(zip(*all_input_concat))
-        collected_tuples = list()
-        for tensor_tuple in all_input_concat:
-            all_lens = tuple(len(x) for x in tensor_tuple)
-
-            if sum(all_lens) > maximum_length:
-                lengths = dict(enumerate(all_lens))
-                lengths_sorted_idxes = list(x[0] for x in sorted(
-                    lengths.items(), key=lambda d: d[1], reverse=True))
-
-                offset = ceil((sum(lengths.values()) - maximum_length) / 3)
-
-                if min(all_lens) > (maximum_length
-                                    // 3) and min(all_lens) > offset:
-                    lengths = dict((k, v - offset) for k, v in lengths.items())
-                else:
-                    while sum(lengths.values()) > maximum_length:
-                        if lengths[lengths_sorted_idxes[0]] > lengths[
-                                lengths_sorted_idxes[1]]:
-                            offset = maximum_length - lengths[
-                                lengths_sorted_idxes[1]] - lengths[
-                                    lengths_sorted_idxes[2]]
-                            if offset > lengths[lengths_sorted_idxes[1]]:
-                                lengths[lengths_sorted_idxes[0]] = offset
-                            else:
-                                lengths[lengths_sorted_idxes[0]] = lengths[
-                                    lengths_sorted_idxes[1]]
-                        elif lengths[lengths_sorted_idxes[0]] == lengths[
-                                lengths_sorted_idxes[1]] > lengths[
-                                    lengths_sorted_idxes[2]]:
-                            offset = (maximum_length
-                                      - lengths[lengths_sorted_idxes[2]]) // 2
-                            if offset > lengths[lengths_sorted_idxes[2]]:
-                                lengths[lengths_sorted_idxes[0]] = lengths[
-                                    lengths_sorted_idxes[1]] = offset
-                            else:
-                                lengths[lengths_sorted_idxes[0]] = lengths[
-                                    lengths_sorted_idxes[1]] = lengths[
-                                        lengths_sorted_idxes[2]]
+    concat_tensor = list(torch.cat(x, dim=0) for x in collected_tuples)
+    all_input_concat_padded = pad_sequence(
+        concat_tensor, batch_first=True, padding_value=pad_idx)
+    return all_input_concat_padded
+
+
+def cut_long_sequences3(all_input_concat: List[List[torch.Tensor]],
+                        maximum_length: int = 512,
+                        pad_idx: int = 1):
+    all_input_concat = list(zip(*all_input_concat))
+    collected_tuples = list()
+    for tensor_tuple in all_input_concat:
+        tensor_tuple = tuple(
+            x.masked_select(x.ne(pad_idx)) for x in tensor_tuple)
+        all_lens = tuple(len(x) for x in tensor_tuple)
+
+        if sum(all_lens) > maximum_length:
+            lengths = dict(enumerate(all_lens))
+            lengths_sorted_idxes = list(x[0] for x in sorted(
+                lengths.items(), key=lambda d: d[1], reverse=True))
+
+            offset = ceil((sum(lengths.values()) - maximum_length) / 3)
+
+            if min(all_lens) > (maximum_length
+                                // 3) and min(all_lens) > offset:
+                lengths = dict((k, v - offset) for k, v in lengths.items())
+            else:
+                while sum(lengths.values()) > maximum_length:
+                    if lengths[lengths_sorted_idxes[0]] > lengths[
+                            lengths_sorted_idxes[1]]:
+                        offset = maximum_length - lengths[lengths_sorted_idxes[
+                            1]] - lengths[lengths_sorted_idxes[2]]
+                        if offset > lengths[lengths_sorted_idxes[1]]:
+                            lengths[lengths_sorted_idxes[0]] = offset
+                        else:
+                            lengths[lengths_sorted_idxes[0]] = lengths[
+                                lengths_sorted_idxes[1]]
+                    elif lengths[lengths_sorted_idxes[0]] == lengths[
+                            lengths_sorted_idxes[1]] > lengths[
+                                lengths_sorted_idxes[2]]:
+                        offset = (maximum_length
+                                  - lengths[lengths_sorted_idxes[2]]) // 2
+                        if offset > lengths[lengths_sorted_idxes[2]]:
+                            lengths[lengths_sorted_idxes[0]] = lengths[
+                                lengths_sorted_idxes[1]] = offset
                         else:
                             lengths[lengths_sorted_idxes[0]] = lengths[
                                 lengths_sorted_idxes[1]] = lengths[
-                                    lengths_sorted_idxes[
-                                        2]] = maximum_length // 3
-
-                new_lens = list(lengths[k] for k in range(0, len(lengths)))
-                new_tensor_tuple = tuple(
-                    x[:y] for x, y in zip(tensor_tuple, new_lens))
-
-                for x, y in zip(new_tensor_tuple, tensor_tuple):
-                    x[-1] = y[-1]
-                collected_tuples.append(new_tensor_tuple)
-            else:
-                collected_tuples.append(tensor_tuple)
-
-        concat_tensor = list(torch.cat(x, dim=0) for x in collected_tuples)
-        all_input_concat_padded = pad_sequence(
-            concat_tensor, batch_first=True, padding_value=pad_idx)
+                                    lengths_sorted_idxes[2]]
+                    else:
+                        lengths[lengths_sorted_idxes[0]] = lengths[
+                            lengths_sorted_idxes[1]] = lengths[
+                                lengths_sorted_idxes[2]] = maximum_length // 3
+
+            new_lens = list(lengths[k] for k in range(0, len(lengths)))
+            new_tensor_tuple = tuple(x[:y]
+                                     for x, y in zip(tensor_tuple, new_lens))
+
+            for x, y in zip(new_tensor_tuple, tensor_tuple):
+                x[-1] = y[-1]
+            collected_tuples.append(new_tensor_tuple)
+        else:
+            collected_tuples.append(tensor_tuple)
 
-        return all_input_concat_padded
+    concat_tensor = list(torch.cat(x, dim=0) for x in collected_tuples)
+    all_input_concat_padded = pad_sequence(
+        concat_tensor, batch_first=True, padding_value=pad_idx)
+    return all_input_concat_padded
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/use/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/science/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .user_satisfaction_estimation import UserSatisfactionEstimation
+    from .protein_structure_pipeline import ProteinStructurePipeline
+
 else:
     _import_structure = {
-        'user_satisfaction_estimation': ['UserSatisfactionEstimation']
+        'protein_structure_pipeline': ['ProteinStructurePipeline']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/models/nlp/use/transformer.py` & `modelscope-1.6.0/modelscope/models/nlp/use/transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/use/user_satisfaction_estimation.py` & `modelscope-1.6.0/modelscope/models/nlp/use/user_satisfaction_estimation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/veco/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/veco/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/veco/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/veco/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/veco/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/veco/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/veco/fill_mask.py` & `modelscope-1.6.0/modelscope/models/nlp/veco/fill_mask.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/veco/text_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/veco/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/veco/token_classification.py` & `modelscope-1.6.0/modelscope/models/nlp/veco/token_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/backbone.py` & `modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/backbone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/nlp/xlm_roberta/configuration.py` & `modelscope-1.6.0/modelscope/models/nlp/xlm_roberta/configuration.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/config.py` & `modelscope-1.6.0/modelscope/models/science/unifold/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/__init__.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/data_ops.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/data_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/msa_pairing.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/msa_pairing.py`

 * *Files 0% similar despite different names*

```diff
@@ -111,15 +111,15 @@
     Args:
         feature: The feature to be padded.
         feature_name: The name of the feature to be padded.
 
     Returns:
         The feature with an additional padding row.
     """
-    assert feature.dtype != np.dtype(np.string_)
+    assert feature.dtype != np.dtype(np.str_)
     if feature_name in (
             'msa_all_seq',
             'msa_mask_all_seq',
             'deletion_matrix_all_seq',
             'deletion_matrix_int_all_seq',
     ):
         num_res = feature.shape[1]
```

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/process.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/process_multimer.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/process_multimer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/protein.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/protein.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/residue_constants.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/residue_constants.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/data/utils.py` & `modelscope-1.6.0/modelscope/models/science/unifold/data/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/dataset.py` & `modelscope-1.6.0/modelscope/models/science/unifold/dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/model.py` & `modelscope-1.6.0/modelscope/models/science/unifold/model.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/alphafold.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/alphafold.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/attentions.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/attentions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/auxillary_heads.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/auxillary_heads.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/common.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/confidence.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/confidence.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/embedders.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/embedders.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/evoformer.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/evoformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/featurization.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/featurization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/frame.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/frame.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/structure_module.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/structure_module.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/template.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/template.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/modules/triangle_multiplication.py` & `modelscope-1.6.0/modelscope/models/science/unifold/modules/triangle_multiplication.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/mmcif.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/mmcif.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/msa_identifiers.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/msa_identifiers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/parsers.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/parsers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/pipeline.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/templates.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/templates.py`

 * *Files 0% similar despite different names*

```diff
@@ -1096,15 +1096,15 @@
                 'template_all_atom_mask':
                 np.zeros((1, num_res, residue_constants.atom_type_num),
                          np.float32),
                 'template_all_atom_positions':
                 np.zeros((1, num_res, residue_constants.atom_type_num, 3),
                          np.float32),
                 'template_domain_names':
-                np.array([''.encode()], dtype=np.object),
+                np.array([''.encode()], dtype=np.object_),
                 'template_sequence':
-                np.array([''.encode()], dtype=np.object),
+                np.array([''.encode()], dtype=np.object_),
                 'template_sum_probs':
                 np.array([0], dtype=np.float32),
             }
         return TemplateSearchResult(
             features=template_features, errors=errors, warnings=warnings)
```

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/__init__.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hhblits.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hhblits.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hhsearch.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hhsearch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hmmbuild.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hmmbuild.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/hmmsearch.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/hmmsearch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/jackhmmer.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/jackhmmer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/kalign.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/kalign.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/tools/utils.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/tools/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/models/science/unifold/msa/utils.py` & `modelscope-1.6.0/modelscope/models/science/unifold/msa/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/auth/auth_config.py` & `modelscope-1.6.0/modelscope/msdatasets/auth/auth_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/context/dataset_context_config.py` & `modelscope-1.6.0/modelscope/msdatasets/context/dataset_context_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,15 +20,14 @@
                  use_streaming: bool, **kwargs):
 
         self._download_config = None
         self._data_meta_config = None
         self._config_kwargs = kwargs
         self._dataset_version_cache_root_dir = None
         self._auth_config = None
-        self._ext_config: dict = {}
 
         # The lock file path for meta-files and data-files
         self._global_meta_lock_file_path = None
         self._global_data_lock_file_path = None
 
         # General arguments for dataset
         self.hub = hub
@@ -96,15 +95,7 @@
     @property
     def auth_config(self) -> BaseAuthConfig:
         return self._auth_config
 
     @auth_config.setter
     def auth_config(self, val: BaseAuthConfig):
         self._auth_config = val
-
-    @property
-    def ext_config(self) -> dict:
-        return self._ext_config
-
-    @ext_config.setter
-    def ext_config(self, val: dict):
-        self._ext_config = val
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/data_files/data_files_manager.py` & `modelscope-1.6.0/modelscope/msdatasets/data_files/data_files_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/data_loader/data_loader.py` & `modelscope-1.6.0/modelscope/msdatasets/data_loader/data_loader.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 from modelscope.hub.api import ModelScopeConfig
 from modelscope.msdatasets.auth.auth_config import OssAuthConfig
 from modelscope.msdatasets.context.dataset_context_config import \
     DatasetContextConfig
 from modelscope.msdatasets.data_files.data_files_manager import \
     DataFilesManager
-from modelscope.msdatasets.dataset_cls.dataset import ExternalDataset
+from modelscope.msdatasets.dataset_cls import ExternalDataset
 from modelscope.msdatasets.meta.data_meta_manager import DataMetaManager
 from modelscope.utils.constant import (DatasetFormations, DatasetPathName,
                                        DownloadMode, VirgoDatasetConfig)
 from modelscope.utils.logger import get_logger
 from modelscope.utils.url_utils import valid_url
 
 logger = get_logger()
@@ -187,73 +187,97 @@
 
         self.dataset_context_config.auth_config = auth_config
 
     def _build(self):
         """
         Fetch virgo meta and build virgo dataset.
         """
-        from modelscope.msdatasets.dataset_cls import VirgoDataset
+        from modelscope.msdatasets.dataset_cls.dataset import VirgoDataset
+        import pandas as pd
 
         meta_manager = DataMetaManager(self.dataset_context_config)
         meta_manager.fetch_virgo_meta()
         self.dataset_context_config = meta_manager.dataset_context_config
-        self.dataset = VirgoDataset(**self.dataset_context_config.ext_config)
+        self.dataset = VirgoDataset(
+            **self.dataset_context_config.config_kwargs)
 
         virgo_cache_dir = os.path.join(
             self.dataset_context_config.cache_root_dir,
             self.dataset_context_config.namespace,
             self.dataset_context_config.dataset_name,
             self.dataset_context_config.version)
         os.makedirs(
             os.path.join(virgo_cache_dir, DatasetPathName.META_NAME),
             exist_ok=True)
         meta_content_cache_file = os.path.join(virgo_cache_dir,
                                                DatasetPathName.META_NAME,
                                                'meta_content.csv')
-        meta_content_df = self.dataset.meta
-        meta_content_df.to_csv(meta_content_cache_file, index=False)
-        self.dataset.meta_content_cache_file = meta_content_cache_file
-        self.dataset.virgo_cache_dir = virgo_cache_dir
-        logger.info(f'Virgo meta content saved to {meta_content_cache_file}')
+
+        if isinstance(self.dataset.meta, pd.DataFrame):
+            meta_content_df = self.dataset.meta
+            meta_content_df.to_csv(meta_content_cache_file, index=False)
+            self.dataset.meta_content_cache_file = meta_content_cache_file
+            self.dataset.virgo_cache_dir = virgo_cache_dir
+            logger.info(
+                f'Virgo meta content saved to {meta_content_cache_file}')
 
     def _prepare_and_download(self):
         """
         Fetch data-files from oss-urls in the virgo meta content.
         """
 
-        if self.dataset_context_config.download_virgo_files:
+        download_virgo_files = self.dataset_context_config.config_kwargs.pop(
+            'download_virgo_files', '')
+
+        if self.dataset.data_type == 0 and download_virgo_files:
             import requests
             import json
             import shutil
             from urllib.parse import urlparse
             from functools import partial
 
             def download_file(meta_info_val, data_dir):
-                file_url = ''
+                file_url_list = []
+                file_path_list = []
                 try:
-                    file_url = json.loads(meta_info_val)['url']
-                    is_url = valid_url(file_url)
-                    if is_url:
-                        url_parse_res = urlparse(file_url)
-                        file_name = os.path.basename(url_parse_res.path)
+                    meta_info_val = json.loads(meta_info_val)
+                    # get url first, if not exist, try to get inner_url
+                    file_url = meta_info_val.get('url', '')
+                    if file_url:
+                        file_url_list.append(file_url)
                     else:
-                        raise ValueError(f'Unsupported url: {file_url}')
-                    file_path = os.path.join(data_dir, file_name)
+                        tmp_inner_member_list = meta_info_val.get(
+                            'inner_url', '')
+                        for item in tmp_inner_member_list:
+                            file_url = item.get('url', '')
+                            if file_url:
+                                file_url_list.append(file_url)
+
+                    for one_file_url in file_url_list:
+                        is_url = valid_url(one_file_url)
+                        if is_url:
+                            url_parse_res = urlparse(file_url)
+                            file_name = os.path.basename(url_parse_res.path)
+                        else:
+                            raise ValueError(f'Unsupported url: {file_url}')
+                        file_path = os.path.join(data_dir, file_name)
+                        file_path_list.append((one_file_url, file_path))
+
                 except Exception as e:
-                    logger.error(e)
-                    file_path = ''
+                    logger.error(f'parse virgo meta info error: {e}')
+                    file_path_list = []
 
-                if file_path and not os.path.exists(file_path):
-                    logger.info(
-                        f'Downloading file from {file_url} to {file_path}')
-                    os.makedirs(data_dir, exist_ok=True)
-                    with open(file_path, 'wb') as f:
-                        f.write(requests.get(file_url).content)
+                for file_url_item, file_path_item in file_path_list:
+                    if file_path_item and not os.path.exists(file_path_item):
+                        logger.info(f'Downloading file to {file_path_item}')
+                        os.makedirs(data_dir, exist_ok=True)
+                        with open(file_path_item, 'wb') as f:
+                            f.write(requests.get(file_url_item).content)
 
-                return file_path
+                return file_path_list
 
             self.dataset.download_virgo_files = True
             download_mode = self.dataset_context_config.download_mode
             data_files_dir = os.path.join(self.dataset.virgo_cache_dir,
                                           DatasetPathName.DATA_FILES_NAME)
 
             if download_mode == DownloadMode.FORCE_REDOWNLOAD:
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/data_loader/data_loader_manager.py` & `modelscope-1.6.0/modelscope/msdatasets/data_loader/data_loader_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,20 +23,14 @@
     from .image_quality_assessment_degradation import ImageQualityAssessmentDegradationDataset
     from .image_quality_assmessment_mos import ImageQualityAssessmentMosDataset
     from .referring_video_object_segmentation import ReferringVideoObjectSegmentationDataset
     from .sidd_image_denoising import SiddImageDenoisingDataset
     from .video_frame_interpolation import VideoFrameInterpolationDataset
     from .video_stabilization import VideoStabilizationDataset
     from .video_super_resolution import VideoSuperResolutionDataset
-    from .image_semantic_segmentation import SegDataset
-    from .face_2d_keypoins import FaceKeypointDataset
-    from .hand_2d_keypoints import HandCocoWholeBodyDataset
-    from .human_wholebody_keypoint import WholeBodyCocoTopDownDataset
-    from .image_classification import ClsDataset
-    from .object_detection import DetDataset, DetImagesMixDataset
     from .ocr_detection import DataLoader, ImageDataset, QuadMeasurer
     from .ocr_recognition_dataset import OCRRecognitionDataset
     from .image_colorization import ImageColorizationDataset
 else:
     _import_structure = {
         'easycv_base': ['EasyCVBaseDataset'],
         'builder': ['CUSTOM_DATASETS', 'build_custom_dataset'],
@@ -62,20 +56,14 @@
         'image_quality_assmessment_mos': ['ImageQualityAssessmentMosDataset'],
         'referring_video_object_segmentation':
         ['ReferringVideoObjectSegmentationDataset'],
         'sidd_image_denoising': ['SiddImageDenoisingDataset'],
         'video_frame_interpolation': ['VideoFrameInterpolationDataset'],
         'video_stabilization': ['VideoStabilizationDataset'],
         'video_super_resolution': ['VideoSuperResolutionDataset'],
-        'image_semantic_segmentation': ['SegDataset'],
-        'face_2d_keypoins': ['FaceKeypointDataset'],
-        'hand_2d_keypoints': ['HandCocoWholeBodyDataset'],
-        'human_wholebody_keypoint': ['WholeBodyCocoTopDownDataset'],
-        'image_classification': ['ClsDataset'],
-        'object_detection': ['DetDataset', 'DetImagesMixDataset'],
         'ocr_detection': ['DataLoader', 'ImageDataset', 'QuadMeasurer'],
         'ocr_recognition_dataset': ['OCRRecognitionDataset'],
         'image_colorization': ['ImageColorizationDataset'],
     }
 
     import sys
     sys.modules[__name__] = LazyImportModule(
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/easycv_base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .face_2d_keypoints_dataset import FaceKeypointDataset
+    from .video_frame_interpolation_dataset import VideoFrameInterpolationDataset
 
 else:
-    _import_structure = {'face_2d_keypoints_dataset': ['FaceKeypointDataset']}
+    _import_structure = {
+        'video_frame_interpolation_dataset':
+        ['VideoFrameInterpolationDataset'],
+    }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
         _import_structure,
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py` & `modelscope-1.6.0/modelscope/trainers/hooks/logger/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
+from modelscope.trainers.utils.log_buffer import LogBuffer
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .hand_2d_keypoints_dataset import HandCocoWholeBodyDataset
-
+    from .base import LoggerHook
+    from .tensorboard_hook import TensorboardHook
+    from .text_logger_hook import TextLoggerHook
 else:
     _import_structure = {
-        'hand_2d_keypoints_dataset': ['HandCocoWholeBodyDataset']
+        'base': ['LoggerHook'],
+        'tensorboard_hook': ['TensorboardHook'],
+        'text_logger_hook': ['TextLoggerHook']
     }
-
     import sys
-
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
         _import_structure,
         module_spec=__spec__,
         extra_objects={},
     )
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py` & `modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
+
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .human_wholebody_keypoint_dataset import WholeBodyCocoTopDownDataset
+    from .base import BaseWarmup
+    from .warmup import ConstantWarmup, ExponentialWarmup, LinearWarmup
 
 else:
     _import_structure = {
-        'human_wholebody_keypoint_dataset': ['WholeBodyCocoTopDownDataset']
+        'base': ['BaseWarmup'],
+        'warmup': ['ConstantWarmup', 'ExponentialWarmup', 'LinearWarmup']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .image_inpainting_dataset import ImageInpaintingDataset
+    from .image_quality_assessment_mos_dataset import ImageQualityAssessmentMosDataset
+
 else:
     _import_structure = {
-        'image_inpainting_dataset': ['ImageInpaintingDataset'],
+        'image_quality_assessment_mos_dataset':
+        ['ImageQualityAssessmentMosDataset'],
     }
+
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
         _import_structure,
         module_spec=__spec__,
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .image_quality_assessment_mos_dataset import ImageQualityAssessmentMosDataset
+    from .video_stabilization_dataset import VideoStabilizationDataset
 
 else:
     _import_structure = {
-        'image_quality_assessment_mos_dataset':
-        ['ImageQualityAssessmentMosDataset'],
+        'video_stabilization_dataset': ['VideoStabilizationDataset'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .detection_dataset import DetDataset, DetImagesMixDataset
+    from .sidd_image_denoising_dataset import SiddImageDenoisingDataset
 
 else:
     _import_structure = {
-        'detection_dataset': ['DetDataset', 'DetImagesMixDataset']
+        'sidd_image_denoising_dataset': ['SiddImageDenoisingDataset'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py`

 * *Files 8% similar despite different names*

```diff
@@ -30,32 +30,34 @@
     return chr(inside_code)
 
 
 @CUSTOM_DATASETS.register_module(
     Tasks.ocr_recognition, module_name=Models.ocr_recognition)
 class OCRRecognitionDataset(TorchCustomDataset):
 
-    def __init__(self, **kwargs):
+    def __init__(self, local_lmdb=None, preprocessor=None, **kwargs):
         split_config = kwargs['split_config']
         cache_root = next(iter(split_config.values()))
         lmdb_path = os.path.join(cache_root, DATASET_STRUCTURE['lmdb'])
+        if local_lmdb is not None:
+            lmdb_path = local_lmdb
         self.env = lmdb.open(
             lmdb_path,
             max_readers=1,
             readonly=True,
             lock=False,
             readahead=False,
             meminit=False)
         if not self.env:
             print('cannot creat lmdb from %s' % (lmdb_path))
             sys.exit(0)
         self.nSamples = 0
         with self.env.begin(write=False) as txn:
             self.nSamples = int(txn.get('num-samples'.encode()))
-        self.reco_preprocess = kwargs['preprocessor']
+        self.reco_preprocess = preprocessor
 
     def __len__(self):
         return self.nSamples
 
     def __getitem__(self, index):
         index += 1
         img_key = 'image-%09d' % index
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .sidd_image_denoising_dataset import SiddImageDenoisingDataset
-
+    from .convnext import convnext_tiny
+    from .vitstr import vitstr_tiny
 else:
     _import_structure = {
-        'sidd_image_denoising_dataset': ['SiddImageDenoisingDataset'],
+        'convnext': ['convnext_tiny'],
+        'vitstr': ['vitstr_tiny']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/veco_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/__init__.py` & `modelscope-1.6.0/modelscope/trainers/hooks/compression/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .video_frame_interpolation_dataset import VideoFrameInterpolationDataset
+    from .sparsity_hook import SparsityHook
+    from .utils import SparseLinear, convert_sparse_network
 
 else:
     _import_structure = {
-        'video_frame_interpolation_dataset':
-        ['VideoFrameInterpolationDataset'],
+        'sparsity_hook': ['SparsityHook'],
+        'utils': ['convert_sparse_network', 'SparseLinear'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/__init__.py` & `modelscope-1.6.0/modelscope/models/nlp/unite/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
+
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .video_stabilization_dataset import VideoStabilizationDataset
-
+    from .configuration import UniTEConfig
+    from .translation_evaluation import UniTEForTranslationEvaluation
 else:
     _import_structure = {
-        'video_stabilization_dataset': ['VideoStabilizationDataset'],
+        'configuration': ['UniTEConfig'],
+        'translation_evaluation': ['UniTEForTranslationEvaluation'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/download/dataset_builder.py` & `modelscope-1.6.0/modelscope/msdatasets/download/dataset_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,16 +14,16 @@
 from datasets.packaged_modules import csv
 from datasets.utils.filelock import FileLock
 from datasets.utils.py_utils import map_nested
 
 from modelscope.hub.api import HubApi
 from modelscope.msdatasets.context.dataset_context_config import \
     DatasetContextConfig
-from modelscope.msdatasets.dataset_cls.dataset import (ExternalDataset,
-                                                       NativeIterableDataset)
+from modelscope.msdatasets.dataset_cls import (ExternalDataset,
+                                               NativeIterableDataset)
 from modelscope.msdatasets.download.download_manager import \
     DataStreamingDownloadManager
 from modelscope.msdatasets.utils.dataset_utils import \
     get_subdir_hash_from_split
 from modelscope.utils.constant import (DEFAULT_DATASET_NAMESPACE,
                                        DatasetPathName, DownloadMode)
 from modelscope.utils.logger import get_logger
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/download/download_config.py` & `modelscope-1.6.0/modelscope/msdatasets/download/download_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/download/download_manager.py` & `modelscope-1.6.0/modelscope/msdatasets/download/download_manager.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/meta/data_meta_config.py` & `modelscope-1.6.0/modelscope/msdatasets/meta/data_meta_config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/meta/data_meta_manager.py` & `modelscope-1.6.0/modelscope/msdatasets/meta/data_meta_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -142,15 +142,15 @@
 
     def fetch_virgo_meta(self) -> None:
         virgo_dataset_id = self.dataset_context_config.dataset_name
         version = int(self.dataset_context_config.version)
 
         meta_content = self.api.get_virgo_meta(
             dataset_id=virgo_dataset_id, version=version)
-        self.dataset_context_config.ext_config = meta_content
+        self.dataset_context_config.config_kwargs.update(meta_content)
 
     def _fetch_meta_from_cache(self, meta_cache_dir):
         local_paths = defaultdict(list)
         dataset_type = None
         for meta_file_name in os.listdir(meta_cache_dir):
             file_ext = os.path.splitext(meta_file_name)[-1]
             if file_ext == DatasetFormations.formation_mark_ext.value:
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/ms_dataset.py` & `modelscope-1.6.0/modelscope/msdatasets/ms_dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,16 +14,15 @@
 from modelscope.msdatasets.context.dataset_context_config import \
     DatasetContextConfig
 from modelscope.msdatasets.data_loader.data_loader import VirgoDownloader
 from modelscope.msdatasets.data_loader.data_loader_manager import (
     LocalDataLoaderManager, LocalDataLoaderType, RemoteDataLoaderManager,
     RemoteDataLoaderType)
 from modelscope.msdatasets.dataset_cls import (ExternalDataset,
-                                               NativeIterableDataset,
-                                               VirgoDataset)
+                                               NativeIterableDataset)
 from modelscope.msdatasets.dataset_cls.custom_datasets.builder import \
     build_custom_dataset
 from modelscope.msdatasets.utils.delete_utils import DatasetDeleteManager
 from modelscope.msdatasets.utils.upload_utils import DatasetUploadManager
 from modelscope.preprocessors import build_preprocessor
 from modelscope.utils.config import Config, ConfigDict
 from modelscope.utils.config_ds import MS_DATASETS_CACHE
@@ -168,15 +167,15 @@
                                                       Sequence[str]]]]] = None,
         download_mode: Optional[DownloadMode] = DownloadMode.
         REUSE_DATASET_IF_EXISTS,
         cache_dir: Optional[str] = MS_DATASETS_CACHE,
         use_streaming: Optional[bool] = False,
         custom_cfg: Optional[Config] = Config(),
         **config_kwargs,
-    ) -> Union[dict, 'MsDataset', NativeIterableDataset, VirgoDataset]:
+    ) -> Union[dict, 'MsDataset', NativeIterableDataset]:
         """Load a MsDataset from the ModelScope Hub, Hugging Face Hub, urls, or a local dataset.
 
             Args:
                 dataset_name (str): Path or name of the dataset.
                     The form of `namespace/dataset_name` is also supported.
                 namespace(str, optional): Namespace of the dataset. It should not be None if you load a remote dataset
                     from Hubs.modelscope,
@@ -186,17 +185,14 @@
                 target (str, optional): Name of the column to output.
                 version (str, optional): Version of the dataset script to load:
                 subset_name (str, optional): Defining the subset_name of the dataset.
                 data_dir (str, optional): Defining the data_dir of the dataset configuration. I
                 data_files (str or Sequence or Mapping, optional): Path(s) to source data file(s).
                 split (str, optional): Which split of the data to load.
                 hub (Hubs or str, optional): When loading from a remote hub, where it is from. default Hubs.modelscope
-                download_mode (DownloadMode or str, optional):
-                    How to treat existing datasets. default DownloadMode.REUSE_DATASET_IF_EXISTS
-                config_kwargs (additional keyword arguments): Keyword arguments to be passed
                 download_mode (DownloadMode or str, optional): How to treat existing datasets. default
                                                                DownloadMode.REUSE_DATASET_IF_EXISTS
                 cache_dir (str, Optional): User-define local cache directory.
                 use_streaming (bool, Optional): If set to True, no need to download all data files.
                                                 Instead, it streams the data progressively, and returns
                                                 NativeIterableDataset or a dict of NativeIterableDataset.
                 custom_cfg (str, Optional): Model configuration, this can be used for custom datasets.
@@ -296,17 +292,14 @@
             if version == DEFAULT_DATASET_REVISION:
                 dataset_context_config.version = VirgoDatasetConfig.default_dataset_version
             if cache_dir == MS_DATASETS_CACHE:
                 from modelscope.utils.config_ds import CACHE_HOME
                 cache_dir = os.path.join(CACHE_HOME, 'virgo', 'hub',
                                          'datasets')
                 dataset_context_config.cache_root_dir = cache_dir
-            if 'download_virgo_files' in config_kwargs:
-                dataset_context_config.download_virgo_files = config_kwargs.pop(
-                    'download_virgo_files')
 
             virgo_downloader = VirgoDownloader(dataset_context_config)
             virgo_downloader.process()
 
             return virgo_downloader.dataset
 
         else:
```

### Comparing `modelscope-1.5.2/modelscope/msdatasets/task_datasets/__init__.py` & `modelscope-1.6.0/modelscope/msdatasets/task_datasets/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/utils/dataset_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/utils/dataset_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/utils/delete_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/utils/delete_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/utils/oss_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/utils/oss_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/msdatasets/utils/upload_utils.py` & `modelscope-1.6.0/modelscope/msdatasets/utils/upload_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/ops/ailut/pyinterfaces.py` & `modelscope-1.6.0/modelscope/ops/ailut/pyinterfaces.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/ops/quadtree_attention/functions/quadtree_attention.py` & `modelscope-1.6.0/modelscope/ops/quadtree_attention/functions/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/ops/quadtree_attention/modules/quadtree_attention.py` & `modelscope-1.6.0/modelscope/ops/quadtree_attention/modules/quadtree_attention.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/outputs/cv_outputs.py` & `modelscope-1.6.0/modelscope/outputs/cv_outputs.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/outputs/nlp_outputs.py` & `modelscope-1.6.0/modelscope/outputs/nlp_outputs.py`

 * *Files 2% similar despite different names*

```diff
@@ -450,7 +450,17 @@
         doc_embs (`Tensor`, *optional*) Then tensor of the doc embeddings.
         loss (`torch.FloatTensor` of shape `(1,)`, *optional*): Sentence Embedding modeling loss.
     """
 
     query_embeddings: Tensor = None
     doc_embeddings: Tensor = None
     loss: Tensor = None
+
+
+@dataclass
+class TranslationEvaluationOutput(ModelOutputBase):
+    """The output class for translation evaluation models.
+    """
+
+    score: Tensor = None
+    loss: Tensor = None
+    input_format: List[str] = None
```

### Comparing `modelscope-1.5.2/modelscope/outputs/outputs.py` & `modelscope-1.6.0/modelscope/outputs/outputs.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,10 +1,14 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from collections import OrderedDict, namedtuple
 from dataclasses import dataclass, fields
+from typing import Dict, List, Tuple
+
+import numpy as np
+import torch
 
 from modelscope.utils.constant import Tasks
 
 
 class OutputKeys(object):
     LOSS = 'loss'
     LOGITS = 'logits'
@@ -46,28 +50,363 @@
     VIDEO_EMBEDDING = 'video_embedding'
     UUID = 'uuid'
     WORD = 'word'
     KWS_LIST = 'kws_list'
     SQL_STRING = 'sql_string'
     SQL_QUERY = 'sql_query'
     HISTORY = 'history'
-    QUERT_RESULT = 'query_result'
+    QUERY_RESULT = 'query_result'
     TIMESTAMPS = 'timestamps'
     SHOT_NUM = 'shot_num'
     SCENE_NUM = 'scene_num'
     SCENE_META_LIST = 'scene_meta_list'
     SHOT_META_LIST = 'shot_meta_list'
     MATCHES = 'matches'
     PCD12 = 'pcd12'
     PCD12_ALIGN = 'pcd12_align'
     TBOUNDS = 'tbounds'
 
 
-TASK_OUTPUTS = {
+OutputTypes = {
+    OutputKeys.LOSS: float,  # checked
+    OutputKeys.LOGITS: np.ndarray,  # checked.
+    OutputKeys.SCORES: List[float],  # checked
+    OutputKeys.SCORE: float,  # checked
+    OutputKeys.LABEL: str,  # checked
+    OutputKeys.LABELS: List[str],  # checked
+    OutputKeys.INPUT_IDS: np.ndarray,  # checked
+    OutputKeys.LABEL_POS: np.ndarray,  # checked
+    OutputKeys.POSES:
+    List[np.ndarray],  # [Tuple(np.ndarray, np.ndarray)]  # checked doubtful
+    OutputKeys.CAPTION: str,
+    OutputKeys.BOXES: np.ndarray,  # checked
+    OutputKeys.KEYPOINTS: np.ndarray,  # checked
+    OutputKeys.MASKS: np.ndarray,  # checked
+    OutputKeys.DEPTHS: List[np.ndarray],  # checked
+    OutputKeys.DEPTHS_COLOR: List[np.ndarray],  # checked
+    OutputKeys.LAYOUT: np.ndarray,  # checked
+    OutputKeys.TEXT: str,  # checked
+    OutputKeys.POLYGONS: np.array,  # checked
+    OutputKeys.OUTPUT: Dict,
+    OutputKeys.OUTPUT_IMG: 'image',  # checked
+    OutputKeys.OUTPUT_IMGS: List[np.ndarray],  # checked
+    OutputKeys.OUTPUT_VIDEO: 'bytes',
+    OutputKeys.OUTPUT_PCM: np.ndarray,
+    OutputKeys.OUTPUT_PCM_LIST: List[np.ndarray],
+    OutputKeys.OUTPUT_WAV: np.ndarray,
+    OutputKeys.OUTPUT_OBJ: Dict,
+    OutputKeys.OUTPUT_MESH: np.ndarray,
+    OutputKeys.IMG_EMBEDDING: np.ndarray,
+    OutputKeys.SPK_EMBEDDING: np.ndarray,
+    OutputKeys.SPO_LIST: List[float],
+    OutputKeys.TEXT_EMBEDDING: np.ndarray,
+    OutputKeys.TRANSLATION: str,
+    OutputKeys.RESPONSE: Dict,
+    OutputKeys.PREDICTION: np.ndarray,  # checked
+    OutputKeys.PREDICTIONS: List[np.ndarray],
+    OutputKeys.PROBABILITIES: np.ndarray,
+    OutputKeys.DIALOG_STATES: object,
+    OutputKeys.VIDEO_EMBEDDING: np.ndarray,
+    OutputKeys.UUID: str,
+    OutputKeys.WORD: str,
+    OutputKeys.KWS_LIST: List[str],
+    OutputKeys.SQL_STRING: str,  # checked
+    OutputKeys.SQL_QUERY: str,  # checked
+    OutputKeys.HISTORY: Dict,  # checked
+    OutputKeys.QUERY_RESULT: Dict,  # checked
+    OutputKeys.TIMESTAMPS: str,
+    OutputKeys.SHOT_NUM: int,
+    OutputKeys.SCENE_NUM: int,
+    OutputKeys.SCENE_META_LIST: List[int],
+    OutputKeys.SHOT_META_LIST: List[int],
+    OutputKeys.MATCHES: List[np.ndarray],
+    OutputKeys.PCD12: np.ndarray,
+    OutputKeys.PCD12_ALIGN: np.ndarray,
+    OutputKeys.TBOUNDS: Dict,
+}
+
+OutputTypeSchema = {
+    OutputKeys.LOSS: {
+        'type': 'number'
+    },  # checked
+    OutputKeys.LOGITS: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked.
+    OutputKeys.SCORES: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.SCORE: {
+        'type': 'number'
+    },  # checked
+    OutputKeys.LABEL: {
+        'type': 'string'
+    },  # checked
+    OutputKeys.LABELS: {
+        'type': 'array',
+        'items': {
+            'type': 'string'
+        }
+    },  # checked
+    OutputKeys.INPUT_IDS: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.LABEL_POS: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.POSES: {
+        'type': 'array',
+        'items': {
+            'type': 'array',
+            'items': {
+                'type': 'number'
+            }
+        }
+    },  # [Tuple(np.ndarray, np.ndarray)]  # checked doubtful
+    OutputKeys.CAPTION: {
+        'type': 'string'
+    },
+    OutputKeys.BOXES: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.KEYPOINTS: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.MASKS: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.DEPTHS: {
+        'type': 'array',
+        'items': {
+            'type': 'array',
+            'items': {
+                'type': 'number'
+            }
+        }
+    },  # checked
+    OutputKeys.DEPTHS_COLOR: {
+        'type': 'array',
+        'items': {
+            'type': 'array',
+            'items': {
+                'type': 'number'
+            }
+        }
+    },  # checked
+    OutputKeys.LAYOUT: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.TEXT: {
+        'type': 'string'
+    },  # checked
+    OutputKeys.POLYGONS: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.OUTPUT: {
+        'type': 'object'
+    },
+    OutputKeys.OUTPUT_IMG: {
+        'type': 'string',
+        'description': 'The base64 encoded image.',
+    },  # checked
+    OutputKeys.OUTPUT_IMGS: {
+        'type': 'array',
+        'items': {
+            'type': 'string',
+            'description': 'The base64 encoded image.',
+        }
+    },  # checked
+    OutputKeys.OUTPUT_VIDEO: {
+        'type': 'string',
+        'description': 'The base64 encoded video.',
+    },
+    OutputKeys.OUTPUT_PCM: {
+        'type': 'string',
+        'description': 'The base64 encoded PCM.',
+    },
+    OutputKeys.OUTPUT_PCM_LIST: {
+        'type': 'array',
+        'items': {
+            'type': 'string',
+            'description': 'The base64 encoded PCM.',
+        }
+    },
+    OutputKeys.OUTPUT_WAV: {
+        'type': 'string',
+        'description': 'The base64 encoded WAV.',
+    },
+    OutputKeys.OUTPUT_OBJ: {
+        'type': 'object'
+    },
+    OutputKeys.OUTPUT_MESH: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.IMG_EMBEDDING: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.SPK_EMBEDDING: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.SPO_LIST: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.TEXT_EMBEDDING: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.TRANSLATION: {
+        'type': 'string'
+    },
+    OutputKeys.RESPONSE: {
+        'type': 'object'
+    },
+    OutputKeys.PREDICTION: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },  # checked
+    OutputKeys.PREDICTIONS: {
+        'type': 'array',
+        'items': {
+            'type': 'array',
+            'items': {
+                'type': 'number'
+            }
+        }
+    },
+    OutputKeys.PROBABILITIES: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.DIALOG_STATES: {
+        'type': 'object'
+    },
+    OutputKeys.VIDEO_EMBEDDING: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.UUID: {
+        'type': 'string'
+    },
+    OutputKeys.WORD: {
+        'type': 'string'
+    },
+    OutputKeys.KWS_LIST: {
+        'type': 'array',
+        'items': {
+            'type': 'string'
+        }
+    },
+    OutputKeys.SQL_STRING: {
+        'type': 'string'
+    },  # checked
+    OutputKeys.SQL_QUERY: {
+        'type': 'string'
+    },  # checked
+    OutputKeys.HISTORY: {
+        'type': 'object'
+    },  # checked
+    OutputKeys.QUERY_RESULT: {
+        'type': 'object'
+    },  # checked
+    OutputKeys.TIMESTAMPS: {
+        'type': 'string'
+    },
+    OutputKeys.SHOT_NUM: {
+        'type': 'integer'
+    },
+    OutputKeys.SCENE_NUM: {
+        'type': 'integer'
+    },
+    OutputKeys.SCENE_META_LIST: {
+        'type': 'array',
+        'items': {
+            'type': 'integer'
+        }
+    },
+    OutputKeys.SHOT_META_LIST: {
+        'type': 'array',
+        'items': {
+            'type': 'integer'
+        }
+    },
+    OutputKeys.MATCHES: {
+        'type': 'array',
+        'items': {
+            'type': 'array',
+            'items': {
+                'type': 'number'
+            }
+        }
+    },
+    OutputKeys.PCD12: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.PCD12_ALIGN: {
+        'type': 'array',
+        'items': {
+            'type': 'number'
+        }
+    },
+    OutputKeys.TBOUNDS: {
+        'type': 'object'
+    },
+}
 
+TASK_OUTPUTS = {
+    Tasks.task_template:
+    [OutputKeys.BOXES, OutputKeys.OUTPUT_IMG, OutputKeys.TEXT_EMBEDDING],
     # ============ vision tasks ===================
 
     # ocr detection result for single sample
     # {
     #   "polygons": np.array with shape [num_text, 8], each polygon is
     #       [x1, y1, x2, y2, x3, y3, x4, y4]
     # }
@@ -384,16 +723,17 @@
     Tasks.video_colorization: [OutputKeys.OUTPUT_VIDEO],
 
     # image quality assessment degradation result for single image
     # {
     #       "scores": [0.885272, 0.014790631, 0.014558001]
     #       "labels": ['', '', ''],
     # }
-    Tasks.image_quality_assessment_degradation:
-    [OutputKeys.SCORES, OutputKeys.LABELS],
+    Tasks.image_quality_assessment_degradation: [
+        OutputKeys.SCORES, OutputKeys.LABELS
+    ],
 
     # live category recognition result for single video
     # {
     #       "scores": [0.885272, 0.014790631, 0.014558001]
     #       "labels": ['/>>/', '/>>', '/>>>>'],
     # }
     Tasks.live_category: [OutputKeys.SCORES, OutputKeys.LABELS],
@@ -1025,14 +1365,18 @@
     # {"text": "this is a text answser. "}
     Tasks.visual_question_answering: [OutputKeys.TEXT],
 
     # VideoQA result for a sample
     # {"text": "this is a text answser. "}
     Tasks.video_question_answering: [OutputKeys.TEXT],
 
+    # Multimodal Dialogue result for a sample
+    # {"text": "this is a text response. "}
+    Tasks.multimodal_dialogue: [OutputKeys.TEXT],
+
     # auto_speech_recognition result for a single sample
     # {
     #    "text": ""
     # }
     Tasks.auto_speech_recognition: [OutputKeys.TEXT],
 
     # {
@@ -1103,17 +1447,17 @@
 
     # image_skychange result for a single sample
     # {
     #    "output_img": np.ndarray with shape [height, width, 3]
     # }
     Tasks.image_skychange: [OutputKeys.OUTPUT_IMG],
     # {
-    #     'scores': [0.1, 0.2, 0.3, ...]
+    #     'score': [0.1, 0.2, 0.3, ...]
     # }
-    Tasks.translation_evaluation: [OutputKeys.SCORES],
+    Tasks.translation_evaluation: [OutputKeys.SCORE],
 
     # video object segmentation result for a single video
     #   {
     #       "masks": [np.array # 3D array with shape [frame_num, height, width]]
     #   }
     Tasks.video_object_segmentation: [OutputKeys.MASKS],
 
@@ -1136,14 +1480,15 @@
     #       "labels": ["dog", "horse", "cow", "cat"],
     #   }
     Tasks.vision_efficient_tuning: [OutputKeys.SCORES, OutputKeys.LABELS],
     Tasks.document_grounded_dialog_generate: [OutputKeys.TEXT],
     Tasks.document_grounded_dialog_rerank: [OutputKeys.OUTPUT],
     Tasks.document_grounded_dialog_retrieval: [OutputKeys.OUTPUT],
     Tasks.video_temporal_grounding: [OutputKeys.SCORES, OutputKeys.TBOUNDS],
+    Tasks.text_to_video_synthesis: [OutputKeys.OUTPUT_VIDEO],
 }
 
 
 class ModelOutputBase(list):
 
     def __post_init__(self):
         self.reconstruct()
```

### Comparing `modelscope-1.5.2/modelscope/pipeline_inputs.py` & `modelscope-1.6.0/modelscope/pipeline_inputs.py`

 * *Files 16% similar despite different names*

```diff
@@ -16,26 +16,61 @@
     IMAGE = 'image'
     TEXT = 'text'
     AUDIO = 'audio'
     VIDEO = 'video'
     BOX = 'box'
     DICT = 'dict'
     LIST = 'list'
-    INT = 'int'
+    NUMBER = 'number'
 
 
 INPUT_TYPE = {
     InputType.IMAGE: (str, np.ndarray, Image.Image),
     InputType.TEXT: str,
     InputType.AUDIO: (str, bytes, np.ndarray),
     InputType.VIDEO: (str, np.ndarray, 'cv2.VideoCapture'),
     InputType.BOX: (list, np.ndarray),
     InputType.DICT: (dict, type(None)),
     InputType.LIST: (list, type(None)),
-    InputType.INT: int,
+    InputType.NUMBER: int,
+}
+
+INPUT_TYPE_SCHEMA = {
+    InputType.IMAGE: {
+        'type': 'string',
+        'description': 'Base64 encoded image file or url string.'
+    },  # support url or base64 encoded file.
+    InputType.AUDIO: {
+        'type': 'string',
+        'description': 'Base64 encoded audio file or url string..'
+    },  # support url or base64 encoded file.
+    InputType.VIDEO: {
+        'type': 'string',
+        'description': 'Base64 encoded video file or url string..'
+    },  # support url or base64 encoded file.
+    InputType.TEXT: {
+        'type': 'string',
+        'description': 'The input text.'
+    },
+    InputType.BOX: {
+        'type': 'array',
+        'description': 'Box coordinate, should be int.',
+        'items': {
+            'type': 'number'
+        }
+    },
+    InputType.DICT: {  # unknown properties
+        'type': 'object',
+    },
+    InputType.LIST: {
+        'type': 'array'
+    },  # unknown item type.
+    InputType.NUMBER: {
+        'type': 'integer'
+    },
 }
 
 
 def check_input_type(input_type, input):
     expected_type = INPUT_TYPE[input_type]
     if input_type == InputType.VIDEO:
         # special type checking using class name, to avoid introduction of opencv dependency into fundamental framework.
@@ -43,20 +78,27 @@
             f'invalid input type for {input_type}, expected {expected_type} but got {type(input)}\n {input}'
     else:
         assert isinstance(input, expected_type), \
             f'invalid input type for {input_type}, expected {expected_type} but got {type(input)}\n {input}'
 
 
 TASK_INPUTS = {
+
+    Tasks.task_template: {
+        'image': InputType.IMAGE,
+        'text': InputType.TEXT
+    },
     # if task input is single var, value is  InputType
     # if task input is a tuple,  value is tuple of InputType
     # if task input is a dict, value is a dict of InputType, where key
     # equals the one needed in pipeline input dict
     # if task input is a list, value is a set of input format, in which
-    # each element corresponds to one input format as described above.
+    # each element corresponds to one input format as described above and
+    # must include a dict format.
+
     # ============ vision tasks ===================
     Tasks.ocr_detection:
     InputType.IMAGE,
     Tasks.ocr_recognition:
     InputType.IMAGE,
     Tasks.face_2d_keypoints:
     InputType.IMAGE,
@@ -69,15 +111,15 @@
     Tasks.face_recognition:
     InputType.IMAGE,
     Tasks.face_reconstruction:
     InputType.IMAGE,
     Tasks.human_detection:
     InputType.IMAGE,
     Tasks.face_image_generation:
-    InputType.INT,
+    InputType.NUMBER,
     Tasks.image_classification:
     InputType.IMAGE,
     Tasks.image_object_detection:
     InputType.IMAGE,
     Tasks.domain_specific_object_detection:
     InputType.IMAGE,
     Tasks.image_segmentation:
@@ -187,16 +229,15 @@
             'text2': InputType.TEXT
         },
     ],
     Tasks.sentence_similarity: (InputType.TEXT, InputType.TEXT),
     Tasks.nli: (InputType.TEXT, InputType.TEXT),
     Tasks.sentiment_classification:
     InputType.TEXT,
-    Tasks.zero_shot_classification:
-    InputType.TEXT,
+    Tasks.zero_shot_classification: InputType.TEXT,
     Tasks.relation_extraction:
     InputType.TEXT,
     Tasks.translation:
     InputType.TEXT,
     Tasks.competency_aware_translation:
     InputType.TEXT,
     Tasks.word_segmentation: [InputType.TEXT, {
@@ -208,15 +249,21 @@
     InputType.TEXT,
     Tasks.text_error_correction:
     InputType.TEXT,
     Tasks.sentence_embedding: {
         'source_sentence': InputType.LIST,
         'sentences_to_compare': InputType.LIST,
     },
-    Tasks.text_ranking: (InputType.TEXT, InputType.TEXT),
+    Tasks.text_ranking: [
+        (InputType.TEXT, InputType.TEXT),
+        {
+            'source_sentence': InputType.LIST,
+            'sentences_to_compare': InputType.LIST
+        }
+    ],
     Tasks.text_generation:
     InputType.TEXT,
     Tasks.fid_dialogue: {
         'history': InputType.TEXT,
         'knowledge': InputType.TEXT,
         'bot_profile': InputType.TEXT,
         'user_profile': InputType.TEXT,
@@ -257,15 +304,15 @@
     Tasks.document_grounded_dialog_retrieval: {
         'query': InputType.LIST,
         'positive': InputType.LIST,
         'negative': InputType.LIST
     },
 
     # ============ audio tasks ===================
-    Tasks.auto_speech_recognition:
+    Tasks.auto_speech_recognition:  # input can be audio, or audio and text.
     [InputType.AUDIO, {
         'wav': InputType.AUDIO,
         'text': InputType.TEXT
     }],
     Tasks.speech_signal_process:
     InputType.AUDIO,
     Tasks.acoustic_echo_cancellation: {
@@ -286,14 +333,17 @@
     # ============ multi-modal tasks ===================
     Tasks.image_captioning: [InputType.IMAGE, {
         'image': InputType.IMAGE,
     }],
     Tasks.video_captioning: [InputType.VIDEO, {
         'video': InputType.VIDEO,
     }],
+    Tasks.multimodal_dialogue: {
+        'messages': InputType.LIST,
+    },
     Tasks.visual_grounding: {
         'image': InputType.IMAGE,
         'text': InputType.TEXT
     },
     Tasks.text_to_image_synthesis: {
         'text': InputType.TEXT,
     },
@@ -328,9 +378,13 @@
     InputType.IMAGE,
     Tasks.image_reid_person:
     InputType.IMAGE,
     Tasks.video_inpainting: {
         'video_input_path': InputType.TEXT,
         'video_output_path': InputType.TEXT,
         'mask_path': InputType.TEXT,
-    }
+    },
+    Tasks.text_to_video_synthesis: {
+        'text': InputType.TEXT
+    },
+    Tasks.video_summarization: InputType.TEXT,
 }
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/ans_dfsmn_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/ans_dfsmn_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/ans_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/ans_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/asr_inference_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/asr_inference_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -50,14 +50,15 @@
                  vad_model_revision: Optional[str] = None,
                  punc_model: Optional[Union[Model, str]] = None,
                  punc_model_revision: Optional[str] = None,
                  lm_model: Optional[Union[Model, str]] = None,
                  lm_model_revision: Optional[str] = None,
                  timestamp_model: Optional[Union[Model, str]] = None,
                  timestamp_model_revision: Optional[str] = None,
+                 ngpu: int = 1,
                  **kwargs):
         """
         Use `model` and `preprocessor` to create an asr pipeline for prediction
         Args:
             model ('Model' or 'str'):
                 The pipeline handles three types of model:
 
@@ -83,15 +84,15 @@
             batch_size('int'):
                 the batch size for inference
             ngpu('int'):
                 the number of gpus, 0 indicates CPU mode
             beam_size('int'):
                 beam size for decoding
             ctc_weight('float'):
-                CTC weight in joint decoding
+                the CTC weight in joint decoding
             lm_weight('float'):
                 lm weight
             decoding_ind('int', defaults to 0):
                 decoding ind
             decoding_mode('str', defaults to 'model1'):
                 decoding mode
             vad_model_file('str'):
@@ -115,56 +116,56 @@
         self.lm_model = lm_model
         self.lm_model_revision = lm_model_revision
         self.timestamp_model = timestamp_model
         self.timestamp_model_revision = timestamp_model_revision
         self.model_cfg = self.model.forward()
 
         self.cmd = self.get_cmd(kwargs, model)
-        if self.cmd['code_base'] == 'funasr':
-            from funasr.bin import asr_inference_launch
-            self.funasr_infer_modelscope = asr_inference_launch.inference_launch(
-                mode=self.cmd['mode'],
-                maxlenratio=self.cmd['maxlenratio'],
-                minlenratio=self.cmd['minlenratio'],
-                batch_size=self.cmd['batch_size'],
-                beam_size=self.cmd['beam_size'],
-                ngpu=self.cmd['ngpu'],
-                ctc_weight=self.cmd['ctc_weight'],
-                lm_weight=self.cmd['lm_weight'],
-                penalty=self.cmd['penalty'],
-                log_level=self.cmd['log_level'],
-                asr_train_config=self.cmd['asr_train_config'],
-                asr_model_file=self.cmd['asr_model_file'],
-                cmvn_file=self.cmd['cmvn_file'],
-                lm_file=self.cmd['lm_file'],
-                token_type=self.cmd['token_type'],
-                key_file=self.cmd['key_file'],
-                lm_train_config=self.cmd['lm_train_config'],
-                bpemodel=self.cmd['bpemodel'],
-                allow_variable_data_keys=self.cmd['allow_variable_data_keys'],
-                output_dir=self.cmd['output_dir'],
-                dtype=self.cmd['dtype'],
-                seed=self.cmd['seed'],
-                ngram_weight=self.cmd['ngram_weight'],
-                nbest=self.cmd['nbest'],
-                num_workers=self.cmd['num_workers'],
-                vad_infer_config=self.cmd['vad_infer_config'],
-                vad_model_file=self.cmd['vad_model_file'],
-                vad_cmvn_file=self.cmd['vad_cmvn_file'],
-                punc_model_file=self.cmd['punc_model_file'],
-                punc_infer_config=self.cmd['punc_infer_config'],
-                timestamp_model_file=self.cmd['timestamp_model_file'],
-                timestamp_infer_config=self.cmd['timestamp_infer_config'],
-                timestamp_cmvn_file=self.cmd['timestamp_cmvn_file'],
-                outputs_dict=self.cmd['outputs_dict'],
-                param_dict=self.cmd['param_dict'],
-                token_num_relax=self.cmd['token_num_relax'],
-                decoding_ind=self.cmd['decoding_ind'],
-                decoding_mode=self.cmd['decoding_mode'],
-            )
+        from funasr.bin import asr_inference_launch
+        self.funasr_infer_modelscope = asr_inference_launch.inference_launch(
+            mode=self.cmd['mode'],
+            maxlenratio=self.cmd['maxlenratio'],
+            minlenratio=self.cmd['minlenratio'],
+            batch_size=self.cmd['batch_size'],
+            beam_size=self.cmd['beam_size'],
+            ngpu=self.cmd['ngpu'],
+            ctc_weight=self.cmd['ctc_weight'],
+            lm_weight=self.cmd['lm_weight'],
+            penalty=self.cmd['penalty'],
+            log_level=self.cmd['log_level'],
+            asr_train_config=self.cmd['asr_train_config'],
+            asr_model_file=self.cmd['asr_model_file'],
+            cmvn_file=self.cmd['cmvn_file'],
+            lm_file=self.cmd['lm_file'],
+            token_type=self.cmd['token_type'],
+            key_file=self.cmd['key_file'],
+            lm_train_config=self.cmd['lm_train_config'],
+            bpemodel=self.cmd['bpemodel'],
+            allow_variable_data_keys=self.cmd['allow_variable_data_keys'],
+            output_dir=self.cmd['output_dir'],
+            dtype=self.cmd['dtype'],
+            seed=self.cmd['seed'],
+            ngram_weight=self.cmd['ngram_weight'],
+            nbest=self.cmd['nbest'],
+            num_workers=self.cmd['num_workers'],
+            vad_infer_config=self.cmd['vad_infer_config'],
+            vad_model_file=self.cmd['vad_model_file'],
+            vad_cmvn_file=self.cmd['vad_cmvn_file'],
+            punc_model_file=self.cmd['punc_model_file'],
+            punc_infer_config=self.cmd['punc_infer_config'],
+            timestamp_model_file=self.cmd['timestamp_model_file'],
+            timestamp_infer_config=self.cmd['timestamp_infer_config'],
+            timestamp_cmvn_file=self.cmd['timestamp_cmvn_file'],
+            outputs_dict=self.cmd['outputs_dict'],
+            param_dict=self.cmd['param_dict'],
+            token_num_relax=self.cmd['token_num_relax'],
+            decoding_ind=self.cmd['decoding_ind'],
+            decoding_mode=self.cmd['decoding_mode'],
+            **kwargs,
+        )
 
     def __call__(self,
                  audio_in: Union[str, bytes],
                  audio_fs: int = None,
                  recog_type: str = None,
                  audio_format: str = None,
                  output_dir: str = None,
@@ -193,49 +194,39 @@
             A dictionary of result or a list of dictionary of result.
 
             The dictionary contain the following keys:
             - **text** ('str') --The asr result.
         """
 
         # code base
-        code_base = self.cmd['code_base']
+        # code_base = self.cmd['code_base']
         self.recog_type = recog_type
         self.audio_format = audio_format
         self.audio_fs = None
         checking_audio_fs = None
         self.raw_inputs = None
         if output_dir is not None:
             self.cmd['output_dir'] = output_dir
         self.cmd['param_dict'] = param_dict
 
-        if code_base == 'funasr':
-            if isinstance(audio_in, str):
-                # for funasr code, generate wav.scp from url or local path
-                self.audio_in, self.raw_inputs = generate_scp_from_url(
-                    audio_in)
-            elif isinstance(audio_in, bytes):
-                self.audio_in = audio_in
-                self.raw_inputs = None
-            else:
-                import numpy
-                import torch
-                if isinstance(audio_in, torch.Tensor):
-                    self.audio_in = None
-                    self.raw_inputs = audio_in
-                elif isinstance(audio_in, numpy.ndarray):
-                    self.audio_in = None
-                    self.raw_inputs = audio_in
-        elif isinstance(audio_in, str):
-            # load pcm data from url if audio_in is url str
-            self.audio_in, checking_audio_fs = load_bytes_from_url(audio_in)
+        if isinstance(audio_in, str):
+            # for funasr code, generate wav.scp from url or local path
+            self.audio_in, self.raw_inputs = generate_scp_from_url(audio_in)
         elif isinstance(audio_in, bytes):
-            # load pcm data from wav data if audio_in is wave format
-            self.audio_in, checking_audio_fs = extract_pcm_from_wav(audio_in)
-        else:
             self.audio_in = audio_in
+            self.raw_inputs = None
+        else:
+            import numpy
+            import torch
+            if isinstance(audio_in, torch.Tensor):
+                self.audio_in = None
+                self.raw_inputs = audio_in
+            elif isinstance(audio_in, numpy.ndarray):
+                self.audio_in = None
+                self.raw_inputs = audio_in
 
         # set the sample_rate of audio_in if checking_audio_fs is valid
         if checking_audio_fs is not None:
             self.audio_fs = checking_audio_fs
 
         if recog_type is None or audio_format is None:
             self.recog_type, self.audio_format, self.audio_in = asr_utils.type_checking(
@@ -313,117 +304,96 @@
             'mode': outputs['mode'],
             'fs': {
                 'model_fs': None,
                 'audio_fs': None
             }
         }
 
-        if self.framework == Frameworks.torch:
-            frontend_conf = None
-            token_num_relax = None
-            decoding_ind = None
-            decoding_mode = None
-            if os.path.exists(outputs['am_model_config']):
-                config_file = open(
-                    outputs['am_model_config'], encoding='utf-8')
-                root = yaml.full_load(config_file)
-                config_file.close()
-                if 'frontend_conf' in root:
-                    frontend_conf = root['frontend_conf']
-            if os.path.exists(outputs['asr_model_config']):
-                config_file = open(
-                    outputs['asr_model_config'], encoding='utf-8')
-                root = yaml.full_load(config_file)
-                config_file.close()
-                if 'token_num_relax' in root:
-                    token_num_relax = root['token_num_relax']
-                if 'decoding_ind' in root:
-                    decoding_ind = root['decoding_ind']
-                if 'decoding_mode' in root:
-                    decoding_mode = root['decoding_mode']
-
-                cmd['beam_size'] = root['beam_size']
-                cmd['penalty'] = root['penalty']
-                cmd['maxlenratio'] = root['maxlenratio']
-                cmd['minlenratio'] = root['minlenratio']
-                cmd['ctc_weight'] = root['ctc_weight']
-                cmd['lm_weight'] = root['lm_weight']
-            cmd['asr_train_config'] = outputs['am_model_config']
-            cmd['lm_file'] = outputs['lm_model_path']
-            cmd['lm_train_config'] = outputs['lm_model_config']
-            cmd['batch_size'] = outputs['model_config']['batch_size']
-            cmd['frontend_conf'] = frontend_conf
-            if frontend_conf is not None and 'fs' in frontend_conf:
-                cmd['fs']['model_fs'] = frontend_conf['fs']
-            cmd['token_num_relax'] = token_num_relax
-            cmd['decoding_ind'] = decoding_ind
-            cmd['decoding_mode'] = decoding_mode
-            if outputs.__contains__('mvn_file'):
-                cmd['cmvn_file'] = outputs['mvn_file']
-            model_config = self.model_cfg['model_config']
-            if model_config.__contains__('vad_model') and self.vad_model != '':
-                self.vad_model = model_config['vad_model']
-            if model_config.__contains__('vad_model_revision'):
-                self.vad_model_revision = model_config['vad_model_revision']
-            if model_config.__contains__(
-                    'punc_model') and self.punc_model != '':
-                self.punc_model = model_config['punc_model']
-            if model_config.__contains__('punc_model_revision'):
-                self.punc_model_revision = model_config['punc_model_revision']
-            if model_config.__contains__(
-                    'timestamp_model') and self.timestamp_model != '':
-                self.timestamp_model = model_config['timestamp_model']
-            if model_config.__contains__('timestamp_model_revision'):
-                self.timestamp_model_revision = model_config[
-                    'timestamp_model_revision']
-            update_local_model(model_config, model_path, extra_args)
-            self.load_vad_model(cmd)
-            self.load_punc_model(cmd)
-            self.load_lm_model(cmd)
-            self.load_timestamp_model(cmd)
-
-            user_args_dict = [
-                'output_dir',
-                'batch_size',
-                'mode',
-                'ngpu',
-                'beam_size',
-                'ctc_weight',
-                'lm_weight',
-                'decoding_ind',
-                'decoding_mode',
-                'vad_model_file',
-                'vad_infer_config',
-                'vad_cmvn_file',
-                'punc_model_file',
-                'punc_infer_config',
-                'param_dict',
-            ]
-
-            for user_args in user_args_dict:
-                if user_args in extra_args and extra_args[
-                        user_args] is not None:
-                    cmd[user_args] = extra_args[user_args]
-
-        elif self.framework == Frameworks.tf:
-            cmd['fs']['model_fs'] = outputs['model_config']['fs']
-            cmd['hop_length'] = outputs['model_config']['hop_length']
-            cmd['feature_dims'] = outputs['model_config']['feature_dims']
-            cmd['predictions_file'] = 'text'
-            cmd['cmvn_file'] = outputs['am_mvn_file']
-            cmd['vocab_file'] = outputs['vocab_file']
-            if 'idx_text' in outputs:
-                cmd['idx_text'] = outputs['idx_text']
-            if 'sampled_ids' in outputs['model_config']:
-                cmd['sampled_ids'] = outputs['model_config']['sampled_ids']
-            if 'sampled_lengths' in outputs['model_config']:
-                cmd['sampled_lengths'] = outputs['model_config'][
-                    'sampled_lengths']
-        else:
-            raise ValueError('model type is mismatching')
+        frontend_conf = None
+        token_num_relax = None
+        decoding_ind = None
+        decoding_mode = None
+        if os.path.exists(outputs['am_model_config']):
+            config_file = open(outputs['am_model_config'], encoding='utf-8')
+            root = yaml.full_load(config_file)
+            config_file.close()
+            if 'frontend_conf' in root:
+                frontend_conf = root['frontend_conf']
+        if os.path.exists(outputs['asr_model_config']):
+            config_file = open(outputs['asr_model_config'], encoding='utf-8')
+            root = yaml.full_load(config_file)
+            config_file.close()
+            if 'token_num_relax' in root:
+                token_num_relax = root['token_num_relax']
+            if 'decoding_ind' in root:
+                decoding_ind = root['decoding_ind']
+            if 'decoding_mode' in root:
+                decoding_mode = root['decoding_mode']
+
+            cmd['beam_size'] = root['beam_size']
+            cmd['penalty'] = root['penalty']
+            cmd['maxlenratio'] = root['maxlenratio']
+            cmd['minlenratio'] = root['minlenratio']
+            cmd['ctc_weight'] = root['ctc_weight']
+            cmd['lm_weight'] = root['lm_weight']
+        cmd['asr_train_config'] = outputs['am_model_config']
+        cmd['lm_file'] = outputs['lm_model_path']
+        cmd['lm_train_config'] = outputs['lm_model_config']
+        cmd['batch_size'] = outputs['model_config']['batch_size']
+        cmd['frontend_conf'] = frontend_conf
+        if frontend_conf is not None and 'fs' in frontend_conf:
+            cmd['fs']['model_fs'] = frontend_conf['fs']
+        cmd['token_num_relax'] = token_num_relax
+        cmd['decoding_ind'] = decoding_ind
+        cmd['decoding_mode'] = decoding_mode
+        if outputs.__contains__('mvn_file'):
+            cmd['cmvn_file'] = outputs['mvn_file']
+        model_config = self.model_cfg['model_config']
+        if model_config.__contains__('vad_model') and self.vad_model != '':
+            self.vad_model = model_config['vad_model']
+        if model_config.__contains__('vad_model_revision'):
+            self.vad_model_revision = model_config['vad_model_revision']
+        if model_config.__contains__('punc_model') and self.punc_model != '':
+            self.punc_model = model_config['punc_model']
+        if model_config.__contains__('punc_model_revision'):
+            self.punc_model_revision = model_config['punc_model_revision']
+        if model_config.__contains__(
+                'timestamp_model') and self.timestamp_model != '':
+            self.timestamp_model = model_config['timestamp_model']
+        if model_config.__contains__('timestamp_model_revision'):
+            self.timestamp_model_revision = model_config[
+                'timestamp_model_revision']
+        update_local_model(model_config, model_path, extra_args)
+        self.load_vad_model(cmd)
+        self.load_punc_model(cmd)
+        self.load_lm_model(cmd)
+        self.load_timestamp_model(cmd)
+
+        user_args_dict = [
+            'output_dir',
+            'batch_size',
+            'mode',
+            'ngpu',
+            'beam_size',
+            'ctc_weight',
+            'lm_weight',
+            'decoding_ind',
+            'decoding_mode',
+            'vad_model_file',
+            'vad_infer_config',
+            'vad_cmvn_file',
+            'punc_model_file',
+            'punc_infer_config',
+            'param_dict',
+        ]
+
+        for user_args in user_args_dict:
+            if user_args in extra_args and extra_args[user_args] is not None:
+                cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def load_vad_model(self, cmd):
         if self.vad_model is not None and self.vad_model != '':
             if os.path.exists(self.vad_model):
                 vad_model = self.vad_model
@@ -510,31 +480,20 @@
     def forward(self, inputs: Dict[str, Any], **kwargs) -> Dict[str, Any]:
         """Decoding
         """
 
         logger.info(f"Decoding with {inputs['audio_format']} files ...")
 
         data_cmd: Sequence[Tuple[str, str, str]]
-        if self.cmd['code_base'] == 'funasr':
-            if isinstance(self.audio_in, bytes):
-                data_cmd = [self.audio_in, 'speech', 'bytes']
-            elif isinstance(self.audio_in, str):
-                data_cmd = [self.audio_in, 'speech', 'sound']
-            elif self.raw_inputs is not None:
-                data_cmd = None
-        else:
-            if inputs['audio_format'] == 'wav' or inputs[
-                    'audio_format'] == 'pcm':
-                data_cmd = ['speech', 'sound']
-            elif inputs['audio_format'] == 'kaldi_ark':
-                data_cmd = ['speech', 'kaldi_ark']
-            elif inputs['audio_format'] == 'tfrecord':
-                data_cmd = ['speech', 'tfrecord']
-            if inputs.__contains__('mvn_file'):
-                data_cmd.append(inputs['mvn_file'])
+        if isinstance(self.audio_in, bytes):
+            data_cmd = [self.audio_in, 'speech', 'bytes']
+        elif isinstance(self.audio_in, str):
+            data_cmd = [self.audio_in, 'speech', 'sound']
+        elif self.raw_inputs is not None:
+            data_cmd = None
 
         # generate asr inference command
         self.cmd['name_and_type'] = data_cmd
         self.cmd['raw_inputs'] = self.raw_inputs
         self.cmd['audio_in'] = self.audio_in
 
         inputs['asr_result'] = self.run_inference(self.cmd, **kwargs)
@@ -608,38 +567,13 @@
                         'value': line_item[1].strip('\n')
                     }
                     ref_list.append(item)
 
         return ref_list
 
     def run_inference(self, cmd, **kwargs):
-        asr_result = []
-        if self.framework == Frameworks.torch and cmd['code_base'] == 'funasr':
-            asr_result = self.funasr_infer_modelscope(
-                cmd['name_and_type'], cmd['raw_inputs'], cmd['output_dir'],
-                cmd['fs'], cmd['param_dict'], **kwargs)
-
-        elif self.framework == Frameworks.tf:
-            from easyasr import asr_inference_paraformer_tf
-            if hasattr(asr_inference_paraformer_tf, 'set_parameters'):
-                asr_inference_paraformer_tf.set_parameters(
-                    language=cmd['lang'])
-            else:
-                # in order to support easyasr-0.0.2
-                cmd['fs'] = cmd['fs']['model_fs']
-
-            asr_result = asr_inference_paraformer_tf.asr_inference(
-                ngpu=cmd['ngpu'],
-                name_and_type=cmd['name_and_type'],
-                audio_lists=cmd['audio_in'],
-                idx_text_file=cmd['idx_text'],
-                asr_model_file=cmd['asr_model_file'],
-                vocab_file=cmd['vocab_file'],
-                am_mvn_file=cmd['cmvn_file'],
-                predictions_file=cmd['predictions_file'],
-                fs=cmd['fs'],
-                hop_length=cmd['hop_length'],
-                feature_dims=cmd['feature_dims'],
-                sampled_ids=cmd['sampled_ids'],
-                sampled_lengths=cmd['sampled_lengths'])
+        asr_result = self.funasr_infer_modelscope(cmd['name_and_type'],
+                                                  cmd['raw_inputs'],
+                                                  cmd['output_dir'], cmd['fs'],
+                                                  cmd['param_dict'], **kwargs)
 
         return asr_result
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/asr_wenet_inference_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/inverse_text_processing_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/inverse_text_processing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/kws_farfield_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/kws_farfield_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/kws_kwsbp_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/kws_kwsbp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/linear_aec_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/linear_aec_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/lm_infer_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/lm_infer_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,15 +31,18 @@
     >>>    task=Tasks.language_score_prediction,
     >>>    model='damo/speech_transformer_lm_zh-cn-common-vocab8404-pytorch')
     >>> text_in='hello    '
     >>> print(inference_pipeline(text_in))
 
     """
 
-    def __init__(self, model: Union[Model, str] = None, **kwargs):
+    def __init__(self,
+                 model: Union[Model, str] = None,
+                 ngpu: int = 1,
+                 **kwargs):
         """
         Use `model` to create a LM pipeline for prediction
         Args:
             model ('Model' or 'str'):
                 The pipeline handles three types of model:
 
                 - A model instance
@@ -84,15 +87,17 @@
             key_file=self.cmd['key_file'],
             train_config=self.cmd['train_config'],
             model_file=self.cmd['model_file'],
             log_base=self.cmd['log_base'],
             split_with_space=self.cmd['split_with_space'],
             seg_dict_file=self.cmd['seg_dict_file'],
             output_dir=self.cmd['output_dir'],
-            param_dict=self.cmd['param_dict'])
+            param_dict=self.cmd['param_dict'],
+            **kwargs,
+        )
 
     def __call__(self,
                  text_in: str = None,
                  output_dir: str = None,
                  param_dict: dict = None) -> Dict[str, Any]:
         """
         Compute PPL
@@ -185,14 +190,15 @@
             'output_dir',
             'param_dict',
         ]
 
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def forward(self, text_in: str = None) -> list:
         """Decoding
         """
         logger.info('Compute PPL : {0} ...'.format(text_in))
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/punctuation_processing_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/punctuation_processing_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,15 +35,18 @@
     >>> pipeline_punc = pipeline(
     >>>    task=Tasks.punctuation, model='damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch')
     >>> text_in='https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_text/punc_example.txt'
     >>> print(pipeline_punc(text_in))
 
     """
 
-    def __init__(self, model: Union[Model, str] = None, **kwargs):
+    def __init__(self,
+                 model: Union[Model, str] = None,
+                 ngpu: int = 1,
+                 **kwargs):
         """use `model` to create an asr pipeline for prediction
         """
         super().__init__(model=model, **kwargs)
         self.model_cfg = self.model.forward()
         self.cmd = self.get_cmd(kwargs, model)
 
         from funasr.bin import punc_inference_launch
@@ -55,15 +58,17 @@
             seed=self.cmd['seed'],
             num_workers=self.cmd['num_workers'],
             log_level=self.cmd['log_level'],
             key_file=self.cmd['key_file'],
             train_config=self.cmd['train_config'],
             model_file=self.cmd['model_file'],
             output_dir=self.cmd['output_dir'],
-            param_dict=self.cmd['param_dict'])
+            param_dict=self.cmd['param_dict'],
+            **kwargs,
+        )
 
     def __call__(self,
                  text_in: str = None,
                  output_dir: str = None,
                  cache: List[Any] = None,
                  param_dict: dict = None) -> Dict[str, Any]:
         if len(text_in) == 0:
@@ -137,14 +142,15 @@
             'lang',
             'param_dict',
         ]
 
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def forward(self, text_in: str = None) -> list:
         """Decoding
         """
         logger.info('Punctuation Processing: {0} ...'.format(text_in))
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/separation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/separation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/speaker_diarization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/speaker_diarization_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,14 +44,15 @@
 
     """
 
     def __init__(self,
                  model: Union[Model, str] = None,
                  sv_model: Optional[Union[Model, str]] = None,
                  sv_model_revision: Optional[str] = None,
+                 ngpu: int = 1,
                  **kwargs):
         """use `model` to create a speaker diarization pipeline for prediction
         Args:
             model ('Model' or 'str'):
                 The pipeline handles three types of model:
 
                 - A model instance
@@ -86,14 +87,15 @@
             model_tag=self.cmd['model_tag'],
             allow_variable_data_keys=self.cmd['allow_variable_data_keys'],
             streaming=self.cmd['streaming'],
             smooth_size=self.cmd['smooth_size'],
             dur_threshold=self.cmd['dur_threshold'],
             out_format=self.cmd['out_format'],
             param_dict=self.cmd['param_dict'],
+            **kwargs,
         )
 
     def __call__(self,
                  audio_in: Union[tuple, str, Any] = None,
                  output_dir: str = None,
                  param_dict: dict = None) -> Dict[str, Any]:
         """
@@ -199,14 +201,15 @@
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 if isinstance(cmd[user_args], dict) and isinstance(
                         extra_args[user_args], dict):
                     cmd[user_args].update(extra_args[user_args])
                 else:
                     cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def load_sv_model(self, cmd):
         if self.sv_model is not None and self.sv_model != '':
             if os.path.exists(self.sv_model):
                 sv_model = self.sv_model
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/speaker_verification_light_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/speaker_verification_light_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/speaker_verification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/speaker_verification_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,15 +37,18 @@
         >>>    task=Tasks.speaker_verification, model='damo/speech_xvector_sv-zh-cn-cnceleb-16k-spk3465-pytorch')
         >>> audio_in=('sv_example_enroll.wav', 'sv_example_same.wav')
         >>> print(pipeline_sv(audio_in))
         >>> # {'label': ['Same', 'Different'], 'scores': [0.8540488358969999, 0.14595116410300013]}
 
     """
 
-    def __init__(self, model: Union[Model, str] = None, **kwargs):
+    def __init__(self,
+                 model: Union[Model, str] = None,
+                 ngpu: int = 1,
+                 **kwargs):
         """use `model` to create an asr pipeline for prediction
         """
         super().__init__(model=model, **kwargs)
         self.model_cfg = self.model.forward()
         self.cmd = self.get_cmd(kwargs, model)
 
         from funasr.bin import sv_inference_launch
@@ -63,14 +66,15 @@
             sv_model_file=self.cmd['sv_model_file'],
             model_tag=self.cmd['model_tag'],
             allow_variable_data_keys=self.cmd['allow_variable_data_keys'],
             streaming=self.cmd['streaming'],
             embedding_node=self.cmd['embedding_node'],
             sv_threshold=self.cmd['sv_threshold'],
             param_dict=self.cmd['param_dict'],
+            **kwargs,
         )
 
     def __call__(self,
                  audio_in: Union[tuple, str, Any] = None,
                  output_dir: str = None,
                  param_dict: dict = None) -> Dict[str, Any]:
         if len(audio_in) == 0:
@@ -164,14 +168,15 @@
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 if isinstance(cmd[user_args], dict) and isinstance(
                         extra_args[user_args], dict):
                     cmd[user_args].update(extra_args[user_args])
                 else:
                     cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def forward(self, audio_in: Union[tuple, str, Any] = None) -> list:
         """Decoding
         """
         logger.info(
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/text_to_speech_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/text_to_speech_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/timestamp_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/timestamp_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,15 +36,18 @@
 
     >>> audio_in='https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_timestamps.wav'
     >>> text_in='                   '
     >>> print(pipeline_infer(audio_in, text_in))
 
     """
 
-    def __init__(self, model: Union[Model, str] = None, **kwargs):
+    def __init__(self,
+                 model: Union[Model, str] = None,
+                 ngpu: int = 1,
+                 **kwargs):
         """
         Use `model` and `preprocessor` to create an asr pipeline for prediction
         Args:
             model ('Model' or 'str'):
                 The pipeline handles three types of model:
 
                 - A model instance
@@ -80,15 +83,17 @@
             timestamp_infer_config=self.cmd['timestamp_infer_config'],
             timestamp_model_file=self.cmd['timestamp_model_file'],
             timestamp_cmvn_file=self.cmd['timestamp_cmvn_file'],
             output_dir=self.cmd['output_dir'],
             allow_variable_data_keys=self.cmd['allow_variable_data_keys'],
             split_with_space=self.cmd['split_with_space'],
             seg_dict_file=self.cmd['seg_dict_file'],
-            param_dict=self.cmd['param_dict'])
+            param_dict=self.cmd['param_dict'],
+            **kwargs,
+        )
 
     def __call__(self,
                  audio_in: Union[str, bytes],
                  text_in: str = None,
                  audio_fs: int = None,
                  recog_type: str = None,
                  audio_format: str = None,
@@ -260,14 +265,15 @@
             'split_with_space',
             'seg_dict_file',
         ]
 
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def forward(self, audio_in: Dict[str, Any], text_in: Dict[str, Any],
                 **kwargs) -> Dict[str, Any]:
         """Decoding
         """
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/audio/voice_activity_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/audio/voice_activity_detection_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,15 +37,18 @@
         >>> pipeline_vad = pipeline(
         >>>    task=Tasks.voice_activity_detection, model='damo/speech_fsmn_vad_zh-cn-16k-common-pytorch')
         >>> audio_in='https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.pcm'
         >>> print(pipeline_vad(audio_in))
 
     """
 
-    def __init__(self, model: Union[Model, str] = None, **kwargs):
+    def __init__(self,
+                 model: Union[Model, str] = None,
+                 ngpu: int = 1,
+                 **kwargs):
         """use `model` to create an vad pipeline for prediction
         """
         super().__init__(model=model, **kwargs)
         config_path = os.path.join(model, ModelFile.CONFIGURATION)
         self.cmd = self.get_cmd(config_path, kwargs, model)
 
         from funasr.bin import vad_inference_launch
@@ -56,15 +59,17 @@
             ngpu=self.cmd['ngpu'],
             seed=self.cmd['seed'],
             num_workers=self.cmd['num_workers'],
             log_level=self.cmd['log_level'],
             key_file=self.cmd['key_file'],
             vad_infer_config=self.cmd['vad_infer_config'],
             vad_model_file=self.cmd['vad_model_file'],
-            vad_cmvn_file=self.cmd['vad_cmvn_file'])
+            vad_cmvn_file=self.cmd['vad_cmvn_file'],
+            **kwargs,
+        )
 
     def __call__(self,
                  audio_in: Union[str, bytes],
                  audio_fs: int = None,
                  recog_type: str = None,
                  audio_format: str = None,
                  output_dir: str = None,
@@ -205,14 +210,15 @@
             'output_dir', 'batch_size', 'mode', 'ngpu', 'param_dict',
             'num_workers', 'fs'
         ]
 
         for user_args in user_args_dict:
             if user_args in extra_args and extra_args[user_args] is not None:
                 cmd[user_args] = extra_args[user_args]
+                del extra_args[user_args]
 
         return cmd
 
     def forward(self, audio_in: Dict[str, Any], **kwargs) -> Dict[str, Any]:
         """Decoding
         """
         logger.info('VAD Processing ...')
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/base.py` & `modelscope-1.6.0/modelscope/pipelines/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -291,15 +291,24 @@
                     batched_out = self._batch(preprocessed_list)
                     batched_out = self.forward(batched_out, **forward_params)
 
             for batch_idx in range(real_batch_size):
                 out = {}
                 for k, element in batched_out.items():
                     if element is not None:
-                        out[k] = element[batch_idx]
+                        if isinstance(element, (tuple, list)):
+                            if isinstance(element[0], torch.Tensor):
+                                out[k] = type(element)(
+                                    e[batch_idx:batch_idx + 1]
+                                    for e in element)
+                            else:
+                                # Compatible with traditional pipelines
+                                out[k] = element[batch_idx]
+                        else:
+                            out[k] = element[batch_idx:batch_idx + 1]
                 out = self.postprocess(out, **postprocess_params)
                 self._check_output(out)
                 output_list.append(out)
 
         return output_list
 
     def _check_input(self, input):
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/builder.py` & `modelscope-1.6.0/modelscope/pipelines/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/cv/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,15 +5,14 @@
 
 if TYPE_CHECKING:
     from .action_recognition_pipeline import ActionRecognitionPipeline
     from .action_detection_pipeline import ActionDetectionPipeline
     from .animal_recognition_pipeline import AnimalRecognitionPipeline
     from .body_2d_keypoints_pipeline import Body2DKeypointsPipeline
     from .body_3d_keypoints_pipeline import Body3DKeypointsPipeline
-    from .hand_2d_keypoints_pipeline import Hand2DKeypointsPipeline
     from .cmdssl_video_embedding_pipeline import CMDSSLVideoEmbeddingPipeline
     from .card_detection_pipeline import CardDetectionPipeline
     from .hicossl_video_embedding_pipeline import HICOSSLVideoEmbeddingPipeline
     from .crowd_counting_pipeline import CrowdCountingPipeline
     from .image_detection_pipeline import ImageDetectionPipeline
     from .image_salient_detection_pipeline import ImageSalientDetectionPipeline
     from .face_detection_pipeline import FaceDetectionPipeline
@@ -25,48 +24,40 @@
     from .face_recognition_onnx_ir_pipeline import FaceRecognitionOnnxIrPipeline
     from .face_recognition_onnx_fm_pipeline import FaceRecognitionOnnxFmPipeline
     from .general_recognition_pipeline import GeneralRecognitionPipeline
     from .image_cartoon_pipeline import ImageCartoonPipeline
     from .image_classification_pipeline import GeneralImageClassificationPipeline
     from .image_color_enhance_pipeline import ImageColorEnhancePipeline
     from .image_colorization_pipeline import ImageColorizationPipeline
-    from .image_classification_pipeline import ImageClassificationPipeline
     from .image_denoise_pipeline import ImageDenoisePipeline
     from .image_deblur_pipeline import ImageDeblurPipeline
     from .image_instance_segmentation_pipeline import ImageInstanceSegmentationPipeline
     from .image_matting_pipeline import ImageMattingPipeline
-    from .image_panoptic_segmentation_pipeline import ImagePanopticSegmentationPipeline
-    from .image_semantic_segmentation_pipeline import ImagePanopticSegmentationEasyCVPipeline
     from .image_portrait_enhancement_pipeline import ImagePortraitEnhancementPipeline
     from .image_reid_person_pipeline import ImageReidPersonPipeline
     from .image_semantic_segmentation_pipeline import ImageSemanticSegmentationPipeline
     from .image_style_transfer_pipeline import ImageStyleTransferPipeline
     from .image_super_resolution_pipeline import ImageSuperResolutionPipeline
     from .image_to_image_generate_pipeline import Image2ImageGenerationPipeline
     from .image_to_image_translation_pipeline import Image2ImageTranslationPipeline
     from .image_inpainting_pipeline import ImageInpaintingPipeline
     from .image_paintbyexample_pipeline import ImagePaintbyexamplePipeline
     from .product_retrieval_embedding_pipeline import ProductRetrievalEmbeddingPipeline
-    from .realtime_object_detection_pipeline import RealtimeObjectDetectionPipeline
     from .live_category_pipeline import LiveCategoryPipeline
     from .ocr_detection_pipeline import OCRDetectionPipeline
     from .ocr_recognition_pipeline import OCRRecognitionPipeline
     from .license_plate_detection_pipeline import LicensePlateDetectionPipeline
     from .table_recognition_pipeline import TableRecognitionPipeline
     from .lineless_table_recognition_pipeline import LinelessTableRecognitionPipeline
     from .skin_retouching_pipeline import SkinRetouchingPipeline
     from .face_reconstruction_pipeline import FaceReconstructionPipeline
     from .tinynas_classification_pipeline import TinynasClassificationPipeline
     from .video_category_pipeline import VideoCategoryPipeline
     from .virtual_try_on_pipeline import VirtualTryonPipeline
     from .shop_segmentation_pipleline import ShopSegmentationPipeline
-    from .easycv_pipelines import (EasyCVDetectionPipeline,
-                                   EasyCVSegmentationPipeline,
-                                   Face2DKeypointsPipeline,
-                                   HumanWholebodyKeypointsPipeline)
     from .text_driven_segmentation_pipleline import TextDrivenSegmentationPipeline
     from .movie_scene_segmentation_pipeline import MovieSceneSegmentationPipeline
     from .mog_face_detection_pipeline import MogFaceDetectionPipeline
     from .ulfd_face_detection_pipeline import UlfdFaceDetectionPipeline
     from .retina_face_detection_pipeline import RetinaFaceDetectionPipeline
     from .facial_expression_recognition_pipeline import FacialExpressionRecognitionPipeline
     from .facial_landmark_confidence_pipeline import FacialLandmarkConfidencePipeline
@@ -119,15 +110,14 @@
 else:
     _import_structure = {
         'action_recognition_pipeline': ['ActionRecognitionPipeline'],
         'action_detection_pipeline': ['ActionDetectionPipeline'],
         'animal_recognition_pipeline': ['AnimalRecognitionPipeline'],
         'body_2d_keypoints_pipeline': ['Body2DKeypointsPipeline'],
         'body_3d_keypoints_pipeline': ['Body3DKeypointsPipeline'],
-        'hand_2d_keypoints_pipeline': ['Hand2DKeypointsPipeline'],
         'card_detection_pipeline': ['CardDetectionPipeline'],
         'cmdssl_video_embedding_pipeline': ['CMDSSLVideoEmbeddingPipeline'],
         'hicossl_video_embedding_pipeline': ['HICOSSLVideoEmbeddingPipeline'],
         'crowd_counting_pipeline': ['CrowdCountingPipeline'],
         'image_detection_pipeline': ['ImageDetectionPipeline'],
         'image_salient_detection_pipeline': ['ImageSalientDetectionPipeline'],
         'face_detection_pipeline': ['FaceDetectionPipeline'],
@@ -136,60 +126,48 @@
         'face_recognition_ood_pipeline': ['FaceRecognitionOodPipeline'],
         'arc_face_recognition_pipeline': ['ArcFaceRecognitionPipeline'],
         'mask_face_recognition_pipeline': ['MaskFaceRecognitionPipeline'],
         'face_recognition_onnx_ir_pipeline': ['FaceRecognitionOnnxIrPipeline'],
         'face_recognition_onnx_fm_pipeline': ['FaceRecognitionOnnxFmPipeline'],
         'general_recognition_pipeline': ['GeneralRecognitionPipeline'],
         'image_classification_pipeline':
-        ['GeneralImageClassificationPipeline', 'ImageClassificationPipeline'],
+        ['GeneralImageClassificationPipeline'],
         'image_cartoon_pipeline': ['ImageCartoonPipeline'],
         'image_denoise_pipeline': ['ImageDenoisePipeline'],
         'image_deblur_pipeline': ['ImageDeblurPipeline'],
         'image_color_enhance_pipeline': ['ImageColorEnhancePipeline'],
         'image_colorization_pipeline': ['ImageColorizationPipeline'],
         'image_instance_segmentation_pipeline':
         ['ImageInstanceSegmentationPipeline'],
         'image_matting_pipeline': ['ImageMattingPipeline'],
-        'image_panoptic_segmentation_pipeline': [
-            'ImagePanopticSegmentationPipeline',
-            'ImagePanopticSegmentationEasyCVPipeline'
-        ],
         'image_portrait_enhancement_pipeline':
         ['ImagePortraitEnhancementPipeline'],
         'image_reid_person_pipeline': ['ImageReidPersonPipeline'],
         'image_semantic_segmentation_pipeline':
         ['ImageSemanticSegmentationPipeline'],
         'image_style_transfer_pipeline': ['ImageStyleTransferPipeline'],
         'image_super_resolution_pipeline': ['ImageSuperResolutionPipeline'],
         'image_to_image_translation_pipeline':
         ['Image2ImageTranslationPipeline'],
         'product_retrieval_embedding_pipeline':
         ['ProductRetrievalEmbeddingPipeline'],
-        'realtime_object_detection_pipeline':
-        ['RealtimeObjectDetectionPipeline'],
         'live_category_pipeline': ['LiveCategoryPipeline'],
         'image_to_image_generate_pipeline': ['Image2ImageGenerationPipeline'],
         'image_inpainting_pipeline': ['ImageInpaintingPipeline'],
         'image_paintbyexample_pipeline': ['ImagePaintbyexamplePipeline'],
         'ocr_detection_pipeline': ['OCRDetectionPipeline'],
         'ocr_recognition_pipeline': ['OCRRecognitionPipeline'],
         'license_plate_detection_pipeline': ['LicensePlateDetectionPipeline'],
         'table_recognition_pipeline': ['TableRecognitionPipeline'],
         'skin_retouching_pipeline': ['SkinRetouchingPipeline'],
         'face_reconstruction_pipeline': ['FaceReconstructionPipeline'],
         'tinynas_classification_pipeline': ['TinynasClassificationPipeline'],
         'video_category_pipeline': ['VideoCategoryPipeline'],
         'virtual_try_on_pipeline': ['VirtualTryonPipeline'],
         'shop_segmentation_pipleline': ['ShopSegmentationPipeline'],
-        'easycv_pipelines': [
-            'EasyCVDetectionPipeline',
-            'EasyCVSegmentationPipeline',
-            'Face2DKeypointsPipeline',
-            'HumanWholebodyKeypointsPipeline',
-        ],
         'text_driven_segmentation_pipleline':
         ['TextDrivenSegmentationPipeline'],
         'movie_scene_segmentation_pipeline':
         ['MovieSceneSegmentationPipeline'],
         'mog_face_detection_pipeline': ['MogFaceDetectionPipeline'],
         'ulfd_face_detection_pipeline': ['UlfdFaceDetectionPipeline'],
         'retina_face_detection_pipeline': ['RetinaFaceDetectionPipeline'],
@@ -198,17 +176,16 @@
         'facial_landmark_confidence_pipeline':
         ['FacialLandmarkConfidencePipeline'],
         'face_processing_base_pipeline': ['FaceProcessingBasePipeline'],
         'face_attribute_recognition_pipeline':
         ['FaceAttributeRecognitionPipeline'],
         'mtcnn_face_detection_pipeline': ['MtcnnFaceDetectionPipeline'],
         'hand_static_pipeline': ['HandStaticPipeline'],
-        'referring_video_object_segmentation_pipeline': [
-            'ReferringVideoObjectSegmentationPipeline'
-        ],
+        'referring_video_object_segmentation_pipeline':
+        ['ReferringVideoObjectSegmentationPipeline'],
         'language_guided_video_summarization_pipeline': [
             'LanguageGuidedVideoSummarizationPipeline'
         ],
         'vision_efficient_tuning_adapter_pipeline': [
             'VisionEfficientTuningAdapterPipeline'
         ],
         'vision_efficient_tuning_prompt_pipeline': [
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/action_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/action_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/action_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/action_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/animal_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/animal_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/arc_face_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/arc_face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/bad_image_detecting_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/bad_image_detecting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/body_2d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/body_3d_keypoints_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/card_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/card_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/cmdssl_video_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/content_check_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/content_check_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/controllable_image_generation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/controllable_image_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/crowd_counting_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/crowd_counting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ddcolor_image_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ddpm_semantic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/base.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/canmt_translation.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,123 +1,109 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import glob
-import os
-import os.path as osp
-from typing import Any
 
-import numpy as np
-from easycv.utils.ms_utils import EasyCVMeta
-from PIL import ImageFile
+import os.path as osp
+from typing import Any, Dict
 
-from modelscope.hub.snapshot_download import snapshot_download
-from modelscope.pipelines.util import is_official_hub_path
+import jieba
+import torch
+from sacremoses import MosesDetokenizer, MosesPunctNormalizer, MosesTokenizer
+from subword_nmt import apply_bpe
+
+from modelscope.metainfo import Preprocessors
+from modelscope.preprocessors.base import Preprocessor
+from modelscope.preprocessors.builder import PREPROCESSORS
 from modelscope.utils.config import Config
-from modelscope.utils.constant import (DEFAULT_MODEL_REVISION, Invoke,
-                                       ModelFile, ThirdParty)
-from modelscope.utils.device import create_device
+from modelscope.utils.constant import Fields, ModelFile
+from .text_clean import TextClean
 
 
-class EasyCVPipeline(object):
-    """Base pipeline for EasyCV.
-    Loading configuration file of modelscope style by default,
-    but it is actually use the predictor api of easycv to predict.
-    So here we do some adaptation work for configuration and predict api.
+@PREPROCESSORS.register_module(
+    Fields.nlp, module_name=Preprocessors.canmt_translation)
+class CanmtTranslationPreprocessor(Preprocessor):
+    """The preprocessor used in text correction task.
     """
 
-    def __init__(self, model: str, model_file_pattern='*.pt', *args, **kwargs):
-        """
-            model (str): model id on modelscope hub or local model path.
-            model_file_pattern (str): model file pattern.
+    def __init__(self,
+                 model_dir: str,
+                 max_length: int = None,
+                 *args,
+                 **kwargs):
+        from fairseq.data import Dictionary
+        """preprocess the data via the vocab file from the `model_dir` path
 
+        Args:
+            model_dir (str): model path
         """
-        self.model_file_pattern = model_file_pattern
+        super().__init__(*args, **kwargs)
+        self.cfg = Config.from_file(
+            osp.join(model_dir, ModelFile.CONFIGURATION))
+        self.vocab_src = Dictionary.load(osp.join(model_dir, 'dict.src.txt'))
+        self.vocab_tgt = Dictionary.load(osp.join(model_dir, 'dict.tgt.txt'))
+        self.padding_value = self.vocab_src.pad()
+        self.max_length = max_length + 1 if max_length is not None else 129  # 1 is eos token
+
+        self.src_lang = self.cfg['preprocessor']['src_lang']
+        self.tgt_lang = self.cfg['preprocessor']['tgt_lang']
+        self.tc = TextClean()
 
-        assert isinstance(model, str)
-        if osp.exists(model):
-            model_dir = model
+        if self.src_lang == 'zh':
+            self.tok = jieba
         else:
-            assert is_official_hub_path(
-                model), 'Only support local model path and official hub path!'
-            model_dir = snapshot_download(
-                model_id=model,
-                revision=DEFAULT_MODEL_REVISION,
-                user_agent={
-                    Invoke.KEY: Invoke.PIPELINE,
-                    ThirdParty.KEY: ThirdParty.EASYCV
-                })
-
-        assert osp.isdir(model_dir)
-        model_files = glob.glob(
-            os.path.join(model_dir, self.model_file_pattern))
-        assert len(
-            model_files
-        ) == 1, f'Need one model file, but find {len(model_files)}: {model_files}'
-
-        model_path = model_files[0]
-        self.model_path = model_path
-        self.model_dir = model_dir
-
-        # get configuration file from source model dir
-        self.config_file = os.path.join(model_dir, ModelFile.CONFIGURATION)
-        assert os.path.exists(
-            self.config_file
-        ), f'Not find "{ModelFile.CONFIGURATION}" in model directory!'
-
-        self.cfg = Config.from_file(self.config_file)
-        if 'device' in kwargs:
-            kwargs['device'] = create_device(kwargs['device'])
-        if 'predictor_config' in kwargs:
-            kwargs.pop('predictor_config')
-        self.predict_op = self._build_predict_op(**kwargs)
-
-    def _build_predict_op(self, **kwargs):
-        """Build EasyCV predictor."""
-        from easycv.predictors.builder import build_predictor
-
-        easycv_config = self._to_easycv_config()
-        pipeline_op = build_predictor(self.cfg.pipeline.predictor_config, {
-            'model_path': self.model_path,
-            'config_file': easycv_config,
-            **kwargs
-        })
-        return pipeline_op
-
-    def _to_easycv_config(self):
-        """Adapt to EasyCV predictor."""
-        # TODO: refine config compatibility problems
-
-        easycv_arch = self.cfg.model.pop(EasyCVMeta.ARCH, None)
-        model_cfg = self.cfg.model
-        # Revert to the configuration of easycv
-        if easycv_arch is not None:
-            model_cfg.update(easycv_arch)
-
-        easycv_config = Config(dict(model=model_cfg))
-
-        reserved_keys = []
-        if hasattr(self.cfg, EasyCVMeta.META):
-            easycv_meta_cfg = getattr(self.cfg, EasyCVMeta.META)
-            reserved_keys = easycv_meta_cfg.get(EasyCVMeta.RESERVED_KEYS, [])
-            for key in reserved_keys:
-                easycv_config.merge_from_dict({key: getattr(self.cfg, key)})
-        if 'test_pipeline' not in reserved_keys:
-            easycv_config.merge_from_dict(
-                {'test_pipeline': self.cfg.dataset.val.get('pipeline', [])})
-
-        return easycv_config
-
-    def _is_single_inputs(self, inputs):
-        if isinstance(inputs, str) or (isinstance(inputs, list)
-                                       and len(inputs) == 1) or isinstance(
-                                           inputs, np.ndarray) or isinstance(
-                                               inputs, ImageFile.ImageFile):
-            return True
-
-        return False
+            self.punct_normalizer = MosesPunctNormalizer(lang=self.src_lang)
+            self.tok = MosesTokenizer(lang=self.src_lang)
 
-    def __call__(self, inputs) -> Any:
-        outputs = self.predict_op(inputs)
-
-        if self._is_single_inputs(inputs):
-            outputs = outputs[0]
+        self.src_bpe_path = osp.join(
+            model_dir, self.cfg['preprocessor']['src_bpe']['file'])
+        self.bpe = apply_bpe.BPE(open(self.src_bpe_path))
+
+    def __call__(self, input: str) -> Dict[str, Any]:
+        """process the raw input data
+
+        Args:
+            data (str): a sentence
+                Example:
+                    ''
+        Returns:
+            Dict[str, Any]: the preprocessed data
+            Example:
+            {'net_input':
+                {'src_tokens':tensor([1,2,3,4]),
+                'src_lengths': tensor([4])}
+            }
+        """
+        if self.src_lang == 'zh':
+            input = self.tc.clean(input)
+            input_tok = self.tok.cut(input)
+            input_tok = ' '.join(list(input_tok))
+        else:
+            input = [self._punct_normalizer.normalize(item) for item in input]
+            input_tok = [
+                self.tok.tokenize(
+                    item, return_str=True, aggressive_dash_splits=True)
+                for item in input
+            ]
+
+        input_bpe = self.bpe.process_line(input_tok).strip().split()
+        text = ' '.join([x for x in input_bpe])
+
+        inputs = self.vocab_src.encode_line(
+            text, append_eos=True, add_if_not_exist=False)
+        prev_inputs = torch.roll(inputs, shifts=1)
+        lengths = inputs.size()[0]
+        max_len = min(self.max_length, lengths)
+
+        padding = torch.tensor(
+            [self.padding_value] *  # noqa: W504
+            (max_len - lengths),
+            dtype=inputs.dtype)
+        sources = torch.unsqueeze(torch.cat([inputs, padding]), dim=0)
+        inputs = torch.unsqueeze(torch.cat([padding, inputs]), dim=0)
+        prev_inputs = torch.unsqueeze(torch.cat([prev_inputs, padding]), dim=0)
+        lengths = torch.tensor([lengths])
+        out = {
+            'src_tokens': inputs,
+            'src_lengths': lengths,
+            'prev_src_tokens': prev_inputs,
+            'sources': sources
+        }
 
-        return outputs
+        return out
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,66 +1,59 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from typing import Any
+import os.path as osp
+from typing import Any, Dict, List, Union
+
+import cv2
+import json
+import numpy as np
+import torch
+from PIL import Image
+from torchvision import transforms
 
 from modelscope.metainfo import Pipelines
+from modelscope.models.cv.stream_yolo import RealtimeVideoDetector
 from modelscope.outputs import OutputKeys
+from modelscope.pipelines import pipeline
+from modelscope.pipelines.base import Input, Model, Pipeline, Tensor
 from modelscope.pipelines.builder import PIPELINES
+from modelscope.preprocessors import load_image
 from modelscope.utils.constant import ModelFile, Tasks
-from modelscope.utils.cv.image_utils import \
-    show_image_object_detection_auto_result
-from .base import EasyCVPipeline
+from modelscope.utils.logger import get_logger
+
+logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.image_object_detection, module_name=Pipelines.easycv_detection)
-@PIPELINES.register_module(
-    Tasks.image_object_detection,
-    module_name=Pipelines.image_object_detection_auto)
-@PIPELINES.register_module(
-    Tasks.domain_specific_object_detection,
-    module_name=Pipelines.hand_detection)
-class EasyCVDetectionPipeline(EasyCVPipeline):
-    """Pipeline for easycv detection task."""
-
-    def __init__(self,
-                 model: str,
-                 model_file_pattern=ModelFile.TORCH_MODEL_FILE,
-                 *args,
-                 **kwargs):
-        """
-            model (str): model id on modelscope hub or local model path.
-            model_file_pattern (str): model file pattern.
-        """
-
-        super(EasyCVDetectionPipeline, self).__init__(
-            model=model,
-            model_file_pattern=model_file_pattern,
-            *args,
-            **kwargs)
-
-    def show_result(self, img_path, result, save_path=None):
-        show_image_object_detection_auto_result(img_path, result, save_path)
-
-    def __call__(self, inputs) -> Any:
-        outputs = self.predict_op(inputs)
-
-        scores = []
-        labels = []
-        boxes = []
-        for output in outputs:
-            for score, label, box in zip(output['detection_scores'],
-                                         output['detection_classes'],
-                                         output['detection_boxes']):
-                scores.append(score)
-                labels.append(self.cfg.CLASSES[label])
-                boxes.append([b for b in box])
+    Tasks.video_object_detection,
+    module_name=Pipelines.realtime_video_object_detection)
+class RealtimeVideoObjectDetectionPipeline(Pipeline):
+
+    def __init__(self, model: str, **kwargs):
+        super().__init__(model=model, **kwargs)
+
+    def preprocess(self, input: Input) -> Dict[Tensor, Union[str, np.ndarray]]:
+        return input
+
+    def forward(self, input: Input) -> Dict[Tensor, Dict[str, np.ndarray]]:
+        self.video_path = input
+        # Processing the whole video and return results for each frame
+        forward_output = self.model.inference_video(self.video_path)
+        return {'forward_output': forward_output}
+
+    def postprocess(self, input: Dict[Tensor, Dict[str, np.ndarray]],
+                    **kwargs) -> str:
+        forward_output = input['forward_output']
+
+        scores, boxes, labels, timestamps = [], [], [], []
+        for result in forward_output:
+            box, score, label, timestamp = result
+            scores.append(score)
+            boxes.append(box)
+            labels.append(label)
+            timestamps.append(timestamp)
 
-        results = [{
+        return {
+            OutputKeys.BOXES: boxes,
             OutputKeys.SCORES: scores,
             OutputKeys.LABELS: labels,
-            OutputKeys.BOXES: boxes
-        } for output in outputs]
-
-        if self._is_single_inputs(inputs):
-            results = results[0]
-
-        return results
+            OutputKeys.TIMESTAMPS: timestamps,
+        }
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/retina_face_detection_pipeline.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,67 +1,59 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import os.path
-from typing import Any
+import os.path as osp
+from typing import Any, Dict
+
+import cv2
+import numpy as np
+import PIL
+import torch
 
 from modelscope.metainfo import Pipelines
+from modelscope.models.cv.face_detection import RetinaFaceDetection
 from modelscope.outputs import OutputKeys
+from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
+from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
-from .base import EasyCVPipeline
+from modelscope.utils.logger import get_logger
+
+logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.human_wholebody_keypoint,
-    module_name=Pipelines.human_wholebody_keypoint)
-class HumanWholebodyKeypointsPipeline(EasyCVPipeline):
-    """Pipeline for human wholebody 2d keypoints detection."""
-
-    def __init__(self,
-                 model: str,
-                 model_file_pattern=ModelFile.TORCH_MODEL_FILE,
-                 *args,
-                 **kwargs):
+    Tasks.face_detection, module_name=Pipelines.retina_face_detection)
+class RetinaFaceDetectionPipeline(Pipeline):
+
+    def __init__(self, model: str, **kwargs):
         """
-            model (str): model id on modelscope hub or local model path.
-            model_file_pattern (str): model file pattern.
+        use `model` to create a face detection pipeline for prediction
+        Args:
+            model: model id on modelscope hub.
         """
-        super(HumanWholebodyKeypointsPipeline, self).__init__(
-            model=model,
-            model_file_pattern=model_file_pattern,
-            *args,
-            **kwargs)
-
-    def _build_predict_op(self, **kwargs):
-        """Build EasyCV predictor."""
-        from easycv.predictors.builder import build_predictor
-        detection_predictor_type = self.cfg['DETECTION']['type']
-        detection_model_path = os.path.join(
-            self.model_dir, self.cfg['DETECTION']['model_path'])
-        detection_cfg_file = os.path.join(self.model_dir,
-                                          self.cfg['DETECTION']['config_file'])
-        detection_score_threshold = self.cfg['DETECTION']['score_threshold']
-        self.cfg.pipeline.predictor_config[
-            'detection_predictor_config'] = dict(
-                type=detection_predictor_type,
-                model_path=detection_model_path,
-                config_file=detection_cfg_file,
-                score_threshold=detection_score_threshold)
-        easycv_config = self._to_easycv_config()
-        pipeline_op = build_predictor(self.cfg.pipeline.predictor_config, {
-            'model_path': self.model_path,
-            'config_file': easycv_config,
-            **kwargs
-        })
-        return pipeline_op
-
-    def __call__(self, inputs) -> Any:
-        outputs = self.predict_op(inputs)
-
-        results = [{
-            OutputKeys.KEYPOINTS: output['keypoints'],
-            OutputKeys.BOXES: output['boxes']
-        } for output in outputs]
-
-        if self._is_single_inputs(inputs):
-            results = results[0]
+        super().__init__(model=model, **kwargs)
+        ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
+        logger.info(f'loading model from {ckpt_path}')
+        detector = RetinaFaceDetection(
+            model_path=ckpt_path, device=self.device)
+        self.detector = detector
+        logger.info('load model done')
+
+    def preprocess(self, input: Input) -> Dict[str, Any]:
+        img = LoadImage.convert_to_ndarray(input)
+        img = img.astype(np.float32)
+        result = {'img': img}
+        return result
+
+    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        result = self.detector(input)
+        assert result is not None
+        bboxes = result[0][:, :4].tolist()
+        scores = result[0][:, 4].tolist()
+        lms = result[1].tolist()
+        return {
+            OutputKeys.SCORES: scores,
+            OutputKeys.BOXES: bboxes,
+            OutputKeys.KEYPOINTS: lms,
+        }
 
-        return results
+    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+        return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_skychange_pipeline.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,47 +1,60 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from typing import Any
+import pdb
+import time
+from typing import Any, Dict, Union
 
+import cv2
 import numpy as np
+import PIL
 
 from modelscope.metainfo import Pipelines
+from modelscope.models.cv.image_skychange import ImageSkyChangePreprocessor
 from modelscope.outputs import OutputKeys
+from modelscope.pipelines.base import Input, Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
+from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import Tasks
-from .base import EasyCVPipeline
+from modelscope.utils.logger import get_logger
+
+logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.image_segmentation, module_name=Pipelines.easycv_segmentation)
-class EasyCVSegmentationPipeline(EasyCVPipeline):
-    """Pipeline for easycv segmentation task."""
+    Tasks.image_skychange, module_name=Pipelines.image_skychange)
+class ImageSkychangePipeline(Pipeline):
+    """
+    Image Sky Change Pipeline. Given two images(sky_image and scene_image), pipeline will replace the sky style
+    of sky_image with the sky style of scene_image.
+
+    Examples:
+
+    >>> from modelscope.pipelines import pipeline
+    >>> detector = pipeline('image-skychange', 'damo/cv_hrnetocr_skychange')
+    >>> detector({
+            'sky_image': 'sky_image.jpg', # sky_image path (str)
+            'scene_image': 'scene_image.jpg', # scene_image path (str)
+        })
+    >>> {"output_img": [H * W * 3] 0~255, we can use cv2.imwrite to save output_img as an image.}
+    """
 
-    def __init__(self, model: str, model_file_pattern='*.pt', *args, **kwargs):
+    def __init__(self, model: str, **kwargs):
         """
-            model (str): model id on modelscope hub or local model path.
-            model_file_pattern (str): model file pattern.
+        use `model` to create a image sky change pipeline for image editing
+        Args:
+            model (`str` or `Model`): model_id on modelscope hub
+            preprocessor(`Preprocessor`, *optional*,  defaults to None): `ImageSkyChangePreprocessor`.
         """
+        super().__init__(model=model, **kwargs)
+        if not isinstance(self.model, Model):
+            logger.error('model object is not initialized.')
+            raise Exception('model object is not initialized.')
+        if self.preprocessor is None:
+            self.preprocessor = ImageSkyChangePreprocessor()
+        logger.info('load model done')
+
+    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        res = self.model.forward(**input)
+        return {OutputKeys.OUTPUT_IMG: res}
 
-        super(EasyCVSegmentationPipeline, self).__init__(
-            model=model,
-            model_file_pattern=model_file_pattern,
-            *args,
-            **kwargs)
-
-    def __call__(self, inputs) -> Any:
-        outputs = self.predict_op(inputs)
-
-        semantic_result = outputs[0]['seg_pred']
-
-        ids = np.unique(semantic_result)[::-1]
-        legal_indices = ids != len(self.predict_op.CLASSES)  # for VOID label
-        ids = ids[legal_indices]
-        segms = (semantic_result[None] == ids[:, None, None])
-        masks = [it.astype(np.int) for it in segms]
-        labels_txt = np.array(self.predict_op.CLASSES)[ids].tolist()
-
-        results = {
-            OutputKeys.MASKS: masks,
-            OutputKeys.LABELS: labels_txt,
-            OutputKeys.SCORES: [0.999 for _ in range(len(labels_txt))]
-        }
-        return results
+    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+        return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_attribute_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_emotion_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_emotion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_human_hand_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_image_generation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_image_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_liveness_ir_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_liveness_ir_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_liveness_xc_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_liveness_xc_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_processing_base_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_processing_base_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_quality_assessment_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_quality_assessment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_ood_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_ood_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/face_reconstruction_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/face_reconstruction_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -130,15 +130,15 @@
         device = create_device(self.device_name)
         self.device = device
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         img = LoadImage.convert_to_ndarray(input)
         if len(img.shape) == 2:
             img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
-        img = img.astype(np.float)
+        img = img.astype(float)
         result = {'img': img}
         return result
 
     def read_data(self, img, lm, lm3d_std, to_tensor=True, image_res=1024):
         # to RGB
         im = PIL.Image.fromarray(img[..., ::-1])
         W, H = im.size
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/facial_expression_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/general_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/general_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/hand_static_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/hand_static_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/human_reconstruction_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/human_reconstruction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_body_reshaping_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_body_reshaping_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_cartoon_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_cartoon_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_classification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_color_enhance_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_color_enhance_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_colorization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_debanding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_debanding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_deblur_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_deblur_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_defrcn_fewshot_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_denoise_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_denoise_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_depth_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_detection_pipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -26,15 +26,15 @@
             model: model id on modelscope hub.
         """
         super().__init__(model=model, auto_collate=False, **kwargs)
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
 
         img = LoadImage.convert_to_ndarray(input)
-        img = img.astype(np.float)
+        img = img.astype(np.float64)
         img = self.model.preprocess(img)
         result = {'img': img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
 
         outputs = self.model.inference(input['img'])
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_driving_perception_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_driving_perception_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_face_fusion_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_face_fusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_human_parsing_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_human_parsing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_inpainting_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_inpainting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_matching_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_matching_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_matting_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_matting_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -49,15 +49,15 @@
                     self.output = self._session.graph.get_tensor_by_name(
                         'output_png:0')
                     self.input_name = 'input_image:0'
                 logger.info('load model done')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         img = LoadImage.convert_to_ndarray(input)
-        img = img.astype(np.float)
+        img = img.astype(float)
         result = {'img': img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
         with self._session.as_default():
             feed_dict = {self.input_name: input['img']}
             output_img = self._session.run(self.output, feed_dict=feed_dict)
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_paintbyexample_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_paintbyexample_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/tbs_detection_pipeline.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,135 +1,149 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from typing import Any, Dict, Union
+
+import colorsys
+import os
+from typing import Any, Dict
 
 import cv2
 import numpy as np
-import PIL
 import torch
+from PIL import Image, ImageDraw, ImageFile, ImageFont
 
 from modelscope.metainfo import Pipelines
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.pipelines.cv.easycv_pipelines.base import EasyCVPipeline
-from modelscope.preprocessors import load_image
+from modelscope.pipelines.cv.tbs_detection_utils.utils import (_get_anchors,
+                                                               generate,
+                                                               post_process)
+from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
+ImageFile.LOAD_TRUNCATED_IMAGES = True
+
 logger = get_logger()
 
+__all__ = ['TBSDetectionPipeline']
+
 
 @PIPELINES.register_module(
-    Tasks.image_segmentation,
-    module_name=Pipelines.image_panoptic_segmentation)
-class ImagePanopticSegmentationPipeline(Pipeline):
+    Tasks.image_object_detection, module_name=Pipelines.tbs_detection)
+class TBSDetectionPipeline(Pipeline):
+    """ TBS Detection Pipeline.
+
+    Example:
+
+    ```python
+    >>> from modelscope.pipelines import pipeline
+
+    >>> tbs_detect = pipeline(Tasks.image_object_detection, model='landingAI/LD_CytoBrainCerv')
+    >>> tbs_detect(input='data/test/images/tbs_detection.jpg')
+       {
+        "boxes": [
+            [
+            446.9007568359375,
+            36.374977111816406,
+            907.0919189453125,
+            337.439208984375
+            ],
+            [
+            454.3310241699219,
+            336.08477783203125,
+            921.26904296875,
+            641.7871704101562
+            ]
+        ],
+        "labels": [
+            ["Positive"]
+        ],
+        "scores": [
+            0.9296008944511414,
+            0.9260380268096924
+        ]
+        }
+    >>> #
+    ```
+    """
+    _defaults = {
+        'class_names': ['positive'],
+        'model_image_size': (416, 416, 3),
+        'confidence': 0.5,
+        'iou': 0.3,
+    }
+
+    @classmethod
+    def get_defaults(cls, n):
+        if n in cls._defaults:
+            return cls._defaults[n]
+        else:
+            return "Unrecognized attribute name '" + n + "'"
 
     def __init__(self, model: str, **kwargs):
         """
-        use `model` to create a image panoptic segmentation pipeline for prediction
-        Args:
             model: model id on modelscope hub.
         """
-        super().__init__(model=model, **kwargs)
-
-        logger.info('panoptic segmentation model, pipeline init')
+        super().__init__(model=model, auto_collate=False, **kwargs)
+        self.__dict__.update(self._defaults)
+        self.anchors = _get_anchors(self)
+        generate(self)
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
-        from mmdet.datasets.pipelines import Compose
-        from mmcv.parallel import collate, scatter
-        from mmdet.datasets import replace_ImageToTensor
-
-        cfg = self.model.cfg
-        # build the data pipeline
-
-        if isinstance(input, str):
-            cfg.data.test.pipeline[0].type = 'LoadImageFromWebcam'
-            img = np.array(load_image(input))
-            img = img[:, :, ::-1]  # convert to bgr
-        elif isinstance(input, PIL.Image.Image):
-            cfg.data.test.pipeline[0].type = 'LoadImageFromWebcam'
-            img = np.array(input.convert('RGB'))
-        elif isinstance(input, np.ndarray):
-            cfg.data.test.pipeline[0].type = 'LoadImageFromWebcam'
-            if len(input.shape) == 2:
-                img = cv2.cvtColor(input, cv2.COLOR_GRAY2BGR)
-            else:
-                img = input
-        else:
-            raise TypeError(f'input should be either str, PIL.Image,'
-                            f' np.array, but got {type(input)}')
-
-        # collect data
-        data = dict(img=img)
-        cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)
-        test_pipeline = Compose(cfg.data.test.pipeline)
-
-        data = test_pipeline(data)
-        # copy from mmdet_model collect data
-        data = collate([data], samples_per_gpu=1)
-        data['img_metas'] = [
-            img_metas.data[0] for img_metas in data['img_metas']
-        ]
-        data['img'] = [img.data[0] for img in data['img']]
-        if next(self.model.parameters()).is_cuda:
-            # scatter to specified GPU
-            data = scatter(data, [next(self.model.parameters()).device])[0]
-
-        return data
+        """
+        Detect objects (bounding boxes) in the image(s) passed as inputs.
 
-    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        results = self.model.inference(input)
+        Args:
+            input (`Image` or `List[Image]`):
+                The pipeline handles three types of images:
 
-        return results
+                - A string containing an HTTP(S) link pointing to an image
+                - A string containing a local path to an image
+                - An image loaded in PIL or opencv directly
 
-    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
-        # bz=1, tcguo
-        pan_results = inputs[0]['pan_results']
-        INSTANCE_OFFSET = 1000
-
-        ids = np.unique(pan_results)[::-1]
-        legal_indices = ids != self.model.num_classes  # for VOID label
-        ids = ids[legal_indices]
-        labels = np.array([id % INSTANCE_OFFSET for id in ids], dtype=np.int64)
-        segms = (pan_results[None] == ids[:, None, None])
-        masks = [it.astype(np.int) for it in segms]
-        labels_txt = np.array(self.model.CLASSES)[labels].tolist()
+                The pipeline accepts either a single image or a batch of images. Images in a batch must all be in the
+                same format.
 
-        outputs = {
-            OutputKeys.MASKS: masks,
-            OutputKeys.LABELS: labels_txt,
-            OutputKeys.SCORES: [0.999 for _ in range(len(labels_txt))]
-        }
-        return outputs
 
+        Return:
+            A dictionary of result or a list of dictionary of result. If the input is an image, a dictionary
+            is returned. If input is a list of image, a list of dictionary is returned.
 
-@PIPELINES.register_module(
-    Tasks.image_segmentation,
-    module_name=Pipelines.image_panoptic_segmentation_easycv)
-class ImagePanopticSegmentationEasyCVPipeline(EasyCVPipeline):
-    """Pipeline built upon easycv for image segmentation."""
+            The dictionary contain the following keys:
 
-    def __init__(self, model: str, model_file_pattern='*.pt', *args, **kwargs):
+            - **scores** (`List[float]`) -- The detection score for each card in the image.
+            - **boxes** (`List[float]) -- The bounding boxe [x1, y1, x2, y2] of detected objects in in image's
+                original size.
+            - **labels** (`List[str]`, optional) -- The boxes's class_names of detected object in image.
         """
-            model (str): model id on modelscope hub or local model path.
-            model_file_pattern (str): model file pattern.
-        """
-        super(ImagePanopticSegmentationEasyCVPipeline, self).__init__(
-            model=model,
-            model_file_pattern=model_file_pattern,
-            *args,
-            **kwargs)
-
-    def __call__(self, inputs) -> Any:
-        outputs = self.predict_op(inputs)
-        easycv_results = outputs[0]
-
-        results = {
-            OutputKeys.MASKS:
-            easycv_results[OutputKeys.MASKS],
-            OutputKeys.LABELS:
-            easycv_results[OutputKeys.LABELS],
-            OutputKeys.SCORES:
-            [0.999 for _ in range(len(easycv_results[OutputKeys.LABELS]))]
-        }
+        img = LoadImage.convert_to_ndarray(input)
+        img = img.astype(float)
+        result = {'img': img, 'img_path': input}
+        return result
 
-        return results
+    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+        img = input['img'].astype(np.uint8)
+        img = cv2.resize(img, (416, 416))
+        img = img.astype(np.float32)
+        tmp_inp = np.transpose(img / 255.0, (2, 0, 1))
+        tmp_inp = torch.from_numpy(tmp_inp).type(torch.FloatTensor)
+        img = torch.unsqueeze(tmp_inp, dim=0)
+        model_path = os.path.join(self.model, 'pytorch_yolov4.pt')
+        model = torch.load(model_path)
+        outputs = model(img.cuda())
+        result = {'data': outputs, 'img_path': input['img_path']}
+        return result
+
+    def postprocess(self, input: Dict[str, Any], *args,
+                    **kwargs) -> Dict[str, Any]:
+
+        bboxes, scores = post_process(self, input['data'], input['img_path'])
+
+        if bboxes is None:
+            outputs = {OutputKeys.SCORES: [], OutputKeys.BOXES: []}
+            return outputs
+        outputs = {
+            OutputKeys.SCORES: scores.tolist(),
+            OutputKeys.LABELS: ['Positive'],
+            OutputKeys.BOXES: bboxes
+        }
+        return outputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_reid_person_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_reid_person_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_restoration_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_restoration_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_salient_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_salient_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_semantic_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_skychange_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/mog_face_detection_pipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,60 +1,55 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import pdb
-import time
-from typing import Any, Dict, Union
+import os.path as osp
+from typing import Any, Dict
 
-import cv2
 import numpy as np
-import PIL
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.image_skychange import ImageSkyChangePreprocessor
+from modelscope.models.cv.face_detection import MogFaceDetector
 from modelscope.outputs import OutputKeys
-from modelscope.pipelines.base import Input, Model, Pipeline
+from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
-from modelscope.utils.constant import Tasks
+from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.image_skychange, module_name=Pipelines.image_skychange)
-class ImageSkychangePipeline(Pipeline):
-    """
-    Image Sky Change Pipeline. Given two images(sky_image and scene_image), pipeline will replace the sky style
-    of sky_image with the sky style of scene_image.
-
-    Examples:
-
-    >>> from modelscope.pipelines import pipeline
-    >>> detector = pipeline('image-skychange', 'damo/cv_hrnetocr_skychange')
-    >>> detector({
-            'sky_image': 'sky_image.jpg', # sky_image path (str)
-            'scene_image': 'scene_image.jpg', # scene_image path (str)
-        })
-    >>> {"output_img": [H * W * 3] 0~255, we can use cv2.imwrite to save output_img as an image.}
-    """
+    Tasks.face_detection, module_name=Pipelines.mog_face_detection)
+class MogFaceDetectionPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
         """
-        use `model` to create a image sky change pipeline for image editing
+        use `model` to create a face detection pipeline for prediction
         Args:
-            model (`str` or `Model`): model_id on modelscope hub
-            preprocessor(`Preprocessor`, *optional*,  defaults to None): `ImageSkyChangePreprocessor`.
+            model: model id on modelscope hub.
         """
         super().__init__(model=model, **kwargs)
-        if not isinstance(self.model, Model):
-            logger.error('model object is not initialized.')
-            raise Exception('model object is not initialized.')
-        if self.preprocessor is None:
-            self.preprocessor = ImageSkyChangePreprocessor()
+        ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
+        logger.info(f'loading model from {ckpt_path}')
+        detector = MogFaceDetector(model_path=ckpt_path, device=self.device)
+        self.detector = detector
         logger.info('load model done')
 
+    def preprocess(self, input: Input) -> Dict[str, Any]:
+        img = LoadImage.convert_to_ndarray(input)
+        img = img.astype(np.float32)
+        result = {'img': img}
+        return result
+
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        res = self.model.forward(**input)
-        return {OutputKeys.OUTPUT_IMG: res}
+
+        result = self.detector(input)
+        assert result is not None
+        bboxes = result[:, :4].tolist()
+        scores = result[:, 4].tolist()
+        return {
+            OutputKeys.SCORES: scores,
+            OutputKeys.BOXES: bboxes,
+            OutputKeys.KEYPOINTS: None,
+        }
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_structured_model_probing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_style_transfer_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_style_transfer_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -69,20 +69,20 @@
         if type(content) is dict:  # for demo service
             style = content['style']
             content = content['content']
 
         content = LoadImage.convert_to_ndarray(content)
         if len(content.shape) == 2:
             content = cv2.cvtColor(content, cv2.COLOR_GRAY2BGR)
-        content_img = content.astype(np.float)
+        content_img = content.astype(float)
 
         style_img = LoadImage.convert_to_ndarray(style)
         if len(style_img.shape) == 2:
             style_img = cv2.cvtColor(style_img, cv2.COLOR_GRAY2BGR)
-        style_img = style_img.astype(np.float)
+        style_img = style_img.astype(float)
 
         result = {'content': content_img, 'style': style_img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
         content_feed, style_feed = input['content'], input['style']
         h = np.shape(content_feed)[0]
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_super_resolution_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_to_image_generate_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_to_image_generate_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/image_to_image_translation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/image_to_image_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/indoor_layout_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/language_guided_video_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/license_plate_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/license_plate_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/lineless_table_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/live_category_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/live_category_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/mask_face_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/mask_face_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/maskdino_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/mobile_image_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/mog_face_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,54 +1,56 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os.path as osp
 from typing import Any, Dict
 
+import cv2
 import numpy as np
+import PIL
+import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.face_detection import MogFaceDetector
+from modelscope.models.cv.face_detection import UlfdFaceDetector
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import LoadImage
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_detection, module_name=Pipelines.mog_face_detection)
-class MogFaceDetectionPipeline(Pipeline):
+    Tasks.face_detection, module_name=Pipelines.ulfd_face_detection)
+class UlfdFaceDetectionPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
         """
         use `model` to create a face detection pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, **kwargs)
         ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
         logger.info(f'loading model from {ckpt_path}')
-        detector = MogFaceDetector(model_path=ckpt_path, device=self.device)
+        detector = UlfdFaceDetector(model_path=ckpt_path, device=self.device)
         self.detector = detector
         logger.info('load model done')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         img = LoadImage.convert_to_ndarray(input)
         img = img.astype(np.float32)
         result = {'img': img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-
         result = self.detector(input)
         assert result is not None
-        bboxes = result[:, :4].tolist()
-        scores = result[:, 4].tolist()
+        bboxes = result[0].tolist()
+        scores = result[1].tolist()
         return {
             OutputKeys.SCORES: scores,
             OutputKeys.BOXES: bboxes,
             OutputKeys.KEYPOINTS: None,
         }
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/motion_generation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/motion_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/movie_scene_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/mtcnn_face_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/nerf_recon_acc_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/object_detection_3d_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/object_detection_3d_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_dla34.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_dla34.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/model_vlpt.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/model_vlpt.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/__init__.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,19 +1,24 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .convnext import convnext_tiny
-    from .vitstr import vitstr_tiny
+    from .clip import CLIPTrainer
+    from .team import TEAMImgClsTrainer
+    from .ofa import OFATrainer
+    from .mplug import MPlugTrainer
+
 else:
     _import_structure = {
-        'convnext': ['convnext_tiny'],
-        'vitstr': ['vitstr_tiny']
+        'clip': ['CLIPTrainer'],
+        'team': ['TEAMImgClsTrainer'],
+        'ofa': ['OFATrainer'],
+        'mplug': ['MPlugTrainer'],
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/convnext.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ocr_modules/vitstr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/ops.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/resnet18_v1.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/resnet_utils.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/resnet_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/table_process.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/table_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ocr_utils/utils.py` & `modelscope-1.6.0/modelscope/pipelines/cv/ocr_utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/panorama_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/pedestrian_attribute_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/product_retrieval_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/product_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/product_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/realtime_video_object_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,59 +1,61 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import os.path as osp
-from typing import Any, Dict, List, Union
+from typing import Any, Dict, Optional, Union
 
-import cv2
-import json
-import numpy as np
 import torch
-from PIL import Image
-from torchvision import transforms
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.stream_yolo import RealtimeVideoDetector
+from modelscope.models.multi_modal.vldoc.model import VLDocForDocVLEmbedding
 from modelscope.outputs import OutputKeys
-from modelscope.pipelines import pipeline
-from modelscope.pipelines.base import Input, Model, Pipeline, Tensor
+from modelscope.pipelines.base import Input, Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.preprocessors import load_image
+from modelscope.preprocessors.multi_modal import (Preprocessor,
+                                                  VLDocPreprocessor)
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.video_object_detection,
-    module_name=Pipelines.realtime_video_object_detection)
-class RealtimeVideoObjectDetectionPipeline(Pipeline):
-
-    def __init__(self, model: str, **kwargs):
-        super().__init__(model=model, **kwargs)
-
-    def preprocess(self, input: Input) -> Dict[Tensor, Union[str, np.ndarray]]:
-        return input
-
-    def forward(self, input: Input) -> Dict[Tensor, Dict[str, np.ndarray]]:
-        self.video_path = input
-        # Processing the whole video and return results for each frame
-        forward_output = self.model.inference_video(self.video_path)
-        return {'forward_output': forward_output}
-
-    def postprocess(self, input: Dict[Tensor, Dict[str, np.ndarray]],
-                    **kwargs) -> str:
-        forward_output = input['forward_output']
-
-        scores, boxes, labels, timestamps = [], [], [], []
-        for result in forward_output:
-            box, score, label, timestamp = result
-            scores.append(score)
-            boxes.append(box)
-            labels.append(label)
-            timestamps.append(timestamp)
-
-        return {
-            OutputKeys.BOXES: boxes,
-            OutputKeys.SCORES: scores,
-            OutputKeys.LABELS: labels,
-            OutputKeys.TIMESTAMPS: timestamps,
-        }
+    Tasks.document_vl_embedding, module_name=Pipelines.document_vl_embedding)
+class DocumentVLEmbeddingPipeline(Pipeline):
+
+    def __init__(self,
+                 model: Union[Model, str],
+                 preprocessor: Optional[Preprocessor] = None,
+                 **kwargs):
+        """ The pipeline for multi-modal document embedding generation.
+
+        Args:
+            model: model id on modelscope hub.
+            preprocessor: type `Preprocessor`. If None, `VLDocPreprocessor` is used.
+
+        Examples:
+
+        >>> from modelscope.models import Model
+        >>> from modelscope.pipelines import pipeline
+        >>> model = Model.from_pretrained(
+            'damo/multi-modal_convnext-roberta-base_vldoc-embedding')
+        >>> doc_VL_emb_pipeline = pipeline(task='document-vl-embedding', model=model)
+        >>> inp = {
+                'images': ['data/demo.png'],
+                'ocr_info_paths': ['data/demo.json']
+            }
+        >>> result = doc_VL_emb_pipeline(inp)
+        """
+
+        super().__init__(model=model, preprocessor=preprocessor, **kwargs)
+        self.model.eval()
+        if preprocessor is None:
+            if isinstance(self.model, VLDocForDocVLEmbedding):
+                self.preprocessor = VLDocPreprocessor(self.model.model_dir)
+            else:
+                raise NotImplementedError
+
+    def forward(self, encodings: Dict[str, Any]) -> Dict[str, Any]:
+        for k, v in encodings.items():
+            encodings[k] = encodings[k].to(self.device)
+        return self.model(**encodings)
+
+    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+        return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/referring_video_object_segmentation_pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -221,15 +221,15 @@
 
         return result
 
 
 def apply_mask(image, mask, color, transparency=0.7):
     mask = mask[..., np.newaxis].repeat(repeats=3, axis=2)
     mask = mask * transparency
-    color_matrix = np.ones(image.shape, dtype=np.float) * color
+    color_matrix = np.ones(image.shape, dtype=np.float64) * color
     out_image = color_matrix * mask + image * (1.0 - mask)
     return out_image
 
 
 def timestamp_format(seconds):
     m, s = divmod(seconds, 60)
     h, m = divmod(m, 60)
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/retina_face_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,59 +1,49 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import os.path as osp
-from typing import Any, Dict
+from typing import Any, Dict, Optional, Union
 
-import cv2
-import numpy as np
-import PIL
 import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.face_detection import RetinaFaceDetection
-from modelscope.outputs import OutputKeys
-from modelscope.pipelines.base import Input, Pipeline
+from modelscope.models.multi_modal import OfaForAllTasks
+from modelscope.pipelines.base import Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.preprocessors import LoadImage
-from modelscope.utils.constant import ModelFile, Tasks
+from modelscope.pipelines.util import batch_process
+from modelscope.preprocessors import OfaPreprocessor, Preprocessor
+from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.face_detection, module_name=Pipelines.retina_face_detection)
-class RetinaFaceDetectionPipeline(Pipeline):
+    Tasks.ocr_recognition, module_name=Pipelines.ofa_ocr_recognition)
+class OcrRecognitionPipeline(Pipeline):
 
-    def __init__(self, model: str, **kwargs):
+    def __init__(self,
+                 model: Union[Model, str],
+                 preprocessor: Optional[Preprocessor] = None,
+                 **kwargs):
         """
-        use `model` to create a face detection pipeline for prediction
+        use `model` and `preprocessor` to create a ocr recognition pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
-        super().__init__(model=model, **kwargs)
-        ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
-        logger.info(f'loading model from {ckpt_path}')
-        detector = RetinaFaceDetection(
-            model_path=ckpt_path, device=self.device)
-        self.detector = detector
-        logger.info('load model done')
-
-    def preprocess(self, input: Input) -> Dict[str, Any]:
-        img = LoadImage.convert_to_ndarray(input)
-        img = img.astype(np.float32)
-        result = {'img': img}
-        return result
-
-    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        result = self.detector(input)
-        assert result is not None
-        bboxes = result[0][:, :4].tolist()
-        scores = result[0][:, 4].tolist()
-        lms = result[1].tolist()
-        return {
-            OutputKeys.SCORES: scores,
-            OutputKeys.BOXES: bboxes,
-            OutputKeys.KEYPOINTS: lms,
-        }
+        super().__init__(model=model, preprocessor=preprocessor, **kwargs)
+        self.model.eval()
+        if preprocessor is None:
+            if isinstance(self.model, OfaForAllTasks):
+                self.preprocessor = OfaPreprocessor(self.model.model_dir)
+
+    def _batch(self, data):
+        if isinstance(self.model, OfaForAllTasks):
+            return batch_process(self.model, data)
+        else:
+            return super(OcrRecognitionPipeline, self)._batch(data)
+
+    def forward(self, inputs: Dict[str, Any],
+                **forward_params) -> Dict[str, Any]:
+        with torch.no_grad():
+            return super().forward(inputs, **forward_params)
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/shop_segmentation_pipleline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/shop_segmentation_pipleline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/skin_retouching_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/skin_retouching_pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -101,15 +101,15 @@
         self.input_size = 512
         self.device = device
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
         img = LoadImage.convert_to_ndarray(input)
         if len(img.shape) == 2:
             img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
-        img = img.astype(np.float)
+        img = img.astype(float)
         result = {'img': img}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
         rgb_image = input['img'].astype(np.uint8)
 
         retouch_local = True
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/table_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/table_recognition_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/tbs_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,149 +1,138 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-import colorsys
 import os
+import os.path as osp
 from typing import Any, Dict
 
 import cv2
+import mmcv
 import numpy as np
 import torch
-from PIL import Image, ImageDraw, ImageFile, ImageFont
+from tqdm import tqdm
 
 from modelscope.metainfo import Pipelines
+from modelscope.models.cv.video_panoptic_segmentation.video_k_net import \
+    VideoKNet
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.pipelines.cv.tbs_detection_utils.utils import (_get_anchors,
-                                                               generate,
-                                                               post_process)
-from modelscope.preprocessors import LoadImage
-from modelscope.utils.constant import Tasks
+from modelscope.utils.config import Config
+from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
-ImageFile.LOAD_TRUNCATED_IMAGES = True
-
 logger = get_logger()
 
-__all__ = ['TBSDetectionPipeline']
-
 
 @PIPELINES.register_module(
-    Tasks.image_object_detection, module_name=Pipelines.tbs_detection)
-class TBSDetectionPipeline(Pipeline):
-    """ TBS Detection Pipeline.
-
-    Example:
-
-    ```python
-    >>> from modelscope.pipelines import pipeline
-
-    >>> tbs_detect = pipeline(Tasks.image_object_detection, model='landingAI/LD_CytoBrainCerv')
-    >>> tbs_detect(input='data/test/images/tbs_detection.jpg')
-       {
-        "boxes": [
-            [
-            446.9007568359375,
-            36.374977111816406,
-            907.0919189453125,
-            337.439208984375
-            ],
-            [
-            454.3310241699219,
-            336.08477783203125,
-            921.26904296875,
-            641.7871704101562
-            ]
-        ],
-        "labels": [
-            ["Positive"]
-        ],
-        "scores": [
-            0.9296008944511414,
-            0.9260380268096924
-        ]
-        }
-    >>> #
-    ```
-    """
-    _defaults = {
-        'class_names': ['positive'],
-        'model_image_size': (416, 416, 3),
-        'confidence': 0.5,
-        'iou': 0.3,
-    }
-
-    @classmethod
-    def get_defaults(cls, n):
-        if n in cls._defaults:
-            return cls._defaults[n]
-        else:
-            return "Unrecognized attribute name '" + n + "'"
+    Tasks.video_panoptic_segmentation,
+    module_name=Pipelines.video_panoptic_segmentation)
+class VideoPanopticSegmentationPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
         """
+        use `model` to create a video panoptic segmentation pipeline for prediction
+        Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, auto_collate=False, **kwargs)
-        self.__dict__.update(self._defaults)
-        self.anchors = _get_anchors(self)
-        generate(self)
+        logger.info(f'loading model from {model}')
+        model_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
+        config_path = osp.join(model, ModelFile.CONFIGURATION)
+        logger.info(f'loading config from {config_path}')
+        self.cfg = Config.from_file(config_path)
+        self.max_video_frames = kwargs.get('max_video_frames', 1000)
+
+        self.model = VideoKNet(model)
+        checkpoint = torch.load(
+            model_path, map_location=torch.device(self.device))
+        self.model.load_state_dict(checkpoint['state_dict'])
+        self.model = self.model.to(self.device).eval()
+        logger.info('load model done')
+
+        self.pad_size_divisor = 32
+        self.mean = np.array([123.675, 116.28, 103.53], np.float32)
+        self.std = np.array([58.395, 57.12, 57.375], np.float32)
+        self.to_rgb = False
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
-        """
-        Detect objects (bounding boxes) in the image(s) passed as inputs.
-
-        Args:
-            input (`Image` or `List[Image]`):
-                The pipeline handles three types of images:
-
-                - A string containing an HTTP(S) link pointing to an image
-                - A string containing a local path to an image
-                - An image loaded in PIL or opencv directly
-
-                The pipeline accepts either a single image or a batch of images. Images in a batch must all be in the
-                same format.
-
-
-        Return:
-            A dictionary of result or a list of dictionary of result. If the input is an image, a dictionary
-            is returned. If input is a list of image, a list of dictionary is returned.
-
-            The dictionary contain the following keys:
-
-            - **scores** (`List[float]`) -- The detection score for each card in the image.
-            - **boxes** (`List[float]) -- The bounding boxe [x1, y1, x2, y2] of detected objects in in image's
-                original size.
-            - **labels** (`List[str]`, optional) -- The boxes's class_names of detected object in image.
-        """
-        img = LoadImage.convert_to_ndarray(input)
-        img = img.astype(np.float)
-        result = {'img': img, 'img_path': input}
+        if not isinstance(input, str):
+            raise TypeError(f'input should be a str,'
+                            f'  but got {type(input)}')
+        frames = []
+        img_metas = []
+        iids = []
+        cap = cv2.VideoCapture(input)
+        self.fps = cap.get(cv2.CAP_PROP_FPS)
+        self.frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
+        frame_idx = 0
+        while (cap.isOpened()):
+            ret, frame = cap.read()
+            if not ret:
+                break
+
+            if frame_idx > self.max_video_frames:
+                break
+
+            norm_frame = mmcv.imnormalize(frame, self.mean, self.std,
+                                          self.to_rgb)
+            pad_frame = mmcv.impad_to_multiple(
+                norm_frame, self.pad_size_divisor, pad_val=0)
+
+            img_meta = {}
+            img_meta['ori_shape'] = frame.shape
+            img_meta['img_shape'] = frame.shape
+            img_meta['pad_shape'] = pad_frame.shape
+            img_meta['batch_input_shape'] = pad_frame.shape[0:2]
+            img_meta['scale_factor'] = 1.0,
+            img_meta['flip'] = False
+            img_meta['flip_direction'] = None
+
+            frames.append(pad_frame)
+            img_metas.append([img_meta])
+            iids.append(frame_idx)
+
+            frame_idx += 1
+
+        result = {
+            'video_name': input,
+            'imgs': np.array(frames),
+            'img_metas': img_metas,
+            'iids': iids,
+        }
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        img = input['img'].astype(np.uint8)
-        img = cv2.resize(img, (416, 416))
-        img = img.astype(np.float32)
-        tmp_inp = np.transpose(img / 255.0, (2, 0, 1))
-        tmp_inp = torch.from_numpy(tmp_inp).type(torch.FloatTensor)
-        img = torch.unsqueeze(tmp_inp, dim=0)
-        model_path = os.path.join(self.model, 'pytorch_yolov4.pt')
-        model = torch.load(model_path)
-        outputs = model(img.cuda())
-        result = {'data': outputs, 'img_path': input['img_path']}
-        return result
-
-    def postprocess(self, input: Dict[str, Any], *args,
-                    **kwargs) -> Dict[str, Any]:
-
-        bboxes, scores = post_process(self, input['data'], input['img_path'])
-
-        if bboxes is None:
-            outputs = {OutputKeys.SCORES: [], OutputKeys.BOXES: []}
-            return outputs
-        outputs = {
-            OutputKeys.SCORES: scores.tolist(),
-            OutputKeys.LABELS: ['Positive'],
-            OutputKeys.BOXES: bboxes
+        scores = []
+        labels = []
+        masks = []
+        boxes = []
+        track_ids = []
+        for ii in tqdm(range(len(input['iids']))):
+            img = input['imgs'][ii]
+            img_meta = input['img_metas'][ii]
+            iid = input['iids'][ii]
+
+            x = np.transpose(img, [2, 0, 1])
+            x = np.expand_dims(x, 0)
+            x = torch.from_numpy(x).to(self.device)
+            with torch.no_grad():
+                segm_results = self.model(x, img_meta, rescale=True, iid=iid)
+
+            _, _, _, vis_sem, vis_tracker, label, binary_mask, track_id, thing_bbox_for_tracking = segm_results
+            scores.append([0.99] * len(label))
+            labels.append(label)
+            masks.append(binary_mask)
+            boxes.append(thing_bbox_for_tracking)
+            track_ids.append(track_id)
+
+        output = {
+            'scores': scores,
+            'labels': labels,
+            'masks': masks,
+            'boxes': boxes,
+            'uuid': track_ids
         }
-        return outputs
+        return output
+
+    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+        return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/tbs_detection_utils/utils.py` & `modelscope-1.6.0/modelscope/pipelines/cv/tbs_detection_utils/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/text_driven_segmentation_pipleline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/tinynas_classification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/tinynas_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/tinynas_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/tinynas_detection_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/ulfd_face_detection_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/pipeline_template.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,57 +1,87 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-import os.path as osp
+
 from typing import Any, Dict
 
-import cv2
 import numpy as np
-import PIL
-import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.face_detection import UlfdFaceDetector
-from modelscope.outputs import OutputKeys
-from modelscope.pipelines.base import Input, Pipeline
+from modelscope.models.base.base_model import Model
+from modelscope.outputs.outputs import OutputKeys
+from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.preprocessors import LoadImage
-from modelscope.utils.constant import ModelFile, Tasks
-from modelscope.utils.logger import get_logger
+from modelscope.utils.constant import Tasks
 
-logger = get_logger()
+__all__ = ['PipelineTemplate']
 
 
 @PIPELINES.register_module(
-    Tasks.face_detection, module_name=Pipelines.ulfd_face_detection)
-class UlfdFaceDetectionPipeline(Pipeline):
+    Tasks.task_template, module_name=Pipelines.pipeline_template)
+class PipelineTemplate(Pipeline):
+    """A pipeline template explain how to define parameters and input and
+       output information. As a rule, the first parameter is the input,
+       followed by the request parameters. The parameter must add type
+       hint information, and set the default value if necessary,
+       for the convenience of use.
+    """
+
+    def __init__(self, model: Model, **kwargs):
+        """A pipeline template to describe input and
+        output and parameter processing
 
-    def __init__(self, model: str, **kwargs):
-        """
-        use `model` to create a face detection pipeline for prediction
         Args:
-            model: model id on modelscope hub.
+            model: A Model instance.
         """
+        # call base init.
         super().__init__(model=model, **kwargs)
-        ckpt_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
-        logger.info(f'loading model from {ckpt_path}')
-        detector = UlfdFaceDetector(model_path=ckpt_path, device=self.device)
-        self.detector = detector
-        logger.info('load model done')
-
-    def preprocess(self, input: Input) -> Dict[str, Any]:
-        img = LoadImage.convert_to_ndarray(input)
-        img = img.astype(np.float32)
-        result = {'img': img}
-        return result
 
-    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        result = self.detector(input)
-        assert result is not None
-        bboxes = result[0].tolist()
-        scores = result[1].tolist()
-        return {
-            OutputKeys.SCORES: scores,
-            OutputKeys.BOXES: bboxes,
-            OutputKeys.KEYPOINTS: None,
-        }
+    def preprocess(self,
+                   input: Any,
+                   max_length: int = 1024,
+                   top_p: float = 0.8) -> Any:
+        """Pipeline preprocess interface.
+
+        Args:
+            input (Any): The pipeline input, ref Tasks.task_template TASK_INPUTS.
+            max_length (int, optional): The max_length parameter. Defaults to 1024.
+            top_p (float, optional): The top_p parameter. Defaults to 0.8.
+
+        Returns:
+            Any: Return result process by forward.
+        """
+        pass
 
-    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
-        return inputs
+    def forward(self,
+                input: Any,
+                max_length: int = 1024,
+                top_p: float = 0.8) -> Any:
+        """The forward interface.
+
+        Args:
+            input (Any): The output of the preprocess.
+            max_length (int, optional): max_length. Defaults to 1024.
+            top_p (float, optional): top_p. Defaults to 0.8.
+
+        Returns:
+            Any: Return result process by postprocess.
+        """
+        pass
+
+    def postprocess(self,
+                    inputs: Any,
+                    postprocess_param1: str = None) -> Dict[str, Any]:
+        """The postprocess interface.
+
+        Args:
+            input (Any): The output of the forward.
+            max_length (int, optional): max_length. Defaults to 1024.
+            top_p (float, optional): top_p. Defaults to 0.8.
+
+        Returns:
+            Any: Return result process by postprocess.
+        """
+        result = {
+            OutputKeys.BOXES: np.zeros(4),
+            OutputKeys.OUTPUT_IMG: np.zeros(10, 4),
+            OutputKeys.TEXT_EMBEDDING: np.zeros(1, 1000)
+        }
+        return result
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_category_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_category_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_colorization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_colorization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_deinterlace_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_deinterlace_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_depth_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_depth_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_frame_interpolation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_human_matting_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_human_matting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_inpainting_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_inpainting_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_instance_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_multi_object_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_object_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_object_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_panoptic_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/vop_retrieval_pipeline.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,138 +1,122 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
+import gzip
+import math
 import os
 import os.path as osp
+import pickle
+import random
+from collections import defaultdict, deque
 from typing import Any, Dict
 
-import cv2
-import mmcv
 import numpy as np
 import torch
 from tqdm import tqdm
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.cv.video_panoptic_segmentation.video_k_net import \
-    VideoKNet
+from modelscope.models import Model
+from modelscope.models.cv.vop_retrieval import (LengthAdaptiveTokenizer, VoP,
+                                                init_transform_dict, load_data,
+                                                load_frames_from_video)
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
+from modelscope.preprocessors import load_image
 from modelscope.utils.config import Config
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.video_panoptic_segmentation,
-    module_name=Pipelines.video_panoptic_segmentation)
-class VideoPanopticSegmentationPipeline(Pipeline):
+    Tasks.vop_retrieval, module_name=Pipelines.vop_retrieval)
+class VopRetrievalPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
         """
-        use `model` to create a video panoptic segmentation pipeline for prediction
+        use `model` to create a vop pipeline for retrieval
         Args:
             model: model id on modelscope hub.
         """
-        super().__init__(model=model, auto_collate=False, **kwargs)
-        logger.info(f'loading model from {model}')
-        model_path = osp.join(model, ModelFile.TORCH_MODEL_FILE)
-        config_path = osp.join(model, ModelFile.CONFIGURATION)
-        logger.info(f'loading config from {config_path}')
-        self.cfg = Config.from_file(config_path)
-        self.max_video_frames = kwargs.get('max_video_frames', 1000)
-
-        self.model = VideoKNet(model)
-        checkpoint = torch.load(
-            model_path, map_location=torch.device(self.device))
-        self.model.load_state_dict(checkpoint['state_dict'])
-        self.model = self.model.to(self.device).eval()
+        super().__init__(model=model, **kwargs)
+
+        # [from pretrain] load model
+        self.model = Model.from_pretrained('damo/cv_vit-b32_retrieval_vop').to(
+            self.device)
         logger.info('load model done')
 
-        self.pad_size_divisor = 32
-        self.mean = np.array([123.675, 116.28, 103.53], np.float32)
-        self.std = np.array([58.395, 57.12, 57.375], np.float32)
-        self.to_rgb = False
+        # others: load transform
+        self.local_pth = model
+        self.cfg = Config.from_file(osp.join(model, ModelFile.CONFIGURATION))
+        self.img_transform = init_transform_dict(
+            self.cfg.hyperparam.input_res)['clip_test']
+        logger.info('load transform done')
+
+        # others: load tokenizer
+        bpe_path = gzip.open(osp.join(
+            model,
+            'bpe_simple_vocab_16e6.txt.gz')).read().decode('utf-8').split('\n')
+        self.tokenizer = LengthAdaptiveTokenizer(self.cfg.hyperparam, bpe_path)
+        logger.info('load tokenizer done')
+
+        # others: load dataset
+        self.database = load_data(
+            osp.join(model, 'VoP_msrvtt9k_features.pkl'), self.device)
+        logger.info('load database done')
 
     def preprocess(self, input: Input) -> Dict[str, Any]:
-        if not isinstance(input, str):
+        if isinstance(input, str):
+            if '.mp4' in input:
+                query = []
+                for video_path in [input]:
+                    video_path = osp.join(self.local_pth, video_path)
+                    imgs, idxs = load_frames_from_video(
+                        video_path, self.cfg.hyperparam.num_frames,
+                        self.cfg.hyperparam.video_sample_type)
+                    imgs = self.img_transform(imgs)
+                    query.append(imgs)
+                query = torch.stack(
+                    query, dim=0).to(
+                        self.device, non_blocking=True)
+                mode = 'v2t'
+            else:
+                query = self.tokenizer(
+                    input, return_tensors='pt', padding=True, truncation=True)
+                if isinstance(query, torch.Tensor):
+                    query = query.to(self.device, non_blocking=True)
+                else:
+                    query = {
+                        key: val.to(self.device, non_blocking=True)
+                        for key, val in query.items()
+                    }
+                mode = 't2v'
+        else:
             raise TypeError(f'input should be a str,'
                             f'  but got {type(input)}')
-        frames = []
-        img_metas = []
-        iids = []
-        cap = cv2.VideoCapture(input)
-        self.fps = cap.get(cv2.CAP_PROP_FPS)
-        self.frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
-        frame_idx = 0
-        while (cap.isOpened()):
-            ret, frame = cap.read()
-            if not ret:
-                break
-
-            if frame_idx > self.max_video_frames:
-                break
-
-            norm_frame = mmcv.imnormalize(frame, self.mean, self.std,
-                                          self.to_rgb)
-            pad_frame = mmcv.impad_to_multiple(
-                norm_frame, self.pad_size_divisor, pad_val=0)
-
-            img_meta = {}
-            img_meta['ori_shape'] = frame.shape
-            img_meta['img_shape'] = frame.shape
-            img_meta['pad_shape'] = pad_frame.shape
-            img_meta['batch_input_shape'] = pad_frame.shape[0:2]
-            img_meta['scale_factor'] = 1.0,
-            img_meta['flip'] = False
-            img_meta['flip_direction'] = None
-
-            frames.append(pad_frame)
-            img_metas.append([img_meta])
-            iids.append(frame_idx)
-
-            frame_idx += 1
-
-        result = {
-            'video_name': input,
-            'imgs': np.array(frames),
-            'img_metas': img_metas,
-            'iids': iids,
-        }
+        result = {'input_data': query, 'mode': mode}
         return result
 
     def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
-        scores = []
-        labels = []
-        masks = []
-        boxes = []
-        track_ids = []
-        for ii in tqdm(range(len(input['iids']))):
-            img = input['imgs'][ii]
-            img_meta = input['img_metas'][ii]
-            iid = input['iids'][ii]
-
-            x = np.transpose(img, [2, 0, 1])
-            x = np.expand_dims(x, 0)
-            x = torch.from_numpy(x).to(self.device)
-            with torch.no_grad():
-                segm_results = self.model(x, img_meta, rescale=True, iid=iid)
-
-            _, _, _, vis_sem, vis_tracker, label, binary_mask, track_id, thing_bbox_for_tracking = segm_results
-            scores.append([0.99] * len(label))
-            labels.append(label)
-            masks.append(binary_mask)
-            boxes.append(thing_bbox_for_tracking)
-            track_ids.append(track_id)
-
-        output = {
-            'scores': scores,
-            'labels': labels,
-            'masks': masks,
-            'boxes': boxes,
-            'uuid': track_ids
-        }
-        return output
+        text_embeds, vid_embeds_pooled, vid_ids, texts = self.database
+        with torch.no_grad():
+            if input['mode'] == 't2v':
+                query_feats = self.model.get_text_features(input['input_data'])
+                score = query_feats @ vid_embeds_pooled.T
+                retrieval_idxs = torch.topk(
+                    score, k=self.cfg.hyperparam.topk,
+                    dim=-1)[1].cpu().numpy()
+                res = np.array(vid_ids)[retrieval_idxs]
+            elif input['mode'] == 'v2t':
+                query_feats = self.model.get_video_features(
+                    input['input_data'])
+                score = query_feats @ text_embeds.T
+                retrieval_idxs = torch.topk(
+                    score, k=self.cfg.hyperparam.topk,
+                    dim=-1)[1].cpu().numpy()
+                res = np.array(texts)[retrieval_idxs]
+            results = {'output_data': res, 'mode': input['mode']}
+            return results
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_single_object_tracking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_stabilization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_stabilization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_summarization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/video_super_resolution_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/video_super_resolution_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/vidt_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/vidt_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/virtual_try_on_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/virtual_try_on_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/vision_middleware_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/vision_middleware_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/cv/vop_retrieval_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/cv/vop_retrieval_se_pipeline.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,53 +1,61 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import gzip
-import math
-import os
 import os.path as osp
-import pickle
-import random
-from collections import defaultdict, deque
 from typing import Any, Dict
 
 import numpy as np
 import torch
-from tqdm import tqdm
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
-from modelscope.models.cv.vop_retrieval import (LengthAdaptiveTokenizer, VoP,
+from modelscope.models.cv.vop_retrieval import (LengthAdaptiveTokenizer,
                                                 init_transform_dict, load_data,
                                                 load_frames_from_video)
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Input, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.preprocessors import load_image
 from modelscope.utils.config import Config
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.vop_retrieval, module_name=Pipelines.vop_retrieval)
-class VopRetrievalPipeline(Pipeline):
+    Tasks.vop_retrieval, module_name=Pipelines.vop_retrieval_se)
+class VopRetrievalSEPipeline(Pipeline):
 
     def __init__(self, model: str, **kwargs):
-        """
-        use `model` to create a vop pipeline for retrieval
-        Args:
-            model: model id on modelscope hub.
+        r""" Card VopRetrievalSE Pipeline.
+
+        Examples:
+        >>>
+        >>>   from modelscope.pipelines import pipeline
+        >>>   vop_pipeline = pipeline(Tasks.vop_retrieval,
+        >>>            model='damo/cv_vit-b32_retrieval_vop_bias')
+        >>>
+        >>>   # IF DO TEXT-TO-VIDEO:
+        >>>   input_text = 'a squid is talking'
+        >>>   result = vop_pipeline(input_text)
+        >>>   result:
+        >>>   {'output_data': array([['video8916']], dtype='<U9'),'mode': 't2v'}
+        >>>
+        >>>   # IF DO VIDEO-TO-TEXT:
+        >>>   input_video = 'video10.mp4'
+        >>>   result = vop_pipeline(input_video)
+        >>>   result:
+        >>>   {'output_data': array([['assorted people are shown holding cute pets']], dtype='<U163'), 'mode': 'v2t'}
+        >>>
         """
         super().__init__(model=model, **kwargs)
 
         # [from pretrain] load model
-        self.model = Model.from_pretrained('damo/cv_vit-b32_retrieval_vop').to(
-            self.device)
+        self.model = Model.from_pretrained(model).to(self.device)
         logger.info('load model done')
 
         # others: load transform
         self.local_pth = model
         self.cfg = Config.from_file(osp.join(model, ModelFile.CONFIGURATION))
         self.img_transform = init_transform_dict(
             self.cfg.hyperparam.input_res)['clip_test']
@@ -57,19 +65,29 @@
         bpe_path = gzip.open(osp.join(
             model,
             'bpe_simple_vocab_16e6.txt.gz')).read().decode('utf-8').split('\n')
         self.tokenizer = LengthAdaptiveTokenizer(self.cfg.hyperparam, bpe_path)
         logger.info('load tokenizer done')
 
         # others: load dataset
-        self.database = load_data(
-            osp.join(model, 'VoP_msrvtt9k_features.pkl'), self.device)
+        if 'vop_bias' in model:
+            self.database = load_data(
+                osp.join(model, 'Bias_msrvtt9k_features.pkl'), self.device)
+        elif 'vop_partial' in model:
+            self.database = load_data(
+                osp.join(model, 'Partial_msrvtt9k_features.pkl'), self.device)
+        elif 'vop_proj' in model:
+            self.database = load_data(
+                osp.join(model, 'Proj_msrvtt9k_features.pkl'), self.device)
+        else:
+            self.database = load_data(
+                osp.join(model, 'VoP_msrvtt9k_features.pkl'), self.device)
         logger.info('load database done')
 
-    def preprocess(self, input: Input) -> Dict[str, Any]:
+    def preprocess(self, input: Input, **preprocess_params) -> Dict[str, Any]:
         if isinstance(input, str):
             if '.mp4' in input:
                 query = []
                 for video_path in [input]:
                     video_path = osp.join(self.local_pth, video_path)
                     imgs, idxs = load_frames_from_video(
                         video_path, self.cfg.hyperparam.num_frames,
@@ -93,15 +111,16 @@
                 mode = 't2v'
         else:
             raise TypeError(f'input should be a str,'
                             f'  but got {type(input)}')
         result = {'input_data': query, 'mode': mode}
         return result
 
-    def forward(self, input: Dict[str, Any]) -> Dict[str, Any]:
+    def forward(self, input: Dict[str, Any],
+                **forward_params) -> Dict[str, Any]:
         text_embeds, vid_embeds_pooled, vid_ids, texts = self.database
         with torch.no_grad():
             if input['mode'] == 't2v':
                 query_feats = self.model.get_text_features(input['input_data'])
                 score = query_feats @ vid_embeds_pooled.T
                 retrieval_idxs = torch.topk(
                     score, k=self.cfg.hyperparam.topk,
@@ -114,9 +133,10 @@
                 retrieval_idxs = torch.topk(
                     score, k=self.cfg.hyperparam.topk,
                     dim=-1)[1].cpu().numpy()
                 res = np.array(texts)[retrieval_idxs]
             results = {'output_data': res, 'mode': input['mode']}
             return results
 
-    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+    def postprocess(self, inputs: Dict[str, Any],
+                    **post_params) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     from .mgeo_ranking_pipeline import MGeoRankingPipeline
     from .document_vl_embedding_pipeline import DocumentVLEmbeddingPipeline
     from .video_captioning_pipeline import VideoCaptioningPipeline
     from .video_question_answering_pipeline import VideoQuestionAnsweringPipeline
     from .diffusers_wrapped import StableDiffusionWrapperPipeline, ChineseStableDiffusionPipeline
     from .soonet_video_temporal_grounding_pipeline import SOONetVideoTemporalGroundingPipeline
     from .text_to_video_synthesis_pipeline import TextToVideoSynthesisPipeline
+    from .multimodal_dialogue_pipeline import MultimodalDialoguePipeline
 else:
     _import_structure = {
         'image_captioning_pipeline': ['ImageCaptioningPipeline'],
         'visual_entailment_pipeline': ['VisualEntailmentPipeline'],
         'visual_grounding_pipeline': ['VisualGroundingPipeline'],
         'multi_modal_embedding_pipeline': ['MultiModalEmbeddingPipeline'],
         'text_to_image_synthesis_pipeline': ['TextToImageSynthesisPipeline'],
@@ -41,14 +42,15 @@
         'video_question_answering_pipeline':
         ['VideoQuestionAnsweringPipeline'],
         'diffusers_wrapped':
         ['StableDiffusionWrapperPipeline', 'ChineseStableDiffusionPipeline'],
         'soonet_video_temporal_grounding_pipeline':
         ['SOONetVideoTemporalGroundingPipeline'],
         'text_to_video_synthesis_pipeline': ['TextToVideoSynthesisPipeline'],
+        'multimodal_dialogue_pipeline': ['MultimodalDialoguePipeline']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/asr_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/asr_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/document_vl_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/sudoku_pipeline.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,61 +1,53 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import Any, Dict, Optional, Union
 
 import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.multi_modal.vldoc.model import VLDocForDocVLEmbedding
-from modelscope.outputs import OutputKeys
-from modelscope.pipelines.base import Input, Model, Pipeline
+from modelscope.models.multi_modal import OfaForAllTasks
+from modelscope.pipelines.base import Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.preprocessors.multi_modal import (Preprocessor,
-                                                  VLDocPreprocessor)
-from modelscope.utils.constant import ModelFile, Tasks
+from modelscope.pipelines.util import batch_process
+from modelscope.preprocessors import OfaPreprocessor, Preprocessor
+from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
-@PIPELINES.register_module(
-    Tasks.document_vl_embedding, module_name=Pipelines.document_vl_embedding)
-class DocumentVLEmbeddingPipeline(Pipeline):
+@PIPELINES.register_module(Tasks.sudoku, module_name=Pipelines.ofa_sudoku)
+class SudokuPipeline(Pipeline):
+    R"""
+    pipeline for sudoku solving
+    """
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
                  **kwargs):
-        """ The pipeline for multi-modal document embedding generation.
-
+        """
+        use `model` and `preprocessor` to create a pipeline for solving sudoku
         Args:
             model: model id on modelscope hub.
-            preprocessor: type `Preprocessor`. If None, `VLDocPreprocessor` is used.
-
-        Examples:
-
-        >>> from modelscope.models import Model
-        >>> from modelscope.pipelines import pipeline
-        >>> model = Model.from_pretrained(
-            'damo/multi-modal_convnext-roberta-base_vldoc-embedding')
-        >>> doc_VL_emb_pipeline = pipeline(task='document-vl-embedding', model=model)
-        >>> inp = {
-                'images': ['data/demo.png'],
-                'ocr_info_paths': ['data/demo.json']
-            }
-        >>> result = doc_VL_emb_pipeline(inp)
         """
-
         super().__init__(model=model, preprocessor=preprocessor, **kwargs)
         self.model.eval()
         if preprocessor is None:
-            if isinstance(self.model, VLDocForDocVLEmbedding):
-                self.preprocessor = VLDocPreprocessor(self.model.model_dir)
+            if isinstance(self.model, OfaForAllTasks):
+                self.preprocessor = OfaPreprocessor(self.model.model_dir)
             else:
-                raise NotImplementedError
+                raise 'no preprocessor is provided'
 
-    def forward(self, encodings: Dict[str, Any]) -> Dict[str, Any]:
-        for k, v in encodings.items():
-            encodings[k] = encodings[k].to(self.device)
-        return self.model(**encodings)
+    def _batch(self, data):
+        if isinstance(self.model, OfaForAllTasks):
+            return batch_process(self.model, data)
+        else:
+            return super(SudokuPipeline, self)._batch(data)
+
+    def forward(self, inputs: Dict[str, Any],
+                **forward_params) -> Dict[str, Any]:
+        with torch.no_grad():
+            return super().forward(inputs, **forward_params)
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/gridvlp_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/gridvlp_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/image_captioning_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/image_captioning_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import Any, Dict, Optional, Union
 
+import numpy as np
 import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.multi_modal import MPlugForAllTasks, OfaForAllTasks
+from modelscope.models.multi_modal import (CLIP_Interrogator, MPlugForAllTasks,
+                                           OfaForAllTasks)
 from modelscope.pipelines.base import Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.pipelines.util import batch_process
-from modelscope.preprocessors import (MPlugPreprocessor, OfaPreprocessor,
-                                      Preprocessor)
+from modelscope.preprocessors import (
+    ImageCaptioningClipInterrogatorPreprocessor, MPlugPreprocessor,
+    OfaPreprocessor, Preprocessor, load_image)
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
@@ -24,25 +27,39 @@
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
                  **kwargs):
         """
         use `model` and `preprocessor` to create a image captioning pipeline for prediction
         Args:
             model: model id on modelscope hub.
+        Examples:
+        from modelscope.pipelines import pipeline
+        from modelscope.utils.constant import Tasks
+
+        model_id = 'damo/cv_clip-interrogator'
+        input_image = "test.png"
+
+        pipeline_ci = pipeline(Tasks.image_captioning, model=model_id)
+        print(pipeline_ci(input_image))
+
+
         """
         super().__init__(model=model, preprocessor=preprocessor, **kwargs)
         self.model.eval()
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
         if preprocessor is None:
 
             if isinstance(self.model, OfaForAllTasks):
                 self.preprocessor = OfaPreprocessor(self.model.model_dir)
             elif isinstance(self.model, MPlugForAllTasks):
                 self.preprocessor = MPlugPreprocessor(self.model.model_dir)
+            elif isinstance(self.model, CLIP_Interrogator):
+                self.preprocessor = ImageCaptioningClipInterrogatorPreprocessor(
+                )
 
     def _batch(self, data):
         if isinstance(self.model, OfaForAllTasks):
             return batch_process(self.model, data)
         elif isinstance(self.model, MPlugForAllTasks):
             from transformers.tokenization_utils_base import BatchEncoding
             batch_data = dict(train=data[0]['train'])
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,37 +12,36 @@
 from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.ocr_recognition, module_name=Pipelines.ofa_ocr_recognition)
-class OcrRecognitionPipeline(Pipeline):
+    Tasks.visual_entailment, module_name=Pipelines.visual_entailment)
+class VisualEntailmentPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
                  **kwargs):
         """
-        use `model` and `preprocessor` to create a ocr recognition pipeline for prediction
+        use `model` and `preprocessor` to create a visual entailment pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, preprocessor=preprocessor, **kwargs)
         self.model.eval()
-        if preprocessor is None:
-            if isinstance(self.model, OfaForAllTasks):
-                self.preprocessor = OfaPreprocessor(self.model.model_dir)
+        if preprocessor is None and isinstance(self.model, OfaForAllTasks):
+            self.preprocessor = OfaPreprocessor(model_dir=self.model.model_dir)
 
     def _batch(self, data):
         if isinstance(self.model, OfaForAllTasks):
             return batch_process(self.model, data)
         else:
-            return super(OcrRecognitionPipeline, self)._batch(data)
+            return super(VisualEntailmentPipeline, self)._batch(data)
 
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
         with torch.no_grad():
             return super().forward(inputs, **forward_params)
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/sudoku_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/text2sql_pipeline.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,42 +11,40 @@
 from modelscope.preprocessors import OfaPreprocessor, Preprocessor
 from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
-@PIPELINES.register_module(Tasks.sudoku, module_name=Pipelines.ofa_sudoku)
-class SudokuPipeline(Pipeline):
+@PIPELINES.register_module(Tasks.text2sql, module_name=Pipelines.ofa_text2sql)
+class TextToSqlPipeline(Pipeline):
     R"""
-    pipeline for sudoku solving
+    pipeline for text to sql task
     """
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
                  **kwargs):
         """
-        use `model` and `preprocessor` to create a pipeline for solving sudoku
+        use `model` and `preprocessor` to create a pipeline for text2sql task
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, preprocessor=preprocessor, **kwargs)
         self.model.eval()
         if preprocessor is None:
             if isinstance(self.model, OfaForAllTasks):
                 self.preprocessor = OfaPreprocessor(self.model.model_dir)
-            else:
-                raise 'no preprocessor is provided'
 
     def _batch(self, data):
         if isinstance(self.model, OfaForAllTasks):
             return batch_process(self.model, data)
         else:
-            return super(SudokuPipeline, self)._batch(data)
+            return super(TextToSqlPipeline, self)._batch(data)
 
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
         with torch.no_grad():
             return super().forward(inputs, **forward_params)
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/text2sql_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py`

 * *Files 11% similar despite different names*

```diff
@@ -11,40 +11,37 @@
 from modelscope.preprocessors import OfaPreprocessor, Preprocessor
 from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
-@PIPELINES.register_module(Tasks.text2sql, module_name=Pipelines.ofa_text2sql)
-class TextToSqlPipeline(Pipeline):
-    R"""
-    pipeline for text to sql task
-    """
+@PIPELINES.register_module(
+    Tasks.visual_grounding, module_name=Pipelines.visual_grounding)
+class VisualGroundingPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
                  **kwargs):
         """
-        use `model` and `preprocessor` to create a pipeline for text2sql task
+        use `model` and `preprocessor` to create a visual grounding pipeline for prediction
         Args:
             model: model id on modelscope hub.
         """
         super().__init__(model=model, preprocessor=preprocessor, **kwargs)
-        self.model.eval()
-        if preprocessor is None:
-            if isinstance(self.model, OfaForAllTasks):
-                self.preprocessor = OfaPreprocessor(self.model.model_dir)
+        self.model.model.eval()
+        if preprocessor is None and isinstance(self.model, OfaForAllTasks):
+            self.preprocessor = OfaPreprocessor(model_dir=self.model.model_dir)
 
     def _batch(self, data):
         if isinstance(self.model, OfaForAllTasks):
             return batch_process(self.model, data)
         else:
-            return super(TextToSqlPipeline, self)._batch(data)
+            return super(VisualGroundingPipeline, self)._batch(data)
 
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
         with torch.no_grad():
             return super().forward(inputs, **forward_params)
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
+import os
 import tempfile
 from typing import Any, Dict, Optional
 
 import cv2
 import torch
 from einops import rearrange
 
@@ -58,25 +59,35 @@
         video = self.model(input)
         return {'video': video}
 
     def postprocess(self, inputs: Dict[str, Any],
                     **post_params) -> Dict[str, Any]:
         video = tensor2vid(inputs['video'])
         output_video_path = post_params.get('output_video', None)
+        temp_video_file = False
         if output_video_path is None:
             output_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name
+            temp_video_file = True
 
         fourcc = cv2.VideoWriter_fourcc(*'mp4v')
         h, w, c = video[0].shape
         video_writer = cv2.VideoWriter(
             output_video_path, fourcc, fps=8, frameSize=(w, h))
         for i in range(len(video)):
             img = cv2.cvtColor(video[i], cv2.COLOR_RGB2BGR)
             video_writer.write(img)
-        return {OutputKeys.OUTPUT_VIDEO: output_video_path}
+        video_writer.release()
+        if temp_video_file:
+            video_file_content = b''
+            with open(output_video_path, 'rb') as f:
+                video_file_content = f.read()
+            os.remove(output_video_path)
+            return {OutputKeys.OUTPUT_VIDEO: video_file_content}
+        else:
+            return {OutputKeys.OUTPUT_VIDEO: output_video_path}
 
 
 def tensor2vid(video, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):
     mean = torch.tensor(
         mean, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw
     std = torch.tensor(
         std, device=video.device).reshape(1, -1, 1, 1, 1)  # ncfhw
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/video_captioning_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/video_captioning_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/video_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/visual_entailment_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,48 +1,63 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import Any, Dict, Optional, Union
 
 import torch
 
 from modelscope.metainfo import Pipelines
-from modelscope.models.multi_modal import OfaForAllTasks
-from modelscope.pipelines.base import Model, Pipeline
+from modelscope.models import Model
+from modelscope.models.multi_modal import MPlugForAllTasks, OfaForAllTasks
+from modelscope.pipelines.base import Pipeline, Tensor
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.pipelines.util import batch_process
-from modelscope.preprocessors import OfaPreprocessor, Preprocessor
+from modelscope.preprocessors import (MPlugPreprocessor, OfaPreprocessor,
+                                      Preprocessor)
 from modelscope.utils.constant import Tasks
-from modelscope.utils.logger import get_logger
 
-logger = get_logger()
+__all__ = ['VisualQuestionAnsweringPipeline']
 
 
 @PIPELINES.register_module(
-    Tasks.visual_entailment, module_name=Pipelines.visual_entailment)
-class VisualEntailmentPipeline(Pipeline):
+    Tasks.visual_question_answering,
+    module_name=Pipelines.visual_question_answering)
+class VisualQuestionAnsweringPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
                  **kwargs):
-        """
-        use `model` and `preprocessor` to create a visual entailment pipeline for prediction
+        """use `model` and `preprocessor` to create a visual question answering pipeline for prediction
+
         Args:
-            model: model id on modelscope hub.
+            model (MPlugForVisualQuestionAnswering): a model instance
+            preprocessor (MPlugVisualQuestionAnsweringPreprocessor): a preprocessor instance
         """
         super().__init__(model=model, preprocessor=preprocessor, **kwargs)
+        if preprocessor is None:
+            if isinstance(self.model, OfaForAllTasks):
+                self.preprocessor = OfaPreprocessor(self.model.model_dir)
+            elif isinstance(self.model, MPlugForAllTasks):
+                self.preprocessor = MPlugPreprocessor(self.model.model_dir)
         self.model.eval()
-        if preprocessor is None and isinstance(self.model, OfaForAllTasks):
-            self.preprocessor = OfaPreprocessor(model_dir=self.model.model_dir)
 
     def _batch(self, data):
         if isinstance(self.model, OfaForAllTasks):
             return batch_process(self.model, data)
         else:
-            return super(VisualEntailmentPipeline, self)._batch(data)
+            return super(VisualQuestionAnsweringPipeline, self)._batch(data)
 
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
         with torch.no_grad():
             return super().forward(inputs, **forward_params)
 
-    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+    def postprocess(self, inputs: Dict[str, Tensor],
+                    **postprocess_params) -> Dict[str, str]:
+        """process the prediction results
+
+        Args:
+            inputs (Dict[str, Any]): _description_
+
+        Returns:
+            Dict[str, str]: the prediction results
+        """
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/visual_grounding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/summarization_pipeline.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,47 +1,65 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import Any, Dict, Optional, Union
 
 import torch
 
-from modelscope.metainfo import Pipelines
-from modelscope.models.multi_modal import OfaForAllTasks
+from modelscope.metainfo import Pipelines, Preprocessors
 from modelscope.pipelines.base import Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.pipelines.util import batch_process
-from modelscope.preprocessors import OfaPreprocessor, Preprocessor
-from modelscope.utils.constant import Tasks
+from modelscope.preprocessors import Preprocessor
+from modelscope.utils.constant import Fields, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.visual_grounding, module_name=Pipelines.visual_grounding)
-class VisualGroundingPipeline(Pipeline):
+    Tasks.text_summarization, module_name=Pipelines.text_generation)
+class SummarizationPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
+                 config_file: str = None,
+                 device: str = 'gpu',
+                 auto_collate=True,
                  **kwargs):
-        """
-        use `model` and `preprocessor` to create a visual grounding pipeline for prediction
+        """Use `model` and `preprocessor` to create a Summarization pipeline for prediction.
+
         Args:
-            model: model id on modelscope hub.
+            model (str or Model): Supply either a local model dir which supported the summarization task,
+            or a model id from the model hub, or a model instance.
+            preprocessor (Preprocessor): An optional preprocessor instance.
+            kwargs (dict, `optional`):
+                Extra kwargs passed into the preprocessor's constructor.
         """
-        super().__init__(model=model, preprocessor=preprocessor, **kwargs)
-        self.model.model.eval()
-        if preprocessor is None and isinstance(self.model, OfaForAllTasks):
-            self.preprocessor = OfaPreprocessor(model_dir=self.model.model_dir)
+        super().__init__(
+            model=model,
+            preprocessor=preprocessor,
+            config_file=config_file,
+            device=device,
+            auto_collate=auto_collate)
+        self.model.eval()
+        if preprocessor is None:
+            if self.model.__class__.__name__ == 'OfaForAllTasks':
+                self.preprocessor = Preprocessor.from_pretrained(
+                    self.model.model_dir,
+                    type=Preprocessors.ofa_tasks_preprocessor,
+                    field=Fields.multi_modal)
+            else:
+                self.preprocessor = Preprocessor.from_pretrained(
+                    self.model.model_dir, **kwargs)
 
     def _batch(self, data):
-        if isinstance(self.model, OfaForAllTasks):
+        if self.model.__class__.__name__ == 'OfaForAllTasks':
             return batch_process(self.model, data)
         else:
-            return super(VisualGroundingPipeline, self)._batch(data)
+            return super(SummarizationPipeline, self)._batch(data)
 
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
         with torch.no_grad():
             return super().forward(inputs, **forward_params)
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/multi_modal/visual_question_answering_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/information_extraction_pipeline.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,63 +1,65 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
+
 from typing import Any, Dict, Optional, Union
 
 import torch
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
-from modelscope.models.multi_modal import MPlugForAllTasks, OfaForAllTasks
-from modelscope.pipelines.base import Pipeline, Tensor
+from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.pipelines.util import batch_process
-from modelscope.preprocessors import (MPlugPreprocessor, OfaPreprocessor,
-                                      Preprocessor)
-from modelscope.utils.constant import Tasks
+from modelscope.preprocessors import Preprocessor
+from modelscope.utils.constant import ModelFile, Tasks
 
-__all__ = ['VisualQuestionAnsweringPipeline']
+__all__ = ['InformationExtractionPipeline']
 
 
 @PIPELINES.register_module(
-    Tasks.visual_question_answering,
-    module_name=Pipelines.visual_question_answering)
-class VisualQuestionAnsweringPipeline(Pipeline):
+    Tasks.information_extraction, module_name=Pipelines.relation_extraction)
+@PIPELINES.register_module(
+    Tasks.relation_extraction, module_name=Pipelines.relation_extraction)
+class InformationExtractionPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
+                 config_file: str = None,
+                 device: str = 'gpu',
+                 auto_collate=True,
+                 sequence_length=512,
                  **kwargs):
-        """use `model` and `preprocessor` to create a visual question answering pipeline for prediction
+        """
 
         Args:
-            model (MPlugForVisualQuestionAnswering): a model instance
-            preprocessor (MPlugVisualQuestionAnsweringPreprocessor): a preprocessor instance
+            model (str or Model): Supply either a local model dir which supported information extraction task, or a
+            model id from the model hub, or a torch model instance.
+            preprocessor (Preprocessor): An optional preprocessor instance, please make sure the preprocessor fits for
+            the model if supplied.
+            kwargs (dict, `optional`):
+                Extra kwargs passed into the preprocessor's constructor.
         """
-        super().__init__(model=model, preprocessor=preprocessor, **kwargs)
-        if preprocessor is None:
-            if isinstance(self.model, OfaForAllTasks):
-                self.preprocessor = OfaPreprocessor(self.model.model_dir)
-            elif isinstance(self.model, MPlugForAllTasks):
-                self.preprocessor = MPlugPreprocessor(self.model.model_dir)
+        super().__init__(
+            model=model,
+            preprocessor=preprocessor,
+            config_file=config_file,
+            device=device,
+            auto_collate=auto_collate)
+
+        assert isinstance(self.model, Model), \
+            f'please check whether model config exists in {ModelFile.CONFIGURATION}'
+
+        if self.preprocessor is None:
+            self.preprocessor = Preprocessor.from_pretrained(
+                self.model.model_dir,
+                sequence_length=sequence_length,
+                **kwargs)
         self.model.eval()
 
-    def _batch(self, data):
-        if isinstance(self.model, OfaForAllTasks):
-            return batch_process(self.model, data)
-        else:
-            return super(VisualQuestionAnsweringPipeline, self)._batch(data)
-
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
         with torch.no_grad():
-            return super().forward(inputs, **forward_params)
+            return self.model(**inputs, **forward_params)
 
-    def postprocess(self, inputs: Dict[str, Tensor],
+    def postprocess(self, inputs: Dict[str, Any],
                     **postprocess_params) -> Dict[str, str]:
-        """process the prediction results
-
-        Args:
-            inputs (Dict[str, Any]): _description_
-
-        Returns:
-            Dict[str, str]: the prediction results
-        """
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/__init__.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/automatic_post_editing_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/canmt_translation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/canmt_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/codegeex_code_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/codegeex_code_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/conversational_text_to_sql_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/dialog_intent_prediction_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -36,15 +36,17 @@
                 Extra kwargs passed into the preprocessor's constructor.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
         if preprocessor is None:
             self.preprocessor = DialogIntentPredictionPreprocessor(
                 self.model.model_dir, **kwargs)
         self.categories = self.preprocessor.categories
 
     def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, str]:
         """process the prediction results
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/dialog_modeling_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/dialog_modeling_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/dialog_state_tracking_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,15 +38,17 @@
         """
 
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         if preprocessor is None:
             self.preprocessor = DialogStateTrackingPreprocessor(
                 self.model.model_dir, **kwargs)
 
         self.tokenizer = self.preprocessor.tokenizer
         self.config = self.preprocessor.config
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/distributed_gpt3_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/distributed_gpt_moe_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/distributed_plug_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/distributed_plug_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/document_grounded_dialog_generate_pipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -42,15 +42,17 @@
             >>> pipe_ins = pipeline('document-grounded-dialog-generate', model='damo/nlp_convai_generate')
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         if preprocessor is None:
             self.preprocessor = DocumentGroundedDialogGeneratePreprocessor(
                 self.model.model_dir, **kwargs)
 
     def forward(self, inputs: Union[list, Dict[str, Any]],
                 **forward_params) -> Dict[str, Any]:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -60,15 +60,17 @@
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
             auto_collate=auto_collate,
-            seed=seed)
+            seed=seed,
+            compile=kwarg.pop('compile', False),
+            compile_options=kwarg.pop('compile_options', {}))
         self.model = model
         self.preprocessor = preprocessor
         self.device = device
         if kwarg['model_resize']:
             self.model.resize_token_embeddings(
                 len(self.preprocessor.tokenizer))
         self.model.to(self.device)
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,15 +51,17 @@
 
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         if preprocessor is None:
             self.preprocessor = DocumentGroundedDialogRetrievalPreprocessor(
                 self.model.model_dir, **kwargs)
         self.per_gpu_batch_size = per_gpu_batch_size
         self.passages_index = []
         self.passages = []
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/document_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/document_segmentation_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,19 +2,17 @@
 
 import re
 from typing import Any, Dict, List, Union
 
 import numpy as np
 import torch
 from datasets import Dataset
-from transformers.models.bert.modeling_bert import BertConfig
 
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
-from modelscope.models.nlp.ponet.configuration import PoNetConfig
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline, Tensor
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import \
     DocumentSegmentationTransformersPreprocessor
 from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
@@ -44,15 +42,19 @@
             the model if supplied.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            **kwargs)
+
+        kwargs.pop('compile', None)
+        kwargs.pop('compile_options', None)
 
         self.model_dir = self.model.model_dir
         self.model_cfg = self.model.model_cfg
         if preprocessor is None:
             self.preprocessor = DocumentSegmentationTransformersPreprocessor(
                 self.model_dir, self.model.config.max_position_embeddings,
                 **kwargs)
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/extractive_summarization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/extractive_summarization_pipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -37,15 +37,19 @@
             **kwargs):
 
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            **kwargs)
+
+        kwargs.pop('compile', None)
+        kwargs.pop('compile_options', None)
 
         self.model_dir = self.model.model_dir
         self.model_cfg = self.model.model_cfg
 
         if preprocessor is None:
             self.preprocessor = DocumentSegmentationTransformersPreprocessor(
                 self.model_dir, self.model.config.max_position_embeddings,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/faq_question_answering_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/faq_question_answering_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/fasttext_text_classification_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/feature_extraction_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/feature_extraction_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -49,15 +49,17 @@
 
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir,
                 padding=padding,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/fid_dialogue_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/fid_dialogue_pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -187,16 +187,16 @@
             history_list[i] = subject + '' + history_list[i]
             subject = '' if subject == '' else ''
         return ''.join(history_list)
 
     def postprocess(self, inputs: TokenGeneratorOutput,
                     **postprocess_params) -> Dict[str, Any]:
 
-        if torch.cuda.is_available():
-            hypotheses = inputs.sequences.detach().cpu().tolist()
+        # if torch.cuda.is_available():
+        hypotheses = inputs.sequences.detach().cpu().tolist()
 
         response = self.preprocessor_tokenizer.decode(
             hypotheses[0], skip_special_tokens=self.is_t5)
 
         token_mapping = {
             '<extra_id_22>': '\n',
             '<extra_id_33>': '\t',
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/fill_mask_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/fill_mask_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -58,15 +58,17 @@
         To view other examples plese check tests/pipelines/test_fill_mask.py.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir,
                 first_sequence=first_sequence,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/glm130b_text_generation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/information_extraction_pipeline.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,65 +1,59 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-from typing import Any, Dict, Optional, Union
+from typing import Any, Dict
 
-import torch
+from transformers import AutoTokenizer
 
-from modelscope.metainfo import Pipelines
-from modelscope.models import Model
-from modelscope.pipelines.base import Pipeline
-from modelscope.pipelines.builder import PIPELINES
+from modelscope.metainfo import Preprocessors
 from modelscope.preprocessors import Preprocessor
-from modelscope.utils.constant import ModelFile, Tasks
+from modelscope.preprocessors.builder import PREPROCESSORS
+from modelscope.utils.constant import Fields, ModeKeys
+from modelscope.utils.type_assert import type_assert
+
+
+@PREPROCESSORS.register_module(
+    Fields.nlp, module_name=Preprocessors.re_tokenizer)
+class RelationExtractionTransformersPreprocessor(Preprocessor):
+
+    def __init__(
+        self,
+        model_dir: str,
+        mode: str = ModeKeys.INFERENCE,
+        **kwargs,
+    ):
+        """The preprocessor for relation Extraction task, based on transformers' tokenizer.
 
-__all__ = ['InformationExtractionPipeline']
-
-
-@PIPELINES.register_module(
-    Tasks.information_extraction, module_name=Pipelines.relation_extraction)
-@PIPELINES.register_module(
-    Tasks.relation_extraction, module_name=Pipelines.relation_extraction)
-class InformationExtractionPipeline(Pipeline):
-
-    def __init__(self,
-                 model: Union[Model, str],
-                 preprocessor: Optional[Preprocessor] = None,
-                 config_file: str = None,
-                 device: str = 'gpu',
-                 auto_collate=True,
-                 sequence_length=512,
-                 **kwargs):
+        Args:
+            model_dir: The model dir used to initialize the tokenizer.
+            mode: The mode for the preprocessor.
         """
 
+        super().__init__(mode)
+        self.model_dir: str = model_dir
+        self.tokenizer = AutoTokenizer.from_pretrained(
+            model_dir, use_fast=True)
+
+    @type_assert(object, str)
+    def __call__(self, data: str, **kwargs) -> Dict[str, Any]:
+        """process the raw input data
+
         Args:
-            model (str or Model): Supply either a local model dir which supported information extraction task, or a
-            model id from the model hub, or a torch model instance.
-            preprocessor (Preprocessor): An optional preprocessor instance, please make sure the preprocessor fits for
-            the model if supplied.
-            kwargs (dict, `optional`):
-                Extra kwargs passed into the preprocessor's constructor.
+            data (str): a sentence
+                Example:
+                    'you are so handsome.'
+
+        Returns:
+            Dict[str, Any]: the preprocessed data
         """
-        super().__init__(
-            model=model,
-            preprocessor=preprocessor,
-            config_file=config_file,
-            device=device,
-            auto_collate=auto_collate)
-
-        assert isinstance(self.model, Model), \
-            f'please check whether model config exists in {ModelFile.CONFIGURATION}'
-
-        if self.preprocessor is None:
-            self.preprocessor = Preprocessor.from_pretrained(
-                self.model.model_dir,
-                sequence_length=sequence_length,
-                **kwargs)
-        self.model.eval()
-
-    def forward(self, inputs: Dict[str, Any],
-                **forward_params) -> Dict[str, Any]:
-        with torch.no_grad():
-            return self.model(**inputs, **forward_params)
-
-    def postprocess(self, inputs: Dict[str, Any],
-                    **postprocess_params) -> Dict[str, str]:
-        return inputs
+
+        # preprocess the data for the model input
+        text = data
+        if 'return_tensors' not in kwargs:
+            kwargs['return_tensors'] = 'pt'
+        output = self.tokenizer([text], **kwargs)
+        return {
+            'text': text,
+            'input_ids': output['input_ids'],
+            'attention_mask': output['attention_mask'],
+            'offsets': output[0].offsets
+        }
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/interactive_translation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/interactive_translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/language_identification_pipline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/language_identification_pipline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/mglm_text_summarization_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/named_entity_recognition_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -51,15 +51,17 @@
             To view other examples plese check the tests/pipelines/test_plugin_model.py.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/sentence_embedding_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/sentence_embedding_pipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -38,15 +38,17 @@
                 Extra kwargs passed into the preprocessor's constructor.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/siamese_uie_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/siamese_uie_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from modelscope.metainfo import Pipelines
 from modelscope.models import Model
 from modelscope.msdatasets import MsDataset
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import Pipeline
 from modelscope.pipelines.builder import PIPELINES
 from modelscope.preprocessors import Preprocessor, SiameseUiePreprocessor
-from modelscope.utils.constant import Tasks
+from modelscope.utils.constant import ModelFile, Tasks
 
 Input = Union[str, tuple, MsDataset, 'Image.Image', 'numpy.ndarray']
 
 logger = logging.getLogger(__name__)
 
 os.environ['TOKENIZERS_PARALLELISM'] = 'true'
 
@@ -63,15 +63,17 @@
             To view other examples plese check tests/pipelines/test_siamese_uie.py.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if self.preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir, **kwargs)
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/summarization_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,66 +1,90 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import Any, Dict, Optional, Union
 
 import torch
 
-from modelscope.metainfo import Pipelines, Preprocessors
+from modelscope.metainfo import Pipelines
+from modelscope.models.multi_modal import MplugOwlForConditionalGeneration
+from modelscope.outputs import OutputKeys, TokenGeneratorOutput
 from modelscope.pipelines.base import Model, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.pipelines.util import batch_process
-from modelscope.preprocessors import Preprocessor
-from modelscope.utils.constant import Fields, Tasks
+from modelscope.preprocessors import MplugOwlPreprocessor, Preprocessor
+from modelscope.utils.constant import Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 
 @PIPELINES.register_module(
-    Tasks.text_summarization, module_name=Pipelines.text_generation)
-class SummarizationPipeline(Pipeline):
+    Tasks.multimodal_dialogue, module_name=Pipelines.multimodal_dialogue)
+class MultimodalDialoguePipeline(Pipeline):
+    r""" Multimodal Dialogue Pipeline.
+
+    Examples:
+    >>> from modelscope.pipelines import pipeline
+    >>> chatbot = pipeline('multimodal-dialogue', 'damo/multi-modal_mplug_owl_multimodal-dialogue_7b')
+    >>> image = 'data/resource/portrait_input.png'
+    >>> system_prompt_1 = 'The following is a conversation between a curious human and AI assistant.'
+    >>> system_prompt_2 = "The assistant gives helpful, detailed, and polite answers to the user's questions."
+    >>> messages = {
+    >>>       'messages': [
+    >>>            {
+    >>>                'role': 'system',
+    >>>                'content': system_prompt_1 + ' ' + system_prompt_2
+    >>>            },
+    >>>            {
+    >>>                'role': 'user',
+    >>>                'content': [{
+    >>>                    'image': image
+    >>>                }]
+    >>>            },
+    >>>            {
+    >>>                'role': 'user',
+    >>>                'content': 'Describe the facial expression of the man.'
+    >>>            },
+    >>>        ]
+    >>>    }
+    >>> chatbot(messages)
+    >>> {
+    >>>     "text": he is angry.
+    >>> }
+    >>>
+    """
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: Optional[Preprocessor] = None,
-                 config_file: str = None,
-                 device: str = 'gpu',
-                 auto_collate=True,
                  **kwargs):
-        """Use `model` and `preprocessor` to create a Summarization pipeline for prediction.
-
+        """
+        use `model` and `preprocessor` to create a multimodal dialogue pipeline for prediction
         Args:
-            model (str or Model): Supply either a local model dir which supported the summarization task,
-            or a model id from the model hub, or a model instance.
-            preprocessor (Preprocessor): An optional preprocessor instance.
-            kwargs (dict, `optional`):
-                Extra kwargs passed into the preprocessor's constructor.
+            model: model id on modelscope hub.
         """
-        super().__init__(
-            model=model,
-            preprocessor=preprocessor,
-            config_file=config_file,
-            device=device,
-            auto_collate=auto_collate)
+        super().__init__(model=model, preprocessor=preprocessor, **kwargs)
         self.model.eval()
         if preprocessor is None:
-            if self.model.__class__.__name__ == 'OfaForAllTasks':
-                self.preprocessor = Preprocessor.from_pretrained(
-                    self.model.model_dir,
-                    type=Preprocessors.ofa_tasks_preprocessor,
-                    field=Fields.multi_modal)
-            else:
-                self.preprocessor = Preprocessor.from_pretrained(
-                    self.model.model_dir, **kwargs)
-
-    def _batch(self, data):
-        if self.model.__class__.__name__ == 'OfaForAllTasks':
-            return batch_process(self.model, data)
-        else:
-            return super(SummarizationPipeline, self)._batch(data)
+            if isinstance(self.model, MplugOwlForConditionalGeneration):
+                self.preprocessor = MplugOwlPreprocessor(self.model.model_dir)
 
     def forward(self, inputs: Dict[str, Any],
                 **forward_params) -> Dict[str, Any]:
+        """
+        the `forward_params` can be the generation configurations listed in transformers library.
+        """
         with torch.no_grad():
             return super().forward(inputs, **forward_params)
 
-    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
+    def postprocess(self, inputs: Dict[str, Any]) -> Dict[str, str]:
+        """process the prediction results
+
+        Args:
+            inputs (Dict[str, Any]): _description_
+
+        Returns:
+            Dict[str, str]: the prediction results
+        """
+        if isinstance(self.model, MplugOwlForConditionalGeneration):
+            output = self.preprocessor.tokenizer.decode(
+                inputs[0], skip_special_tokens=True)
+            inputs = {OutputKeys.TEXT: output}
         return inputs
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/table_question_answering_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/table_question_answering_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -47,15 +47,17 @@
                 Extra kwargs passed into the preprocessor's constructor.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if preprocessor is None:
             self.preprocessor = TableQuestionAnsweringPreprocessor(
                 self.model.model_dir, **kwargs)
@@ -397,14 +399,14 @@
         else:
             tabledata = {'header_id': [], 'header_name': [], 'rows': []}
 
         output = {
             OutputKeys.SQL_STRING: sql.string,
             OutputKeys.SQL_QUERY: sql.query,
             OutputKeys.HISTORY: result['sql'],
-            OutputKeys.QUERT_RESULT: tabledata,
+            OutputKeys.QUERY_RESULT: tabledata,
         }
 
         return {OutputKeys.OUTPUT: output}
 
     def _collate_fn(self, data):
         return data
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/text_classification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/text_classification_pipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -72,15 +72,15 @@
             if self.model.__class__.__name__ == 'OfaForAllTasks':
                 self.preprocessor = Preprocessor.from_pretrained(
                     model_name_or_path=self.model.model_dir,
                     type=Preprocessors.ofa_tasks_preprocessor,
                     field=Fields.multi_modal,
                     **kwargs)
             else:
-                first_sequence = kwargs.pop('first_sequence', 'first_sequence')
+                first_sequence = kwargs.pop('first_sequence', 'text')
                 second_sequence = kwargs.pop('second_sequence', None)
                 sequence_length = kwargs.pop('sequence_length', 512)
                 self.preprocessor = Preprocessor.from_pretrained(
                     self.model.model_dir, **{
                         'first_sequence': first_sequence,
                         'second_sequence': second_sequence,
                         'sequence_length': sequence_length,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/text_error_correction_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/text_error_correction_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/text_generation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/text_generation_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -54,15 +54,17 @@
             To view other examples plese check tests/pipelines/test_text_generation.py.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir, first_sequence=first_sequence, **kwargs)
         self.model.eval()
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/text_ranking_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/text_ranking_pipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -39,15 +39,17 @@
                 Extra kwargs passed into the preprocessor's constructor.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/token_classification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/token_classification_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,15 +47,17 @@
                 Extra kwargs passed into the preprocessor's constructor.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if preprocessor is None:
             self.preprocessor = Preprocessor.from_pretrained(
                 self.model.model_dir,
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/translation_evaluation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/translation_evaluation_pipeline.py`

 * *Files 24% similar despite different names*

```diff
@@ -5,20 +5,19 @@
 from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
 import torch
 
 from modelscope.metainfo import Pipelines
 from modelscope.models.base import Model
-from modelscope.models.nlp.unite.configuration_unite import EvaluationMode
+from modelscope.models.nlp.unite.configuration import InputFormat
 from modelscope.outputs import OutputKeys
 from modelscope.pipelines.base import InputModel, Pipeline
 from modelscope.pipelines.builder import PIPELINES
-from modelscope.preprocessors import (Preprocessor,
-                                      TranslationEvaluationPreprocessor)
+from modelscope.preprocessors import Preprocessor
 from modelscope.utils.config import Config
 from modelscope.utils.constant import ModelFile, Tasks
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 __all__ = ['TranslationEvaluationPipeline']
@@ -27,65 +26,67 @@
 @PIPELINES.register_module(
     Tasks.translation_evaluation, module_name=Pipelines.translation_evaluation)
 class TranslationEvaluationPipeline(Pipeline):
 
     def __init__(self,
                  model: InputModel,
                  preprocessor: Optional[Preprocessor] = None,
-                 eval_mode: EvaluationMode = EvaluationMode.SRC_REF,
+                 input_format: InputFormat = InputFormat.SRC_REF,
                  device: str = 'gpu',
                  **kwargs):
-        r"""Build a translation pipeline with a model dir or a model id in the model hub.
+        r"""Build a translation evaluation pipeline with a model dir or a model id in the model hub.
 
         Args:
             model: A Model instance.
-            eval_mode: Evaluation mode, choosing one from `"EvaluationMode.SRC_REF"`,
-                `"EvaluationMode.SRC"`, `"EvaluationMode.REF"`. Aside from hypothesis, the
+            preprocessor: The preprocessor for this pipeline.
+            input_format: Input format, choosing one from `"InputFormat.SRC_REF"`,
+                `"InputFormat.SRC"`, `"InputFormat.REF"`. Aside from hypothesis, the
                 source/reference/source+reference can be presented during evaluation.
+            device: Used device for this pipeline.
         """
-        super().__init__(model=model, preprocessor=preprocessor)
+        super().__init__(
+            model=model,
+            preprocessor=preprocessor,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
-        self.eval_mode = eval_mode
-        self.checking_eval_mode()
+        self.input_format = input_format
+        self.checking_input_format()
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
-        self.preprocessor = TranslationEvaluationPreprocessor(
-            self.model.model_dir,
-            self.eval_mode) if preprocessor is None else preprocessor
-
         self.model.load_checkpoint(
             osp.join(self.model.model_dir, ModelFile.TORCH_MODEL_BIN_FILE),
-            self.device)
+            device=self.device,
+            plm_only=False)
         self.model.eval()
 
         return
 
-    def checking_eval_mode(self):
-        if self.eval_mode == EvaluationMode.SRC:
+    def checking_input_format(self):
+        if self.input_format == InputFormat.SRC:
             logger.info('Evaluation mode: source-only')
-        elif self.eval_mode == EvaluationMode.REF:
+        elif self.input_format == InputFormat.REF:
             logger.info('Evaluation mode: reference-only')
-        elif self.eval_mode == EvaluationMode.SRC_REF:
+        elif self.input_format == InputFormat.SRC_REF:
             logger.info('Evaluation mode: source-reference-combined')
         else:
-            raise ValueError(
-                'Evaluation mode should be one choice among'
-                '\'EvaluationMode.SRC\', \'EvaluationMode.REF\', and'
-                '\'EvaluationMode.SRC_REF\'.')
+            raise ValueError('Evaluation mode should be one choice among'
+                             '\'InputFormat.SRC\', \'InputFormat.REF\', and'
+                             '\'InputFormat.SRC_REF\'.')
 
-    def change_eval_mode(self,
-                         eval_mode: EvaluationMode = EvaluationMode.SRC_REF):
+    def change_input_format(self,
+                            input_format: InputFormat = InputFormat.SRC_REF):
         logger.info('Changing the evaluation mode.')
-        self.eval_mode = eval_mode
-        self.checking_eval_mode()
-        self.preprocessor.eval_mode = eval_mode
+        self.input_format = input_format
+        self.checking_input_format()
+        self.preprocessor.change_input_format(input_format)
         return
 
-    def __call__(self, input: Dict[str, Union[str, List[str]]], **kwargs):
+    def __call__(self, input_dict: Dict[str, Union[str, List[str]]], **kwargs):
         r"""Implementation of __call__ function.
 
         Args:
             input: The formatted dict containing the inputted sentences.
             An example of the formatted dict:
                 ```
                 input = {
@@ -100,16 +101,16 @@
                     'ref': [
                         'It is a sentence.',
                         'It is another sentence.',
                     ]
                 }
                 ```
         """
-        return super().__call__(input=input, **kwargs)
+        return super().__call__(input=input_dict, **kwargs)
 
-    def forward(self,
-                input_ids: List[torch.Tensor]) -> Dict[str, torch.Tensor]:
-        return self.model(input_ids)
+    def forward(
+            self, input_dict: Dict[str,
+                                   torch.Tensor]) -> Dict[str, torch.Tensor]:
+        return self.model(**input_dict)
 
     def postprocess(self, output: torch.Tensor) -> Dict[str, Any]:
-        result = {OutputKeys.SCORES: output.cpu().tolist()}
-        return result
+        return output
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/translation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/translation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/translation_quality_estimation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/user_satisfaction_estimation_pipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -22,15 +22,16 @@
 class UserSatisfactionEstimationPipeline(Pipeline):
 
     def __init__(self,
                  model: Union[Model, str],
                  preprocessor: DialogueClassificationUsePreprocessor = None,
                  config_file: str = None,
                  device: str = 'gpu',
-                 auto_collate=True):
+                 auto_collate=True,
+                 **kwargs):
         """The inference pipeline for the user satisfaction estimation task.
 
         Args:
             model (str or Model): Supply either a local model dir which supported user satisfaction estimation task, or
             a model id from the model hub, or a torch model instance.
             preprocessor (DialogueClassificationUsePreprocessor): An optional preprocessor instance.
             device (str): device str, should be either cpu, cuda, gpu, gpu:X or cuda:X
@@ -45,15 +46,17 @@
             >>> print(pipeline_ins(input))
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
 
         if hasattr(self.preprocessor, 'id2label'):
             self.id2label = self.preprocessor.id2label
 
         self.model.eval()
 
     def forward(self, inputs: Dict[str, Any],
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/word_alignment_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/word_alignment_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/word_segmentation_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/word_segmentation_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/nlp/zero_shot_classification_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -62,15 +62,17 @@
             To view other examples plese check tests/pipelines/test_zero_shot_classification.py.
         """
         super().__init__(
             model=model,
             preprocessor=preprocessor,
             config_file=config_file,
             device=device,
-            auto_collate=auto_collate)
+            auto_collate=auto_collate,
+            compile=kwargs.pop('compile', False),
+            compile_options=kwargs.pop('compile_options', {}))
         self.entailment_id = 0
         self.contradiction_id = 2
 
         assert isinstance(self.model, Model), \
             f'please check whether model config exists in {ModelFile.CONFIGURATION}'
 
         if preprocessor is None:
```

### Comparing `modelscope-1.5.2/modelscope/pipelines/science/protein_structure_pipeline.py` & `modelscope-1.6.0/modelscope/pipelines/science/protein_structure_pipeline.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/pipelines/util.py` & `modelscope-1.6.0/modelscope/pipelines/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,38 +16,39 @@
     from .cv import (ImageClassificationMmcvPreprocessor,
                      ImageRestorationPreprocessor,
                      ControllableImageGenerationPreprocessor)
     from .kws import WavToLists
     from .tts import KanttsDataPreprocessor
     from .multi_modal import (DiffusionImageGenerationPreprocessor,
                               OfaPreprocessor, MPlugPreprocessor,
-                              HiTeAPreprocessor)
+                              HiTeAPreprocessor, MplugOwlPreprocessor,
+                              ImageCaptioningClipInterrogatorPreprocessor)
     from .nlp import (
         DocumentSegmentationTransformersPreprocessor,
         FaqQuestionAnsweringTransformersPreprocessor,
         FillMaskPoNetPreprocessor, FillMaskTransformersPreprocessor,
         TextRankingTransformersPreprocessor,
         RelationExtractionTransformersPreprocessor,
         SentenceEmbeddingTransformersPreprocessor,
         TextClassificationTransformersPreprocessor,
         TextGenerationSentencePiecePreprocessor,
         TokenClassificationTransformersPreprocessor,
         TextErrorCorrectionPreprocessor, TextGenerationT5Preprocessor,
         WordAlignmentPreprocessor, TextGenerationTransformersPreprocessor,
         Tokenize, WordSegmentationBlankSetToLabelPreprocessor,
-        CodeGeeXPreprocessor, MGLMSummarizationPreprocessor,
+        MGLMSummarizationPreprocessor,
         ZeroShotClassificationTransformersPreprocessor,
         TextGenerationJiebaPreprocessor, SentencePiecePreprocessor,
         DialogIntentPredictionPreprocessor, DialogModelingPreprocessor,
         DialogStateTrackingPreprocessor, ConversationalTextToSqlPreprocessor,
         TableQuestionAnsweringPreprocessor, NERPreprocessorViet,
         NERPreprocessorThai, WordSegmentationPreprocessorThai,
-        TranslationEvaluationPreprocessor, CanmtTranslationPreprocessor,
-        DialogueClassificationUsePreprocessor, SiameseUiePreprocessor,
-        DocumentGroundedDialogGeneratePreprocessor,
+        TranslationEvaluationTransformersPreprocessor,
+        CanmtTranslationPreprocessor, DialogueClassificationUsePreprocessor,
+        SiameseUiePreprocessor, DocumentGroundedDialogGeneratePreprocessor,
         DocumentGroundedDialogRetrievalPreprocessor,
         DocumentGroundedDialogRerankPreprocessor)
     from .video import ReadVideoData, MovieSceneSegmentationPreprocessor
 
 else:
     _import_structure = {
         'base': ['Preprocessor'],
@@ -66,15 +67,16 @@
             'ImageRestorationPreprocessor',
             'ControllableImageGenerationPreprocessor'
         ],
         'kws': ['WavToLists'],
         'tts': ['KanttsDataPreprocessor'],
         'multi_modal': [
             'DiffusionImageGenerationPreprocessor', 'OfaPreprocessor',
-            'MPlugPreprocessor', 'HiTeAPreprocessor'
+            'MPlugPreprocessor', 'HiTeAPreprocessor', 'MplugOwlPreprocessor',
+            'ImageCaptioningClipInterrogatorPreprocessor'
         ],
         'nlp': [
             'DocumentSegmentationTransformersPreprocessor',
             'FaqQuestionAnsweringTransformersPreprocessor',
             'FillMaskPoNetPreprocessor', 'FillMaskTransformersPreprocessor',
             'NLPTokenizerPreprocessorBase',
             'TextRankingTransformersPreprocessor',
@@ -92,15 +94,15 @@
             'TextGenerationJiebaPreprocessor', 'SentencePiecePreprocessor',
             'NERPreprocessorViet', 'NERPreprocessorThai',
             'WordSegmentationPreprocessorThai',
             'DialogIntentPredictionPreprocessor', 'DialogModelingPreprocessor',
             'DialogStateTrackingPreprocessor',
             'ConversationalTextToSqlPreprocessor',
             'TableQuestionAnsweringPreprocessor',
-            'TranslationEvaluationPreprocessor',
+            'TranslationEvaluationTransformersPreprocessor',
             'CanmtTranslationPreprocessor',
             'DialogueClassificationUsePreprocessor', 'SiameseUiePreprocessor',
             'DialogueClassificationUsePreprocessor',
             'DocumentGroundedDialogGeneratePreprocessor',
             'DocumentGroundedDialogRetrievalPreprocessor',
             'DocumentGroundedDialogRerankPreprocessor'
         ],
```

### Comparing `modelscope-1.5.2/modelscope/preprocessors/audio.py` & `modelscope-1.6.0/modelscope/preprocessors/audio.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/base.py` & `modelscope-1.6.0/modelscope/preprocessors/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/builder.py` & `modelscope-1.6.0/modelscope/preprocessors/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/common.py` & `modelscope-1.6.0/modelscope/preprocessors/common.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/action_detection_mapper.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/action_detection_mapper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/bad_image_detecting_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/controllable_image_generation.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/controllable_image_generation.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/cv2_transforms.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/cv2_transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/image_classification_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/image_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/image_quality_assessment_man.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/image_quality_assessment_man.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/image_quality_assessment_mos.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/image_quality_assessment_mos.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/image_restoration_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/image_restoration_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/mmcls_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/mmcls_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/timer.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/timer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/util.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/util.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/video_stabilization.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/video_stabilization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/cv/video_super_resolution.py` & `modelscope-1.6.0/modelscope/preprocessors/cv/video_super_resolution.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/image.py` & `modelscope-1.6.0/modelscope/preprocessors/image.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/kws.py` & `modelscope-1.6.0/modelscope/preprocessors/kws.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/movie_scene_segmentation/transforms.py` & `modelscope-1.6.0/modelscope/preprocessors/movie_scene_segmentation/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/multi_modal.py` & `modelscope-1.6.0/modelscope/preprocessors/multi_modal.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os.path as osp
+import re
 from io import BytesIO
 from typing import Any, Dict, List, Tuple, Union
 
 import decord
 import json
 import numpy as np
 import torch
@@ -25,15 +26,15 @@
 from .builder import PREPROCESSORS
 from .ofa import *  # noqa
 from .ofa.utils.collate import collate_fn
 from .ofa.utils.constant import OFA_TASK_KEY_MAPPING
 
 __all__ = [
     'DiffusionImageGenerationPreprocessor', 'OfaPreprocessor',
-    'MPlugPreprocessor', 'HiTeAPreprocessor'
+    'MPlugPreprocessor', 'HiTeAPreprocessor', 'MplugOwlPreprocessor'
 ]
 
 
 @PREPROCESSORS.register_module(
     Fields.multi_modal,
     module_name=Preprocessors.diffusion_image_generation_preprocessor)
 class DiffusionImageGenerationPreprocessor(Preprocessor):
@@ -638,7 +639,163 @@
                 'video': video,
                 'question_input_ids': question.input_ids.squeeze(),
                 'question_attention_mask': question.attention_mask.squeeze(),
                 'answer_input_ids': answer.input_ids.squeeze(),
                 'answer_attention_mask': answer.attention_mask.squeeze(),
             }
             return output
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal, module_name=Preprocessors.mplug_owl_preprocessor)
+class MplugOwlPreprocessor(Preprocessor):
+
+    def __init__(self,
+                 model_dir: str,
+                 mode: str = ModeKeys.INFERENCE,
+                 *args,
+                 **kwargs):
+        super().__init__(*args, **kwargs)
+        self.model_dir = model_dir
+        self.mode = mode
+
+        self._tokenizer = None
+        self._patch_resize_transform = None
+        self.media_token = {'<image>': 65}
+        self._image_map = {}
+
+    @property
+    def tokenizer(self):
+        from modelscope.models.nlp.llama import LlamaTokenizer
+
+        if self._tokenizer is None:
+            self._tokenizer = LlamaTokenizer.from_pretrained(self.model_dir)
+        return self._tokenizer
+
+    @property
+    def patch_resize_transform(self):
+        if self._patch_resize_transform is None:
+            from torchvision import transforms
+
+            mean = (0.48145466, 0.4578275, 0.40821073)
+            std = (0.26862954, 0.26130258, 0.27577711)
+
+            self._patch_resize_transform = transforms.Compose([
+                transforms.Resize((224, 224), interpolation=Image.BICUBIC),
+                transforms.ToTensor(),
+                transforms.Normalize(mean=mean, std=std),
+            ])
+        return self._patch_resize_transform
+
+    def image_open(self, path: str) -> Tuple[Image.Image, int]:
+        if path not in self._image_map:
+            index = len(self._image_map)
+            self._image_map[path] = (load_image(path), index)
+        return self._image_map[path]
+
+    def tokenize_text(self, text: str) -> List[int]:
+        media_tokens = {
+            k: -int(i + 1)
+            for i, k in enumerate(self.media_token.keys())
+        }
+        media_lengths = self.media_token.copy()
+
+        prompt_chunk = [self.tokenizer.bos_token_id]
+
+        # Pure Text
+        condition = [
+            media_token not in text for media_token in media_tokens.keys()
+        ]
+        if all(condition):
+            enc_chunk = prompt_chunk + \
+                self.tokenizer(text, add_special_tokens=False)['input_ids']
+
+        # Multi-Modal Text
+        else:
+            enc_chunk = prompt_chunk
+            pattern = '|'.join(map(re.escape, list(media_tokens.keys())))
+            chunk_strs = re.split(f'({pattern})', text)
+            chunk_strs = [x for x in chunk_strs if len(x) > 0]
+            for idx, chunk_str in enumerate(chunk_strs):
+                if chunk_str in media_tokens:
+                    enc_chunk += [media_tokens[chunk_str]] * \
+                        media_lengths[chunk_str]
+                else:
+                    tmp_chunk = self.tokenizer(
+                        chunk_str, add_special_tokens=False)['input_ids']
+                    enc_chunk += tmp_chunk
+        return enc_chunk
+
+    def convert(self, messages: Dict[str, List[Dict]]) -> str:
+        texts = []
+        image = []
+        messages = messages['messages']
+        for turn in messages:
+            if turn['role'] == 'system':
+                role = ''
+            elif turn['role'] == 'user':
+                role = 'Human: '
+            else:
+                role = 'AI: '
+            if isinstance(turn['content'], str):
+                text = f"{role}{turn['content']}"
+                texts.append(text)
+            else:
+                for t in turn['content']:
+                    if isinstance(t, str):
+                        text = f'{role}{t}'
+                    else:
+                        text = f'{role}<image>'
+                        image.append(t['image'])
+                    texts.append(text)
+        texts = '\n'.join(texts)
+        texts += '\nAI: '
+        return image, texts
+
+    def __call__(self, messages: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Args:
+            messages: {[
+                {'role': 'system', 'content': 'message1'},
+                {'role': 'user', 'content': 'message2'},
+                {'role': 'user', 'content': ['message2', {"image": 'image_path'}, 'message3', ...]},
+            ]}
+            The 'role' should be choose from ['system', 'user', 'assistant'].
+            The 'content' can be either str or List[Union[str, Dict]]
+        Return:
+            output: Dict[str, Tensor]
+        """
+        output = {}
+        images, text = self.convert(messages)
+
+        if len(images) > 0:
+            pixel_values = []
+            for image in images:
+                pixel_values.append(
+                    self.patch_resize_transform(self.image_open(image)[0]))
+                pixel_values = torch.stack(pixel_values, dim=0)
+        else:
+            pixel_values = None
+
+        input_ids = self.tokenize_text(text)
+        input_ids = torch.LongTensor([input_ids])
+
+        output = {
+            'pixel_values': pixel_values,
+            'input_ids': input_ids,
+        }
+
+        return output
+
+
+@PREPROCESSORS.register_module(
+    Fields.multi_modal,
+    module_name=Preprocessors.image_captioning_clip_interrogator_preprocessor)
+class ImageCaptioningClipInterrogatorPreprocessor(Preprocessor):
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    def __call__(self, data) -> Dict[str, Any]:
+        image = load_image(data)
+        data = np.array(image).transpose(2, 0, 1)
+        return data
```

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     from .space import (DialogIntentPredictionPreprocessor,
                         DialogModelingPreprocessor,
                         DialogStateTrackingPreprocessor, InputFeatures,
                         MultiWOZBPETextField, IntentBPETextField)
     from .space_T_en import ConversationalTextToSqlPreprocessor
     from .space_T_cn import TableQuestionAnsweringPreprocessor
     from .mglm_summarization_preprocessor import MGLMSummarizationPreprocessor
-    from .translation_evaluation_preprocessor import TranslationEvaluationPreprocessor
+    from .translation_evaluation_preprocessor import TranslationEvaluationTransformersPreprocessor
     from .canmt_translation import CanmtTranslationPreprocessor
     from .dialog_classification_use_preprocessor import DialogueClassificationUsePreprocessor
     from .siamese_uie_preprocessor import SiameseUiePreprocessor
     from .document_grounded_dialog_generate_preprocessor import DocumentGroundedDialogGeneratePreprocessor
     from .document_grounded_dialog_retrieval_preprocessor import DocumentGroundedDialogRetrievalPreprocessor
     from .document_grounded_dialog_rerank_preprocessor import DocumentGroundedDialogRerankPreprocessor
 else:
@@ -86,15 +86,15 @@
             'InputFeatures',
             'MultiWOZBPETextField',
             'IntentBPETextField',
         ],
         'space_T_en': ['ConversationalTextToSqlPreprocessor'],
         'space_T_cn': ['TableQuestionAnsweringPreprocessor'],
         'translation_evaluation_preprocessor':
-        ['TranslationEvaluationPreprocessor'],
+        ['TranslationEvaluationTransformersPreprocessor'],
         'canmt_translation': [
             'CanmtTranslationPreprocessor',
         ],
         'dialog_classification_use_preprocessor':
         ['DialogueClassificationUsePreprocessor'],
         'siamese_uie_preprocessor': ['SiameseUiePreprocessor'],
         'document_grounded_dialog_generate_preprocessor':
```

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/bert_seq_cls_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/canmt_translation.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/text_ranking_preprocessor.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,109 +1,100 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-import os.path as osp
 from typing import Any, Dict
 
-import jieba
-import torch
-from sacremoses import MosesDetokenizer, MosesPunctNormalizer, MosesTokenizer
-from subword_nmt import apply_bpe
+from transformers import AutoTokenizer
 
 from modelscope.metainfo import Preprocessors
-from modelscope.preprocessors.base import Preprocessor
+from modelscope.preprocessors import Preprocessor
 from modelscope.preprocessors.builder import PREPROCESSORS
-from modelscope.utils.config import Config
-from modelscope.utils.constant import Fields, ModelFile
-from .text_clean import TextClean
+from modelscope.utils.constant import Fields, ModeKeys
+from modelscope.utils.type_assert import type_assert
 
 
-@PREPROCESSORS.register_module(
-    Fields.nlp, module_name=Preprocessors.canmt_translation)
-class CanmtTranslationPreprocessor(Preprocessor):
-    """The preprocessor used in text correction task.
-    """
+class TextRankingPreprocessorBase(Preprocessor):
 
     def __init__(self,
-                 model_dir: str,
-                 max_length: int = None,
-                 *args,
-                 **kwargs):
-        from fairseq.data import Dictionary
-        """preprocess the data via the vocab file from the `model_dir` path
+                 mode: str = ModeKeys.INFERENCE,
+                 first_sequence='source_sentence',
+                 second_sequence='sentences_to_compare',
+                 label='labels',
+                 qid='qid'):
+        """The tokenizer preprocessor class for the text ranking preprocessor.
 
         Args:
-            model_dir (str): model path
+            first_sequence(str, `optional`): The key of the first sequence.
+            second_sequence(str, `optional`): The key of the second sequence.
+            label(str, `optional`): The keys of the label columns, default `labels`.
+            qid(str, `optional`): The qid info.
+            mode: The mode for the preprocessor.
         """
-        super().__init__(*args, **kwargs)
-        self.cfg = Config.from_file(
-            osp.join(model_dir, ModelFile.CONFIGURATION))
-        self.vocab_src = Dictionary.load(osp.join(model_dir, 'dict.src.txt'))
-        self.vocab_tgt = Dictionary.load(osp.join(model_dir, 'dict.tgt.txt'))
-        self.padding_value = self.vocab_src.pad()
-        self.max_length = max_length + 1 if max_length is not None else 129  # 1 is eos token
-
-        self.src_lang = self.cfg['preprocessor']['src_lang']
-        self.tgt_lang = self.cfg['preprocessor']['tgt_lang']
-        self.tc = TextClean()
-
-        if self.src_lang == 'zh':
-            self.tok = jieba
-        else:
-            self.punct_normalizer = MosesPunctNormalizer(lang=self.src_lang)
-            self.tok = MosesTokenizer(lang=self.src_lang)
-
-        self.src_bpe_path = osp.join(
-            model_dir, self.cfg['preprocessor']['src_bpe']['file'])
-        self.bpe = apply_bpe.BPE(open(self.src_bpe_path))
+        super().__init__(mode)
+        self.first_sequence = first_sequence
+        self.second_sequence = second_sequence
+        self.label = label
+        self.qid = qid
+
+
+@PREPROCESSORS.register_module(
+    Fields.nlp, module_name=Preprocessors.text_ranking)
+class TextRankingTransformersPreprocessor(TextRankingPreprocessorBase):
 
-    def __call__(self, input: str) -> Dict[str, Any]:
-        """process the raw input data
+    def __init__(self,
+                 model_dir: str,
+                 mode: str = ModeKeys.INFERENCE,
+                 first_sequence='source_sentence',
+                 second_sequence='sentences_to_compare',
+                 label='labels',
+                 qid='qid',
+                 max_length=None,
+                 padding='max_length',
+                 truncation=True,
+                 use_fast=True,
+                 **kwargs):
+        """The tokenizer preprocessor class for the text ranking preprocessor.
 
         Args:
-            data (str): a sentence
-                Example:
-                    ''
-        Returns:
-            Dict[str, Any]: the preprocessed data
-            Example:
-            {'net_input':
-                {'src_tokens':tensor([1,2,3,4]),
-                'src_lengths': tensor([4])}
-            }
+            model_dir(str, `optional`): The model dir used to parse the label mapping, can be None.
+            max_length: The max sequence length which the model supported,
+                will be passed into tokenizer as the 'max_length' param.
         """
-        if self.src_lang == 'zh':
-            input = self.tc.clean(input)
-            input_tok = self.tok.cut(input)
-            input_tok = ' '.join(list(input_tok))
-        else:
-            input = [self._punct_normalizer.normalize(item) for item in input]
-            input_tok = [
-                self.tok.tokenize(
-                    item, return_str=True, aggressive_dash_splits=True)
-                for item in input
-            ]
-
-        input_bpe = self.bpe.process_line(input_tok).strip().split()
-        text = ' '.join([x for x in input_bpe])
-
-        inputs = self.vocab_src.encode_line(
-            text, append_eos=True, add_if_not_exist=False)
-        prev_inputs = torch.roll(inputs, shifts=1)
-        lengths = inputs.size()[0]
-        max_len = min(self.max_length, lengths)
-
-        padding = torch.tensor(
-            [self.padding_value] *  # noqa: W504
-            (max_len - lengths),
-            dtype=inputs.dtype)
-        sources = torch.unsqueeze(torch.cat([inputs, padding]), dim=0)
-        inputs = torch.unsqueeze(torch.cat([padding, inputs]), dim=0)
-        prev_inputs = torch.unsqueeze(torch.cat([prev_inputs, padding]), dim=0)
-        lengths = torch.tensor([lengths])
-        out = {
-            'src_tokens': inputs,
-            'src_lengths': lengths,
-            'prev_src_tokens': prev_inputs,
-            'sources': sources
-        }
-
-        return out
+        super().__init__(
+            mode=mode,
+            first_sequence=first_sequence,
+            second_sequence=second_sequence,
+            label=label,
+            qid=qid)
+        self.model_dir = model_dir
+        self.sequence_length = max_length if max_length is not None else kwargs.get(
+            'sequence_length', 128)
+        kwargs.pop('sequence_length', None)
+        self.tokenize_kwargs = kwargs
+        self.tokenize_kwargs['padding'] = padding
+        self.tokenize_kwargs['truncation'] = truncation
+        self.tokenizer = AutoTokenizer.from_pretrained(
+            self.model_dir, use_fast=use_fast)
+
+    @type_assert(object, dict)
+    def __call__(self, data: Dict, **kwargs) -> Dict[str, Any]:
+        sentence1 = data.get(self.first_sequence)
+        sentence2 = data.get(self.second_sequence)
+        labels = data.get(self.label)
+        qid = data.get(self.qid)
+
+        if isinstance(sentence2, str):
+            sentence2 = [sentence2]
+        if isinstance(sentence1, str):
+            sentence1 = [sentence1]
+        sentence1 = sentence1 * len(sentence2)
+        kwargs['max_length'] = kwargs.get(
+            'max_length', kwargs.pop('sequence_length', self.sequence_length))
+        if 'return_tensors' not in kwargs:
+            kwargs['return_tensors'] = 'pt'
+
+        self.tokenize_kwargs.update(kwargs)
+        feature = self.tokenizer(sentence1, sentence2, **self.tokenize_kwargs)
+        if labels is not None:
+            feature['labels'] = labels
+        if qid is not None:
+            feature['qid'] = qid
+        return feature
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/dialog_classification_use_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/document_segmentation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/faq_question_answering_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/feature_extraction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/fill_mask_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/fill_mask_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/mgeo_ranking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/mglm_summarization_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/relation_extraction_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/siamese_uie_preprocessor.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,59 +1,48 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-from typing import Any, Dict
+from typing import Any, Dict, Union
 
 from transformers import AutoTokenizer
 
 from modelscope.metainfo import Preprocessors
 from modelscope.preprocessors import Preprocessor
 from modelscope.preprocessors.builder import PREPROCESSORS
 from modelscope.utils.constant import Fields, ModeKeys
-from modelscope.utils.type_assert import type_assert
+from modelscope.utils.hub import get_model_type
+from .transformers_tokenizer import NLPTokenizer
 
 
 @PREPROCESSORS.register_module(
-    Fields.nlp, module_name=Preprocessors.re_tokenizer)
-class RelationExtractionTransformersPreprocessor(Preprocessor):
+    Fields.nlp, module_name=Preprocessors.siamese_uie_preprocessor)
+class SiameseUiePreprocessor(Preprocessor):
+    """The tokenizer preprocessor used in zero shot classification.
+    """
 
     def __init__(
         self,
         model_dir: str,
         mode: str = ModeKeys.INFERENCE,
         **kwargs,
     ):
-        """The preprocessor for relation Extraction task, based on transformers' tokenizer.
-
+        """preprocess the data
         Args:
-            model_dir: The model dir used to initialize the tokenizer.
-            mode: The mode for the preprocessor.
+            model_dir (str): model path
         """
-
         super().__init__(mode)
         self.model_dir: str = model_dir
         self.tokenizer = AutoTokenizer.from_pretrained(
             model_dir, use_fast=True)
 
-    @type_assert(object, str)
-    def __call__(self, data: str, **kwargs) -> Dict[str, Any]:
+    def __call__(self, data: list, **kwargs) -> Dict[str, Any]:
         """process the raw input data
 
         Args:
-            data (str): a sentence
+            data (str or dict): a sentence
                 Example:
                     'you are so handsome.'
 
         Returns:
             Dict[str, Any]: the preprocessed data
         """
-
-        # preprocess the data for the model input
-        text = data
-        if 'return_tensors' not in kwargs:
-            kwargs['return_tensors'] = 'pt'
-        output = self.tokenizer([text], **kwargs)
-        return {
-            'text': text,
-            'input_ids': output['input_ids'],
-            'attention_mask': output['attention_mask'],
-            'offsets': output[0].offsets
-        }
+        features = self.tokenizer(data, **kwargs)
+        return features
```

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/sentence_embedding_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/args.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/batch.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/batch.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/data_loader.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/data_loader.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/dialog_modeling_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/dst_processors.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/dst_processors.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/gen_field.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/gen_field.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/fields/intent_field.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/fields/intent_field.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/lazy_dataset.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/lazy_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/preprocess.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/preprocess.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/sampler.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/sampler.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/tensorlistdataset.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/tensorlistdataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space/tokenizer.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space/tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/database.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/database.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/schema_link.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/fields/struct.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/common_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/parse.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/parse.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/space_T_en/fields/process_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/text_classification_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/text_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/text_clean.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/text_clean.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/text_error_correction.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/text_error_correction.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/text_generation_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/text_generation_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/token_classification_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/token_classification_preprocessor.py`

 * *Files 0% similar despite different names*

```diff
@@ -197,15 +197,15 @@
 class TokenClassificationTransformersPreprocessor(
         TokenClassificationPreprocessorBase):
     """The tokenizer preprocessor used in normal NER task.
     """
 
     def __init__(self,
                  model_dir: str = None,
-                 first_sequence: str = None,
+                 first_sequence: str = 'text',
                  label: str = 'label',
                  label2id: Dict = None,
                  label_all_tokens: bool = False,
                  mode: str = ModeKeys.INFERENCE,
                  max_length=None,
                  use_fast=None,
                  keep_original_columns=None,
```

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/token_classification_thai_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/token_classification_viet_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/transformers_tokenizer.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/transformers_tokenizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/utils.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/word_alignment_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/word_alignment_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py` & `modelscope-1.6.0/modelscope/preprocessors/nlp/zero_shot_classification_preprocessor.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/__init__.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/asr.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/asr.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/base.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/image_captioning.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/image_captioning.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/image_classification.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/image_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/ocr_recognition.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/ocr_recognition.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/sudoku.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/sudoku.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/summarization.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/summarization.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/text2sql.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/text2sql.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/text_classification.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/text_classification.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/text_to_image_synthesis.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/text_to_image_synthesis.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/audio_helper.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/audio_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/bridge_content_encoder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/collate.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/collate.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/constant.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/constant.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/get_tables.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/get_tables.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/random_help.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/random_help.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/text2phone.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/text2phone.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/transforms.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/transforms.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/utils/vision_helper.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/utils/vision_helper.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/visual_entailment.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/visual_entailment.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/visual_grounding.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/visual_grounding.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/ofa/visual_question_answering.py` & `modelscope-1.6.0/modelscope/preprocessors/ofa/visual_question_answering.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/science/uni_fold.py` & `modelscope-1.6.0/modelscope/preprocessors/science/uni_fold.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/tts.py` & `modelscope-1.6.0/modelscope/preprocessors/tts.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/preprocessors/video.py` & `modelscope-1.6.0/modelscope/preprocessors/video.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/tools/eval.py` & `modelscope-1.6.0/modelscope/tools/eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/tools/speech_tts_autolabel.py` & `modelscope-1.6.0/modelscope/tools/speech_tts_autolabel.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/tools/train.py` & `modelscope-1.6.0/modelscope/tools/train.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/__init__.py` & `modelscope-1.6.0/modelscope/trainers/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,14 +11,16 @@
                      ImagePortraitEnhancementTrainer,
                      MovieSceneSegmentationTrainer, ImageInpaintingTrainer,
                      ReferringVideoObjectSegmentationTrainer)
     from .multi_modal import CLIPTrainer
     from .nlp import SequenceClassificationTrainer, TextRankingTrainer, SiameseUIETrainer
     from .nlp_trainer import NlpEpochBasedTrainer, VecoTrainer
     from .trainer import EpochBasedTrainer
+    from .training_args import TrainingArgs, build_dataset_from_file
+    from .hooks import Hook, Priority
 
 else:
     _import_structure = {
         'audio': ['ANSTrainer', 'KanttsTrainer'],
         'base': ['DummyTrainer'],
         'builder': ['build_trainer'],
         'cv': [
@@ -28,15 +30,17 @@
         ],
         'multi_modal': ['CLIPTrainer'],
         'nlp': [
             'SequenceClassificationTrainer', 'TextRankingTrainer',
             'SiameseUIETrainer'
         ],
         'nlp_trainer': ['NlpEpochBasedTrainer', 'VecoTrainer'],
-        'trainer': ['EpochBasedTrainer']
+        'trainer': ['EpochBasedTrainer'],
+        'training_args': ['TrainingArgs', 'build_dataset_from_file'],
+        'hooks': ['Hook']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/__init__.py` & `modelscope-1.6.0/modelscope/trainers/audio/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/ans_trainer.py` & `modelscope-1.6.0/modelscope/trainers/audio/ans_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/asr_trainer.py` & `modelscope-1.6.0/modelscope/trainers/audio/asr_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_farfield_trainer.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_farfield_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_nearfield_trainer.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_nearfield_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_utils/__init__.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_utils/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_utils/batch_utils.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_utils/batch_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_utils/det_utils.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_utils/det_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_utils/file_utils.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_utils/model_utils.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_utils/model_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/kws_utils/runtime_utils.py` & `modelscope-1.6.0/modelscope/trainers/audio/kws_utils/runtime_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/separation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/audio/separation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/audio/tts_trainer.py` & `modelscope-1.6.0/modelscope/trainers/audio/tts_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/base.py` & `modelscope-1.6.0/modelscope/trainers/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/builder.py` & `modelscope-1.6.0/modelscope/trainers/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/__init__.py` & `modelscope-1.6.0/modelscope/trainers/cv/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/action_detection_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/action_detection_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/card_detection_scrfd_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/card_detection_scrfd_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/cartoon_translation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/cartoon_translation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/face_detection_scrfd_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/face_detection_scrfd_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/image_classifition_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/image_classifition_trainer.py`

 * *Files 1% similar despite different names*

```diff
@@ -494,14 +494,19 @@
         rank, _ = get_dist_info()
         if rank == 0:
             results = {}
             logger = get_logger()
             metric_options = self.cfg.evaluation.get('metric_options', {})
             if 'topk' in metric_options.keys():
                 metric_options['topk'] = tuple(metric_options['topk'])
+            # mmcls will set the default value of topk to (1, 5) which
+            # will cause error when number of classes less then 5.
+            # set topk as (1,) if len(CLASSES) < 5:
+            elif len(CLASSES) < 5:
+                metric_options['topk'] = (1, )
             if self.cfg.evaluation.metrics:
                 eval_results = dataset.evaluate(
                     results=outputs,
                     metric=self.cfg.evaluation.metrics,
                     metric_options=metric_options,
                     logger=logger)
                 results.update(eval_results)
```

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/image_defrcn_fewshot_detection_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/image_detection_damoyolo_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/image_detection_damoyolo_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/image_inpainting_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/image_inpainting_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/image_instance_segmentation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/image_instance_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/image_portrait_enhancement_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/image_portrait_enhancement_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/movie_scene_segmentation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/movie_scene_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/nerf_recon_acc_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/nerf_recon_acc_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/ocr_detection_db_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/ocr_detection_db_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/ocr_recognition_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/ocr_recognition_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/referring_video_object_segmentation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/cv/vision_efficient_tuning_trainer.py` & `modelscope-1.6.0/modelscope/trainers/cv/vision_efficient_tuning_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/default_config.py` & `modelscope-1.6.0/modelscope/trainers/default_config.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,45 +1,13 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 from typing import Dict, List, Optional, Tuple
 
 from modelscope.utils.config import Config
 
-DEFAULT_CONFIG = Config({
-    'framework': 'pytorch',
-    'train': {
-        'work_dir': '/tmp',
-        'max_epochs': 10,
-        'dataloader': {
-            'batch_size_per_gpu': 16,
-            'workers_per_gpu': 0
-        },
-        'optimizer': {
-            'type': 'SGD',
-            'lr': 1e-3
-        },
-        'lr_scheduler': {
-            'type': 'StepLR',
-            'step_size': 2
-        },
-        'checkpoint': {
-            'period': {
-                'interval': 1
-            }
-        }
-    },
-    'evaluation': {
-        'dataloader': {
-            'batch_size_per_gpu': 16,
-            'workers_per_gpu': 0,
-            'shuffle': False
-        },
-    }
-})
-
 DEFAULT_HOOKS_CONFIG = {
     'train.hooks': [{
         'type': 'CheckpointHook',
         'interval': 1
     }, {
         'type': 'TextLoggerHook',
         'interval': 10
@@ -64,15 +32,15 @@
     Aegs:
         cfg: The input cfg to be merged into.
     """
     cfg.merge_from_dict(DEFAULT_HOOKS_CONFIG, force=False)
 
 
 def merge_hooks(cfg: Config) -> List[Dict]:
-    hooks = cfg.train.hooks.copy()
+    hooks = getattr(cfg.train, 'hooks', []).copy()
     for hook_type, key_chain in _HOOK_KEY_CHAIN_MAP.items():
         hook = _key_chain_to_hook(cfg, key_chain, hook_type)
         if hook is not None:
             hooks.append(hook)
     return hooks
 
 
@@ -103,15 +71,16 @@
     return hook
 
 
 def _check_basic_hook(cfg: Config, key_chain: str, hook_type: str) -> bool:
     if cfg.safe_get(key_chain) is None:
         return False
     hooks = list(
-        filter(lambda hook: hook['type'] == hook_type, cfg.train.hooks))
+        filter(lambda hook: hook['type'] == hook_type,
+               getattr(cfg.train, 'hooks', [])))
     assert len(hooks) == 0, f'The key_chain {key_chain} and the traditional hook ' \
                             f'cannot exist at the same time, ' \
                             f'please delete {hook_type} in the configuration file.'
     return True
 
 
 def _hook_split(hook: Dict) -> Tuple[str, Dict]:
```

### Comparing `modelscope-1.5.2/modelscope/trainers/easycv/trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/plug_trainer.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,183 +1,202 @@
-# Copyright (c) Alibaba, Inc. and its affiliates.
-from copy import deepcopy
-from functools import partial
-from typing import Callable, Optional, Tuple, Union
+import os
+from typing import Union
 
 import torch
-from easycv.utils.checkpoint import load_checkpoint as ev_load_checkpoint
+from deepspeed import DeepSpeedEngine
+from megatron_util import mpu
 from torch import nn
-from torch.utils.data import Dataset
 
 from modelscope.metainfo import Trainers
 from modelscope.models.base import TorchModel
-from modelscope.msdatasets import MsDataset
-from modelscope.preprocessors import Preprocessor
-from modelscope.trainers import EpochBasedTrainer
-from modelscope.trainers.base import TRAINERS
-from modelscope.trainers.easycv.utils import register_util
-from modelscope.trainers.hooks import HOOKS
-from modelscope.trainers.parallel.builder import build_parallel
-from modelscope.trainers.parallel.utils import is_parallel
-from modelscope.utils.config import Config
-from modelscope.utils.constant import DEFAULT_MODEL_REVISION
-from modelscope.utils.import_utils import LazyImportModule
-from modelscope.utils.registry import default_group
-
-
-@TRAINERS.register_module(module_name=Trainers.easycv)
-class EasyCVEpochBasedTrainer(EpochBasedTrainer):
-    """Epoch based Trainer for EasyCV.
-
-    Args:
-        cfg_file(str): The config file of EasyCV.
-        model (:obj:`torch.nn.Module` or :obj:`TorchModel` or `str`): The model to be run, or a valid model dir
-            or a model id. If model is None, build_model method will be called.
-        train_dataset (`MsDataset` or `torch.utils.data.Dataset`, *optional*):
-            The dataset to use for training.
-            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a
-            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a
-            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will
-            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally
-            sets the seed of the RNGs used.
-        eval_dataset (`MsDataset` or `torch.utils.data.Dataset`, *optional*): The dataset to use for evaluation.
-        preprocessor (:obj:`Preprocessor`, *optional*): The optional preprocessor.
-            NOTE: If the preprocessor has been called before the dataset fed into this trainer by user's custom code,
-            this parameter should be None, meanwhile remove the 'preprocessor' key from the cfg_file.
-            Else the preprocessor will be instantiated from the cfg_file or assigned from this parameter and
-            this preprocessing action will be executed every time the dataset's __getitem__ is called.
-        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler]`, *optional*): A tuple
-            containing the optimizer and the scheduler to use.
-        max_epochs: (int, optional): Total training epochs.
-    """
-
-    def __init__(
-            self,
-            cfg_file: Optional[str] = None,
-            model: Optional[Union[TorchModel, nn.Module, str]] = None,
-            arg_parse_fn: Optional[Callable] = None,
-            train_dataset: Optional[Union[MsDataset, Dataset]] = None,
-            eval_dataset: Optional[Union[MsDataset, Dataset]] = None,
-            preprocessor: Optional[Preprocessor] = None,
-            optimizers: Tuple[torch.optim.Optimizer,
-                              torch.optim.lr_scheduler._LRScheduler] = (None,
-                                                                        None),
-            model_revision: Optional[str] = DEFAULT_MODEL_REVISION,
-            **kwargs):
-
-        register_util.register_parallel()
-        register_util.register_part_mmcv_hooks_to_ms()
-
-        super(EasyCVEpochBasedTrainer, self).__init__(
-            model=model,
-            cfg_file=cfg_file,
-            arg_parse_fn=arg_parse_fn,
-            preprocessor=preprocessor,
-            optimizers=optimizers,
-            model_revision=model_revision,
-            train_dataset=train_dataset,
-            eval_dataset=eval_dataset,
-            **kwargs)
-
-        # reset data_collator
-        from mmcv.parallel import collate
-
-        self.train_data_collator = partial(
-            collate,
-            samples_per_gpu=self.cfg.train.dataloader.batch_size_per_gpu)
-        self.eval_data_collator = partial(
-            collate,
-            samples_per_gpu=self.cfg.evaluation.dataloader.batch_size_per_gpu)
-
-        # load pretrained model
-        load_from = self.cfg.get('load_from', None)
-        if load_from is not None:
-            ev_load_checkpoint(
-                self.model,
-                filename=load_from,
-                map_location=self.device,
-                strict=False,
-            )
-
-        # reset parallel
-        if not self._dist:
-            assert not is_parallel(
-                self.model
-            ), 'Not support model wrapped by custom parallel if not in distributed mode!'
-            dp_cfg = dict(
-                type='MMDataParallel',
-                module=self.model,
-                device_ids=[torch.cuda.current_device()])
-            self.model = build_parallel(dp_cfg)
-
-    def rebuild_config(self, cfg: Config):
-        cfg = super().rebuild_config(cfg)
-        # Register easycv hooks dynamicly. If the hook already exists in modelscope,
-        # the hook in modelscope will be used, otherwise register easycv hook into ms.
-        # We must manually trigger lazy import to detect whether the hook is in modelscope.
-        # TODO: use ast index to detect whether the hook is in modelscope
-        for h_i in cfg.train.get('hooks', []):
-            sig = ('HOOKS', default_group, h_i['type'])
-            LazyImportModule.import_module(sig)
-            if h_i['type'] not in HOOKS._modules[default_group]:
-                if h_i['type'] in [
-                        'TensorboardLoggerHookV2', 'WandbLoggerHookV2'
-                ]:
-                    raise ValueError(
-                        'Not support hook %s now, we will support it in the future!'
-                        % h_i['type'])
-                register_util.register_hook_to_ms(h_i['type'])
-        return cfg
+from modelscope.models.nlp.plug import DistributedPlug
+from modelscope.models.nlp.plug.backbone import BertLayerNorm
+from modelscope.models.nlp.plug.generator import TextGenerator
+from modelscope.utils.constant import ModeKeys
+from ..base import TRAINERS
+from ..nlp_trainer import NlpEpochBasedTrainer
+
+
+@TRAINERS.register_module(module_name=Trainers.nlp_plug_trainer)
+class PlugTrainer(NlpEpochBasedTrainer):
+
+    def build_model(self) -> Union[nn.Module, TorchModel]:
+        rank = int(os.environ.get('LOCAL_RANK', -1))
+        master_ip = os.environ.get('MASTER_ADDR', '127.0.0.1')
+        master_port = os.environ.get('MASTER_PORT', '29500')
+        model = DistributedPlug(
+            self.model_dir,
+            rank,
+            master_ip=master_ip,
+            master_port=master_port,
+            **self.cfg.model)
+        self.unwrap_module(model.model).model_dir = self.model_dir
+        return model.model
+
+    def to_parallel(self, model) -> Union[nn.Module, TorchModel]:
+        from modelscope.utils.nlp.distributed import DistributedDataParallel as DDP
+        return DDP(model)
+
+    def _get_params_for_weight_decay_optimization(self, module):
+
+        weight_decay_params = {'params': []}
+        no_weight_decay_params = {'params': [], 'weight_decay': 0.0}
+        for module_ in module.modules():
+            if isinstance(module_, (BertLayerNorm, torch.nn.LayerNorm)):
+                no_weight_decay_params['params'].extend([
+                    p for p in list(module_._parameters.values())
+                    if p is not None
+                ])
+            else:
+                weight_decay_params['params'].extend([
+                    p for n, p in list(module_._parameters.items())
+                    if p is not None and 'mask_score' not in n
+                    and 'mask' not in n and n != 'bias'
+                ])
+                no_weight_decay_params['params'].extend([
+                    p for n, p in list(module_._parameters.items())
+                    if p is not None and n == 'bias'
+                ])
+
+        return weight_decay_params, no_weight_decay_params
 
     def create_optimizer_and_scheduler(self):
-        """ Create optimizer and lr scheduler
-        """
         optimizer, lr_scheduler = self.optimizers
-        if optimizer is None:
-            optimizer_cfg = self.cfg.train.get('optimizer', None)
-        else:
-            optimizer_cfg = None
-
-        optim_options = {}
+        optimizer_cfg = self.cfg.train.get('optimizer', None)
+        # optim_options = {}
         if optimizer_cfg is not None:
             optim_options = optimizer_cfg.pop('options', {})
-            from easycv.apis.train import build_optimizer
-            optimizer = build_optimizer(self.model, optimizer_cfg)
+        from deepspeed.ops.adam import DeepSpeedCPUAdam
+        model = self.model
 
-        if lr_scheduler is None:
-            lr_scheduler_cfg = self.cfg.train.get('lr_scheduler', None)
-        else:
-            lr_scheduler_cfg = None
+        embeddings = model.module.model.bert.embeddings
+        layers = model.module.model.bert.encoder.layer
+        dec_layers = model.module.model.decoder.decoder
+        param_groups = []
+        param_groups += list(
+            self._get_params_for_weight_decay_optimization(layers))
+        param_groups += list(
+            self._get_params_for_weight_decay_optimization(embeddings))
+        param_groups += list(
+            self._get_params_for_weight_decay_optimization(dec_layers))
+
+        for param_group in param_groups:
+            for param in param_group['params']:
+                if not hasattr(param, 'model_parallel'):
+                    param.model_parallel = False
+        optimizer = DeepSpeedCPUAdam(
+            param_groups,
+            lr=optimizer_cfg.lr,
+            weight_decay=optimizer_cfg.weight_decay)
+
+        lr_scheduler_cfg = self.cfg.train.get('lr_scheduler', None)
 
-        lr_options = {}
-        # Adapt to mmcv lr scheduler hook.
-        # Please refer to: https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py
         if lr_scheduler_cfg is not None:
             assert optimizer is not None
             lr_options = lr_scheduler_cfg.pop('options', {})
-            assert 'policy' in lr_scheduler_cfg
-            policy_type = lr_scheduler_cfg.pop('policy')
-            if policy_type == policy_type.lower():
-                policy_type = policy_type.title()
-            hook_type = policy_type + 'LrUpdaterHook'
-            lr_scheduler_cfg['type'] = hook_type
-
-            self.cfg.train.lr_scheduler_hook = lr_scheduler_cfg
+        from modelscope.models.nlp.plug.AnnealingLR import AnnealingLR
+        num_iters = self.max_iters
+        lr_scheduler = AnnealingLR(
+            optimizer,
+            start_lr=optimizer_cfg.lr,
+            warmup_iter=lr_scheduler_cfg.warmup * num_iters,
+            num_iters=num_iters,
+            decay_style=lr_scheduler_cfg.decay_style,
+            last_iter=-1)
 
         self.optimizer = optimizer
         self.lr_scheduler = lr_scheduler
-
         return self.optimizer, self.lr_scheduler, optim_options, lr_options
 
-    def to_parallel(self, model) -> Union[nn.Module, TorchModel]:
-        if self.cfg.get('parallel', None) is not None:
-            dp_cfg = deepcopy(self.cfg['parallel'])
-            dp_cfg.update(
-                dict(module=model, device_ids=[torch.cuda.current_device()]))
-            return build_parallel(dp_cfg)
-
-        dp_cfg = dict(
-            type='MMDistributedDataParallel',
-            module=model,
-            device_ids=[torch.cuda.current_device()])
+    def _get_masks_and_position_ids(self, data, eod_token):
+        # Extract batch size and sequence length.
+        batch_size, seq_length = data.size()
+
+        # Attention mask (lower triangular).
+        att_mask_batch = 1
+        attention_mask = torch.tril(
+            torch.ones((att_mask_batch, seq_length, seq_length),
+                       device=data.device)).view(att_mask_batch, 1, seq_length,
+                                                 seq_length)
+
+        # Loss mask.
+        loss_mask = torch.ones(
+            data.size(), dtype=torch.float, device=data.device)
+        loss_mask[data == eod_token] = 0.0
+
+        # Position ids.
+        position_ids = torch.arange(
+            seq_length, dtype=torch.long, device=data.device)
+        position_ids = position_ids.unsqueeze(0).expand_as(data)
+        return attention_mask, loss_mask, position_ids
+
+    def train_step(self, model, inputs):
+        self._mode = ModeKeys.TRAIN
+        # format inputs
+        checkpoint_activations = getattr(self.cfg.train,
+                                         'checkpoint_activations', True)
+        tgt_tokens = inputs['labels'][:, :-1].contiguous()
+        tgt_labels = inputs['labels'][:, 1:].contiguous()
+        tgt_attention_mask, dec_loss_mask, position_ids = self._get_masks_and_position_ids(
+            tgt_tokens, 0)
+        if getattr(self.cfg.train, 'fp16', None):
+            tgt_attention_mask = tgt_attention_mask.half()
+
+        # forward step
+        _, output = model(
+            inputs['input_ids'],
+            None,
+            inputs['attention_mask'],
+            tgt_tokens,
+            position_ids,
+            tgt_attention_mask,
+            checkpoint_activations=checkpoint_activations)
+
+        losses = mpu.vocab_parallel_cross_entropy(output.contiguous().float(),
+                                                  tgt_labels)
+        dec_loss_mask = dec_loss_mask.view(-1)
+        loss = torch.sum(losses.view(-1) * dec_loss_mask) / dec_loss_mask.sum()
+
+        # add model output info to log
+        self.train_outputs = {'loss': loss}
+        self.log_buffer.update(self.train_outputs)
+
+    def evaluation_step(self, data):
+        # wapper 1: DeepspeedEngine, wapper 2: DDP
+        # model = self.model.module
+        if isinstance(self.model, DeepSpeedEngine):
+            model = self.model.module
+        else:
+            model = self.model
+
+        model.eval()
 
-        return build_parallel(dp_cfg)
+        # model: fp16 wapper; model.module : distributedPlug
+        vocab_size = self.unwrap_module(self.model).config.original_vocab_size
+        batch_size = data['input_ids'].shape[0]
+        beam_generator = TextGenerator(model,
+                                       self.eval_preprocessor.nlp_tokenizer,
+                                       None)
+
+        with torch.no_grad():
+            tokens = data['input_ids'].long()
+            padding_mask = data['attention_mask'].byte()
+            target_ids = data['labels'].long()
+            target_labels = target_ids[:, 1:].contiguous()
+            encoder_inputs = [tokens, None, padding_mask]
+            result = beam_generator.translate_batch(encoder_inputs)
+            pred_list = result['predictions']
+            target_list = target_labels.cpu().numpy().tolist()
+            result['preds'] = []
+            data['tgts'] = []
+            for i in range(batch_size):
+                pred_ids = pred_list[i][0]
+                pred_ids[pred_ids > vocab_size - 1] = 100
+                pred_ids = pred_ids.cpu().numpy().tolist()
+
+                gold_string = self.eval_preprocessor.decode(
+                    target_list[i], skip_special_tokens=True)
+                pred_string = self.eval_preprocessor.decode(
+                    pred_ids, skip_special_tokens=True)
+                result['preds'].append(pred_string)
+                data['tgts'].append(gold_string)
+        return result
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/compression/__init__.py` & `modelscope-1.6.0/modelscope/models/cv/image_instance_segmentation/backbones/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .sparsity_hook import SparsityHook
-    from .utils import SparseLinear, convert_sparse_network
+    from .swin_transformer import SwinTransformer
+    from .swin_transformer import D2SwinTransformer
+    from .resnet import build_resnet_backbone
 
 else:
     _import_structure = {
-        'sparsity_hook': ['SparsityHook'],
-        'utils': ['convert_sparse_network', 'SparseLinear'],
+        'swin_transformer': ['SwinTransformer', 'D2SwinTransformer'],
+        'resnet': ['build_resnet_backbone']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/compression/sparsity_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/compression/sparsity_hook.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 
-from modelscope import __version__
 from modelscope.metainfo import Hooks
 from modelscope.trainers.hooks.builder import HOOKS
 from modelscope.trainers.hooks.hook import Hook
 from modelscope.trainers.hooks.priority import Priority
 from modelscope.utils.checkpoint import save_checkpoint
 from modelscope.utils.torch_utils import is_master
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/compression/utils.py` & `modelscope-1.6.0/modelscope/trainers/hooks/compression/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/ddp_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/distributed/ddp_hook.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from modelscope.metainfo import Hooks
+from modelscope.trainers.hooks.builder import HOOKS
+from modelscope.trainers.hooks.hook import Hook
+from modelscope.trainers.hooks.priority import Priority
 from modelscope.utils.constant import DistributedParallelType
 from modelscope.utils.device import create_device
 from modelscope.utils.torch_utils import get_local_rank, init_dist
-from .builder import HOOKS
-from .hook import Hook
-from .priority import Priority
 
 
 @HOOKS.register_module(module_name=Hooks.DDPHook)
 class DDPHook(Hook):
 
     PRIORITY = Priority.LOW
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/deepspeed_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/distributed/deepspeed_hook.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,117 +4,99 @@
 
 import deepspeed
 import torch
 from deepspeed import DeepSpeedEngine
 from megatron_util import mpu, print_rank_0
 
 from modelscope.metainfo import Hooks
+from modelscope.trainers.hooks import LoadCheckpointHook
 from modelscope.trainers.hooks.builder import HOOKS
+from modelscope.trainers.hooks.checkpoint.checkpoint_hook import (
+    BestCkptSaverHook, CheckpointHook)
 from modelscope.trainers.hooks.hook import Hook
 from modelscope.trainers.hooks.priority import Priority
 from modelscope.utils.checkpoint import save_checkpoint
 from modelscope.utils.logger import get_logger
-from .checkpoint_hook import CheckpointHook, LoadCheckpointHook
-from .megatron_hook import MegatronHook
-
-
-@HOOKS.register_module(module_name=Hooks.DeepspeedHook)
-class DeepspeedHook(MegatronHook):
-    PRIORITY = Priority.VERY_HIGH
-
-    def __init__(self,
-                 deepspeed_activation_checkpointing=True,
-                 save_zero_checkpoint=False,
-                 with_mpu=True):
-        self.save_zero_checkpoint = save_zero_checkpoint
-        self.deepspeed_activation_checkpointing = deepspeed_activation_checkpointing
-        # TODO without mpu
-        self.with_mpu = with_mpu
-        assert with_mpu, 'DeepspeedHook now is only for mpu models.'
-
-    def register_strategy(self):
-        Hook.overload(name='OptimizerHook.backward', function=self.backward)
-        Hook.overload(
-            name='OptimizerHook.initialize_optimizer', function=self.idle)
-        Hook.overload(name='LrSchedulerHook.step', function=self.idle)
-        Hook.overload(
-            name='CheckpointHook.save_checkpoints',
-            function=self.save_checkpoints)
-        Hook.overload(
-            name='LoadCheckpointHook.load_checkpoints',
-            function=self.load_checkpoints)
-        Hook.overload(
-            name='CheckpointHook.remove_checkpoints',
-            function=self.remove_checkpoints)
-        Hook.overload(
-            name='CheckpointHook.prepare_output', function=self.prepare_output)
-        if self.with_mpu:
-            Hook.overload(
-                name='CheckpointHook.should_save_on_rank',
-                function=self.should_save_on_rank)
-
-    def backward(self, trainer, loss_keys, cumulative_iters, grad_clip):
-        # assert cumulative_iters == 1, 'DeepSpeed only support cumulative_iters=1'
-        # The `trainer.model` here is actually a deepspeed engine object.
-        # backward step
-        for k in loss_keys:
-            loss = trainer.train_outputs[k]
-            trainer.model.backward(loss)
-
-        # update parameters
-        trainer.model.step()
-
-    def idle(self, *args, **kwargs):
-        pass
+from ..checkpoint.checkpoint_processor import CheckpointProcessor
+from ..lr_scheduler_hook import LrSchedulerProcessor
+from ..optimizer.base import OptimizerHook, OptimizerProcessor
+
+
+class DeepspeedProcessor(CheckpointProcessor, LrSchedulerProcessor,
+                         OptimizerProcessor):
+
+    _BIN_FILE_DIR = 'model'
+
+    def rank_name(self):
+        # TODO
+        try:
+            tp_world_size = mpu.get_tensor_model_parallel_world_size()
+            if tp_world_size == 1:
+                return ''
+            mp_rank = mpu.get_tensor_model_parallel_rank()
+            return '_mp_rank_{:02d}'.format(mp_rank)
+        except (ImportError, AssertionError):
+            return ''
+
+    def get_bin_file(self):
+        mp_rank = mpu.get_tensor_model_parallel_rank()
+        rank = '{:02d}'.format(mp_rank)
+        return f'mp_rank_{rank}_model_states.pt'
 
     def save_checkpoints(self,
                          trainer,
                          checkpoint_path_prefix,
-                         output_sub_dir,
+                         output_dir,
                          meta=None):
         model = trainer.unwrap_module(trainer.model)
         _train_state_file = checkpoint_path_prefix + self.rank_name(
-        ) + CheckpointHook.TRAINER_STATE_SUFFIX
+        ) + CheckpointProcessor.TRAINER_STATE_SUFFIX
         # Save pth file without model state_dict
         save_checkpoint(
             model, _train_state_file, None, None, meta=meta, with_model=False)
 
         save_dir = os.path.dirname(checkpoint_path_prefix)
         prefix = os.path.basename(checkpoint_path_prefix)
         trainer.model.save_checkpoint(save_dir, prefix)
 
         bin_file = self.get_bin_file()
         src_file = os.path.join(checkpoint_path_prefix, bin_file)
-        dest_file = os.path.join(save_dir, output_sub_dir, self._BIN_FILE_DIR,
-                                 bin_file)
+        dest_file = os.path.join(output_dir, self._BIN_FILE_DIR, bin_file)
         if os.path.isfile(dest_file):
             os.unlink(dest_file)
 
-        os.link(src_file, dest_file)
+        try:
+            os.link(src_file, dest_file)
+        except OSError as e:
+            get_logger().error(
+                f'Link {src_file} to {dest_file} error: {e}, '
+                'changing to copy the bin file, this may case more space usage.'
+            )
+            shutil.copyfile(src_file, dest_file)
 
     def remove_checkpoints(self, trainer, checkpoint_path_prefix):
         _train_state_file = checkpoint_path_prefix + self.rank_name(
-        ) + CheckpointHook.TRAINER_STATE_SUFFIX
+        ) + CheckpointProcessor.TRAINER_STATE_SUFFIX
         if os.path.isfile(_train_state_file):
             os.remove(_train_state_file)
 
         shutil.rmtree(checkpoint_path_prefix, ignore_errors=True)
 
     def load_checkpoints(self, checkpoint_path_prefix, trainer, load_all_state,
                          strict):
         assert os.path.isdir(checkpoint_path_prefix)
         path = os.path.dirname(checkpoint_path_prefix)
         tag = os.path.basename(checkpoint_path_prefix)
 
         meta = {}
         _train_state_file = checkpoint_path_prefix + self.rank_name(
-        ) + CheckpointHook.TRAINER_STATE_SUFFIX
+        ) + CheckpointProcessor.TRAINER_STATE_SUFFIX
         if os.path.isfile(_train_state_file):
-            meta = LoadCheckpointHook.load_trainer_state(
-                trainer, _train_state_file, load_all_state)
+            meta = self.load_trainer_state(trainer, _train_state_file,
+                                           load_all_state)
 
         if isinstance(trainer.model, DeepSpeedEngine):
             # DeepSpeedEngine is initialized
             trainer.model.load_checkpoint(
                 path,
                 tag,
                 load_module_strict=strict,
@@ -134,14 +116,65 @@
                     print_rank_0('Skip key: ' + key)
                 else:
                     print_rank_0('Loading key: ' + key)
             trainer.unwrap_module(trainer.model).load_state_dict(
                 checkpoint, strict=strict)
         return meta
 
+    def backward(self, trainer, loss_keys, cumulative_iters, grad_clip):
+        # assert cumulative_iters == 1, 'DeepSpeed only support cumulative_iters=1'
+        # The `trainer.model` here is actually a deepspeed engine object.
+        # backward step
+        for k in loss_keys:
+            loss = trainer.train_outputs[k]
+            trainer.model.backward(loss)
+
+        # update parameters
+        trainer.model.step()
+
+    def initialize_optimizer(self, trainer):
+        pass
+
+    def step(self, trainer):
+        pass
+
+
+@HOOKS.register_module(module_name=Hooks.DeepspeedHook)
+class DeepspeedHook(Hook):
+    PRIORITY = Priority.VERY_HIGH
+
+    def __init__(self,
+                 deepspeed_activation_checkpointing=True,
+                 save_zero_checkpoint=False,
+                 with_mpu=True):
+        self.save_zero_checkpoint = save_zero_checkpoint
+        self.deepspeed_activation_checkpointing = deepspeed_activation_checkpointing
+        # TODO without mpu
+        self.with_mpu = with_mpu
+        assert with_mpu, 'DeepspeedHook now is only for mpu models.'
+
+    def register_processor(self, trainer):
+        processor = DeepspeedProcessor()
+        optimizer_hook = trainer.get_hook(OptimizerHook)
+        if len(optimizer_hook) > 0 and not isinstance(
+                optimizer_hook[0].processor, DeepspeedProcessor):
+            optimizer_hook[0].set_processor(processor)
+        ckpt_hook = trainer.get_hook(CheckpointHook)
+        if len(ckpt_hook) > 0 and not isinstance(ckpt_hook[0].processor,
+                                                 DeepspeedProcessor):
+            ckpt_hook[0].set_processor(processor)
+        best_ckpt_hook = trainer.get_hook(BestCkptSaverHook)
+        if len(best_ckpt_hook) > 0 and not isinstance(
+                best_ckpt_hook[0].processor, DeepspeedProcessor):
+            best_ckpt_hook[0].set_processor(processor)
+        load_ckpt_hook = trainer.get_hook(LoadCheckpointHook)
+        if len(load_ckpt_hook) > 0 and not isinstance(
+                load_ckpt_hook[0].processor, DeepspeedProcessor):
+            load_ckpt_hook[0].set_processor(processor)
+
     def before_val(self, trainer):
         pass
 
     def before_run(self, trainer):
         if not hasattr(trainer, 'logger'):
             self.logger = get_logger()
         else:
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/early_stop_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/early_stop_hook.py`

 * *Files 15% similar despite different names*

```diff
@@ -5,48 +5,58 @@
 from modelscope.metainfo import Hooks
 from modelscope.utils.logger import get_logger
 from .builder import HOOKS
 from .hook import Hook
 from .priority import Priority
 
 
+class EarlyStopStrategy:
+    by_epoch = 'by_epoch'
+    by_step = 'by_step'
+    no = 'no'
+
+
 @HOOKS.register_module(module_name=Hooks.EarlyStopHook)
 class EarlyStopHook(Hook):
     """Early stop when a specific metric stops improving.
 
     Args:
         metric_key (str):  Metric key to be monitored.
         rule (str): Comparison rule for best score. Support "max" and "min".
-            If rule is "max", the training will stop when `metric_key` has stopped increaing.
+            If rule is "max", the training will stop when `metric_key` has stopped increasing.
             If rule is "min", the training will stop when `metric_key` has stopped decreasing.
         patience (int): Trainer will stop if the monitored metric did not improve for the last `patience` times.
-        min_delta (float): Minimum change in the monitored metric to quailfy as an improvement.
+        min_delta (float): Minimum change in the monitored metric to qualify as an improvement.
         check_finite (bool): If true, stops training when the metric becomes NaN or infinite.
-        by_epoch (int): Saving checkpoints by epoch or by iteration.
-        interval (int): The frequency to trigger early stop check. If `by_epoch=True`,
-            it means the number of epochs, else means the number of iterations.
+        early_stop_strategy (str): The strategy to early stop, can be by_epoch/by_step/none
+        interval (int): The frequency to trigger early stop check, by epoch or step.
     """
 
     PRIORITY = Priority.VERY_LOW
     rule_map = {'max': lambda x, y: x > y, 'min': lambda x, y: x < y}
 
     def __init__(self,
                  metric_key: str,
                  rule: str = 'max',
                  patience: int = 3,
                  min_delta: float = 0.0,
                  check_finite: bool = True,
-                 by_epoch: bool = True,
-                 interval: int = 1):
+                 early_stop_strategy: str = EarlyStopStrategy.by_epoch,
+                 interval: int = 1,
+                 **kwargs):
         self.metric_key = metric_key
         self.rule = rule
         self.patience = patience
         self.min_delta = min_delta
         self.check_finite = check_finite
-        self.by_epoch = by_epoch
+        if 'by_epoch' in kwargs:
+            self.early_stop_strategy = EarlyStopStrategy.by_epoch if kwargs[
+                'by_epoch'] else EarlyStopStrategy.by_step
+        else:
+            self.early_stop_strategy = early_stop_strategy
         self.interval = interval
 
         self.wait_count = 0
         self.best_score = float('inf') if rule == 'min' else -float('inf')
 
     def before_run(self, trainer):
         if not hasattr(trainer, 'logger'):
@@ -85,25 +95,25 @@
         return should_stop
 
     def _stop_training(self, trainer):
         self.logger.info('Early Stopping!')
         trainer._stop_training = True
 
     def after_train_epoch(self, trainer):
-        if not self.by_epoch:
+        if self.early_stop_strategy != EarlyStopStrategy.by_epoch:
             return
 
         if not self.every_n_epochs(trainer, self.interval):
             return
 
         if self._should_stop(trainer):
             self._stop_training(trainer)
 
     def after_train_iter(self, trainer):
-        if self.by_epoch:
+        if self.early_stop_strategy != EarlyStopStrategy.by_step:
             return
 
         if not self.every_n_iters(trainer, self.interval):
             return
 
         if self._should_stop(trainer):
             self._stop_training(trainer)
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/evaluation_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/evaluation_hook.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,42 +1,62 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from collections import OrderedDict
+from typing import Optional
 
 from modelscope.metainfo import Hooks
 from .builder import HOOKS
 from .hook import Hook
 
 
+class EvaluationStrategy:
+    by_epoch = 'by_epoch'
+    by_step = 'by_step'
+    no = 'no'
+
+
 @HOOKS.register_module(module_name=Hooks.EvaluationHook)
 class EvaluationHook(Hook):
     """
     Evaluation hook.
 
     Args:
         interval (int): Evaluation interval.
         by_epoch (bool): Evaluate by epoch or by iteration.
         start_idx (int or None, optional): The epoch or iterations validation begins.
             Default: None, validate every interval epochs/iterations from scratch.
     """
 
-    def __init__(self, interval=1, by_epoch=True, start_idx=None):
+    def __init__(self,
+                 interval: Optional[int] = 1,
+                 eval_strategy: Optional[str] = EvaluationStrategy.by_epoch,
+                 start_idx: Optional[int] = None,
+                 **kwargs):
         assert interval > 0, 'interval must be a positive number'
         self.interval = interval
         self.start_idx = start_idx
-        self.by_epoch = by_epoch
+        self.last_eval_tag = (None, None)
+        if 'by_epoch' in kwargs:
+            self.eval_strategy = EvaluationStrategy.by_epoch if kwargs[
+                'by_epoch'] else EvaluationStrategy.by_step
+        else:
+            self.eval_strategy = eval_strategy
 
     def after_train_iter(self, trainer):
         """Called after every training iter to evaluate the results."""
-        if not self.by_epoch and self._should_evaluate(trainer):
+        if self.eval_strategy == EvaluationStrategy.by_step and self._should_evaluate(
+                trainer):
             self.do_evaluate(trainer)
+            self.last_eval_tag = ('iter', trainer.iter)
 
     def after_train_epoch(self, trainer):
         """Called after every training epoch to evaluate the results."""
-        if self.by_epoch and self._should_evaluate(trainer):
+        if self.eval_strategy == EvaluationStrategy.by_epoch and self._should_evaluate(
+                trainer):
             self.do_evaluate(trainer)
+            self.last_eval_tag = ('epoch', trainer.epoch)
 
     def add_visualization_info(self, trainer, results):
         if trainer.visualization_buffer.output.get('eval_results',
                                                    None) is None:
             trainer.visualization_buffer.output['eval_results'] = OrderedDict()
 
             trainer.visualization_buffer.output['eval_results'].update(
@@ -60,15 +80,15 @@
            current epochs/iters.
         3. It will not perform evaluation when current epochs/iters is larger than
            the ``start_idx`` but during epoch/iteration interval.
 
         Returns:
             bool: The flag indicating whether to perform evaluation.
         """
-        if self.by_epoch:
+        if self.eval_strategy == EvaluationStrategy.by_epoch:
             current = trainer.epoch
             check_time = self.every_n_epochs
         else:
             current = trainer.iter
             check_time = self.every_n_iters
 
         if self.start_idx is None:
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/iter_timer_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/iter_timer_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/logger/__init__.py` & `modelscope-1.6.0/modelscope/trainers/hooks/optimizer/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
-from modelscope.trainers.utils.log_buffer import LogBuffer
 from modelscope.utils.import_utils import LazyImportModule
 
 if TYPE_CHECKING:
-    from .base import LoggerHook
-    from .tensorboard_hook import TensorboardHook
-    from .text_logger_hook import TextLoggerHook
+    from .apex_optimizer_hook import ApexAMPOptimizerHook
+    from .base import OptimizerHook, NoneOptimizerHook
+    from .torch_optimizer_hook import TorchAMPOptimizerHook
 else:
     _import_structure = {
-        'base': ['LoggerHook'],
-        'tensorboard_hook': ['TensorboardHook'],
-        'text_logger_hook': ['TextLoggerHook']
+        'apex_optimizer_hook': ['ApexAMPOptimizerHook'],
+        'base': ['OptimizerHook', 'NoneOptimizerHook'],
+        'torch_optimizer_hook': ['TorchAMPOptimizerHook']
     }
     import sys
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
         _import_structure,
         module_spec=__spec__,
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/logger/base.py` & `modelscope-1.6.0/modelscope/trainers/hooks/logger/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/logger/tensorboard_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/logger/tensorboard_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/logger/text_logger_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/logger/text_logger_hook.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/lr_scheduler_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/lr_scheduler_hook.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,60 +1,92 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
+
 from modelscope.metainfo import Hooks
 from modelscope.trainers.lrscheduler.builder import build_lr_scheduler
 from modelscope.utils.constant import LogKeys
 from modelscope.utils.logger import get_logger
 from modelscope.utils.torch_utils import is_master
 from .builder import HOOKS
 from .hook import Hook
 from .priority import Priority
 
 
+class LrSchedulerProcessor:
+
+    def __init__(self):
+        self.lr_strategy = None
+        self.warmup_lr_scheduler = None
+
+    def set_lr_strategy(self, lr_strategy):
+        self.lr_strategy = lr_strategy
+
+    def set_warmup_lr_scheduler(self, warmup_lr_scheduler):
+        self.warmup_lr_scheduler = warmup_lr_scheduler
+
+    def initialize_lr_scheduler(self, trainer):
+        """Initialize the lr scheduler.
+
+        This is a strategic function which can be registered by other hook's function.
+        """
+        pass
+
+    def step(self, trainer):
+        """Do lr scheduler's step.
+
+        This is a strategic function which can be registered by other hook's function.
+        """
+        if self.warmup_lr_scheduler is not None:
+            self.warmup_lr_scheduler.step()
+        else:
+            trainer.lr_scheduler.step()
+
+
+class LrStrategy:
+    by_epoch = 'by_epoch'
+    by_step = 'by_step'
+    no = 'no'
+
+
 @HOOKS.register_module(module_name=Hooks.LrSchedulerHook)
 class LrSchedulerHook(Hook):
     """Lr scheduler.
 
     Args:
         by_epoch (bool): Whether lr changes by epoch
         warmup (dict): warm up config
     """
     PRIORITY = Priority.LOW
 
-    def __init__(self, by_epoch=True, warmup=None, **kwargs) -> None:
+    def __init__(self,
+                 lr_strategy=LrStrategy.by_epoch,
+                 warmup=None,
+                 **kwargs) -> None:
         super().__init__()
-        self.by_epoch = by_epoch
+        if 'by_epoch' in kwargs:
+            self.lr_strategy = LrStrategy.by_epoch if kwargs[
+                'by_epoch'] else LrStrategy.by_step
+        else:
+            self.lr_strategy = lr_strategy
         self.warmup = warmup
         self.warmup_lr_scheduler = None
+        self.processor = LrSchedulerProcessor()
+
+    def set_processor(self, processor):
+        self.processor = processor
 
     def before_run(self, trainer):
-        self.initialize_lr_scheduler(trainer)
+        self.processor.set_lr_strategy(self.lr_strategy)
         if self.warmup is not None:
             assert isinstance(self.warmup, dict) and 'type' in self.warmup
             self.warmup_lr_scheduler = build_lr_scheduler(
                 cfg=self.warmup,
                 default_args={'base_scheduler': trainer.lr_scheduler})
+            self.processor.set_warmup_lr_scheduler(self.warmup_lr_scheduler)
 
-    @Hook.overload_func(name='LrSchedulerHook.initialize_lr_scheduler')
-    def initialize_lr_scheduler(self, trainer):
-        """Initialize the lr scheduler.
-
-        This is a strategic function which can be registered by other hook's function.
-        """
-        pass
-
-    @Hook.overload_func(name='LrSchedulerHook.step')
-    def step(self, trainer):
-        """Do lr scheduler's step.
-
-        This is a strategic function which can be registered by other hook's function.
-        """
-        if self.warmup_lr_scheduler is not None:
-            self.warmup_lr_scheduler.step()
-        else:
-            trainer.lr_scheduler.step()
+        self.processor.initialize_lr_scheduler(trainer)
 
     def get_current_lr(self, trainer):
         import torch
 
         if isinstance(trainer.optimizer, torch.optim.Optimizer):
             lr = [group['lr'] for group in trainer.optimizer.param_groups]
         elif isinstance(trainer.optimizer, dict):
@@ -63,25 +95,25 @@
                 lr[name] = [group['lr'] for group in optim.param_groups]
         else:
             raise RuntimeError(
                 'lr is not applicable because optimizer does not exist.')
         return lr
 
     def after_train_iter(self, trainer):
-        if not self.by_epoch and trainer.iter >= getattr(
+        if self.lr_strategy == LrStrategy.by_step and trainer.iter >= getattr(
                 trainer, 'cumulative_iters', 1) - 1:
-            self.step(trainer)
+            self.processor.step(trainer)
         trainer.log_buffer.output[LogKeys.LR] = self._get_log_lr(trainer)
 
     def before_train_epoch(self, trainer):
         trainer.log_buffer.output[LogKeys.LR] = self._get_log_lr(trainer)
 
     def after_train_epoch(self, trainer):
-        if self.by_epoch:
-            self.step(trainer)
+        if self.lr_strategy == LrStrategy.by_epoch:
+            self.processor.step(trainer)
 
     def _get_log_lr(self, trainer):
         cur_lr = self.get_current_lr(trainer)
         # only record lr of the first param group
         if isinstance(cur_lr, list):
             lr = cur_lr[0]
         else:
@@ -90,53 +122,65 @@
             for k, lr_ in cur_lr.items():
                 assert isinstance(lr_, list)
                 lr.update({k: lr_[0]})
 
         return lr
 
 
+class PlateauLrSchedulerProcessor(LrSchedulerProcessor):
+
+    def __init__(self, metric_key):
+        super().__init__()
+        self.metric_key = metric_key
+
+    def step(self, trainer):
+        # adapt to evaluation interval is greater than 1
+        if trainer.metric_values is None:
+            if is_master():
+                print(
+                    f'Current epoch {trainer.epoch} has no evaluation metric values, skip lr_scheduler.step() !'
+                )
+            return
+
+        metrics = trainer.metric_values[self.metric_key]
+        if self.lr_strategy == LrStrategy.by_epoch:
+            if self.warmup_lr_scheduler is not None:
+                self.warmup_lr_scheduler.step(metrics=metrics)
+            else:
+                trainer.lr_scheduler.step(metrics=metrics)
+
+
 @HOOKS.register_module(module_name=Hooks.PlateauLrSchedulerHook)
 class PlateauLrSchedulerHook(Hook):
     """Lr scheduler hook for `ReduceLROnPlateau`.
 
     Args:
         metric_key (str): Metric key returned from `trainer.metric_values`,
             get the value of metric key and pass it to `ReduceLROnPlateau.step`.
     """
     PRIORITY = Priority.LOW  # should be after EvaluationHook
 
     def __init__(self, metric_key, **kwargs):
+        super().__init__()
         self.metric_key = metric_key
 
-    def register_strategy(self):
-        Hook.overload(name='LrSchedulerHook.step', function=self.step)
+    def register_processor(self, trainer):
+        lr_scheduler_hook = trainer.get_hook(LrSchedulerHook)
+        if len(lr_scheduler_hook) > 0 and type(
+                lr_scheduler_hook[0].processor) in (type(None),
+                                                    LrSchedulerProcessor):
+            lr_scheduler_hook[0].set_processor(
+                PlateauLrSchedulerProcessor(self.metric_key))
 
     def before_run(self, trainer):
         if not hasattr(trainer, 'logger'):
             self.logger = get_logger()
         else:
             self.logger = trainer.logger
 
-    def step(self, trainer):
-        # adapt to evaluation intervel is greater than 1
-        if trainer.metric_values is None:
-            if is_master():
-                self.logger.warning(
-                    f'Current epoch {trainer.epoch} has no evaluation metric values, skip lr_scheduler.step() !'
-                )
-            return
-
-        metrics = trainer.metric_values[self.metric_key]
-        lr_scheduler_hook = trainer.get_hook(LrSchedulerHook)[0]
-        if lr_scheduler_hook.by_epoch:
-            if lr_scheduler_hook.warmup_lr_scheduler is not None:
-                lr_scheduler_hook.warmup_lr_scheduler.step(metrics=metrics)
-            else:
-                trainer.lr_scheduler.step(metrics=metrics)
-
 
 @HOOKS.register_module(module_name=Hooks.NoneLrSchedulerHook)
 class NoneLrSchedulerHook(LrSchedulerHook):
 
     PRIORITY = Priority.LOW  # should be after EvaluationHook
 
     def __init__(self, by_epoch=True, warmup=None) -> None:
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/megatron_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/distributed/megatron_hook.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,78 +1,33 @@
 import os
-from copy import deepcopy
+import shutil
 
 import torch
 from megatron_util import mpu
 
 from modelscope.metainfo import Hooks
+from modelscope.trainers import EpochBasedTrainer
 from modelscope.trainers.hooks.builder import HOOKS
+from modelscope.trainers.hooks.checkpoint.checkpoint_hook import (
+    BestCkptSaverHook, CheckpointHook, CheckpointProcessor)
+from modelscope.trainers.hooks.checkpoint.load_checkpoint_hook import \
+    LoadCheckpointHook
 from modelscope.trainers.hooks.hook import Hook
-from modelscope.trainers.parallel.builder import build_parallel
 from modelscope.utils.checkpoint import load_checkpoint, save_checkpoint
 from modelscope.utils.constant import DistributedParallelType
 from modelscope.utils.device import create_device
+from modelscope.utils.logger import get_logger
 from modelscope.utils.megatron_utils import is_megatron_initialized
 from modelscope.utils.torch_utils import get_local_rank
-from .checkpoint_hook import CheckpointHook, LoadCheckpointHook
 
 
-@HOOKS.register_module(module_name=Hooks.MegatronHook)
-class MegatronHook(Hook):
+class MpuProcessor(CheckpointProcessor):
 
     _BIN_FILE_DIR = 'model'
 
-    def __init__(self):
-        self.wrapped = False
-
-    def register_strategy(self):
-        Hook.overload(
-            name='CheckpointHook.should_save_on_rank',
-            function=self.should_save_on_rank)
-        Hook.overload(
-            name='CheckpointHook.save_checkpoints',
-            function=self.save_checkpoints)
-        Hook.overload(
-            name='LoadCheckpointHook.load_checkpoints',
-            function=self.load_checkpoints)
-        Hook.overload(
-            name='CheckpointHook.remove_checkpoints',
-            function=self.remove_checkpoints)
-        Hook.overload(
-            name='CheckpointHook.prepare_output', function=self.prepare_output)
-
-    def after_init(self, trainer):
-        assert is_megatron_initialized()
-        local_rank = get_local_rank()
-        trainer.device = create_device(f'cuda:{local_rank}')
-        trainer.model.to(trainer.device)
-        trainer.parallel_groups[
-            DistributedParallelType.DP] = mpu.get_data_parallel_group()
-        trainer.parallel_groups[DistributedParallelType.
-                                TP] = mpu.get_tensor_model_parallel_group()
-        trainer.parallel_groups[DistributedParallelType.
-                                PP] = mpu.get_pipeline_model_parallel_group()
-
-    def before_run(self, trainer):
-        self.wrap_module(trainer)
-
-    def before_val(self, trainer):
-        self.wrap_module(trainer)
-
-    def wrap_module(self, trainer):
-        if trainer._dist:
-            if not self.wrapped:
-                trainer.model = trainer.to_parallel(trainer.model)
-                self.wrapped = True
-
-    def should_save_on_rank(self, trainer):
-        # TODO
-        return (not torch.distributed.is_initialized()
-                ) or mpu.get_data_parallel_rank() == 0
-
     def rank_name(self):
         # TODO
         try:
             tp_world_size = mpu.get_tensor_model_parallel_world_size()
             if tp_world_size == 1:
                 return ''
             mp_rank = mpu.get_tensor_model_parallel_rank()
@@ -81,22 +36,35 @@
             return ''
 
     def get_bin_file(self):
         mp_rank = mpu.get_tensor_model_parallel_rank()
         rank = '{:02d}'.format(mp_rank)
         return f'mp_rank_{rank}_model_states.pt'
 
+    def should_save_on_rank(self, trainer):
+        # TODO
+        return (not torch.distributed.is_initialized()
+                ) or mpu.get_data_parallel_rank() == 0
+
+    def prepare_output(self, trainer, output_dir):
+        config = trainer.cfg
+        CheckpointProcessor.copy_files_and_dump_config(trainer, output_dir,
+                                                       config,
+                                                       self._BIN_FILE_DIR)
+        os.makedirs(
+            os.path.join(output_dir, self._BIN_FILE_DIR), exist_ok=True)
+
     def save_checkpoints(self,
                          trainer,
                          checkpoint_path_prefix,
-                         output_sub_dir,
+                         output_dir,
                          meta=None):
         model = trainer.unwrap_module(trainer.model)
         _train_state_file = checkpoint_path_prefix + self.rank_name(
-        ) + CheckpointHook.TRAINER_STATE_SUFFIX
+        ) + CheckpointProcessor.TRAINER_STATE_SUFFIX
         # Save pth file without model state_dict
         save_checkpoint(
             model,
             _train_state_file,
             trainer.optimizer,
             trainer.lr_scheduler,
             meta=meta,
@@ -105,24 +73,30 @@
         save_dir = os.path.dirname(checkpoint_path_prefix)
         prefix = os.path.basename(checkpoint_path_prefix)
         bin_file = self.get_bin_file()
         prefix_bin_file = os.path.join(save_dir, prefix + '_' + bin_file)
         save_checkpoint(model, prefix_bin_file, with_meta=False)
 
         src_file = prefix_bin_file
-        dest_file = os.path.join(save_dir, output_sub_dir, self._BIN_FILE_DIR,
-                                 bin_file)
+        dest_file = os.path.join(output_dir, self._BIN_FILE_DIR, bin_file)
         if os.path.isfile(dest_file):
             os.unlink(dest_file)
 
-        os.link(src_file, dest_file)
+        try:
+            os.link(src_file, dest_file)
+        except OSError as e:
+            get_logger().error(
+                f'Link {src_file} to {dest_file} error: {e}, '
+                'changing to copy the bin file, this may case more space usage.'
+            )
+            shutil.copyfile(src_file, dest_file)
 
     def remove_checkpoints(self, trainer, checkpoint_path_prefix):
         _train_state_file = checkpoint_path_prefix + self.rank_name(
-        ) + CheckpointHook.TRAINER_STATE_SUFFIX
+        ) + CheckpointProcessor.TRAINER_STATE_SUFFIX
         if os.path.isfile(_train_state_file):
             os.remove(_train_state_file)
 
         save_dir = os.path.dirname(checkpoint_path_prefix)
         prefix = os.path.basename(checkpoint_path_prefix)
         bin_file = self.get_bin_file()
         absolute_file = os.path.join(save_dir, prefix + '_' + bin_file)
@@ -135,25 +109,66 @@
         if os.path.isdir(checkpoint_path_prefix):
             save_dir = checkpoint_path_prefix
             bin_file = self.get_bin_file()
             model_file = os.path.join(save_dir, bin_file)
             load_checkpoint(model_file, model, None, None)
         else:
             _train_state_file = checkpoint_path_prefix + self.rank_name(
-            ) + CheckpointHook.TRAINER_STATE_SUFFIX
+            ) + CheckpointProcessor.TRAINER_STATE_SUFFIX
             meta = LoadCheckpointHook.load_trainer_state(
                 trainer, _train_state_file, load_all_state)
 
             save_dir = os.path.dirname(checkpoint_path_prefix)
             prefix = os.path.basename(checkpoint_path_prefix)
             bin_file = self.get_bin_file()
 
             model_file = os.path.join(save_dir, prefix + '_' + bin_file)
             load_checkpoint(model_file, model, None, None)
             return meta
 
-    def prepare_output(self, trainer, output_dir):
-        config = trainer.cfg
-        CheckpointHook.copy_files_and_dump_config(trainer, output_dir, config,
-                                                  self._BIN_FILE_DIR)
-        os.makedirs(
-            os.path.join(output_dir, self._BIN_FILE_DIR), exist_ok=True)
+
+@HOOKS.register_module(module_name=Hooks.MegatronHook)
+class MegatronHook(Hook):
+
+    _BIN_FILE_DIR = 'model'
+
+    def __init__(self):
+        self.wrapped = False
+
+    def register_processor(self, trainer: EpochBasedTrainer):
+        processor = MpuProcessor()
+        ckpt_hook = trainer.get_hook(CheckpointHook)
+        if len(ckpt_hook) > 0 and not isinstance(ckpt_hook[0].processor,
+                                                 MpuProcessor):
+            ckpt_hook[0].set_processor(processor)
+        best_ckpt_hook = trainer.get_hook(BestCkptSaverHook)
+        if len(best_ckpt_hook) > 0 and not isinstance(
+                best_ckpt_hook[0].processor, MpuProcessor):
+            best_ckpt_hook[0].set_processor(processor)
+        load_ckpt_hook = trainer.get_hook(LoadCheckpointHook)
+        if len(load_ckpt_hook) > 0 and not isinstance(
+                load_ckpt_hook[0].processor, MpuProcessor):
+            load_ckpt_hook[0].set_processor(processor)
+
+    def after_init(self, trainer):
+        assert is_megatron_initialized()
+        local_rank = get_local_rank()
+        trainer.device = create_device(f'cuda:{local_rank}')
+        trainer.model.to(trainer.device)
+        trainer.parallel_groups[
+            DistributedParallelType.DP] = mpu.get_data_parallel_group()
+        trainer.parallel_groups[DistributedParallelType.
+                                TP] = mpu.get_tensor_model_parallel_group()
+        trainer.parallel_groups[DistributedParallelType.
+                                PP] = mpu.get_pipeline_model_parallel_group()
+
+    def before_run(self, trainer):
+        self.wrap_module(trainer)
+
+    def before_val(self, trainer):
+        self.wrap_module(trainer)
+
+    def wrap_module(self, trainer):
+        if trainer._dist:
+            if not self.wrapped:
+                trainer.model = trainer.to_parallel(trainer.model)
+                self.wrapped = True
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,48 +3,22 @@
 
 import torch
 from packaging import version
 
 from modelscope.metainfo import Hooks
 from modelscope.trainers.hooks import Hook
 from modelscope.trainers.hooks.builder import HOOKS
-from .base import OptimizerHook
+from .base import OptimizerHook, OptimizerProcessor
 
 
-@HOOKS.register_module(module_name=Hooks.ApexAMPOptimizerHook)
-class ApexAMPOptimizerHook(Hook):
-    """
-    Fp16 optimizer, if torch version is less than 1.6.0,
-    you must install apex (https://www.github.com/nvidia/apex) else use torch.cuda.amp by default
+class ApexOptimizerProcessor(OptimizerProcessor):
 
-    Args:
-        opt_level (str): "O0" and "O3" are not true mixed precision,
-            but they are useful for establishing accuracy and speed baselines, respectively.
-            "O1" and "O2" are different implementations of mixed precision.
-            Try both, and see what gives the best speedup and accuracy for your model.
-    """
-
-    PRIORITY = OptimizerHook.PRIORITY
-
-    def __init__(self, opt_level='O1', **kwargs):
+    def __init__(self, opt_level):
         self.opt_level = opt_level
 
-        try:
-            from apex import amp
-        except ImportError:
-            raise ValueError(
-                'apex not installed, please install apex from https://www.github.com/nvidia/apex.'
-            )
-
-    def register_strategy(self):
-        Hook.overload(
-            name='OptimizerHook.initialize_optimizer',
-            function=self.initialize_optimizer)
-        Hook.overload(name='OptimizerHook.backward', function=self.backward)
-
     def initialize_optimizer(self, trainer):
         from apex import amp
 
         if version.parse(torch.__version__) >= version.parse('1.9.0'):
             trainer.logger.warning(
                 'ApexAMPOptimizerHook is only tested on torch version 1.8.x,'
                 'if it works abnormally please consider downgrading your torch version to 1.8.x.'
@@ -64,14 +38,48 @@
 
         from apex import amp
         for k in loss_keys:
             with amp.scale_loss(trainer.train_outputs[k],
                                 trainer.optimizer) as scaled_loss:
                 scaled_loss.backward()
 
-        if self.every_n_iters(trainer, cumulative_iters):
+        if Hook.every_n_iters(trainer, cumulative_iters):
             if grad_clip is not None:
-                OptimizerHook.clip_grads(trainer.model.parameters(),
-                                         **grad_clip)
+                OptimizerProcessor.clip_grads(trainer.model.parameters(),
+                                              **grad_clip)
 
             trainer.optimizer.step()
             trainer.optimizer.zero_grad()
+
+
+@HOOKS.register_module(module_name=Hooks.ApexAMPOptimizerHook)
+class ApexAMPOptimizerHook(Hook):
+    """
+    Fp16 optimizer, if torch version is less than 1.6.0,
+    you must install apex (https://www.github.com/nvidia/apex) else use torch.cuda.amp by default
+
+    Args:
+        opt_level (str): "O0" and "O3" are not true mixed precision,
+            but they are useful for establishing accuracy and speed baselines, respectively.
+            "O1" and "O2" are different implementations of mixed precision.
+            Try both, and see what gives the best speedup and accuracy for your model.
+    """
+
+    PRIORITY = OptimizerHook.PRIORITY
+
+    def __init__(self, opt_level='O1', **kwargs):
+        self.opt_level = opt_level
+
+        try:
+            from apex import amp
+        except ImportError:
+            raise ValueError(
+                'apex not installed, please install apex from https://www.github.com/nvidia/apex.'
+            )
+
+    def register_processor(self, trainer):
+        optimizer_hook = trainer.get_hook(OptimizerHook)
+        if len(optimizer_hook) > 0 and type(
+                optimizer_hook[0].processor) in (type(None),
+                                                 OptimizerProcessor):
+            optimizer_hook[0].set_processor(
+                ApexOptimizerProcessor(self.opt_level))
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/optimizer/base.py` & `modelscope-1.6.0/modelscope/trainers/hooks/optimizer/base.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,14 +6,56 @@
 from modelscope.metainfo import Hooks
 from modelscope.outputs import OutputKeys
 from modelscope.trainers.hooks.builder import HOOKS
 from modelscope.trainers.hooks.hook import Hook
 from modelscope.trainers.hooks.priority import Priority
 
 
+class OptimizerProcessor:
+
+    def initialize_optimizer(self, trainer):
+        """Initialize the optimizer.
+
+        This is a strategic function which can be registered by other hook's function.
+        """
+        trainer.optimizer.zero_grad()
+
+    def before_forward(self, trainer):
+        pass
+
+    def backward(self, trainer, loss_keys, cumulative_iters, grad_clip):
+        """Do module backward, optimizer's step and zero_grad and clip the grads.
+
+        This is a strategic function which can be registered by other hook's function.
+
+        Args:
+            trainer(`EpochBasedTrainer`): The trainer instance.
+            loss_keys(`list`): The list of loss keys.
+            cumulative_iters(`int`): The cumulative iters for gradients.
+            grad_clip(`dict`): The grad clipping options.
+        """
+        for k in loss_keys:
+            trainer.train_outputs[k] /= cumulative_iters
+            trainer.train_outputs[k].backward()
+
+        if Hook.every_n_iters(trainer, cumulative_iters):
+            if grad_clip is not None:
+                self.clip_grads(trainer.model.parameters(), **grad_clip)
+
+            trainer.optimizer.step()
+            trainer.optimizer.zero_grad()
+
+    @staticmethod
+    def clip_grads(params, **clip_args):
+        params = list(
+            filter(lambda p: p.requires_grad and p.grad is not None, params))
+        if len(params) > 0:
+            return clip_grad.clip_grad_norm_(params, **clip_args)
+
+
 @HOOKS.register_module(module_name=Hooks.OptimizerHook)
 class OptimizerHook(Hook):
     """Optimizer hook
 
     Args:
         cumulative_iters (int): interval of gradients accumulation. Default: 1
         grad_clip (dict): Default None. Containing keys:
@@ -32,60 +74,29 @@
                  **kwargs) -> None:
         if isinstance(loss_keys, str):
             loss_keys = [loss_keys]
         assert isinstance(loss_keys, (tuple, list))
         self.loss_keys = loss_keys
         self.cumulative_iters = cumulative_iters
         self.grad_clip = grad_clip
+        self.processor = OptimizerProcessor()
 
-    @staticmethod
-    def clip_grads(params, **clip_args):
-        params = list(
-            filter(lambda p: p.requires_grad and p.grad is not None, params))
-        if len(params) > 0:
-            return clip_grad.clip_grad_norm_(params, **clip_args)
-
-    @Hook.overload_func(name='OptimizerHook.initialize_optimizer')
-    def initialize_optimizer(self, trainer):
-        """Initialize the optimizer.
-
-        This is a strategic function which can be registered by other hook's function.
-        """
-        trainer.optimizer.zero_grad()
+    def set_processor(self, processor):
+        self.processor = processor
 
     def before_run(self, trainer):
-        self.initialize_optimizer(trainer)
         trainer.cumulative_iters = self.cumulative_iters
+        self.processor.initialize_optimizer(trainer)
 
-    @Hook.overload_func(name='OptimizerHook.backward')
-    def backward(self, trainer, loss_keys, cumulative_iters, grad_clip):
-        """Do module backward, optimizer's step and zero_grad and clip the grads.
-
-        This is a strategic function which can be registered by other hook's function.
-
-        Args:
-            trainer(`EpochBasedTrainer`): The trainer instance.
-            loss_keys(`list`): The list of loss keys.
-            cumulative_iters(`int`): The cumulative iters for gradients.
-            grad_clip(`dict`): The grad clipping options.
-        """
-        for k in loss_keys:
-            trainer.train_outputs[k] /= cumulative_iters
-            trainer.train_outputs[k].backward()
-
-        if self.every_n_iters(trainer, cumulative_iters):
-            if grad_clip is not None:
-                self.clip_grads(trainer.model.parameters(), **grad_clip)
-
-            trainer.optimizer.step()
-            trainer.optimizer.zero_grad()
+    def before_train_iter(self, trainer):
+        self.processor.before_forward(trainer)
 
     def after_train_iter(self, trainer):
-        self.backward(trainer, self.loss_keys, self.cumulative_iters,
-                      self.grad_clip)
+        self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
+                                self.grad_clip)
 
 
 @HOOKS.register_module(module_name=Hooks.NoneOptimizerHook)
 class NoneOptimizerHook(OptimizerHook):
 
     def __init__(self, cumulative_iters=1, grad_clip=None, loss_keys='loss'):
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py` & `modelscope-1.6.0/modelscope/trainers/hooks/optimizer/torch_optimizer_hook.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,14 +1,52 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import logging
 
 from modelscope.metainfo import Hooks
 from modelscope.trainers.hooks import Hook
 from modelscope.trainers.hooks.builder import HOOKS
-from .base import OptimizerHook
+from .base import OptimizerHook, OptimizerProcessor
+
+
+class TorchAMPOptimizerProcessor(OptimizerProcessor):
+
+    def __init__(self, scaler, scale_update_param):
+        self.scaler = scaler
+        self.scale_update_param = scale_update_param
+
+    def before_forward(self, trainer):
+        from torch.cuda import amp
+        setattr(self._model, 'forward', amp.autocast()(self._model.forward))
+
+    def initialize_optimizer(self, trainer):
+        logging.info('open fp16')
+        trainer.optimizer.zero_grad()
+
+        model = trainer.unwrap_module(trainer.model)
+        self._ori_model_forward = model.forward
+        self._model = model
+
+    def backward(self, trainer, loss_keys, cumulative_iters, grad_clip):
+        for k in loss_keys:
+            trainer.train_outputs[k] /= cumulative_iters
+
+        for k in loss_keys:
+            self.scaler.scale(trainer.train_outputs[k]).backward()
+
+        if Hook.every_n_iters(trainer, cumulative_iters):
+            self.scaler.unscale_(trainer.optimizer)
+            if grad_clip is not None:
+                OptimizerProcessor.clip_grads(trainer.model.parameters(),
+                                              **grad_clip)
+
+            self.scaler.step(trainer.optimizer)
+            self.scaler.update(self.scale_update_param)
+            trainer.optimizer.zero_grad()
+
+        setattr(self._model, 'forward', self._ori_model_forward)
 
 
 @HOOKS.register_module(module_name=Hooks.TorchAMPOptimizerHook)
 class TorchAMPOptimizerHook(Hook):
     """
     Fp16 optimizer, if torch version is less than 1.6.0,
     you must install apex (https://www.github.com/nvidia/apex) else use torch.cuda.amp by default
@@ -40,43 +78,15 @@
         elif isinstance(loss_scale, dict):
             self.scaler = amp.GradScaler(**loss_scale)
         else:
             raise ValueError(
                 '`loss_scale` type must be in [float, dict], but got {loss_scale}'
             )
 
-    def register_strategy(self):
-        Hook.overload(
-            name='OptimizerHook.initialize_optimizer',
-            function=self.initialize_optimizer)
-        Hook.overload(name='OptimizerHook.backward', function=self.backward)
-
-    def initialize_optimizer(self, trainer):
-        logging.info('open fp16')
-        trainer.optimizer.zero_grad()
-
-        model = trainer.unwrap_module(trainer.model)
-        self._ori_model_forward = model.forward
-        self._model = model
-
-    def before_train_iter(self, trainer):
-        from torch.cuda import amp
-        setattr(self._model, 'forward', amp.autocast()(self._model.forward))
-
-    def backward(self, trainer, loss_keys, cumulative_iters, grad_clip):
-        for k in loss_keys:
-            trainer.train_outputs[k] /= cumulative_iters
-
-        for k in loss_keys:
-            self.scaler.scale(trainer.train_outputs[k]).backward()
-
-        if self.every_n_iters(trainer, cumulative_iters):
-            self.scaler.unscale_(trainer.optimizer)
-            if grad_clip is not None:
-                OptimizerHook.clip_grads(trainer.model.parameters(),
-                                         **grad_clip)
-
-            self.scaler.step(trainer.optimizer)
-            self.scaler.update(self._scale_update_param)
-            trainer.optimizer.zero_grad()
-
-        setattr(self._model, 'forward', self._ori_model_forward)
+    def register_processor(self, trainer):
+        optimizer_hook = trainer.get_hook(OptimizerHook)
+        if len(optimizer_hook) > 0 and type(
+                optimizer_hook[0].processor) in (type(None),
+                                                 OptimizerProcessor):
+            optimizer_hook[0].set_processor(
+                TorchAMPOptimizerProcessor(self.scaler,
+                                           self._scale_update_param))
```

### Comparing `modelscope-1.5.2/modelscope/trainers/hooks/priority.py` & `modelscope-1.6.0/modelscope/trainers/hooks/priority.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/lrscheduler/__init__.py` & `modelscope-1.6.0/modelscope/trainers/lrscheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/lrscheduler/builder.py` & `modelscope-1.6.0/modelscope/trainers/lrscheduler/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/lrscheduler/warmup/base.py` & `modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/base.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/lrscheduler/warmup/warmup.py` & `modelscope-1.6.0/modelscope/trainers/lrscheduler/warmup/warmup.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/clip/clip_trainer.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/clip/clip_trainer.py`

 * *Files 2% similar despite different names*

```diff
@@ -172,19 +172,18 @@
                     resolution=model.model_info['image_resolution']),
             }
 
         # dataset related
         self.dataset_cfg = cfg.dataset
         if hasattr(self.dataset_cfg, 'column_map'):
             # cases where dataset key names are not "img" and "text"
-            img_key_name = getattr(self.dataset_cfg.column_map, 'img', 'img')
+            img_key_name = self.dataset_cfg['column_map'].get('img', 'img')
             preprocessor[ConfigKeys.train].set_input_img_key(img_key_name)
             preprocessor[ConfigKeys.val].set_input_img_key(img_key_name)
-            text_key_name = getattr(self.dataset_cfg.column_map, 'text',
-                                    'text')
+            text_key_name = self.dataset_cfg['column_map'].get('text', 'text')
             preprocessor[ConfigKeys.train].set_input_text_key(text_key_name)
             preprocessor[ConfigKeys.val].set_input_text_key(text_key_name)
         self.global_batch_size = cfg.train.dataloader.batch_size_per_gpu * world_size
 
         super().__init__(
             model=model,
             cfg_file=cfg_file,
```

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/clip/clip_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/mgeo_ranking_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/mplug/mplug_trainer.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/mplug/mplug_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/ofa/ofa_trainer.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/ofa/ofa_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/ofa/ofa_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/team/team_trainer.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/team/team_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/multi_modal/team/team_trainer_utils.py` & `modelscope-1.6.0/modelscope/trainers/multi_modal/team/team_trainer_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/__init__.py` & `modelscope-1.6.0/modelscope/trainers/nlp/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,22 +6,24 @@
 if TYPE_CHECKING:
     from .sequence_classification_trainer import SequenceClassificationTrainer
     from .csanmt_translation_trainer import CsanmtTranslationTrainer
     from .text_ranking_trainer import TextRankingTrainer
     from .text_generation_trainer import TextGenerationTrainer
     from .sentence_embedding_trainer import SentenceEmbeddingTrainer
     from .siamese_uie_trainer import SiameseUIETrainer
+    from .translation_evaluation_trainer import TranslationEvaluationTrainer
 else:
     _import_structure = {
         'sequence_classification_trainer': ['SequenceClassificationTrainer'],
         'csanmt_translation_trainer': ['CsanmtTranslationTrainer'],
         'text_ranking_trainer': ['TextRankingTrainer'],
         'text_generation_trainer': ['TextGenerationTrainer'],
         'sentence_emebedding_trainer': ['SentenceEmbeddingTrainer'],
-        'siamese_uie_trainer': ['SiameseUIETrainer']
+        'siamese_uie_trainer': ['SiameseUIETrainer'],
+        'translation_evaluation_trainer': ['TranslationEvaluationTrainer']
     }
 
     import sys
 
     sys.modules[__name__] = LazyImportModule(
         __name__,
         globals()['__file__'],
```

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/csanmt_translation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/csanmt_translation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/document_grounded_dialog_generate_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/document_grounded_dialog_rerank_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/document_grounded_dialog_retrieval_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/faq_question_answering_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/faq_question_answering_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/gpt3_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/gpt3_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/gpt_moe_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/gpt_moe_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/sentence_embedding_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/sentence_embedding_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/sequence_classification_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/sequence_classification_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/siamese_uie_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/siamese_uie_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/space/dialog_intent_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/space/dialog_intent_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/space/dialog_modeling_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/space/dialog_modeling_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/space/eval.py` & `modelscope-1.6.0/modelscope/trainers/nlp/space/eval.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/space/metrics/metrics_tracker.py` & `modelscope-1.6.0/modelscope/trainers/nlp/space/metrics/metrics_tracker.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/space/trainer/gen_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/space/trainer/gen_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/space/trainer/intent_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/space/trainer/intent_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/table_question_answering_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/table_question_answering_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/text_generation_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/text_generation_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp/text_ranking_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp/text_ranking_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/nlp_trainer.py` & `modelscope-1.6.0/modelscope/trainers/nlp_trainer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/optimizer/builder.py` & `modelscope-1.6.0/modelscope/trainers/optimizer/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py` & `modelscope-1.6.0/modelscope/trainers/optimizer/child_tuning_adamw_optimizer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/parallel/builder.py` & `modelscope-1.6.0/modelscope/trainers/parallel/builder.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/parallel/utils.py` & `modelscope-1.6.0/modelscope/trainers/parallel/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/trainer.py` & `modelscope-1.6.0/modelscope/trainers/trainer.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from functools import partial
 from typing import Callable, Dict, List, Optional, Tuple, Union
 
 import json
 import torch
 from torch import distributed as dist
 from torch import nn
-from torch.utils.data import DataLoader, Dataset
+from torch.utils.data import DataLoader, Dataset, Sampler
 from torch.utils.data.dataloader import default_collate
 from torch.utils.data.distributed import DistributedSampler
 
 from modelscope.metainfo import Trainers
 from modelscope.metrics import build_metric, task_default_metrics
 from modelscope.metrics.prediction_saving_wrapper import \
     PredictionSavingWrapper
@@ -84,15 +84,15 @@
         remove_unused_data: Automatically remove unused data keys in mini-batches.
             The remove action based on the `inspect` on the model's forward method, the removed columns will be
             moved to the mini-batch's attributes.
         compile (bool, optional): Compile the model with torch 2.0, default False
         compile_options (dict, optional): The compile options if compile=True,
             default None to use the default params of 'TorchModel.compile'.
         efficient_tuners (dict, optional): The tuners to use to train the model
-
+        samplers: (:obj:`Sampler` or `Dict[Sampler]`, *optional*): samplers used in the train/eval DataLoader.
         Examples of cfg_modify_fn:
             >>> def cfg_modify_fn(cfg):
             >>>     cfg.preprocessor.first_sequence= 'text1'
             >>>     cfg.preprocessor.second_sequence='text2'
             >>>     return cfg
     """
 
@@ -110,14 +110,15 @@
                                          Dict[str, Preprocessor]]] = None,
             optimizers: Tuple[torch.optim.Optimizer,
                               torch.optim.lr_scheduler._LRScheduler] = (None,
                                                                         None),
             model_revision: Optional[str] = DEFAULT_MODEL_REVISION,
             seed: int = 42,
             callbacks: Optional[List[Hook]] = None,
+            samplers: Optional[Union[Sampler, Dict[str, Sampler]]] = None,
             efficient_tuners: List[Dict] = None,
             **kwargs):
 
         self._seed = seed
         set_random_seed(self._seed)
         self._metric_values = None
         self.optimizers = optimizers
@@ -128,14 +129,15 @@
         self._inner_iter = 0
         self._stop_training = False
         self._compile = kwargs.get('compile', False)
 
         self.train_dataloader = None
         self.eval_dataloader = None
         self.data_loader = None
+        self._samplers = samplers
 
         if isinstance(model, str):
             third_party = kwargs.get(ThirdParty.KEY)
             if third_party is not None:
                 kwargs.pop(ThirdParty.KEY)
 
             self.model_dir = self.get_or_download_model_dir(
@@ -220,17 +222,14 @@
         self.device = kwargs.get('device')
         self.tune_module(efficient_tuners)
 
         # The parallel_groups field will be initialized in the hooks' after_init stage.
         # Please check the DDPHook and MegatronHook for details.
         self.parallel_groups = {}
 
-        # Clear the Hook overload functions to avoid duplication.
-        Hook.clear_strategies()
-
         if self.launcher is not None and not self.cfg.safe_get(
                 'train.hooks.DDPHook'):
             # A logic to fit the current code
             # Put a DDPHook in if launcher is provided.
             if 'hooks' not in self.cfg.train:
                 self.cfg.train['hooks'] = ConfigDict([])
             self.cfg.train['hooks'].append({
@@ -677,14 +676,15 @@
                 strict(`boolean`): If strict, any unmatched keys will cause an error.
         """
 
         self._mode = ModeKeys.TRAIN
         self.train_dataloader = self.get_train_dataloader()
         self.data_loader = self.train_dataloader
         self.register_optimizers_hook()
+        self.register_processors()
         self.print_hook_info()
         self.set_checkpoint_file_to_hook(checkpoint_path, load_all_state,
                                          kwargs.get('strict', False))
         self.model.train()
 
         self.train_loop(self.train_dataloader)
 
@@ -716,14 +716,15 @@
 
             checkpoint_path(`str`, `optional`): The previous saving checkpoint to read,
                 usually it's a `some-file-name.pth` file or a pure PyTorch `some-file.bin` file
                 generated by this trainer.
 
             strict(`boolean`): If strict, any unmatched keys will cause an error.
         """
+        self.register_processors()
         self.print_hook_info()
         if checkpoint_path is not None:
             from modelscope.trainers.hooks import LoadCheckpointHook
             LoadCheckpointHook.load_checkpoint(
                 checkpoint_path, self, strict=strict)
         self.model.eval()
         self._mode = ModeKeys.EVAL
@@ -754,14 +755,15 @@
                 >>>         predictions = np.argmax(outputs['logits'].cpu().numpy(), axis=1)
                 >>>         with open(self.filename, 'a') as f:
                 >>>             for id, pred in zip(ids, predictions):
                 >>>                 f.writelines(f'{id}, {pred}')
             kwargs:
                 strict(`boolean`): If strict, any unmatched keys will cause an error.
         """
+        self.register_processors()
         self.print_hook_info()
         if checkpoint_path is not None:
             from modelscope.trainers.hooks import LoadCheckpointHook
             LoadCheckpointHook.load_checkpoint(
                 checkpoint_path, self, strict=kwargs.get('strict', False))
         self.model.eval()
         self._mode = ModeKeys.EVAL
@@ -893,39 +895,52 @@
 
         We provide a reasonable default that works well. If you want to use something else, you can change
         the config for data.train in configuration file, or subclass and override this method
         (or `get_train_dataloader` in a subclass.
         """
         if self.train_dataset is None:
             raise 'The train_dataset cannot be None.'
+
+        sampler_cfg = {}
+        if self._samplers is not None:
+            sampler_cfg['sampler'] = self._samplers[
+                ConfigKeys.train] if isinstance(self._samplers,
+                                                dict) else self._samplers
         data_loader = self._build_dataloader_with_dataset(
             self.train_dataset,
             dist=self._dist,
             seed=self._seed,
             collate_fn=self.train_data_collator,
+            **sampler_cfg,
             **self.cfg.train.get('dataloader', {}))
         return data_loader
 
     def get_eval_data_loader(self):
         """ Builder torch dataloader for evaluation.
 
         We provide a reasonable default that works well. If you want to use something else, you can change
         the config for dataset.eval in configuration file, or subclass and override this method in a subclass.
         pass
         """
         if self.eval_dataset is None:
             raise 'The eval_dataset cannot be None.'
 
+        sampler_cfg = {}
+        if self._samplers is not None:
+            sampler_cfg['sampler'] = self._samplers[
+                ConfigKeys.val] if isinstance(self._samplers,
+                                              dict) else self._samplers
         default_config = {'shuffle': False}
         default_config.update(self.cfg.evaluation.get('dataloader', {}))
         data_loader = self._build_dataloader_with_dataset(
             self.eval_dataset,
             dist=self._dist,
             seed=self._seed,
             collate_fn=self.eval_data_collator,
+            **sampler_cfg,
             **default_config)
         return data_loader
 
     def get_predict_dataloader(self, predict_datasets: Union[Dataset,
                                                              List[Dataset]]):
         """ Builder torch dataloader for prediction with the config of evaluation.
 
@@ -934,21 +949,27 @@
         """
         dataset = self.build_dataset(
             datasets=predict_datasets,
             model_cfg=self.cfg,
             mode=ModeKeys.EVAL,
             preprocessor=self.eval_preprocessor)
 
+        sampler_cfg = {}
+        if self._samplers is not None:
+            sampler_cfg['sampler'] = self._samplers[
+                ConfigKeys.val] if isinstance(self._samplers,
+                                              dict) else self._samplers
         default_config = {'shuffle': False}
         default_config.update(self.cfg.evaluation.get('dataloader', {}))
         data_loader = self._build_dataloader_with_dataset(
             dataset,
             dist=self._dist,
             seed=self._seed,
             collate_fn=self.eval_data_collator,
+            **sampler_cfg,
             **default_config)
         return data_loader
 
     def build_optimizer(self, cfg: ConfigDict, default_args: dict = None):
         try:
             return build_optimizer(
                 self.unwrap_module(self.model),
@@ -1128,21 +1149,27 @@
             # number of training samples on each GPU.
             batch_size = batch_size_per_gpu
             num_workers = workers_per_gpu
         else:
             batch_size = batch_size_per_gpu
             num_workers = workers_per_gpu
 
-        if dist and not isinstance(dataset, torch.utils.data.IterableDataset):
-            sampler = DistributedSampler(
-                dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)
-        else:
-            sampler = None
-            if not isinstance(dataset, torch.utils.data.IterableDataset):
-                kwargs['shuffle'] = shuffle
+        sampler = kwargs.pop('sampler', None)
+        if sampler is None:
+            if dist and not isinstance(dataset,
+                                       torch.utils.data.IterableDataset):
+                sampler = DistributedSampler(
+                    dataset,
+                    num_replicas=world_size,
+                    rank=rank,
+                    shuffle=shuffle)
+            else:
+                sampler = None
+                if not isinstance(dataset, torch.utils.data.IterableDataset):
+                    kwargs['shuffle'] = shuffle
 
         batch_sampler = None
 
         init_fn = partial(
             worker_init_fn, num_workers=num_workers, rank=rank,
             seed=seed) if seed is not None else None
 
@@ -1165,27 +1192,26 @@
 
         return data_loader
 
     def train_loop(self, data_loader):
         """ Training loop used by `EpochBasedTrainer.train()`
         """
         self.invoke_hook(TrainerStages.before_run)
-        kwargs = {}
         self.model.train()
         for _ in range(self._epoch, self._max_epochs):
             self.invoke_hook(TrainerStages.before_train_epoch)
             for i, data_batch in enumerate(data_loader):
                 if i < self.inner_iter:
                     # inner_iter may be read out from the checkpoint file, so skip the trained iters in the epoch.
                     continue
                 data_batch = to_device(data_batch, self.device)
                 self.data_batch = data_batch
                 self._inner_iter = i
                 self.invoke_hook(TrainerStages.before_train_iter)
-                self.train_step(self.model, data_batch, **kwargs)
+                self.train_step(self.model, data_batch)
                 self.invoke_hook(TrainerStages.after_train_iter)
                 # Value changed after the hooks are invoked, do not move them above the invoke_hook code.
                 del self.data_batch
                 self._iter += 1
                 self._mode = ModeKeys.TRAIN
 
                 if i + 1 >= self.iters_per_epoch:
@@ -1316,20 +1342,25 @@
             A list of instances of registered hooks.
         """
         hook_cfg = hook_cfg.copy()
         assert isinstance(hook_cfg, list)
         hooks = []
         for cfg_i in hook_cfg:
             hook = build_from_cfg(cfg_i, HOOKS)
-            if hasattr(hook, 'register_strategy'):
-                hook.register_strategy()
             self.register_hook(hook)
             hooks.append(hook)
         return hooks
 
+    def register_processors(self):
+        """Register processors to hooks
+        """
+        for hook in self.hooks:
+            if hasattr(hook, 'register_processor'):
+                hook.register_processor(self)
+
     def get_hook(self, cls):
         return [h for h in self._hooks if h.__class__ == cls]
 
     def invoke_hook(self, fn_name: str) -> None:
         """Call all hooks.
 
         Args:
@@ -1377,22 +1408,15 @@
             hook_infos = stage_hook_map[stage]
             if len(hook_infos) > 0:
                 info = f'Stage: {stage}:\n    '
                 info += '\n    '.join(hook_infos)
                 info += '\n -------------------- '
                 stage_hook_infos.append(info)
         stage_hook_infos = '\n'.join(stage_hook_infos)
-
-        strategy_info = '\n --- Hook strategies info --- \n'
-        for consumer, methods in Hook._strategies.items():
-            strategy_info += f'Method: {consumer} ' \
-                             f'replaced by: ' \
-                             f'{[method.__self__.__class__.__name__ + "." + method.__name__ for method in methods]}\n'
-        strategy_info += '\n --- Hook strategies info end --- \n'
-        return stage_hook_infos + strategy_info
+        return stage_hook_infos
 
 
 def worker_init_fn(worker_id, num_workers, rank, seed):
     # The seed of each worker equals to
     # num_worker * rank + worker_id + user_seed
     worker_seed = num_workers * rank + worker_id + seed
     set_random_seed(worker_seed)
```

### Comparing `modelscope-1.5.2/modelscope/trainers/utils/inference.py` & `modelscope-1.6.0/modelscope/trainers/utils/inference.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/trainers/utils/log_buffer.py` & `modelscope-1.6.0/modelscope/trainers/utils/log_buffer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/tuners/control_sd_lora.py` & `modelscope-1.6.0/modelscope/tuners/control_sd_lora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/tuners/lora.py` & `modelscope-1.6.0/modelscope/tuners/lora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/tuners/sd_lora.py` & `modelscope-1.6.0/modelscope/tuners/sd_lora.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/ast_index_file.py` & `modelscope-1.6.0/modelscope/utils/ast_index_file.py`

 * *Files 11% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.7631412250656355%*

 * *Differences: {"'files_mtime'": "{'TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py': 1684724969.5255067, "*

 * *                  "'TEMPLATE_PATH/models/nlp/palm_v2/configuration.py': 1684724969.5255067, "*

 * *                  "'TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py': 1684724969.5255067, "*

 * *                  "'TEMPLATE_PATH/models/nlp/gpt2/backbone.py': 1684724969.5175066, "*

 * *                  "'TEMPLATE_PATH/models/nlp/bart/text_error_correction.py': 1684724969.5135067, "*

 * *                  "'TEMPLATE_PATH/models/n []*

```diff
@@ -1,1672 +1,1673 @@
 {
     "files_mtime": {
-        "TEMPLATE_PATH/exporters/base.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/builder.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/tf_model_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/exporters/torch_model_exporter.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/accuracy_metric.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/action_detection_evaluator.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/audio_noise_metric.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/base.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/bleu_metric.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/builder.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/ciderD/ciderD.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/ciderD/ciderD_scorer.py": 1682131573.075432,
-        "TEMPLATE_PATH/metrics/image_color_enhance_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_colorization_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_denoise_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_inpainting_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/inbatch_recall_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/loss_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/map_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/ned_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/ocr_recognition_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/ppl_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/sequence_classification_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/text_generation_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/text_ranking_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/token_classification_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_stabilization_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_summarization_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/matlab_functions.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/metric_util.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/niqe.py": 1682131573.0794325,
-        "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/layers/activations.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/layers/affine_transform.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/layers/deep_fsmn.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/layers/layer_base.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/layers/uni_deep_fsmn.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/network/loss.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/network/modulation_loss.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/aec/network/se_net.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/complex_nn.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/conv_stft.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/denoise_net.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/frcrn.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/layers/activations.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/layers/affine_transform.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/layers/layer_base.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/layers/uni_deep_fsmn.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/se_module_complex.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/ans/unet.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v2.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/kws/farfield/model.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/kws/farfield/model_def.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py": 1682131573.0794325,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/cmvn.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/fsmn.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/kws/nearfield/model.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/separation/layer_norm.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/separation/mossformer.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/separation/mossformer_block.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/separation/mossformer_conv_module.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/sv/DTDNN.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/sv/DTDNN_layers.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/audio/tts/voice.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/base/base_head.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/base/base_model.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/base/base_torch_head.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/base/base_torch_model.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/builder.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_detection/action_detection_onnx.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_detection/modules/action_detection_pytorch.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_detection/modules/resnet.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_recognition/models.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_recognition/s3dg.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_recognition/tada_convnext.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/action_recognition/temporal_patch_shift_transformer.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/animal_recognition/resnet.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/animal_recognition/splat.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_basic_modules.py": 1682131573.0834327,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_2d_keypoints/w48.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/backbone.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/block.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/directed_graph.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/skeleton.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/LK/lk.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/config.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_detector.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_landmark.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/facelib/facer.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/loss.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/model_tf.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/network.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cartoon/utils.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/c3d.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet2p1d.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet3d.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/annotator.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/api.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/vit.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/utils.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/utils.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/body.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/hand.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/model.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/util.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py": 1682131573.087433,
-        "TEMPLATE_PATH/models/cv/crowd_counting/hrnet_aspp_relu.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/easycv_base.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_2d_keypoints/face_2d_keypoints_align.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogface.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogprednet.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/resnet.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/utils.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/box_utils.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/first_stage.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/get_nets.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/LK/lk.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_detector.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_landmark.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/facer.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/net.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/retinaface.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/retinaface/utils.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py": 1682131573.0914333,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/box_utils.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/transforms.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_emotion/efficient/model.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_emotion/efficient/utils.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_emotion/emotion_infer.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face_align.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_generation/op/conv2d_gradfix.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_generation/op/fused_act.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_generation/op/upfirdn2d.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_generation/stylegan2.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/ghost_pan.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/nanodet_plus_head.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/one_stage_detector.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/shufflenetv2.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_human_hand_detection/utils.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/align_face.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/common.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_irse.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_resnet.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/bfm.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/de_retouching_module.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/losses.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/networks.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/nv_diffrast.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/opt.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/networks.py": 1682131573.0954337,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/renderer.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/models/unet.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/face_reconstruction/utils.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/transforms.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/vgg.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/hand_2d_keypoints/hand_2d_keypoints.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/hand_static/hand_model.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/hand_static/networks.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Embedding.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/PixToMesh.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Res_backbone.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Surface_head.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/detectors.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/geometry.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/human_segmenter.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/models/networks.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_reconstruction/utils.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/bnext.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/model.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/person_info.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/body.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/model.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/util.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_body_reshaping/slim_utils.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_classification/utils.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py": 1682131573.099434,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/csrnet.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpfnet.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/loss.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/convnext.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/position_encoding.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/transformer_utils.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/unet.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/vgg.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/unet/unet.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_colorization/unet/utils.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/evaluator.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/calibration_layer.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/defrcn.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/fast_rcnn.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/gdl.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/resnet.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/roi_heads.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/coco_register.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/register_data.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/requirements_check.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/voc_register.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/NAFNet_arch.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/arch_util.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_depth.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_layers.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_utils.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/swin_transformer.py": 1682131573.1034343,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/uper_crf_head.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/bts_model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/decoder.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/encoder.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/utils.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_driving_perception/utils.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/gan_wrap.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/fused_act.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/upfirdn2d.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/align_trans.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/matlab_cp2tform.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aad_layer.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aei_flow_net.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/bfm.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/dense_motion.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/facerecon_model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/model_irse.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_face_fusion/network/ops.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/backbone/deeplab_resnet.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_decoder.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_encoder.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_human_parsing/parsing_utils.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/base.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/default.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/model.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/base.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/resnet.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/adversarial.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/feature_matching.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ffc.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/inception.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/perceptual.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/modules/pix2pixhd.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_inpainting/refinement.py": 1682131573.1074345,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/swin_transformer.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/datasets/transforms.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/dino_decoder.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/position_encoding.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/utils.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_swin.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_instance_segmentation/postprocess_utils.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/config/default.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_matching/utils/misc.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/cas_mvsnet.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/depth_filter.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/general_eval_dataset.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/module.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/utils.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/r50_panseg_model.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/align_faces.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/fqa.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/model_resnet.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/gpen.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/helpers.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/losses.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/model_irse.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/detection.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/net.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/utils.py": 1682131573.111435,
-        "TEMPLATE_PATH/models/cv/image_probing_model/backbone.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_probing_model/model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_probing_model/utils.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/degradation_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/maniqa.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/swin.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/backbones/resnet.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/heads/simple_head.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_reid_person/transreid_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_restoration/demoire_models/nets.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/utils.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/segformer.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/BlockModules.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_backnone.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/unet.py": 1682131573.1154351,
-        "TEMPLATE_PATH/models/cv/image_skychange/skychange.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/data/transforms.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/model.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/autoencoder.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/clip.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/diffusion.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/losses.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/data/transforms.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/model_translation.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/autoencoder.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/clip.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/apps.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/degradation.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/diffusion.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/losses.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/metrics.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_color.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_mask.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/svd.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/utils.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/fourier.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/panostretch.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/post_proc.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/modality/layout.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/panovit.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/utils.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/layers.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/models.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/modules.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/sub_layers.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/motion_generation/model.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/cfg_sampler.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/gaussian_diffusion.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/mdm.py": 1682131573.1194355,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/respace.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/rotation2xyz.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/motion_generation/modules/smpl.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/get_model.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/head.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/save_op.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/shot_encoder.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/trn.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/read_write_model.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/nerf.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/segmenter.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/utils.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/dino.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/checkpoint.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py": 1682131573.1234357,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/result_vis.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_detection/model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/dbnet.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_detection/modules/seg_detector_loss.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_detection/utils.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnext.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnextvit.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/crnn.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/timm_tinyc.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/vitstr.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/equi.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/layers.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/mobilenet.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/resnet.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/unifuse.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/util.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/common.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_detection.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_embedding.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/product_segmentation/net.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/backbone.py": 1682131573.1274362,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/criterion.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/matcher.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/misc.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/mttr.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/postprocessing.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/segmentation.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/swin_transformer.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/robust_image_classification/easyrobust_model.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/backbone/Res2Net_v1b.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/modules.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/senet.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/u2net.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/salient_detection/models/utils.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/common.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/head_fpn.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/models.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/neck_fpn.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_base.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/shop_segmentation/utils.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_module.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_unet_in.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/gconv.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/inpainting_unet.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/box_utils.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/net.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/network.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/predict_single.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/prior_box.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/utils.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/unet_deploy.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/utils.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/skin_retouching/weights_init.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/data/data_augment.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/base_exp.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/build.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/default/streamyolo.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/exp/yolox_base.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/darknet.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/dfp_pafpn.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/network_blocks.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/streamyolo.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/models/tal_head.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/utils/boxes.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/stream_yolo/utils/format.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/super_resolution/arch_util.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/super_resolution/ecb.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/super_resolution/rrdbnet_arch.py": 1682131573.1314363,
-        "TEMPLATE_PATH/models/cv/table_recognition/lineless_table_process.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_detector.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_processor.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/clip.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_base.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_blocks.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_net.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_vit.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/model.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/text_driven_segmentation/simple_tokenizer.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/basic_blocks.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/global_utils.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/master_net.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/model_zoo.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/plain_net_utils.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_blocks.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_idwexkx.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_k1kxk1.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_kxkx.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_evaluater.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_inference.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/base_ops.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ops.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/utils.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/weight_init.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py": 1682131573.1354368,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/detectors/detector.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/bounding_box.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/boxlist_ops.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/image_list.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/boxes.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/model_utils.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/scheduler.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/detector.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/tinynas_detection/utils.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/deinterlace_arch.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/archs.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/deep_fourier_upsampling.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/enh.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/fre.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_deinterlace/models/utils.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/configs/default_config.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera_utils.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose_utils.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_checkpoint.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_utils.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_wrapper.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sfm_model_mf.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sup_model_mf.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/layers.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/extractor.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/update.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/augmentations.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/config.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/depth.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/horovod.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image_gt.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/load.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/misc.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/types.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_arch.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py": 1682131573.139437,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/corr.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/extractor.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/raft.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/update.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/UNet.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/flow_reversal.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/transformer_layers.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/scene_change_detection.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/utils.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_human_matting/model.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/decoder.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/deep_guided_filter.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/effv2.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/lraspp.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_human_matting/models/matting.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/utils.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/common.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/decode.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/model.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/yolo.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/basetrack.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/matching.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/multitracker.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/image.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/kalman_filter.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/utils.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/visualization.py": 1682131573.1434374,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/aggregate.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/cbam.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/eval_network.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_core.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_memory_bank.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/mod_resnet.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/modules.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_object_segmentation/network.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_head.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/mask.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/track_heads.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/neck/fpn.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/visualizer.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/config/ostrack.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn_blocks.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/head.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/patch_embed.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/ostrack.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/utils.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/procontext.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/utils.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/vit_ce.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/ostrack.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/procontext.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_single_object_tracking/utils/utils.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/DUT_raft.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/MotionPro.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/corr.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/extractor.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/raft.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/update.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/Smoother.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/config.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_module.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_so.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/IterativeSmooth.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/MedianFilter.py": 1682131573.1474376,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/ProjectionUtils.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/RAFTUtils.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/WarpUtils.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/image_utils.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_stabilization/utils/math_utils.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_summarization/base_model.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_auto.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_nonlin.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_summarization/pgl_sum.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/basicvsr_net.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/common.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_net.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vidt/backbone.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vidt/deformable_transformer.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vidt/fpn_fusion.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vidt/head.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vidt/model.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/virual_tryon/sdafnet.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/backbone.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/head.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/petl.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_helpers.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_vision_transformer.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_weight_init.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/vision_efficient_tuning.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_middleware/backbone.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_middleware/head.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_middleware/model.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vision_middleware/vim.py": 1682131573.151438,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/backbone.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/basic_utils.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/model.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/model_se.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/cv/vop_retrieval/tokenization_clip.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/clip/bert_tokenizer.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/clip/configuration_bert.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/clip/model.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/clip/modeling_bert.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/diffusion.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/model.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/structbert.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/tokenizer.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_generator.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_1024.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_256.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/dpm_solver_pytorch.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_base.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/gemm/tokenizer.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/gaussian_diffusion.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/respace.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/script.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/unet.py": 1682131573.1554382,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/text_classification.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/text_ranking.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mgeo/token_classification.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/dataloaders/rawvideo_util.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/dynamic_inverted_softmax.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/modeling.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_clip.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_cross.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/tokenization_clip.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mmr/models/until_module.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mplug/clip/clip.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mplug/configuration_mplug.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mplug/modeling_mplug.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mplug/mvit.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mplug/predictor.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/clip.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/decoder.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/prior.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/tokenizer.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/upsampler.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/xglm.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_mmspeech.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_ofa.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/incremental_decoding_utils.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/multihead_attention.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/ngram_repeat_block.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/search.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/sequence_generator.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/token_generation_constraints.py": 1682131573.1594386,
-        "TEMPLATE_PATH/models/multi_modal/ofa/generate/utils.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_mmspeech.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_ofa.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/resnet.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa_fast.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/utils/constant.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/utils/utils.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa/vit.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/rleg/model.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/soonet/blocks.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/soonet/clip.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/soonet/model.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/soonet/swin_transformer.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/soonet/tokenizer.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/soonet/utils.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/team/team_model.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/team/utils.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/autoencoder.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/diffusion.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/video_synthesis/unet_sd.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/conv_fpn_trans.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/convnext.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/model.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/modeling_layout_roberta.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/processing.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/tokenization.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/multi_modal/vldoc/transformer_local.py": 1682131573.1634388,
-        "TEMPLATE_PATH/models/nlp/T5/backbone.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/T5/configuration.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/backbone.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/configuration.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/fill_mask.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/text_classification.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/text_ranking.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/token_classification.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bert/word_alignment.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/bloom/backbone.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/canmt/canmt_model.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/canmt/sequence_generator.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/codegeex/inference.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/codegeex/tokenizer.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/csanmt/translation.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/configuration.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization_fast.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/dgds/backbone.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py": 1682131573.1674392,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/fid_plug/backbone.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/fid_plug/configuration.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/glm_130b/generation/strategies.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/glm_130b/initialize.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/functional.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/layers.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt2/backbone.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt3/backbone.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt3/configuration.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt3/distributed_gpt3.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt3/tokenizer.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/backbone.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/checkpointing.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/configuration.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/distributed_gpt_moe.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/experts.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/layer.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/mappings.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/sharded_moe.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/utils.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_moe/tokenizer.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/gpt_neo/backbone.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/crf_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py": 1682131573.1714394,
-        "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/llama/backbone.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/llama/configuration.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/llama/convert_llama_weights_to_hf.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/llama/text_generation.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/llama/tokenization.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/llama/tokenization_fast.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/lstm/backbone.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/lstm/token_classification.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/configuration.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/arguments.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/blocklm_utils.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/configure_data.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/corpora.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/datasets.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/extraction.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/file_utils.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/lazy_loader.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/samplers.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/sp_tokenizer.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization_gpt2.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/data_utils/wordpiece.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/generation_utils.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/model/distributed.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/model/downstream.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_bert.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_glm.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/model/prompt.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/model/transformer.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/process_grid.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/run_test.py": 1682131573.1754398,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/data_utils.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/eval_utils.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/dataset.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/detokenizer.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/finetune.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/dataset.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/evaluate.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/finetune.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/dataset.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/evaluate.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/finetune.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/pvp.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/test/test_block.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/test/test_rel_shift.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/train_utils.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/mglm/utils.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/palm_v2/configuration.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/peer/backbone.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/peer/configuration.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/peer/sas_utils.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/peer/text_classification.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug/AnnealingLR.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug/backbone.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug/configuration.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug/distributed_plug.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug/generator.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug_mental/adv_utils.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug_mental/configuration.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/ponet/backbone.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/ponet/configuration.py": 1682131573.17944,
-        "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/ponet/tokenization.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/configuration.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/model/gen_unified_transformer.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/model/generator.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/model/intent_unified_transformer.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/model/model_base.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/model/tokenization_space.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/model/unified_transformer.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/modules/embedder.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/modules/feedforward.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/modules/functions.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/modules/multihead_attention.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space/modules/transformer_block.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/backbone.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/configuration.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/adv_utils.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/backbone.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/configuration.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/text_classification.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/structbert/token_classification.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/task_model.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/text_classification.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/text_generation.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/task_models/token_classification.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/unite/configuration_unite.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/unite/modeling_unite.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/use/transformer.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/veco/backbone.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/veco/configuration.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/veco/fill_mask.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/veco/text_classification.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/veco/token_classification.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/nlp/xlm_roberta/configuration.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/science/unifold/config.py": 1682131573.1834404,
-        "TEMPLATE_PATH/models/science/unifold/data/data_ops.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/data/msa_pairing.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/data/process.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/data/process_multimer.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/data/protein.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/data/residue_constants.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/data/utils.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/dataset.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/model.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/alphafold.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/attentions.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/auxillary_heads.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/common.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/confidence.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/embedders.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/evoformer.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/featurization.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/frame.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/structure_module.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/template.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/modules/triangle_multiplication.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/mmcif.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/msa_identifiers.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/parsers.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/pipeline.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/templates.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhblits.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhsearch.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmbuild.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmsearch.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/jackhmmer.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/kalign.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/tools/utils.py": 1682131573.1874406,
-        "TEMPLATE_PATH/models/science/unifold/msa/utils.py": 1682131573.1874406,
-        "TEMPLATE_PATH/msdatasets/audio/asr_dataset.py": 1682131573.1874406,
-        "TEMPLATE_PATH/msdatasets/auth/auth_config.py": 1682131573.1874406,
-        "TEMPLATE_PATH/msdatasets/context/dataset_context_config.py": 1682131573.1874406,
-        "TEMPLATE_PATH/msdatasets/data_files/data_files_manager.py": 1682131573.1874406,
-        "TEMPLATE_PATH/msdatasets/data_loader/data_loader.py": 1682131573.1874406,
-        "TEMPLATE_PATH/msdatasets/data_loader/data_loader_manager.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/builder.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/easycv_base.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py": 1682131573.191441,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/dataset_cls/dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/download/dataset_builder.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/download/download_config.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/download/download_manager.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/meta/data_meta_config.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/meta/data_meta_manager.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/ms_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/task_datasets/gopro_image_deblurring_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/task_datasets/reds_image_deblurring_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/task_datasets/sidd_image_denoising.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/task_datasets/torch_base_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/task_datasets/video_summarization_dataset.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/utils/dataset_utils.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/utils/delete_utils.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/utils/oss_utils.py": 1682131573.1954415,
-        "TEMPLATE_PATH/msdatasets/utils/upload_utils.py": 1682131573.1954415,
-        "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py": 1682131573.1954415,
-        "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py": 1682131573.1954415,
-        "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/base.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/builder.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/base.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/segmentation_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_processing_base_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/hand_2d_keypoints_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py": 1682131573.1994417,
-        "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/nerf_recon_acc_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_convnext_transformer.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_dla34.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet18_half.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_vlpt.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/convnext.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/vitstr.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ops.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet18_v1.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet_utils.py": 1682131573.203442,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/table_process.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/ocr_utils/utils.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/tbs_detection_utils/utils.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py": 1682131573.2074423,
-        "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_modeling_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py": 1682131573.2114427,
-        "TEMPLATE_PATH/pipelines/util.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/asr.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/audio.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/base.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/builder.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/common.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/action_detection_mapper.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/cv2_transforms.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py": 1682131573.2114427,
-        "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/cv/timer.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/cv/util.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/cv/video_stabilization.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/cv/video_super_resolution.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/image.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/kws.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/movie_scene_segmentation/transforms.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/multi_modal.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/args.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/batch.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/data_loader.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/dst_processors.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/fields/gen_field.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/fields/intent_field.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/lazy_dataset.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/preprocess.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/sampler.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/tensorlistdataset.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space/tokenizer.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/database.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/schema_link.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/struct.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/common_utils.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/parse.py": 1682131573.215443,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/process_dataset.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/text_clean.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/transformers_tokenizer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/utils.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/asr.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/base.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/image_captioning.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/image_classification.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/ocr_recognition.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/sudoku.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/summarization.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/text2sql.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/text_classification.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/text_to_image_synthesis.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/audio_helper.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/bridge_content_encoder.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/collate.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/constant.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/get_tables.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/random_help.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/text2phone.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/transforms.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/utils/vision_helper.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_entailment.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_grounding.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/ofa/visual_question_answering.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/science/uni_fold.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/tts.py": 1682131573.2194433,
-        "TEMPLATE_PATH/preprocessors/video.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/ans_trainer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/asr_trainer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/batch_utils.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/det_utils.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/file_utils.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/model_utils.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/kws_utils/runtime_utils.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/separation_trainer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/audio/tts_trainer.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/base.py": 1682131573.2194433,
-        "TEMPLATE_PATH/trainers/builder.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/default_config.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/easycv/trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/easycv/utils/hooks.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/easycv/utils/metric.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/easycv/utils/register_util.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/builder.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/compression/sparsity_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/compression/utils.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/ddp_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/deepspeed_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/logger/base.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/megatron_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/base.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/hooks/priority.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/lrscheduler/builder.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/lrscheduler/warmup/base.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer_utils.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer_utils.py": 1682131573.2234435,
-        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer_utils.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/plug_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/space/eval.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/space/metrics/metrics_tracker.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/space/trainer/gen_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/space/trainer/intent_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/nlp_trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/optimizer/builder.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/optimizer/child_tuning_adamw_optimizer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/parallel/builder.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/parallel/utils.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/trainer.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/training_args.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/utils/inference.py": 1682131573.227444,
-        "TEMPLATE_PATH/trainers/utils/log_buffer.py": 1682131573.227444
+        "TEMPLATE_PATH/exporters/audio/ans_dfsmn_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/base.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/builder.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/nlp/sbert_for_sequence_classification_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/nlp/sbert_for_zero_shot_classification_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/tf_model_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/exporters/torch_model_exporter.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/accuracy_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/action_detection_evaluator.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/audio_noise_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/base.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/bleu_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/builder.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/ciderD/ciderD.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/ciderD/ciderD_scorer.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_color_enhance_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_colorization_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_denoise_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_inpainting_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/inbatch_recall_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/loss_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/map_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/movie_scene_segmentation_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/ned_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/ocr_recognition_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/ppl_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/sequence_classification_metric.py": 1684724969.4295056,
+        "TEMPLATE_PATH/metrics/text_generation_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/text_ranking_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/token_classification_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/translation_evaluation_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_stabilization_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_summarization_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/matlab_functions.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/metric_util.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/niqe.py": 1684724969.4335055,
+        "TEMPLATE_PATH/metrics/video_super_resolution_metric/video_super_resolution_metric.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/layers/activations.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/layers/affine_transform.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/layers/deep_fsmn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/layers/layer_base.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/layers/uni_deep_fsmn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/network/loss.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/network/modulation_loss.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/aec/network/se_net.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/complex_nn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/conv_stft.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/denoise_net.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/frcrn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/layers/activations.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/layers/affine_transform.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/layers/layer_base.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/layers/uni_deep_fsmn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/se_module_complex.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/ans/unet.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v2.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/farfield/fsmn_sele_v3.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/farfield/model.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/farfield/model_def.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/cmvn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/fsmn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/kws/nearfield/model.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/separation/layer_norm.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/separation/mossformer.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/separation/mossformer_block.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/separation/mossformer_conv_module.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/DTDNN.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/DTDNN_layers.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/ERes2Net.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/fusion.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/pooling_layers.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/rdino.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/sv/speaker_change_locator.py": 1684724969.4335055,
+        "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/audio/tts/voice.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/base/base_head.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/base/base_model.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/base/base_torch_head.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/base/base_torch_model.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/builder.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_detection/action_detection_onnx.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_detection/modules/action_detection_pytorch.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_detection/modules/resnet.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_recognition/models.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_recognition/s3dg.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_recognition/tada_convnext.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/action_recognition/temporal_patch_shift_transformer.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/animal_recognition/resnet.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/animal_recognition/splat.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_basic_modules.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_2d_keypoints/w48.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/canonical_pose_modules.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/backbone.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/block.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/directed_graph.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/skeleton.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/LK/lk.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/config.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_detector.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/face_landmark.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/facelib/facer.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/loss.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/model_tf.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/align_trans.py": 1684724969.4375057,
+        "TEMPLATE_PATH/models/cv/cartoon/mtcnn_pytorch/src/matlab_cp2tform.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/cartoon/network.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/cartoon/utils.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/c3d.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet2p1d.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/cmdssl_video_embedding/resnet3d.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/annotator.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/api.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/base_model.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/blocks.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/dpt_depth.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/midas_net_custom.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/transforms.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/midas/vit.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/midas/utils.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/mbv2_mlsd_large.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/mlsd/utils.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/body.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/hand.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/model.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/annotator/openpose/util.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/crowd_counting/hrnet_aspp_relu.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogface.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/mogprednet.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/resnet.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mogface/models/utils.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/box_utils.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/first_stage.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/get_nets.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/LK/lk.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_detector.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/face_landmark.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/peppa_pig_face/facer.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/net.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/models/retinaface.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/retinaface/utils.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py": 1684724969.4415057,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/bbox/transforms.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/core/post_processing/bbox_nms.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/base.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/box_utils.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/mb_tiny.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/data_preprocessing.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/fd_config.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/mb_tiny_fd.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/predictor.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/ssd/ssd.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/vision/transforms.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_emotion/efficient/model.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_emotion/efficient/utils.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_emotion/emotion_infer.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_emotion/face_alignment/face_align.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_generation/op/conv2d_gradfix.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_generation/op/fused_act.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_generation/op/upfirdn2d.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_generation/stylegan2.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/ghost_pan.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/nanodet_plus_head.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/one_stage_detector.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/shufflenetv2.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_human_hand_detection/utils.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_recognition/align_face.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/arcface_backbone.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/common.py": 1684724969.4455059,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/facemask_backbone.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_irse.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/backbone/model_resnet.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/bfm.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/de_retouching_module.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/large_base_lmks_infer.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_base_lmks_net.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facelandmark/nets/large_eyeball_net.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/losses.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/networks.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/nv_diffrast.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/opt.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/networks.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_model.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/pix2pix/pix2pix_options.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/renderer.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/models/unet.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/face_reconstruction/utils.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/transforms.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/vgg.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/hand_static/hand_model.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/hand_static/networks.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Embedding.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/PixToMesh.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Res_backbone.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/Surface_head.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/detectors.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/geometry.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/human_segmenter.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/models/networks.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/human_reconstruction/utils.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_binary_quant_classification/bnext.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/model.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/person_info.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/body.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/model.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/pose_estimator/util.py": 1684724969.4495058,
+        "TEMPLATE_PATH/models/cv/image_body_reshaping/slim_utils.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_classification/utils.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/csrnet.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpfnet.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/loss.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/convnext.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/position_encoding.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/transformer_utils.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/unet.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/utils/vgg.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/unet/unet.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_colorization/unet/utils.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/coco_evaluation.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/evaluator.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/evaluation/pascal_voc_evaluation.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/calibration_layer.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/defrcn.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/fast_rcnn.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/gdl.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/resnet.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/models/roi_heads.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/coco_register.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/configuration_mapper.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/model_surgery_op.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/register_data.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/requirements_check.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/utils/voc_register.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/NAFNet_arch.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet/arch_util.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_depth.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_layers.py": 1684724969.453506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/newcrf_utils.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/swin_transformer.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/networks/uper_crf_head.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/bts_model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/decoder.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/encoder.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/networks/utils.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_driving_perception/utils.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/gan_wrap.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/conv2d_gradfix.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/fused_act.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facegan/op/upfirdn2d.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/align_trans.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/facelib/matlab_cp2tform.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aad_layer.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/aei_flow_net.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/bfm.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/dense_motion.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/facerecon_model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/model_irse.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_face_fusion/network/ops.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/backbone/deeplab_resnet.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_decoder.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp/m2fp_encoder.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_human_parsing/parsing_utils.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/base.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/default.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/model.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/base.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ade20k/resnet.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/adversarial.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/feature_matching.py": 1684724969.457506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/ffc.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/inception.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/perceptual.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/modules/pix2pixhd.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_inpainting/refinement.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/resnet.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/backbones/swin_transformer.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/datasets/transforms.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst_model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/dino_decoder.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/position_encoding.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino/utils.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_swin.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_instance_segmentation/postprocess_utils.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/config/default.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/backbone/resnet_fpn.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/fine_preprocess.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/linear_attention.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/quadtree_attention.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/loftr_module/transformer.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/coarse_matching.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/fine_matching.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/loftr_quadtree/utils/position_encoding.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_matching/utils/misc.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/cas_mvsnet.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/colmap2mvsnet.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/depth_filter.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/general_eval_dataset.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/module.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/utils.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/align_faces.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/fqa.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/eqface/model_resnet.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/gpen.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/helpers.py": 1684724969.461506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/losses.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/losses/model_irse.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/detection.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/net.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/models/retinaface.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_portrait_enhancement/retinaface/utils.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_probing_model/backbone.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_probing_model/model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_probing_model/utils.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/degradation_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/maniqa.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_man/swin.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/backbones/resnet.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/censeo_ivqa_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/heads/simple_head.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_reid_person/transreid_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_restoration/demoire_models/nets.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_seg/utils.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/base_panoptic_fusion_head.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/adapter_modules.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/base_decode_head.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/base_segmentor.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/builder.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/seg_func.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/BlockModules.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_backnone.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/hrnet_super_and_ocr.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/ptsemseg/unet.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/skychange.py": 1684724969.465506,
+        "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/data/transforms.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/model.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/autoencoder.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/models/clip.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/diffusion.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_generation/ops/losses.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/data/transforms.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/model_translation.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/autoencoder.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/models/clip.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/apps.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/degradation.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/diffusion.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/losses.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/metrics.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_color.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/random_mask.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/svd.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/image_to_image_translation/ops/utils.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/resnet_DA.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/backbone/vit_horizon_pry_image.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/fourier.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/panostretch.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/misc/post_proc.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/modality/layout.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/panovit.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/networks/utils.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/layers.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/models.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/modules.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/language_guided_video_summarization/transformer/sub_layers.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/model.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/cfg_sampler.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/gaussian_diffusion.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/mdm.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/respace.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/rotation2xyz.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/motion_generation/modules/smpl.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/get_model.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py": 1684724969.469506,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/head.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/save_op.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/shot_encoder.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/movie_scene_segmentation/utils/trn.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/dataloader/read_write_model.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/nerf.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/segmenter.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/nerf_recon_acc/network/utils.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/checkpoint.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/utils/convModule_norm.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/util.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/depth_net.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py": 1684724969.4735062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/object_detection_3d/depe/result_vis.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_detection/model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/dbnet.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_detection/modules/seg_detector_loss.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_detection/utils.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnext.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/convnextvit.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/crnn.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/timm_tinyc.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/modules/vitstr.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/equi.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/layers.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/mobilenet.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/resnet.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/unifuse.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/networks/util.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/common.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/pointnet2_utils.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/sf_rcp.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_detection.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_embedding.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/product_segmentation/net.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/backbone.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/criterion.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/matcher.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/misc.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/mttr.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/multimodal_transformer.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/position_encoding_2d.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/postprocessing.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/segmentation.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/utils/swin_transformer.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/robust_image_classification/easyrobust_model.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/backbone/Res2Net_v1b.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/modules.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/senet.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/u2net.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/salient_detection/models/utils.py": 1684724969.4775062,
+        "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/common.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/head_fpn.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/models.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/neck_fpn.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_base.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/shop_segmentation/utils.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_module.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/detection_model/detection_unet_in.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/gconv.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/inpainting_model/inpainting_unet.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/box_utils.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/net.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/network.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/predict_single.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/prior_box.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/retinaface/utils.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/unet_deploy.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/utils.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/skin_retouching/weights_init.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/data/data_augment.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/base_exp.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/build.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/default/streamyolo.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/exp/yolox_base.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/darknet.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/dfp_pafpn.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/network_blocks.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/streamyolo.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/models/tal_head.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/utils/boxes.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/stream_yolo/utils/format.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/super_resolution/arch_util.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/super_resolution/ecb.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/super_resolution/rrdbnet_arch.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/table_recognition/lineless_table_process.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_detector.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/table_recognition/modules/lore_processor.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/clip.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_base.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_blocks.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_net.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_vit.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/model.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/text_driven_segmentation/simple_tokenizer.py": 1684724969.4815063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/basic_blocks.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/global_utils.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/master_net.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/model_zoo.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/plain_net_utils.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_blocks.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_idwexkx.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_k1kxk1.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_classfication/super_res_kxkx.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_evaluater.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/apis/detector_inference.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/box_level_augs.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/color_augs.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/gaussian_maps.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/box_level_augs/geometric_augs.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/augmentations/scale_aware_aug.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/darknet.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_csp.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/backbones/tinynas_res.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/base_ops.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/neck_ops.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ops.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/ota_assigner.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/repvgg_block.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/utils.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/core/weight_init.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/gfocal_v2_tiny.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/heads/zero_head.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/distill_loss.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/losses/gfocal_loss.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_config.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/base_models/necks/giraffe_fpn_btn.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/detectors/detector.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/bounding_box.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/boxlist_ops.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/structures/image_list.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/boxes.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/model_utils.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/damo/utils/scheduler.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/detector.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/tinynas_detection/utils.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/deinterlace_arch.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/archs.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/deep_fourier_upsampling.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/enh.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/fre.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_deinterlace/models/utils.py": 1684724969.4855063,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/configs/default_config.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/camera_utils.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/geometry/pose_utils.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_checkpoint.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_utils.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/model_wrapper.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sfm_model_mf.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/models/sup_model_mf.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/depth_pose/depth_pose_net.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/depth_decoder.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/layers.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/pose_decoder.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/layers/resnet/resnet_encoder.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/extractor.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/networks/optim/update.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/augmentations.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/config.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/depth.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/horovod.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/image_gt.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/load.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/misc.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_depth_estimation/utils/types.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_arch.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/corr.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/extractor.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/raft.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/flow_model/update.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/IFNet_swin.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/UNet.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/flow_reversal.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/refinenet_arch.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/interp_model/transformer_layers.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/scene_change_detection.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_frame_interpolation/utils/utils.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_human_matting/model.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/decoder.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/deep_guided_filter.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/effv2.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/lraspp.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_human_matting/models/matting.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py": 1684724969.4895062,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/utils.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/common.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/decode.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/model.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/models/yolo.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/basetrack.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/matching.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/tracker/multitracker.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/image.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/kalman_filter.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/utils.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_multi_object_tracking/utils/visualization.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/aggregate.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/cbam.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/eval_network.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_core.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/inference_memory_bank.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/mod_resnet.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/modules.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_object_segmentation/network.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_checkpoint.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/backbone/swin_transformer.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_updator.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/mask.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/track_heads.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/neck/fpn.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/visualizer.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/config/ostrack.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/attn_blocks.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/head.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/layers/patch_embed.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/base_backbone.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/ostrack.py": 1684724969.4935064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/utils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/ostrack/vit_ce.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/procontext.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/utils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/models/procontext/vit_ce.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/ostrack.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/tracker/procontext.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_single_object_tracking/utils/utils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/DUT_raft.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/MotionPro.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/corr.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/extractor.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/raft.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/RAFT/update.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/Smoother.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/config.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_module.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUT/rf_det_so.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/IterativeSmooth.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/MedianFilter.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/ProjectionUtils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/RAFTUtils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/WarpUtils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/image_utils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_stabilization/utils/math_utils.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/exp/longshortnet_base.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_long.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/dfp_pafpn_short.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/models/longshort_backbone_neck.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_summarization/base_model.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_auto.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_summarization/kts/cpd_nonlin.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_summarization/pgl_sum.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/basicvsr_net.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/common.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_net.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/vidt/backbone.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/vidt/deformable_transformer.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/vidt/fpn_fusion.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/vidt/head.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/vidt/model.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/virual_tryon/sdafnet.py": 1684724969.4975064,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/backbone.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/head.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/petl.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_helpers.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_vision_transformer.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/timm_weight_init.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_efficient_tuning/vision_efficient_tuning.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_middleware/backbone.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_middleware/head.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_middleware/model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vision_middleware/vim.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/backbone.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/basic_utils.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/model_se.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/cv/vop_retrieval/tokenization_clip.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/clip/bert_tokenizer.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/clip/configuration_bert.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/clip/model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/clip/modeling_bert.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/clip_interrogator/model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/diffusion.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/structbert.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/tokenizer.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_generator.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_1024.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/diffusion/unet_upsampler_256.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/dpm_solver_pytorch.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_base.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/gemm/tokenizer.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/gaussian_diffusion.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/respace.py": 1684724969.5015066,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/script.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/guided_diffusion/unet.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/text_classification.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/text_ranking.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mgeo/token_classification.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/dataloaders/rawvideo_util.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/dynamic_inverted_softmax.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/modeling.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_clip.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/module_cross.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/tokenization_clip.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mmr/models/until_module.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug/clip/clip.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug/configuration_mplug.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug/modeling_mplug.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug/mvit.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug/predictor.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug_owl/configuration_mplug_owl.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/mplug_owl/modeling_mplug_owl.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/clip.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/decoder.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/prior.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/tokenizer.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/upsampler.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/xglm.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_mmspeech.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/ofa/configuration_ofa.py": 1684724969.5055065,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/incremental_decoding_utils.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/multihead_attention.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/ngram_repeat_block.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/search.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/sequence_generator.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/token_generation_constraints.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/generate/utils.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_mmspeech.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/modeling_ofa.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/resnet.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/tokenization_ofa_fast.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/utils/constant.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/utils/utils.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa/vit.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/rleg/model.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/soonet/blocks.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/soonet/clip.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/soonet/model.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/soonet/swin_transformer.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/soonet/tokenizer.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/soonet/utils.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/team/team_model.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/team/utils.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/autoencoder.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/diffusion.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/video_synthesis/unet_sd.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/conv_fpn_trans.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/convnext.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/model.py": 1684724969.5095067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/modeling_layout_roberta.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/processing.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/tokenization.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/multi_modal/vldoc/transformer_local.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/T5/backbone.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/T5/configuration.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/backbone.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/configuration.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/fill_mask.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/text_classification.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/text_ranking.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/token_classification.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bert/word_alignment.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/bloom/backbone.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/canmt/canmt_model.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/canmt/sequence_generator.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/codegeex/inference.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/codegeex/tokenizer.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/csanmt/translation.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/configuration.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization.py": 1684724969.5135067,
+        "TEMPLATE_PATH/models/nlp/deberta_v2/tokenization_fast.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/dgds/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/fid_plug/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/fid_plug/configuration.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/glm_130b/generation/strategies.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/glm_130b/initialize.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/functional.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/glm_130b/quantization/layers.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt2/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt3/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt3/configuration.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt3/distributed_gpt3.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt3/tokenizer.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/checkpointing.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/configuration.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/distributed_gpt_moe.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/experts.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/layer.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/mappings.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/sharded_moe.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/moe/utils.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_moe/tokenizer.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/gpt_neo/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/crf_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/text_generation_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/text_ranking_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py": 1684724969.5175066,
+        "TEMPLATE_PATH/models/nlp/llama/backbone.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/llama/configuration.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/llama/convert_llama_weights_to_hf.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/llama/text_generation.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/llama/tokenization.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/llama/tokenization_fast.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/lstm/backbone.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/lstm/token_classification.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/configuration.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/arguments.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/blocklm_utils.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/configure_data.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/corpora.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/datasets.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/extraction.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/file_utils.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/lazy_loader.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/samplers.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/sp_tokenizer.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/tokenization_gpt2.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/data_utils/wordpiece.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/generation_utils.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/model/distributed.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/model/downstream.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_bert.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/model/modeling_glm.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/model/prompt.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/model/transformer.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/process_grid.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/run_test.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/data_utils.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/eval_utils.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/dataset.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/detokenizer.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/language_model/finetune.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/dataset.py": 1684724969.5215068,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/evaluate.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/seq2seq/finetune.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/dataset.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/evaluate.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/finetune.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/tasks/superglue/pvp.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/test/test_block.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/test/test_rel_shift.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/train_utils.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/mglm/utils.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/palm_v2/configuration.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/palm_v2/dureader_eval.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/peer/backbone.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/peer/configuration.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/peer/sas_utils.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/peer/text_classification.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug/AnnealingLR.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug/backbone.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug/configuration.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug/distributed_plug.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug/generator.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug_mental/adv_utils.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug_mental/configuration.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/ponet/backbone.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/ponet/configuration.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/ponet/tokenization.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/configuration.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/model/gen_unified_transformer.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/model/generator.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/model/intent_unified_transformer.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/model/model_base.py": 1684724969.5255067,
+        "TEMPLATE_PATH/models/nlp/space/model/tokenization_space.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space/model/unified_transformer.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space/modules/embedder.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space/modules/feedforward.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space/modules/functions.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space/modules/multihead_attention.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space/modules/transformer_block.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/backbone.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/configuration.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/adv_utils.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/backbone.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/configuration.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/text_classification.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/structbert/token_classification.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/task_model.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/text_classification.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/text_generation.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/text_ranking.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/task_models/token_classification.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/unite/configuration.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/unite/translation_evaluation.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/use/transformer.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/veco/backbone.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/veco/configuration.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/veco/fill_mask.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/veco/text_classification.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/veco/token_classification.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/nlp/xlm_roberta/configuration.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/config.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/data_ops.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/msa_pairing.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/process.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/process_multimer.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/protein.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/residue_constants.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/data/utils.py": 1684724969.529507,
+        "TEMPLATE_PATH/models/science/unifold/dataset.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/model.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/alphafold.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/attentions.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/auxillary_heads.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/common.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/confidence.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/embedders.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/evoformer.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/featurization.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/frame.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/structure_module.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/template.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/modules/triangle_multiplication.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/mmcif.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/msa_identifiers.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/parsers.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/pipeline.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/templates.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhblits.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hhsearch.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmbuild.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/hmmsearch.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/jackhmmer.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/kalign.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/tools/utils.py": 1684724969.5335069,
+        "TEMPLATE_PATH/models/science/unifold/msa/utils.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/audio/asr_dataset.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/auth/auth_config.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/context/dataset_context_config.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/data_files/data_files_manager.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/data_loader/data_loader.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/data_loader/data_loader_manager.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/asr_dataset.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_farfield_dataset.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_dataset.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/audio/kws_nearfield_processor.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/builder.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/build.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/collate_batch.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/coco.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/datasets/mosaic_wrapper.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/evaluation/coco/coco_eval.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py": 1684724969.5335069,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/easycv_base.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/augment_data.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/data_process.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_border_map.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_icdar_data.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/make_seg_detection_data.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/normalize_image.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_detection/processes/random_crop_data.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/transformers.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/data_utils.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/transforms.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/torch_custom_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/data_utils.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_summarization_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/dataset_cls/dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/download/dataset_builder.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/download/download_config.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/download/download_manager.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/meta/data_meta_config.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/meta/data_meta_manager.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/ms_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/task_datasets/gopro_image_deblurring_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/task_datasets/reds_image_deblurring_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/task_datasets/sidd_image_denoising.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/task_datasets/torch_base_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/task_datasets/video_summarization_dataset.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/utils/dataset_utils.py": 1684724969.537507,
+        "TEMPLATE_PATH/msdatasets/utils/delete_utils.py": 1684724969.541507,
+        "TEMPLATE_PATH/msdatasets/utils/maxcompute_utils.py": 1684724969.541507,
+        "TEMPLATE_PATH/msdatasets/utils/oss_utils.py": 1684724969.541507,
+        "TEMPLATE_PATH/msdatasets/utils/upload_utils.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/speaker_change_locating_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_eres2net_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/speaker_verification_rdino_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/text_to_speech_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/base.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/builder.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py": 1684724969.541507,
+        "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_human_hand_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_processing_base_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/fast_instance_segmentation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/nerf_recon_acc_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py": 1684724969.545507,
+        "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_convnext_transformer.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_dla34.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet18_half.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/model_vlpt.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/convnext.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/timm_tinyc.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ocr_modules/vitstr.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/ops.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet18_v1.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/resnet_utils.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/table_process.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ocr_utils/utils.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/tbs_detection_utils/utils.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/asr_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/diffusers_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/utils.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py": 1684724969.5495071,
+        "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/multimodal_dialogue_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/ocr_recognition_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_grounding_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_generation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/codegeex_code_translation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/conversational_text_to_sql_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_modeling_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt3_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_gpt_moe_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/distributed_plug_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/glm130b_text_generation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/text_error_correction_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/word_segmentation_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/nlp/zero_shot_classification_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/pipeline_template.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py": 1684724969.553507,
+        "TEMPLATE_PATH/pipelines/util.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/asr.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/audio.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/base.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/builder.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/common.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/action_detection_mapper.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/cv2_transforms.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py": 1684724969.553507,
+        "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/cv/timer.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/cv/util.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/cv/video_stabilization.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/cv/video_super_resolution.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/image.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/kws.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/movie_scene_segmentation/transforms.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/multi_modal.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/document_segmentation_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/faq_question_answering_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/feature_extraction_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/sentence_embedding_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/args.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/batch.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/data_loader.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/dst_processors.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/fields/gen_field.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/fields/intent_field.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/lazy_dataset.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/preprocess.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/sampler.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/tensorlistdataset.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space/tokenizer.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/database.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/schema_link.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/fields/struct.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/common_utils.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/parse.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/preprocess_dataset.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/space_T_en/fields/process_dataset.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/text_clean.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/text_error_correction.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/transformers_tokenizer.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/utils.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py": 1684724969.5575073,
+        "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/asr.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/base.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/image_captioning.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/image_classification.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/ocr_recognition.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/sudoku.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/summarization.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/text2sql.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/text_classification.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/text_to_image_synthesis.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/audio_helper.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/bridge_content_encoder.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/collate.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/constant.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/get_tables.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/random_help.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/text2phone.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/transforms.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/utils/vision_helper.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_entailment.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_grounding.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/ofa/visual_question_answering.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/science/uni_fold.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/tts.py": 1684724969.5615072,
+        "TEMPLATE_PATH/preprocessors/video.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/ans_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/asr_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/batch_utils.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/det_utils.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/file_utils.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/model_utils.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/kws_utils/runtime_utils.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/separation_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/audio/tts_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/base.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/builder.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cli_argument_parser.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/image_portrait_enhancement_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py": 1684724969.5615072,
+        "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/default_config.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/builder.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_processor.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/checkpoint/load_checkpoint_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/compression/sparsity_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/compression/utils.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/distributed/ddp_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/distributed/deepspeed_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/distributed/megatron_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/logger/base.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/base.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/optimizer/torch_optimizer_hook.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/hooks/priority.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/lrscheduler/builder.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/lrscheduler/warmup/base.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer_utils.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/mplug/mplug_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer_utils.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer_utils.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/plug_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/space/eval.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/space/metrics/metrics_tracker.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/space/trainer/gen_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/space/trainer/intent_trainer.py": 1684724969.5655074,
+        "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/nlp/translation_evaluation_trainer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/nlp_trainer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/optimizer/builder.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/optimizer/child_tuning_adamw_optimizer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/parallel/builder.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/parallel/utils.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/trainer.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/training_args.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/utils/inference.py": 1684724969.5695074,
+        "TEMPLATE_PATH/trainers/utils/log_buffer.py": 1684724969.5695074
     },
     "index": {
         "('ATTENTION', 'default', 'PETRMultiheadAttention')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "mmdet",
                 "mmcv",
-                "math",
-                "torch",
                 "warnings",
+                "typing",
                 "copy",
-                "typing"
+                "math",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('BACKBONES', 'backbone', 'bloom')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bloom/backbone.py",
             "imports": [
                 "transformers"
@@ -1686,246 +1687,197 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.gpt2.backbone"
         },
         "('BACKBONES', 'default', 'BASEBEiT')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/base/beit.py",
             "imports": [
-                "mmdet",
-                "timm",
+                "functools",
                 "mmcv",
                 "math",
                 "torch",
-                "functools"
+                "timm",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.base.beit"
         },
         "('BACKBONES', 'default', 'BEiTAdapter')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/backbone/beit_adapter.py",
             "imports": [
+                "logging",
                 "mmdet",
                 "math",
-                "timm",
-                "logging",
-                "torch"
+                "torch",
+                "timm"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.beit_adapter"
         },
         "('BACKBONES', 'default', 'BEiTv2')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/backbones/beit_v2.py",
             "imports": [
+                "functools",
                 "mmcv",
-                "math",
-                "warnings",
                 "einops",
-                "os",
+                "typing",
                 "mmcls",
                 "torch",
-                "collections",
-                "functools",
+                "warnings",
+                "math",
                 "itertools",
-                "typing"
+                "os",
+                "collections"
             ],
             "module": "modelscope.models.cv.image_classification.backbones.beit_v2"
         },
         "('BACKBONES', 'default', 'MasterNet')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/master_net.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.master_net"
         },
         "('BACKBONES', 'default', 'MobileNetV1')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/mobilenet.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.mobilenet"
         },
         "('BACKBONES', 'default', 'NextViT')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/backbones/nextvit.py",
             "imports": [
+                "functools",
                 "mmcv",
-                "math",
-                "warnings",
                 "einops",
-                "os",
+                "typing",
                 "mmcls",
                 "torch",
-                "collections",
-                "functools",
+                "warnings",
+                "math",
                 "itertools",
-                "typing"
+                "os",
+                "collections"
             ],
             "module": "modelscope.models.cv.image_classification.backbones.nextvit"
         },
         "('BACKBONES', 'default', 'ResNetV1e')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/backbones/resnet.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.resnet"
         },
         "('BACKBONES', 'default', 'ViT')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/backbones/vit.py",
             "imports": [
+                "functools",
                 "mmdet",
                 "math",
-                "timm",
                 "torch",
-                "functools"
+                "timm"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.backbones.vit"
         },
         "('BACKBONES', 'default', 'VoVNet')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/backbones/vovnet.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
                 "collections",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.backbones.vovnet"
         },
         "('BBOX_ASSIGNERS', 'default', 'HungarianAssigner3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py",
             "imports": [
-                "mmdet",
+                "scipy",
                 "torch",
-                "scipy"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d"
         },
         "('BBOX_ASSIGNERS', 'default', 'MaskHungarianAssignerVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "scipy",
                 "numpy",
-                "scipy"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
         },
         "('BBOX_CODERS', 'default', 'NMSFreeCoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/coders/nms_free_coder.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.coders.nms_free_coder"
         },
         "('CUSTOM_DATASETS', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/bad_image_detecting/bad_image_detecting_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.bad_image_detecting.bad_image_detecting_dataset"
         },
-        "('CUSTOM_DATASETS', 'domain-specific-object-detection', 'DetImagesMixDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.object_detection.detection_dataset"
-        },
-        "('CUSTOM_DATASETS', 'face-2d-keypoints', 'FaceKeypointDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.face_2d_keypoins.face_2d_keypoints_dataset"
-        },
-        "('CUSTOM_DATASETS', 'hand-2d-keypoints', 'HandCocoWholeBodyDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.hand_2d_keypoints.hand_2d_keypoints_dataset"
-        },
-        "('CUSTOM_DATASETS', 'human-wholebody-keypoint', 'WholeBodyCocoTopDownDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.human_wholebody_keypoint.human_wholebody_keypoint_dataset"
-        },
-        "('CUSTOM_DATASETS', 'image-classification', 'ClsDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_classification.classification_dataset"
-        },
         "('CUSTOM_DATASETS', 'image-colorization', 'ddcolor')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_colorization.image_colorization_dataset"
         },
         "('CUSTOM_DATASETS', 'image-deblurring', 'GoproDataset')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/gopro_image_deblurring_dataset.py",
             "imports": [
-                "numpy",
-                "cv2"
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.gopro_image_deblurring_dataset"
         },
         "('CUSTOM_DATASETS', 'image-deblurring', 'RedsDataset')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/reds_image_deblurring_dataset.py",
             "imports": [
-                "numpy",
-                "cv2"
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.reds_image_deblurring_dataset"
         },
         "('CUSTOM_DATASETS', 'image-denoising', 'SiddDataset')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/sidd_image_denoising/sidd_image_denoising_dataset.py",
             "imports": [
-                "numpy",
-                "cv2"
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.sidd_image_denoising_dataset"
         },
         "('CUSTOM_DATASETS', 'image-inpainting', 'FFTInpainting')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py",
             "imports": [
-                "numpy",
                 "albumentations",
+                "cv2",
                 "glob",
+                "numpy",
                 "enum",
-                "os",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.image_inpainting_dataset"
         },
-        "('CUSTOM_DATASETS', 'image-object-detection', 'DetDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.object_detection.detection_dataset"
-        },
-        "('CUSTOM_DATASETS', 'image-object-detection', 'DetImagesMixDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.object_detection.detection_dataset"
-        },
         "('CUSTOM_DATASETS', 'image-portrait-enhancement', 'PairedDataset')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py",
             "imports": [
-                "numpy",
-                "cv2"
+                "cv2",
+                "numpy"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_portrait_enhancement.image_portrait_enhancement_dataset"
         },
         "('CUSTOM_DATASETS', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py",
             "imports": [
                 "torchvision"
@@ -1933,242 +1885,236 @@
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assessment_degradation.image_quality_assessment_degradation_dataset"
         },
         "('CUSTOM_DATASETS', 'image-quality-assessment-mos', 'image-quality-assessment-mos')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assmessment_mos.image_quality_assessment_mos_dataset"
         },
-        "('CUSTOM_DATASETS', 'image-segmentation', 'DetDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.object_detection.detection_dataset"
-        },
-        "('CUSTOM_DATASETS', 'image-segmentation', 'SegDataset')": {
-            "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_semantic_segmentation.segmentation_dataset"
-        },
         "('CUSTOM_DATASETS', 'image-segmentation', 'cascade_mask_rcnn_swin')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/image_instance_segmentation_coco_dataset.py",
             "imports": [
                 "pycocotools",
                 "numpy",
                 "os"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.image_instance_segmentation_coco_dataset"
         },
         "('CUSTOM_DATASETS', 'language-guided-video-summarization', 'clip-it-language-guided-video-summarization')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/language_guided_video_summarization_dataset.py",
             "imports": [
-                "numpy",
                 "json",
                 "h5py",
+                "numpy",
                 "os",
                 "torch"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.language_guided_video_summarization_dataset"
         },
         "('CUSTOM_DATASETS', 'movie-scene-segmentation', 'resnet50-bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py",
             "imports": [
                 "json",
-                "os",
+                "copy",
+                "torchvision",
                 "torch",
                 "random",
-                "copy",
-                "torchvision"
+                "os"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.movie_scene_segmentation_dataset"
         },
         "('CUSTOM_DATASETS', 'nli', 'veco')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/veco_dataset.py",
             "imports": [
-                "datasets",
                 "numpy",
+                "datasets",
                 "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.veco_dataset"
         },
         "('CUSTOM_DATASETS', 'ocr-recognition', 'OCRRecognition')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/ocr_recognition_dataset.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "lmdb",
+                "six",
                 "json",
-                "os",
+                "PIL",
+                "cv2",
+                "numpy",
                 "torch",
-                "six",
-                "cv2"
+                "lmdb",
+                "os"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_recognition_dataset"
         },
         "('CUSTOM_DATASETS', 'referring-video-object-segmentation', 'swinT-referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/referring_video_object_segmentation/referring_video_object_segmentation_dataset.py",
             "imports": [
-                "pycocotools",
-                "numpy",
-                "glob",
+                "pandas",
                 "json",
+                "torchvision",
+                "tqdm",
                 "h5py",
+                "glob",
+                "numpy",
+                "pycocotools",
                 "os",
-                "pandas",
-                "torch",
-                "tqdm",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.referring_video_object_segmentation_dataset"
         },
         "('CUSTOM_DATASETS', 'sentence-embedding', 'bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py",
             "imports": [
-                "torch",
                 "random",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'text-ranking', 'bert')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/text_ranking_dataset.py",
             "imports": [
-                "torch",
                 "random",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'text-ranking', 'mgeo')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/mgeo_ranking_dataset.py",
             "imports": [
-                "torch",
                 "json",
                 "random",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.mgeo_ranking_dataset"
         },
         "('CUSTOM_DATASETS', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_frame_interpolation/video_frame_interpolation_dataset.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.video_frame_interpolation.video_frame_interpolation_dataset"
         },
         "('CUSTOM_DATASETS', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_stabilization/video_stabilization_dataset.py",
             "imports": [],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.video_stabilization.video_stabilization_dataset"
         },
         "('CUSTOM_DATASETS', 'video-super-resolution', 'real-basicvsr')": {
             "filepath": "TEMPLATE_PATH/msdatasets/dataset_cls/custom_datasets/video_super_resolution/video_super_resolution_dataset.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "collections",
-                "cv2"
+                "torch",
+                "collections"
             ],
             "module": "modelscope.msdatasets.dataset_cls.custom_datasets.video_super_resolution.video_super_resolution_dataset"
         },
         "('DATASETS', 'default', 'CustomNuScenesDataset')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/nuscenes_dataset.py",
             "imports": [
-                "mmdet",
+                "numpy",
                 "mmdet3d",
-                "numpy"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.nuscenes_dataset"
         },
         "('DATASETS', 'default', 'RetinaFaceDataset')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/retinaface.py",
             "imports": [
-                "mmdet",
-                "numpy"
+                "numpy",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.retinaface"
         },
         "('DETECTORS', 'default', 'CustomSingleStageDetector')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/single_stage.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.single_stage"
         },
         "('DETECTORS', 'default', 'EncoderDecoderMask2Former')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/segmentors/encoder_decoder_mask2former.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.encoder_decoder_mask2former"
         },
         "('DETECTORS', 'default', 'Petr3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/detectors/petr3d.py",
             "imports": [
+                "mmcv",
                 "mmdet",
                 "numpy",
-                "mmcv",
-                "mmdet3d",
-                "torch"
+                "torch",
+                "mmdet3d"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.detectors.petr3d"
         },
         "('DETECTORS', 'default', 'SCRFD')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/scrfd.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.scrfd"
         },
         "('DETECTORS', 'default', 'TinyMog')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/detectors/tinymog.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.tinymog"
         },
+        "('EXPORTERS', 'acoustic-noise-suppression', 'speech_dfsmn_ans')": {
+            "filepath": "TEMPLATE_PATH/exporters/audio/ans_dfsmn_exporter.py",
+            "imports": [
+                "torch",
+                "os"
+            ],
+            "module": "modelscope.exporters.audio.ans_dfsmn_exporter"
+        },
         "('EXPORTERS', 'default', 'cartoon-translation')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/cartoon_translation_exporter.py",
             "imports": [
                 "tensorflow",
                 "packaging",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.exporters.cv.cartoon_translation_exporter"
         },
         "('EXPORTERS', 'face-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/face_detection_scrfd_exporter.py",
             "imports": [
-                "numpy",
+                "functools",
                 "onnx",
-                "os",
-                "torch",
                 "typing",
-                "functools"
+                "numpy",
+                "torch",
+                "os"
             ],
             "module": "modelscope.exporters.cv.face_detection_scrfd_exporter"
         },
         "('EXPORTERS', 'image-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/exporters/cv/object_detection_damoyolo_exporter.py",
             "imports": [
-                "numpy",
+                "functools",
                 "onnx",
-                "os",
-                "torch",
                 "typing",
-                "functools"
+                "numpy",
+                "torch",
+                "os"
             ],
             "module": "modelscope.exporters.cv.object_detection_damoyolo_exporter"
         },
         "('EXPORTERS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
                 "torch",
@@ -2276,16 +2222,16 @@
             ],
             "module": "modelscope.exporters.nlp.model_for_token_classification_exporter"
         },
         "('EXPORTERS', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/csanmt_for_translation_exporter.py",
             "imports": [
                 "tensorflow",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.exporters.nlp.csanmt_for_translation_exporter"
         },
         "('EXPORTERS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/exporters/nlp/model_for_token_classification_exporter.py",
             "imports": [
                 "torch",
@@ -2316,269 +2262,269 @@
                 "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.anchor_head"
         },
         "('HEADS', 'default', 'ConvFCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'ConvKernelHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_head.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_head"
         },
         "('HEADS', 'default', 'FCNMaskNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/mask_heads/fcn_mask_head.py",
             "imports": [
+                "mmcv",
                 "mmdet",
                 "numpy",
-                "mmcv",
-                "torch",
-                "warnings"
+                "warnings",
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.mask_heads.fcn_mask_head"
         },
         "('HEADS', 'default', 'KernelFrameIterHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_frame_iter_head.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_frame_iter_head"
         },
         "('HEADS', 'default', 'KernelIterHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_iter_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_iter_head"
         },
         "('HEADS', 'default', 'KernelUpdateHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_update_head.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_update_head"
         },
         "('HEADS', 'default', 'KernelUpdateHeadVideo')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/kernel_update_head.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.track.kernel_update_head"
         },
         "('HEADS', 'default', 'Mask2FormerHeadFromMMSeg')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/models/decode_heads/mask2former_head_from_mmseg.py",
             "imports": [
-                "mmdet",
-                "torch",
                 "copy",
-                "mmcv"
+                "mmcv",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.mask2former_head_from_mmseg"
         },
         "('HEADS', 'default', 'MaskFormerSemanticHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/pan_merge/maskformer_semantic_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.pan_merge.maskformer_semantic_head"
         },
         "('HEADS', 'default', 'MaskScoringNRoIHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.mask_scoring_roi_head"
         },
         "('HEADS', 'default', 'PETRv2DEDNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/dense_heads/petrv2_dednhead.py",
             "imports": [
+                "mmcv",
+                "copy",
                 "mmdet",
                 "numpy",
-                "mmcv",
-                "math",
-                "mmdet3d",
                 "torch",
-                "copy"
+                "math",
+                "mmdet3d"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.petrv2_dednhead"
         },
         "('HEADS', 'default', 'RPNNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py",
             "imports": [
-                "mmdet",
-                "torch",
                 "copy",
-                "mmcv"
+                "mmcv",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.rpn_head"
         },
         "('HEADS', 'default', 'SCRFDHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/models/dense_heads/scrfd_head.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.dense_heads.scrfd_head"
         },
         "('HEADS', 'default', 'Shared2FCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'Shared4Conv1FCBBoxNHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/roi_heads/bbox_heads/convfc_bbox_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head"
         },
         "('HEADS', 'default', 'VideoKernelIterHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_iter_head.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_iter_head"
         },
         "('HEADS', 'default', 'VideoKernelUpdateHead')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/kernel_update_head.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.kernel_update_head"
         },
         "('HEADS', 'fill-mask', 'bert-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'fill-mask', 'roberta-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/torch_pretrain_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.torch_pretrain_head"
         },
         "('HEADS', 'fill-mask', 'xlm-roberta-mlm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/fill_mask_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.fill_mask_head"
         },
         "('HEADS', 'information-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.models.nlp.heads.infromation_extraction_head"
         },
         "('HEADS', 'named-entity-recognition', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'named-entity-recognition', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'named-entity-recognition', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'nli', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/text_classification_head.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_classification_head"
         },
         "('HEADS', 'part-of-speech', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'part-of-speech', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'part-of-speech', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'relation-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/infromation_extraction_head.py",
             "imports": [
                 "torch"
@@ -2624,194 +2570,186 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.heads.text_ranking_head"
         },
         "('HEADS', 'token-classification', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'token-classification', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/token_classification_head.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.heads.token_classification_head"
         },
         "('HEADS', 'token-classification', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'word-segmentation', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
         "('HEADS', 'word-segmentation', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/heads/crf_head.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.models.nlp.heads.crf_head"
         },
-        "('HOOKS', 'default', 'AddLrLogHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/easycv/utils/hooks.py",
-            "imports": [],
-            "module": "modelscope.trainers.easycv.utils.hooks"
-        },
         "('HOOKS', 'default', 'ApexAMPOptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/apex_optimizer_hook.py",
             "imports": [
+                "logging",
                 "packaging",
-                "torch",
-                "logging"
+                "torch"
             ],
             "module": "modelscope.trainers.hooks.optimizer.apex_optimizer_hook"
         },
         "('HOOKS', 'default', 'BestCkptSaverHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py",
+            "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py",
             "imports": [
-                "packaging",
+                "typing",
+                "time",
                 "numpy",
-                "re",
-                "os",
                 "torch",
                 "random",
-                "time"
+                "os"
             ],
-            "module": "modelscope.trainers.hooks.checkpoint_hook"
+            "module": "modelscope.trainers.hooks.checkpoint.checkpoint_hook"
         },
         "('HOOKS', 'default', 'CheckpointHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py",
+            "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint/checkpoint_hook.py",
             "imports": [
-                "packaging",
+                "typing",
+                "time",
                 "numpy",
-                "re",
-                "os",
                 "torch",
                 "random",
-                "time"
+                "os"
             ],
-            "module": "modelscope.trainers.hooks.checkpoint_hook"
+            "module": "modelscope.trainers.hooks.checkpoint.checkpoint_hook"
         },
         "('HOOKS', 'default', 'ClipClampLogitScaleHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/clip_clamp_logit_scale_hook.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.trainers.hooks.clip_clamp_logit_scale_hook"
         },
         "('HOOKS', 'default', 'DDPHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/hooks/ddp_hook.py",
+            "filepath": "TEMPLATE_PATH/trainers/hooks/distributed/ddp_hook.py",
             "imports": [],
-            "module": "modelscope.trainers.hooks.ddp_hook"
+            "module": "modelscope.trainers.hooks.distributed.ddp_hook"
         },
         "('HOOKS', 'default', 'DeepspeedHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/hooks/deepspeed_hook.py",
+            "filepath": "TEMPLATE_PATH/trainers/hooks/distributed/deepspeed_hook.py",
             "imports": [
-                "megatron_util",
                 "shutil",
+                "megatron_util",
                 "os",
-                "deepspeed",
-                "torch"
+                "torch",
+                "deepspeed"
             ],
-            "module": "modelscope.trainers.hooks.deepspeed_hook"
+            "module": "modelscope.trainers.hooks.distributed.deepspeed_hook"
         },
         "('HOOKS', 'default', 'EarlyStopHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/early_stop_hook.py",
             "imports": [
                 "numpy"
             ],
             "module": "modelscope.trainers.hooks.early_stop_hook"
         },
         "('HOOKS', 'default', 'EvaluationHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/evaluation_hook.py",
             "imports": [
-                "collections"
+                "collections",
+                "typing"
             ],
             "module": "modelscope.trainers.hooks.evaluation_hook"
         },
         "('HOOKS', 'default', 'IterTimerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/iter_timer_hook.py",
             "imports": [
                 "time"
             ],
             "module": "modelscope.trainers.hooks.iter_timer_hook"
         },
         "('HOOKS', 'default', 'LoadCheckpointHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint_hook.py",
+            "filepath": "TEMPLATE_PATH/trainers/hooks/checkpoint/load_checkpoint_hook.py",
             "imports": [
-                "packaging",
+                "typing",
                 "numpy",
-                "re",
-                "os",
-                "torch",
+                "packaging",
                 "random",
-                "time"
+                "torch"
             ],
-            "module": "modelscope.trainers.hooks.checkpoint_hook"
+            "module": "modelscope.trainers.hooks.checkpoint.load_checkpoint_hook"
         },
         "('HOOKS', 'default', 'LrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.lr_scheduler_hook"
         },
         "('HOOKS', 'default', 'MegatronHook')": {
-            "filepath": "TEMPLATE_PATH/trainers/hooks/megatron_hook.py",
+            "filepath": "TEMPLATE_PATH/trainers/hooks/distributed/megatron_hook.py",
             "imports": [
                 "torch",
+                "shutil",
                 "megatron_util",
-                "copy",
                 "os"
             ],
-            "module": "modelscope.trainers.hooks.megatron_hook"
+            "module": "modelscope.trainers.hooks.distributed.megatron_hook"
         },
         "('HOOKS', 'default', 'NoneLrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.lr_scheduler_hook"
         },
         "('HOOKS', 'default', 'NoneOptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/base.py",
             "imports": [
-                "torch",
-                "logging"
+                "logging",
+                "torch"
             ],
             "module": "modelscope.trainers.hooks.optimizer.base"
         },
         "('HOOKS', 'default', 'OptimizerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/optimizer/base.py",
             "imports": [
-                "torch",
-                "logging"
+                "logging",
+                "torch"
             ],
             "module": "modelscope.trainers.hooks.optimizer.base"
         },
         "('HOOKS', 'default', 'PlateauLrSchedulerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/lr_scheduler_hook.py",
             "imports": [],
             "module": "modelscope.trainers.hooks.lr_scheduler_hook"
@@ -2822,25 +2760,25 @@
                 "os"
             ],
             "module": "modelscope.trainers.hooks.compression.sparsity_hook"
         },
         "('HOOKS', 'default', 'TensorboardHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/logger/tensorboard_hook.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.trainers.hooks.logger.tensorboard_hook"
         },
         "('HOOKS', 'default', 'TextLoggerHook')": {
             "filepath": "TEMPLATE_PATH/trainers/hooks/logger/text_logger_hook.py",
             "imports": [
-                "json",
                 "datetime",
+                "json",
                 "os",
                 "torch",
                 "collections"
             ],
             "module": "modelscope.trainers.hooks.logger.text_logger_hook"
         },
         "('HOOKS', 'default', 'TorchAMPOptimizerHook')": {
@@ -2864,38 +2802,28 @@
             "filepath": "TEMPLATE_PATH/trainers/lrscheduler/warmup/warmup.py",
             "imports": [],
             "module": "modelscope.trainers.lrscheduler.warmup.warmup"
         },
         "('MATCH_COST', 'default', 'BBox3DL1Cost')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/core/bbox/match_costs/match_cost.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.match_costs.match_cost"
         },
         "('MATCH_COST', 'default', 'MaskCost')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/track/mask_hungarian_assigner.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "scipy",
                 "numpy",
-                "scipy"
-            ],
-            "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
-        },
-        "('METRICS', 'default', 'EasyCVMetric')": {
-            "filepath": "TEMPLATE_PATH/trainers/easycv/utils/metric.py",
-            "imports": [
                 "torch",
-                "numpy",
-                "typing",
-                "itertools"
+                "mmdet"
             ],
-            "module": "modelscope.trainers.easycv.utils.metric"
+            "module": "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner"
         },
         "('METRICS', 'default', 'accuracy')": {
             "filepath": "TEMPLATE_PATH/metrics/accuracy_metric.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
@@ -2907,118 +2835,118 @@
                 "typing"
             ],
             "module": "modelscope.metrics.audio_noise_metric"
         },
         "('METRICS', 'default', 'bleu')": {
             "filepath": "TEMPLATE_PATH/metrics/bleu_metric.py",
             "imports": [
+                "itertools",
                 "sacrebleu",
-                "typing",
-                "itertools"
+                "typing"
             ],
             "module": "modelscope.metrics.bleu_metric"
         },
         "('METRICS', 'default', 'image-color-enhance-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_color_enhance_metric.py",
             "imports": [
+                "cv2",
                 "numpy",
-                "typing",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.metrics.image_color_enhance_metric"
         },
         "('METRICS', 'default', 'image-colorization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_colorization_metric.py",
             "imports": [
-                "numpy",
-                "scipy",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "cv2",
+                "numpy",
+                "scipy",
+                "torch"
             ],
             "module": "modelscope.metrics.image_colorization_metric"
         },
         "('METRICS', 'default', 'image-denoise-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_denoise_metric.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "typing",
-                "cv2"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.metrics.image_denoise_metric"
         },
         "('METRICS', 'default', 'image-inpainting-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_inpainting_metric.py",
             "imports": [
                 "scipy",
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.metrics.image_inpainting_metric"
         },
         "('METRICS', 'default', 'image-ins-seg-coco-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_instance_segmentation_metric.py",
             "imports": [
+                "typing",
                 "pycocotools",
                 "numpy",
                 "tempfile",
                 "os",
-                "collections",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.metrics.image_instance_segmentation_metric"
         },
         "('METRICS', 'default', 'image-portrait-enhancement-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_portrait_enhancement_metric.py",
             "imports": [
+                "cv2",
                 "numpy",
-                "typing",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.metrics.image_portrait_enhancement_metric"
         },
         "('METRICS', 'default', 'image-quality-assessment-degradation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_quality_assessment_degradation_metric.py",
             "imports": [
-                "numpy",
+                "sys",
                 "typing",
                 "tqdm",
+                "cv2",
+                "numpy",
+                "torch",
                 "scipy",
                 "tempfile",
                 "os",
-                "torch",
-                "collections",
-                "sys",
-                "cv2"
+                "collections"
             ],
             "module": "modelscope.metrics.image_quality_assessment_degradation_metric"
         },
         "('METRICS', 'default', 'image-quality-assessment-mos-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/image_quality_assessment_mos_metric.py",
             "imports": [
-                "numpy",
                 "sys",
+                "typing",
+                "tqdm",
+                "cv2",
+                "numpy",
+                "os",
                 "scipy",
                 "tempfile",
-                "os",
-                "torch",
-                "tqdm",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.metrics.image_quality_assessment_mos_metric"
         },
         "('METRICS', 'default', 'inbatch_recall')": {
             "filepath": "TEMPLATE_PATH/metrics/inbatch_recall_metric.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.metrics.inbatch_recall_metric"
         },
         "('METRICS', 'default', 'loss-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/loss_metric.py",
             "imports": [
@@ -3051,27 +2979,27 @@
                 "typing"
             ],
             "module": "modelscope.metrics.ned_metric"
         },
         "('METRICS', 'default', 'ocr-recognition-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/ocr_recognition_metric.py",
             "imports": [
+                "numpy",
                 "torch",
                 "edit_distance",
-                "numpy",
                 "typing"
             ],
             "module": "modelscope.metrics.ocr_recognition_metric"
         },
         "('METRICS', 'default', 'ppl')": {
             "filepath": "TEMPLATE_PATH/metrics/ppl_metric.py",
             "imports": [
                 "math",
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.metrics.ppl_metric"
         },
         "('METRICS', 'default', 'prediction-saving-wrapper')": {
             "filepath": "TEMPLATE_PATH/metrics/prediction_saving_wrapper.py",
             "imports": [
@@ -3080,19 +3008,19 @@
                 "typing"
             ],
             "module": "modelscope.metrics.prediction_saving_wrapper"
         },
         "('METRICS', 'default', 'referring-video-object-segmentation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/referring_video_object_segmentation_metric.py",
             "imports": [
+                "typing",
+                "tqdm",
                 "pycocotools",
                 "numpy",
-                "torch",
-                "tqdm",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.metrics.referring_video_object_segmentation_metric"
         },
         "('METRICS', 'default', 'seq-cls-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/sequence_classification_metric.py",
             "imports": [
                 "numpy",
@@ -3101,57 +3029,66 @@
             ],
             "module": "modelscope.metrics.sequence_classification_metric"
         },
         "('METRICS', 'default', 'text-gen-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/text_generation_metric.py",
             "imports": [
                 "nltk",
-                "typing",
-                "rouge"
+                "rouge",
+                "typing"
             ],
             "module": "modelscope.metrics.text_generation_metric"
         },
         "('METRICS', 'default', 'text-ranking-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/text_ranking_metric.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.metrics.text_ranking_metric"
         },
         "('METRICS', 'default', 'token-cls-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/token_classification_metric.py",
             "imports": [
+                "importlib",
                 "numpy",
-                "typing",
-                "importlib"
+                "typing"
             ],
             "module": "modelscope.metrics.token_classification_metric"
         },
+        "('METRICS', 'default', 'translation-evaluation-metric')": {
+            "filepath": "TEMPLATE_PATH/metrics/translation_evaluation_metric.py",
+            "imports": [
+                "pandas",
+                "importlib",
+                "typing"
+            ],
+            "module": "modelscope.metrics.translation_evaluation_metric"
+        },
         "('METRICS', 'default', 'video-frame-interpolation-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_frame_interpolation_metric.py",
             "imports": [
+                "typing",
                 "numpy",
-                "math",
                 "lpips",
-                "torch",
-                "typing"
+                "math",
+                "torch"
             ],
             "module": "modelscope.metrics.video_frame_interpolation_metric"
         },
         "('METRICS', 'default', 'video-stabilization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_stabilization_metric.py",
             "imports": [
-                "numpy",
                 "sys",
-                "tempfile",
-                "os",
-                "tqdm",
                 "typing",
-                "cv2"
+                "tqdm",
+                "cv2",
+                "numpy",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.metrics.video_stabilization_metric"
         },
         "('METRICS', 'default', 'video-summarization-metric')": {
             "filepath": "TEMPLATE_PATH/metrics/video_summarization_metric.py",
             "imports": [
                 "numpy",
@@ -3174,90 +3111,90 @@
             ],
             "module": "modelscope.models.audio.ans.denoise_net"
         },
         "('MODELS', 'acoustic-noise-suppression', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/models/audio/ans/frcrn.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.ans.frcrn"
         },
         "('MODELS', 'auto-speech-recognition', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'auto-speech-recognition', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'auto-speech-recognition', 'wenet-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/wenet_automatic_speech_recognition.py",
             "imports": [
                 "wenetruntime",
                 "json",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.wenet_automatic_speech_recognition"
         },
         "('MODELS', 'backbone', 'T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/T5/backbone.py",
             "imports": [
-                "math",
+                "typing",
+                "copy",
                 "transformers",
-                "os",
                 "torch",
                 "warnings",
-                "copy",
-                "typing"
+                "math",
+                "os"
             ],
             "module": "modelscope.models.nlp.T5.backbone"
         },
         "('MODELS', 'backbone', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/backbone.py",
             "imports": [
                 "packaging",
-                "torch",
                 "math",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.bert.backbone"
         },
         "('MODELS', 'backbone', 'deberta_v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/deberta_v2/backbone.py",
             "imports": [
+                "transformers",
                 "torch",
                 "collections",
-                "transformers",
                 "typing"
             ],
             "module": "modelscope.models.nlp.deberta_v2.backbone"
         },
         "('MODELS', 'backbone', 'llama')": {
             "filepath": "TEMPLATE_PATH/models/nlp/llama/backbone.py",
             "imports": [
                 "math",
-                "torch",
                 "transformers",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.llama.backbone"
         },
         "('MODELS', 'backbone', 'lstm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/lstm/backbone.py",
             "imports": [
@@ -3265,65 +3202,65 @@
             ],
             "module": "modelscope.models.nlp.lstm.backbone"
         },
         "('MODELS', 'backbone', 'megatron-bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/megatron_bert/backbone.py",
             "imports": [
                 "math",
-                "torch",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.megatron_bert.backbone"
         },
         "('MODELS', 'backbone', 'mgeo')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mgeo/backbone.py",
             "imports": [
-                "math",
-                "warnings",
-                "dataclasses",
+                "typing",
                 "transformers",
+                "dataclasses",
+                "warnings",
                 "os",
-                "torch",
+                "math",
                 "random",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.backbone"
         },
         "('MODELS', 'backbone', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/backbone.py",
             "imports": [
+                "typing",
+                "transformers",
+                "dataclasses",
                 "packaging",
                 "math",
-                "dataclasses",
-                "transformers",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.nlp.plug_mental.backbone"
         },
         "('MODELS', 'backbone', 'ponet')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/backbone.py",
             "imports": [
+                "transformers",
                 "packaging",
-                "distutils",
                 "math",
-                "transformers",
-                "torch"
+                "torch",
+                "distutils"
             ],
             "module": "modelscope.models.nlp.ponet.backbone"
         },
         "('MODELS', 'backbone', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/backbone.py",
             "imports": [
+                "typing",
+                "transformers",
+                "dataclasses",
                 "packaging",
                 "math",
-                "dataclasses",
-                "transformers",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.nlp.structbert.backbone"
         },
         "('MODELS', 'backbone', 'transformers')": {
             "filepath": "TEMPLATE_PATH/models/nlp/hf_transformers/backbone.py",
             "imports": [
                 "transformers"
@@ -3337,152 +3274,152 @@
             ],
             "module": "modelscope.models.nlp.veco.backbone"
         },
         "('MODELS', 'backbone', 'xlm-roberta')": {
             "filepath": "TEMPLATE_PATH/models/nlp/xlm_roberta/backbone.py",
             "imports": [
                 "packaging",
-                "torch",
                 "math",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.xlm_roberta.backbone"
         },
         "('MODELS', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/models/cv/bad_image_detecting/bad_image_detecting.py",
             "imports": [
+                "typing",
+                "torchvision",
                 "numpy",
                 "os",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.models.cv.bad_image_detecting.bad_image_detecting"
         },
         "('MODELS', 'body-2d-keypoints', 'body-2d-keypoints')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_2d_keypoints/hrnet_v2.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.body_2d_keypoints.hrnet_v2"
         },
         "('MODELS', 'body-3d-keypoints', 'body-3d-keypoints')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_3d_keypoints/cannonical_pose/body_3d_pose.py",
             "imports": [
+                "typing",
+                "logging",
                 "numpy",
                 "os",
-                "logging",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.cv.body_3d_keypoints.cannonical_pose.body_3d_pose"
         },
         "('MODELS', 'body-3d-keypoints', 'hdformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/body_3d_keypoints/hdformer/hdformer_detector.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer_detector"
         },
         "('MODELS', 'card-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py",
             "imports": [
+                "typing",
+                "copy",
                 "numpy",
                 "os",
-                "torch",
-                "copy",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.scrfd_detect"
         },
         "('MODELS', 'code-generation', 'codegeex')": {
             "filepath": "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_generation.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.codegeex.codegeex_for_code_generation"
         },
         "('MODELS', 'code-translation', 'codegeex')": {
             "filepath": "TEMPLATE_PATH/models/nlp/codegeex/codegeex_for_code_translation.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.codegeex.codegeex_for_code_translation"
         },
         "('MODELS', 'competency-aware-translation', 'canmt')": {
             "filepath": "TEMPLATE_PATH/models/nlp/canmt/canmt_translation.py",
             "imports": [
+                "typing",
                 "numpy",
-                "math",
                 "os",
-                "torch",
-                "typing"
+                "math",
+                "torch"
             ],
             "module": "modelscope.models.nlp.canmt.canmt_translation"
         },
         "('MODELS', 'controllable-image-generation', 'controllable-image-generation')": {
             "filepath": "TEMPLATE_PATH/models/cv/controllable_image_generation/controlnet.py",
             "imports": [
-                "numpy",
-                "typing",
-                "math",
                 "einops",
+                "sys",
+                "typing",
                 "PIL",
-                "control_ldm",
-                "tempfile",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
+                "control_ldm",
+                "math",
                 "random",
-                "sys",
-                "cv2"
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.models.cv.controllable_image_generation.controlnet"
         },
         "('MODELS', 'crowd-counting', 'HRNetCrowdCounting')": {
             "filepath": "TEMPLATE_PATH/models/cv/crowd_counting/cc_model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.crowd_counting.cc_model"
         },
         "('MODELS', 'document-grounded-dialog-generate', 'doc2bot')": {
             "filepath": "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_generate.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.dgds.document_grounded_dialog_generate"
         },
         "('MODELS', 'document-grounded-dialog-rerank', 'doc2bot')": {
             "filepath": "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_rerank.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.dgds.document_grounded_dialog_rerank"
         },
         "('MODELS', 'document-grounded-dialog-retrieval', 'doc2bot')": {
             "filepath": "TEMPLATE_PATH/models/nlp/dgds/document_grounded_dialog_retrieval.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.dgds.document_grounded_dialog_retrieval"
         },
         "('MODELS', 'document-segmentation', 'bert-for-document-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/document_segmentation.py",
             "imports": [
                 "torch",
@@ -3497,403 +3434,396 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.ponet.document_segmentation"
         },
         "('MODELS', 'document-vl-embedding', 'vldoc')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/vldoc/model.py",
             "imports": [
-                "re",
                 "sys",
-                "math",
                 "json",
-                "os",
+                "copy",
                 "logging",
+                "torchvision",
                 "torch",
-                "copy",
-                "torchvision"
+                "math",
+                "os",
+                "re"
             ],
             "module": "modelscope.models.multi_modal.vldoc.model"
         },
-        "('MODELS', 'domain-specific-object-detection', 'YOLOX')": {
-            "filepath": "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.object_detection.yolox_pai"
-        },
         "('MODELS', 'domain-specific-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py",
             "imports": [],
             "module": "modelscope.models.cv.tinynas_detection.tinynas_damoyolo"
         },
         "('MODELS', 'efficient-diffusion-tuning', 'efficient-diffusion-tuning')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/efficient_diffusion_tuning/efficient_stable_diffusion.py",
             "imports": [
+                "functools",
+                "typing",
                 "transformers",
-                "os",
                 "diffusers",
                 "torch",
-                "typing",
-                "functools"
+                "os"
             ],
             "module": "modelscope.models.multi_modal.efficient_diffusion_tuning.efficient_stable_diffusion"
         },
         "('MODELS', 'extractive-summarization', 'ponet-for-document-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/document_segmentation.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.ponet.document_segmentation"
         },
-        "('MODELS', 'face-2d-keypoints', 'face-2d-keypoints')": {
-            "filepath": "TEMPLATE_PATH/models/cv/face_2d_keypoints/face_2d_keypoints_align.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.face_2d_keypoints.face_2d_keypoints_align"
-        },
         "('MODELS', 'face-2d-keypoints', 'flc')": {
             "filepath": "TEMPLATE_PATH/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py",
             "imports": [
-                "numpy",
                 "PIL",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.facial_landmark_confidence.flc.facial_landmark_confidence"
         },
         "('MODELS', 'face-attribute-recognition', 'fairface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py",
             "imports": [
-                "numpy",
+                "torchvision",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "torchvision",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.models.cv.face_attribute_recognition.fair_face.face_attribute_recognition"
         },
         "('MODELS', 'face-detection', 'damofd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/damofd_detect.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "os",
                 "typing"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.damofd_detect"
         },
         "('MODELS', 'face-detection', 'mogface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/mogface/models/detectors.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "os",
-                "cv2"
+                "torch",
+                "os"
             ],
             "module": "modelscope.models.cv.face_detection.mogface.models.detectors"
         },
         "('MODELS', 'face-detection', 'mtcnn')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/mtcnn/models/detector.py",
             "imports": [
-                "torch",
                 "numpy",
-                "os",
-                "PIL"
+                "torch",
+                "PIL",
+                "os"
             ],
             "module": "modelscope.models.cv.face_detection.mtcnn.models.detector"
         },
         "('MODELS', 'face-detection', 'retinaface')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/retinaface/detection.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.retinaface.detection"
         },
         "('MODELS', 'face-detection', 'scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/scrfd_detect.py",
             "imports": [
+                "typing",
+                "copy",
                 "numpy",
                 "os",
-                "torch",
-                "copy",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.scrfd_detect"
         },
         "('MODELS', 'face-detection', 'tinymog')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/tinymog_detect.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "os",
                 "typing"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.tinymog_detect"
         },
         "('MODELS', 'face-detection', 'ulfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/ulfd_slim/detection.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "os",
-                "cv2"
+                "torch",
+                "os"
             ],
             "module": "modelscope.models.cv.face_detection.ulfd_slim.detection"
         },
         "('MODELS', 'face-emotion', 'face-emotion')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_emotion/emotion_model.py",
             "imports": [
-                "torch",
                 "sys",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.face_emotion.emotion_model"
         },
         "('MODELS', 'face-human-hand-detection', 'face-human-hand-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_human_hand_detection/det_infer.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.face_human_hand_detection.det_infer"
         },
         "('MODELS', 'face-recognition', 'rts-backbone')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_recognition/torchkit/rts_backbone.py",
             "imports": [
                 "math",
                 "torch",
-                "collections",
-                "os"
+                "os",
+                "collections"
             ],
             "module": "modelscope.models.cv.face_recognition.torchkit.rts_backbone"
         },
         "('MODELS', 'face-reconstruction', 'face_reconstruction')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_reconstruction/models/facerecon_model.py",
             "imports": [
+                "cv2",
                 "numpy",
                 "os",
                 "torch",
-                "collections",
-                "cv2"
+                "collections"
             ],
             "module": "modelscope.models.cv.face_reconstruction.models.facerecon_model"
         },
         "('MODELS', 'facial-expression-recognition', 'fer')": {
             "filepath": "TEMPLATE_PATH/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py",
             "imports": [
-                "numpy",
                 "PIL",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.facial_expression_recognition.fer.facial_expression_recognition"
         },
         "('MODELS', 'faq-question-answering', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/faq_question_answering.py",
             "imports": [
-                "math",
+                "typing",
                 "os",
+                "math",
                 "torch",
-                "collections",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.models.nlp.structbert.faq_question_answering"
         },
         "('MODELS', 'feature-extraction', 'feature-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/feature_extraction.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.feature_extraction"
         },
         "('MODELS', 'fid-dialogue', 'fid-T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/fid_T5/text_generation.py",
             "imports": [
-                "io",
                 "torch",
+                "io",
                 "transformers",
                 "os"
             ],
             "module": "modelscope.models.nlp.fid_T5.text_generation"
         },
         "('MODELS', 'fid-dialogue', 'fid-plug')": {
             "filepath": "TEMPLATE_PATH/models/nlp/fid_plug/text_generation.py",
             "imports": [
-                "io",
                 "torch",
+                "io",
                 "transformers",
                 "os"
             ],
             "module": "modelscope.models.nlp.fid_plug.text_generation"
         },
         "('MODELS', 'fill-mask', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/fill_mask.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'deberta_v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/deberta_v2/fill_mask.py",
             "imports": [
-                "torch",
                 "transformers",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.deberta_v2.fill_mask"
         },
         "('MODELS', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/fill_mask.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.fill_mask"
         },
         "('MODELS', 'fill-mask', 'megatron-bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/megatron_bert/fill_mask.py",
             "imports": [
-                "torch",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.megatron_bert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'ponet')": {
             "filepath": "TEMPLATE_PATH/models/nlp/ponet/fill_mask.py",
             "imports": [
-                "torch",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.ponet.fill_mask"
         },
         "('MODELS', 'fill-mask', 'structbert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/structbert/fill_mask.py",
             "imports": [
-                "torch",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.structbert.fill_mask"
         },
         "('MODELS', 'fill-mask', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/fill_mask.py",
             "imports": [
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.fill_mask"
         },
         "('MODELS', 'generative-multi-modal-embedding', 'gemm-generative-multi-modal')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/gemm/gemm_model.py",
             "imports": [
-                "numpy",
-                "PIL",
+                "typing",
                 "json",
+                "torchvision",
+                "PIL",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.gemm.gemm_model"
         },
         "('MODELS', 'generative-multi-modal-embedding', 'rleg-generative-multi-modal')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/rleg/rleg.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.rleg.rleg"
         },
-        "('MODELS', 'hand-2d-keypoints', 'HRNet-Hand2D-Keypoints')": {
-            "filepath": "TEMPLATE_PATH/models/cv/hand_2d_keypoints/hand_2d_keypoints.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.hand_2d_keypoints.hand_2d_keypoints"
-        },
         "('MODELS', 'hand-static', 'hand-static')": {
             "filepath": "TEMPLATE_PATH/models/cv/hand_static/hand_model.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "sys",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch",
+                "os"
             ],
             "module": "modelscope.models.cv.hand_static.hand_model"
         },
         "('MODELS', 'human-detection', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_model"
         },
         "('MODELS', 'human-reconstruction', 'human-reconstruction')": {
             "filepath": "TEMPLATE_PATH/models/cv/human_reconstruction/Reconstruction.py",
             "imports": [
-                "numpy",
-                "skimage",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "skimage",
+                "torch"
             ],
             "module": "modelscope.models.cv.human_reconstruction.Reconstruction"
         },
-        "('MODELS', 'human-wholebody-keypoint', 'human-wholebody-keypoint')": {
-            "filepath": "TEMPLATE_PATH/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.human_wholebody_keypoint.human_wholebody_keypoint"
-        },
         "('MODELS', 'image-body-reshaping', 'image-body-reshaping')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_body_reshaping/image_body_reshaping.py",
             "imports": [
+                "typing",
+                "cv2",
                 "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.image_body_reshaping.image_body_reshaping"
         },
+        "('MODELS', 'image-captioning', 'clip-interrogator')": {
+            "filepath": "TEMPLATE_PATH/models/multi_modal/clip_interrogator/model.py",
+            "imports": [
+                "safetensors",
+                "typing",
+                "transformers",
+                "PIL",
+                "dataclasses",
+                "time",
+                "requests",
+                "hashlib",
+                "torch",
+                "os",
+                "torchvision",
+                "tqdm",
+                "open_clip",
+                "numpy",
+                "math"
+            ],
+            "module": "modelscope.models.multi_modal.clip_interrogator.model"
+        },
         "('MODELS', 'image-captioning', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'image-captioning', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'image-classification', 'ClassificationModel')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/mmcls_model.py",
             "imports": [
                 "os"
@@ -3908,252 +3838,231 @@
             ],
             "module": "modelscope.models.cv.robust_image_classification.easyrobust_model"
         },
         "('MODELS', 'image-classification', 'bnext')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_binary_quant_classification/binary_quant_model.py",
             "imports": [
                 "torch",
-                "collections",
-                "os"
+                "os",
+                "collections"
             ],
             "module": "modelscope.models.cv.image_binary_quant_classification.binary_quant_model"
         },
         "('MODELS', 'image-classification', 'content-check')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_classification/resnet50_cc.py",
             "imports": [
-                "math",
+                "torchvision",
                 "os",
+                "math",
                 "torch",
-                "collections",
-                "torchvision"
+                "collections"
             ],
             "module": "modelscope.models.cv.image_classification.resnet50_cc"
         },
         "('MODELS', 'image-classification', 'image-probing-model')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_probing_model/model.py",
             "imports": [
-                "torch",
                 "json",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_probing_model.model"
         },
         "('MODELS', 'image-classification', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'image-color-enhancement', 'adaint')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/adaint/adaint.py",
             "imports": [
+                "typing",
+                "torchvision",
                 "numbers",
                 "os",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.models.cv.image_color_enhance.adaint.adaint"
         },
         "('MODELS', 'image-color-enhancement', 'csrnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/image_color_enhance.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_color_enhance.image_color_enhance"
         },
         "('MODELS', 'image-color-enhancement', 'deeplpfnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_color_enhance/deeplpf/deeplpf_image_color_enhance.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_color_enhance.deeplpf.deeplpf_image_color_enhance"
         },
         "('MODELS', 'image-colorization', 'ddcolor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_colorization/ddcolor/ddcolor_for_image_colorization.py",
             "imports": [
+                "typing",
+                "copy",
                 "numpy",
                 "os",
-                "torch",
-                "copy",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.cv.image_colorization.ddcolor.ddcolor_for_image_colorization"
         },
         "('MODELS', 'image-debanding', 'rrdb')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_debanding/rrdb/rrdb_image_debanding.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_debanding.rrdb.rrdb_image_debanding"
         },
         "('MODELS', 'image-deblurring', 'nafnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_deblur/nafnet_for_image_deblur.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_deblur.nafnet_for_image_deblur"
         },
         "('MODELS', 'image-demoireing', 'image-restoration')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_restoration/image_restoration_model.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "os",
-                "cv2"
+                "torch",
+                "os"
             ],
             "module": "modelscope.models.cv.image_restoration.image_restoration_model"
         },
         "('MODELS', 'image-denoising', 'nafnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_denoise/nafnet_for_image_denoise.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_denoise.nafnet_for_image_denoise"
         },
         "('MODELS', 'image-depth-estimation', 'bts-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_depth_estimation_bts/depth_estimation_bts_model.py",
             "imports": [
                 "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.image_depth_estimation_bts.depth_estimation_bts_model"
         },
         "('MODELS', 'image-depth-estimation', 'newcrfs-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_depth_estimation/newcrfs_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.image_depth_estimation.newcrfs_model"
         },
         "('MODELS', 'image-driving-perception', 'yolopv2')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_driving_perception/image_driving_percetion_model.py",
             "imports": [
+                "typing",
+                "cv2",
                 "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.image_driving_perception.image_driving_percetion_model"
         },
         "('MODELS', 'image-face-fusion', 'image-face-fusion')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_face_fusion/image_face_fusion.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
-                "collections",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch",
+                "os",
+                "collections"
             ],
             "module": "modelscope.models.cv.image_face_fusion.image_face_fusion"
         },
         "('MODELS', 'image-fewshot-detection', 'defrcn')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_defrcn_fewshot/defrcn_for_fewshot.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_defrcn_fewshot.defrcn_for_fewshot"
         },
         "('MODELS', 'image-inpainting', 'FFTInpainting')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_inpainting/model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_inpainting.model"
         },
         "('MODELS', 'image-matching', 'quadtree-attention-image-matching')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_matching/quadtree_attention_model.py",
             "imports": [
-                "numpy",
                 "pathlib",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.models.cv.image_matching.quadtree_attention_model"
         },
         "('MODELS', 'image-multi-view-depth-estimation', 'image-casmvs-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_mvs_depth_estimation/casmvs_model.py",
             "imports": [
-                "numpy",
                 "easydict",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.image_mvs_depth_estimation.casmvs_model"
         },
-        "('MODELS', 'image-object-detection', 'DINO')": {
-            "filepath": "TEMPLATE_PATH/models/cv/object_detection/dino.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.object_detection.dino"
-        },
         "('MODELS', 'image-object-detection', 'MaskScoring')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_model"
         },
-        "('MODELS', 'image-object-detection', 'YOLOX')": {
-            "filepath": "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.object_detection.yolox_pai"
-        },
         "('MODELS', 'image-object-detection', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_model"
         },
-        "('MODELS', 'image-object-detection', 'image-object-detection-auto')": {
-            "filepath": "TEMPLATE_PATH/models/cv/object_detection/yolox_pai.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.object_detection.yolox_pai"
-        },
         "('MODELS', 'image-object-detection', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_damoyolo.py",
             "imports": [],
             "module": "modelscope.models.cv.tinynas_detection.tinynas_damoyolo"
         },
         "('MODELS', 'image-object-detection', 'tinynas-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/tinynas_detection/tinynas_detector.py",
@@ -4167,174 +4076,169 @@
                 "os"
             ],
             "module": "modelscope.models.cv.vidt.model"
         },
         "('MODELS', 'image-paintbyexample', 'Stablediffusion-Paintbyexample')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_paintbyexample/model.py",
             "imports": [
+                "typing",
                 "omegaconf",
                 "paint_ldm",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.cv.image_paintbyexample.model"
         },
         "('MODELS', 'image-portrait-enhancement', 'gpen')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_portrait_enhancement/image_portrait_enhancement.py",
             "imports": [
                 "math",
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_portrait_enhancement.image_portrait_enhancement"
         },
         "('MODELS', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_degradation/image_quality_assessment_degradation.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_degradation.image_quality_assessment_degradation"
         },
         "('MODELS', 'image-quality-assessment-mos', 'image-quality-assessment-man')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_man/image_quality_assessment_man.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_man.image_quality_assessment_man"
         },
         "('MODELS', 'image-quality-assessment-mos', 'image-quality-assessment-mos')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_quality_assessment_mos/image_quality_assessment_mos.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_quality_assessment_mos.image_quality_assessment_mos"
         },
         "('MODELS', 'image-reid-person', 'passvitb')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_reid_person/pass_model.py",
             "imports": [
-                "torch",
                 "enum",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.image_reid_person.pass_model"
         },
-        "('MODELS', 'image-segmentation', 'Segformer')": {
-            "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/segformer.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.image_semantic_segmentation.segformer"
-        },
         "('MODELS', 'image-segmentation', 'cascade_mask_rcnn_swin')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_instance_segmentation/model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_instance_segmentation.model"
         },
+        "('MODELS', 'image-segmentation', 'fastinst')": {
+            "filepath": "TEMPLATE_PATH/models/cv/image_instance_segmentation/fastinst_model.py",
+            "imports": [
+                "torch",
+                "os",
+                "typing"
+            ],
+            "module": "modelscope.models.cv.image_instance_segmentation.fastinst_model"
+        },
         "('MODELS', 'image-segmentation', 'm2fp')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_human_parsing/m2fp_net.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_human_parsing.m2fp_net"
         },
         "('MODELS', 'image-segmentation', 'maskdino_swin')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_instance_segmentation/maskdino_model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_instance_segmentation.maskdino_model"
         },
-        "('MODELS', 'image-segmentation', 'r50-panoptic-segmentation')": {
-            "filepath": "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/r50_panseg_model.py",
-            "imports": [
-                "easycv"
-            ],
-            "module": "modelscope.models.cv.image_panoptic_segmentation.r50_panseg_model"
-        },
         "('MODELS', 'image-segmentation', 'swinL-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_panoptic_segmentation/panseg_model.py",
             "imports": [
                 "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.image_panoptic_segmentation.panseg_model"
         },
         "('MODELS', 'image-segmentation', 'swinL-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model"
         },
         "('MODELS', 'image-segmentation', 'vision-middleware')": {
             "filepath": "TEMPLATE_PATH/models/cv/vision_middleware/model.py",
             "imports": [
-                "torch",
                 "json",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.vision_middleware.model"
         },
         "('MODELS', 'image-segmentation', 'vitadapter-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/semantic_seg_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model"
         },
         "('MODELS', 'image-skychange', 'image-skychange')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_skychange/skychange_model.py",
             "imports": [
                 "pdb",
-                "math",
-                "time",
+                "typing",
                 "json",
+                "cv2",
+                "time",
                 "os",
+                "math",
                 "torch",
-                "collections",
-                "typing",
-                "cv2"
+                "collections"
             ],
             "module": "modelscope.models.cv.image_skychange.skychange_model"
         },
         "('MODELS', 'image-super-resolution', 'ecbsr')": {
             "filepath": "TEMPLATE_PATH/models/cv/super_resolution/ecbsr_model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.super_resolution.ecbsr_model"
         },
         "('MODELS', 'image-text-retrieval', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'indoor-layout-estimation', 'panovit-layout-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/indoor_layout_estimation/panovit.py",
             "imports": [
                 "torch",
@@ -4351,121 +4255,146 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.information_extraction"
         },
         "('MODELS', 'inverse-text-processing', 'generic-itn')": {
             "filepath": "TEMPLATE_PATH/models/audio/itn/generic_inverse_text_processing.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.itn.generic_inverse_text_processing"
         },
         "('MODELS', 'keyword-spotting', 'kws-kwsbp')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/generic_key_word_spotting.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.kws.generic_key_word_spotting"
         },
         "('MODELS', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/models/audio/kws/farfield/model.py",
             "imports": [
                 "tempfile",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.kws.farfield.model"
         },
-        "('MODELS', 'keyword-spotting', 'speech_kws_fsmn_char_ctc_nearfield')": {
-            "filepath": "TEMPLATE_PATH/models/audio/kws/nearfield/model.py",
+        "('MODELS', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield_iot')": {
+            "filepath": "TEMPLATE_PATH/models/audio/kws/farfield/model.py",
             "imports": [
                 "tempfile",
                 "os",
-                "torch",
-                "sys",
                 "typing"
             ],
+            "module": "modelscope.models.audio.kws.farfield.model"
+        },
+        "('MODELS', 'keyword-spotting', 'speech_kws_fsmn_char_ctc_nearfield')": {
+            "filepath": "TEMPLATE_PATH/models/audio/kws/nearfield/model.py",
+            "imports": [
+                "sys",
+                "typing",
+                "os",
+                "tempfile",
+                "torch"
+            ],
             "module": "modelscope.models.audio.kws.nearfield.model"
         },
         "('MODELS', 'language-guided-video-summarization', 'clip-it-language-guided-video-summarization')": {
             "filepath": "TEMPLATE_PATH/models/cv/language_guided_video_summarization/summarizer.py",
             "imports": [
-                "numpy",
-                "argparse",
                 "bmt_clipit",
+                "typing",
                 "videofeatures_clipit",
+                "argparse",
+                "numpy",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.cv.language_guided_video_summarization.summarizer"
         },
         "('MODELS', 'language-score-prediction', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'lineless-table-recognition', 'LoreModel')": {
             "filepath": "TEMPLATE_PATH/models/cv/table_recognition/model_lore.py",
             "imports": [
+                "typing",
+                "copy",
                 "numpy",
-                "math",
-                "os",
                 "torch",
-                "copy",
-                "typing"
+                "math",
+                "os"
             ],
             "module": "modelscope.models.cv.table_recognition.model_lore"
         },
         "('MODELS', 'movie-scene-segmentation', 'resnet50-bert')": {
             "filepath": "TEMPLATE_PATH/models/cv/movie_scene_segmentation/model.py",
             "imports": [
-                "numpy",
-                "math",
+                "typing",
+                "torchvision",
                 "shotdetect_scenedetect_lgss",
-                "einops",
+                "tqdm",
                 "PIL",
+                "math",
+                "numpy",
                 "os",
-                "torch",
-                "tqdm",
-                "typing",
-                "torchvision"
+                "einops",
+                "torch"
             ],
             "module": "modelscope.models.cv.movie_scene_segmentation.model"
         },
         "('MODELS', 'multi-modal-embedding', 'clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/clip/model.py",
             "imports": [
-                "numpy",
+                "typing",
                 "json",
+                "numpy",
                 "os",
                 "torch",
-                "collections",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.models.multi_modal.clip.model"
         },
         "('MODELS', 'multi-modal-similarity', 'team-multi-modal-similarity')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/team/team_model.py",
             "imports": [
-                "numpy",
-                "tokenizers",
-                "PIL",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "tokenizers",
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.team.team_model"
         },
+        "('MODELS', 'multimodal-dialogue', 'mplug-owl')": {
+            "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_owl/modeling_mplug_owl.py",
+            "imports": [
+                "typing",
+                "copy",
+                "logging",
+                "transformers",
+                "dataclasses",
+                "io",
+                "torch",
+                "math",
+                "random",
+                "os"
+            ],
+            "module": "modelscope.models.multi_modal.mplug_owl.modeling_mplug_owl"
+        },
         "('MODELS', 'named-entity-recognition', 'lstm-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/lstm/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.lstm.token_classification"
         },
         "('MODELS', 'named-entity-recognition', 'token-classification-for-ner')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
@@ -4482,21 +4411,21 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'nerf-recon-acc', 'nerf-recon-acc')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_recon_acc.py",
             "imports": [
+                "tqdm",
+                "cv2",
+                "time",
                 "numpy",
                 "glob",
                 "os",
-                "torch",
-                "tqdm",
-                "time",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.nerf_recon_acc.nerf_recon_acc"
         },
         "('MODELS', 'nli', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.text_classification"
@@ -4507,16 +4436,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'nli', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'nli', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -4536,73 +4465,73 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'object-detection-3d', 'depe')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/depe_detect.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.depe_detect"
         },
         "('MODELS', 'ocr-detection', 'OCRDetection')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_detection/model.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.ocr_detection.model"
         },
         "('MODELS', 'ocr-recognition', 'OCRRecognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_recognition/model.py",
             "imports": [
                 "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.ocr_recognition.model"
         },
         "('MODELS', 'ocr-recognition', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'open-vocabulary-detection', 'open-vocabulary-detection-vild')": {
             "filepath": "TEMPLATE_PATH/models/cv/open_vocabulary_detection_vild/vild.py",
             "imports": [
-                "numpy",
                 "tensorflow",
-                "scipy",
+                "typing",
                 "clip",
-                "os",
+                "numpy",
                 "torch",
-                "typing"
+                "scipy",
+                "os"
             ],
             "module": "modelscope.models.cv.open_vocabulary_detection_vild.vild"
         },
         "('MODELS', 'panorama-depth-estimation', 'unifuse-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/panorama_depth_estimation/unifuse_model.py",
             "imports": [
-                "torch",
+                "torchvision",
                 "numpy",
-                "os",
-                "torchvision"
+                "torch",
+                "os"
             ],
             "module": "modelscope.models.cv.panorama_depth_estimation.unifuse_model"
         },
         "('MODELS', 'part-of-speech', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.token_classification"
@@ -4641,74 +4570,74 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'pedestrian-attribute-recognition', 'pedestrian-attribute-recognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/pedestrian_attribute_recognition/model.py",
             "imports": [
-                "torch",
+                "torchvision",
                 "numpy",
-                "os",
-                "torchvision"
+                "torch",
+                "os"
             ],
             "module": "modelscope.models.cv.pedestrian_attribute_recognition.model"
         },
         "('MODELS', 'pointcloud-sceneflow-estimation', 'rcp-sceneflow-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/pointcloud_sceneflow_estimation/rcp_model.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "os"
             ],
             "module": "modelscope.models.cv.pointcloud_sceneflow_estimation.rcp_model"
         },
         "('MODELS', 'product-retrieval-embedding', 'product-retrieval-embedding')": {
             "filepath": "TEMPLATE_PATH/models/cv/product_retrieval_embedding/item_model.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.product_retrieval_embedding.item_model"
         },
         "('MODELS', 'product-segmentation', 'product-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/product_segmentation/seg_infer.py",
             "imports": [
                 "cv2",
-                "torch",
                 "numpy",
+                "torch",
                 "PIL"
             ],
             "module": "modelscope.models.cv.product_segmentation.seg_infer"
         },
         "('MODELS', 'protein-structure', 'unifold')": {
             "filepath": "TEMPLATE_PATH/models/science/unifold/model.py",
             "imports": [
                 "argparse",
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.science.unifold.model"
         },
         "('MODELS', 'punctuation', 'generic-punc')": {
             "filepath": "TEMPLATE_PATH/models/audio/punc/generic_punctuation.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.punc.generic_punctuation"
         },
         "('MODELS', 'referring-video-object-segmentation', 'swinT-referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/referring_video_object_segmentation/model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.referring_video_object_segmentation.model"
         },
         "('MODELS', 'relation-extraction', 'information-extraction')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/information_extraction.py",
             "imports": [
                 "numpy",
@@ -4716,28 +4645,28 @@
             ],
             "module": "modelscope.models.nlp.task_models.information_extraction"
         },
         "('MODELS', 'semantic-segmentation', 'ddpm')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py",
             "imports": [
                 "torch",
-                "typing",
                 "os",
-                "ddpm_guided_diffusion"
+                "ddpm_guided_diffusion",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.ddpm_segmentation_model"
         },
         "('MODELS', 'semantic-segmentation', 'detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/salient_detection/salient_model.py",
             "imports": [
+                "torchvision",
                 "PIL",
+                "cv2",
                 "os",
-                "torch",
-                "torchvision",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.salient_detection.salient_model"
         },
         "('MODELS', 'sentence-embedding', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/sentence_embedding.py",
             "imports": [
                 "torch"
@@ -4755,16 +4684,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'sentence-similarity', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'sentence-similarity', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -4796,16 +4725,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'sentiment-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'sentiment-classification', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -4825,144 +4754,178 @@
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'shop-segmentation', 'shop-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/shop_segmentation/shop_seg_model.py",
             "imports": [
-                "numpy",
-                "PIL",
                 "json",
-                "os",
+                "typing",
+                "PIL",
+                "numpy",
                 "torch",
-                "typing"
+                "os"
             ],
             "module": "modelscope.models.cv.shop_segmentation.shop_seg_model"
         },
         "('MODELS', 'siamese-uie', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/siamese_uie.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.bert.siamese_uie"
         },
         "('MODELS', 'speaker-diarization', 'generic-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.sv.generic_speaker_verification"
         },
+        "('MODELS', 'speaker-diarization', 'scl-sd')": {
+            "filepath": "TEMPLATE_PATH/models/audio/sv/speaker_change_locator.py",
+            "imports": [
+                "typing",
+                "torchaudio",
+                "numpy",
+                "os",
+                "torch",
+                "collections"
+            ],
+            "module": "modelscope.models.audio.sv.speaker_change_locator"
+        },
         "('MODELS', 'speaker-verification', 'cam++-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/DTDNN.py",
             "imports": [
+                "typing",
                 "torchaudio",
                 "os",
                 "torch",
-                "collections",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.models.audio.sv.DTDNN"
         },
         "('MODELS', 'speaker-verification', 'ecapa-tdnn-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/ecapa_tdnn.py",
             "imports": [
+                "typing",
                 "torchaudio",
-                "math",
                 "os",
-                "torch",
-                "typing"
+                "math",
+                "torch"
             ],
             "module": "modelscope.models.audio.sv.ecapa_tdnn"
         },
+        "('MODELS', 'speaker-verification', 'eres2net-sv')": {
+            "filepath": "TEMPLATE_PATH/models/audio/sv/ERes2Net.py",
+            "imports": [
+                "typing",
+                "torchaudio",
+                "os",
+                "math",
+                "torch"
+            ],
+            "module": "modelscope.models.audio.sv.ERes2Net"
+        },
         "('MODELS', 'speaker-verification', 'generic-sv')": {
             "filepath": "TEMPLATE_PATH/models/audio/sv/generic_speaker_verification.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.sv.generic_speaker_verification"
         },
+        "('MODELS', 'speaker-verification', 'rdino_ecapa-tdnn-sv')": {
+            "filepath": "TEMPLATE_PATH/models/audio/sv/rdino.py",
+            "imports": [
+                "typing",
+                "torchaudio",
+                "os",
+                "math",
+                "torch"
+            ],
+            "module": "modelscope.models.audio.sv.rdino"
+        },
         "('MODELS', 'speech-separation', 'speech_mossformer_separation_temporal_8k')": {
             "filepath": "TEMPLATE_PATH/models/audio/separation/mossformer.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "os",
                 "typing"
             ],
             "module": "modelscope.models.audio.separation.mossformer"
         },
         "('MODELS', 'speech-timestamp', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'sudoku', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'table-question-answering', 'space-T-cn')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space_T_cn/table_question_answering.py",
             "imports": [
-                "numpy",
+                "typing",
                 "transformers",
+                "numpy",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.nlp.space_T_cn.table_question_answering"
         },
         "('MODELS', 'table-question-answering', 'space-T-en')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space_T_en/text_to_sql.py",
             "imports": [
-                "text2sql_lgesql",
                 "torch",
-                "typing",
-                "os"
+                "text2sql_lgesql",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space_T_en.text_to_sql"
         },
         "('MODELS', 'task-oriented-conversation', 'space-dst')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_state_tracking.py",
             "imports": [
-                "torch",
                 "transformers",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.space.dialog_state_tracking"
         },
         "('MODELS', 'task-oriented-conversation', 'space-intent')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_intent_prediction.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space.dialog_intent_prediction"
         },
         "('MODELS', 'task-oriented-conversation', 'space-modeling')": {
             "filepath": "TEMPLATE_PATH/models/nlp/space/dialog_modeling.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.space.dialog_modeling"
         },
         "('MODELS', 'text-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.text_classification"
@@ -4973,30 +4936,30 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'text-classification', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'text-classification', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -5017,81 +4980,81 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.text_classification"
         },
         "('MODELS', 'text-classification', 'user-satisfaction-estimation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/use/user_satisfaction_estimation.py",
             "imports": [
-                "numpy",
+                "typing",
                 "transformers",
+                "numpy",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.nlp.use.user_satisfaction_estimation"
         },
         "('MODELS', 'text-classification', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/text_classification.py",
             "imports": [
                 "transformers"
             ],
             "module": "modelscope.models.nlp.veco.text_classification"
         },
         "('MODELS', 'text-driven-segmentation', 'text-driven-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/text_driven_segmentation/lseg_model.py",
             "imports": [
-                "numpy",
-                "PIL",
                 "json",
-                "os",
+                "typing",
+                "PIL",
+                "numpy",
                 "torch",
-                "typing"
+                "os"
             ],
             "module": "modelscope.models.cv.text_driven_segmentation.lseg_model"
         },
         "('MODELS', 'text-error-correction', 'bart')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bart/text_error_correction.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.nlp.bart.text_error_correction"
         },
         "('MODELS', 'text-generation', 'glm130b')": {
             "filepath": "TEMPLATE_PATH/models/nlp/glm_130b/text_generation.py",
             "imports": [
-                "stat",
-                "re",
-                "sys",
+                "functools",
                 "SwissArmyTransformer",
+                "sys",
+                "typing",
+                "copy",
+                "stat",
                 "time",
-                "os",
                 "torch",
                 "random",
-                "copy",
-                "functools",
-                "typing"
+                "os",
+                "re"
             ],
             "module": "modelscope.models.nlp.glm_130b.text_generation"
         },
         "('MODELS', 'text-generation', 'gpt-moe')": {
             "filepath": "TEMPLATE_PATH/models/nlp/gpt_moe/text_generation.py",
             "imports": [
-                "typing",
-                "transformers"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.models.nlp.gpt_moe.text_generation"
         },
         "('MODELS', 'text-generation', 'gpt3')": {
             "filepath": "TEMPLATE_PATH/models/nlp/gpt3/text_generation.py",
             "imports": [
+                "transformers",
                 "torch",
                 "collections",
-                "transformers",
                 "typing"
             ],
             "module": "modelscope.models.nlp.gpt3.text_generation"
         },
         "('MODELS', 'text-generation', 'llama')": {
             "filepath": "TEMPLATE_PATH/models/nlp/llama/text_generation.py",
             "imports": [
@@ -5099,34 +5062,34 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.llama.text_generation"
         },
         "('MODELS', 'text-generation', 'palm-v2')": {
             "filepath": "TEMPLATE_PATH/models/nlp/palm_v2/text_generation.py",
             "imports": [
-                "numpy",
-                "typing",
-                "codecs",
-                "math",
+                "subprocess",
+                "copy",
                 "json",
-                "dataclasses",
+                "typing",
                 "transformers",
-                "os",
+                "dataclasses",
+                "numpy",
                 "torch",
-                "copy",
-                "subprocess"
+                "codecs",
+                "math",
+                "os"
             ],
             "module": "modelscope.models.nlp.palm_v2.text_generation"
         },
         "('MODELS', 'text-generation', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/text_generation.py",
             "imports": [
-                "torch",
                 "numpy",
                 "transformers",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.text_generation"
         },
         "('MODELS', 'text-ranking', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/text_ranking.py",
             "imports": [],
@@ -5146,125 +5109,125 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.text_ranking"
         },
         "('MODELS', 'text-summarization', 'mglm')": {
             "filepath": "TEMPLATE_PATH/models/nlp/mglm/mglm_for_text_summarization.py",
             "imports": [
-                "megatron_util",
+                "typing",
                 "numpy",
-                "os",
                 "torch",
+                "os",
                 "random",
-                "typing"
+                "megatron_util"
             ],
             "module": "modelscope.models.nlp.mglm.mglm_for_text_summarization"
         },
         "('MODELS', 'text-summarization', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text-to-image-synthesis', 'diffusion-text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/diffusion/model.py",
             "imports": [
-                "numpy",
                 "json",
+                "typing",
+                "numpy",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.diffusion.model"
         },
         "('MODELS', 'text-to-image-synthesis', 'multi-stage-diffusion-text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/multi_stage_diffusion/model.py",
             "imports": [
-                "numpy",
-                "math",
-                "PIL",
                 "json",
+                "typing",
+                "PIL",
+                "numpy",
                 "os",
-                "torch",
-                "typing"
+                "math",
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.multi_stage_diffusion.model"
         },
         "('MODELS', 'text-to-image-synthesis', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_text_to_image_synthesis_model.py",
             "imports": [
+                "typing",
+                "json",
+                "torchvision",
+                "PIL",
                 "numpy",
                 "taming",
-                "PIL",
-                "pkg_resources",
-                "json",
                 "os",
-                "torch",
-                "typing",
-                "torchvision"
+                "pkg_resources",
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_text_to_image_synthesis_model"
         },
         "('MODELS', 'text-to-speech', 'sambert-hifigan')": {
             "filepath": "TEMPLATE_PATH/models/audio/tts/sambert_hifi.py",
             "imports": [
-                "numpy",
-                "yaml",
-                "shutil",
+                "datetime",
                 "zipfile",
+                "wave",
+                "shutil",
                 "json",
-                "datetime",
-                "os",
-                "matplotlib",
                 "__future__",
-                "wave"
+                "numpy",
+                "matplotlib",
+                "yaml",
+                "os"
             ],
             "module": "modelscope.models.audio.tts.sambert_hifi"
         },
         "('MODELS', 'text-to-video-synthesis', 'latent-text-to-video-synthesis')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/video_synthesis/text_to_video_synthesis_model.py",
             "imports": [
-                "einops",
-                "os",
-                "torch",
+                "typing",
                 "open_clip",
-                "typing"
+                "os",
+                "einops",
+                "torch"
             ],
             "module": "modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model"
         },
         "('MODELS', 'text2sql', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'text2text-generation', 'T5')": {
             "filepath": "TEMPLATE_PATH/models/nlp/T5/text2text_generation.py",
             "imports": [
+                "typing",
+                "copy",
                 "transformers",
-                "torch",
                 "warnings",
-                "copy",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.models.nlp.T5.text2text_generation"
         },
         "('MODELS', 'token-classification', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/token_classification.py",
             "imports": [],
             "module": "modelscope.models.nlp.bert.token_classification"
@@ -5311,16 +5274,16 @@
                 "typing"
             ],
             "module": "modelscope.models.nlp.task_models.token_classification"
         },
         "('MODELS', 'token-classification', 'veco')": {
             "filepath": "TEMPLATE_PATH/models/nlp/veco/token_classification.py",
             "imports": [
-                "torch",
-                "transformers"
+                "transformers",
+                "torch"
             ],
             "module": "modelscope.models.nlp.veco.token_classification"
         },
         "('MODELS', 'transformer-crf', 'transformer-crf')": {
             "filepath": "TEMPLATE_PATH/models/nlp/task_models/token_classification.py",
             "imports": [
                 "torch",
@@ -5335,210 +5298,210 @@
                 "math",
                 "collections",
                 "typing"
             ],
             "module": "modelscope.models.nlp.csanmt.translation"
         },
         "('MODELS', 'translation-evaluation', 'unite')": {
-            "filepath": "TEMPLATE_PATH/models/nlp/unite/modeling_unite.py",
+            "filepath": "TEMPLATE_PATH/models/nlp/unite/translation_evaluation.py",
             "imports": [
-                "packaging",
-                "numpy",
-                "math",
-                "dataclasses",
+                "typing",
                 "transformers",
-                "torch",
+                "dataclasses",
+                "numpy",
+                "packaging",
                 "warnings",
-                "typing"
+                "math",
+                "torch"
             ],
-            "module": "modelscope.models.nlp.unite.modeling_unite"
+            "module": "modelscope.models.nlp.unite.translation_evaluation"
         },
         "('MODELS', 'video-captioning', 'hitea')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'video-deinterlace', 'video-deinterlace')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_deinterlace/UNet_for_video_deinterlace.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "os",
                 "typing"
             ],
             "module": "modelscope.models.cv.video_deinterlace.UNet_for_video_deinterlace"
         },
         "('MODELS', 'video-depth-estimation', 'dro-resnet18-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_depth_estimation/dro_model.py",
             "imports": [
-                "numpy",
+                "tqdm",
+                "cv2",
                 "glob",
-                "os",
+                "numpy",
                 "torch",
-                "tqdm",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.models.cv.video_depth_estimation.dro_model"
         },
         "('MODELS', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_frame_interpolation/VFINet_for_video_frame_interpolation.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "os",
                 "typing"
             ],
             "module": "modelscope.models.cv.video_frame_interpolation.VFINet_for_video_frame_interpolation"
         },
         "('MODELS', 'video-human-matting', 'video-human-matting')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_human_matting/model.py",
             "imports": [
+                "typing",
+                "torchvision",
                 "numpy",
                 "os",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.models.cv.video_human_matting.model"
         },
         "('MODELS', 'video-inpainting', 'video-inpainting')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_inpainting/inpainting_model.py",
             "imports": [
+                "torchvision",
                 "math",
-                "torch",
                 "numpy",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.models.cv.video_inpainting.inpainting_model"
         },
         "('MODELS', 'video-instance-segmentation', 'swinb-video-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/video_knet.py",
             "imports": [
-                "mmdet",
-                "torch"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.video_knet"
         },
         "('MODELS', 'video-multi-modal-embedding', 'video-clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mmr/models/clip_for_mm_video_embedding.py",
             "imports": [
-                "numpy",
-                "uuid",
-                "decord",
+                "json",
+                "typing",
                 "urllib",
                 "PIL",
-                "tempfile",
-                "json",
-                "os",
+                "numpy",
                 "torch",
+                "uuid",
                 "random",
-                "typing"
+                "tempfile",
+                "os",
+                "decord"
             ],
             "module": "modelscope.models.multi_modal.mmr.models.clip_for_mm_video_embedding"
         },
         "('MODELS', 'video-object-detection', 'longshortnet')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_streaming_perception/longshortnet/longshortnet.py",
             "imports": [
-                "numpy",
-                "argparse",
                 "json",
-                "os",
                 "logging",
-                "torch",
                 "tqdm",
+                "argparse",
                 "time",
-                "cv2"
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.models.cv.video_streaming_perception.longshortnet.longshortnet"
         },
         "('MODELS', 'video-object-detection', 'realtime-video-object-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/stream_yolo/realtime_video_detector.py",
             "imports": [
-                "numpy",
-                "argparse",
                 "json",
-                "os",
                 "logging",
-                "torch",
                 "tqdm",
+                "argparse",
                 "time",
-                "cv2"
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.models.cv.stream_yolo.realtime_video_detector"
         },
         "('MODELS', 'video-object-segmentation', 'video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_object_segmentation/model.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_object_segmentation.model"
         },
         "('MODELS', 'video-panoptic-segmentation', 'swinb-video-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/video_k_net.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.video_k_net"
         },
         "('MODELS', 'video-question-answering', 'hitea')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_stabilization/DUTRAFTStabilizer.py",
             "imports": [
-                "numpy",
+                "sys",
                 "typing",
+                "cv2",
+                "numpy",
+                "torch",
                 "math",
                 "tempfile",
-                "os",
-                "torch",
-                "sys",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.models.cv.video_stabilization.DUTRAFTStabilizer"
         },
         "('MODELS', 'video-summarization', 'pgl-video-summarization')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_summarization/summarizer.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_summarization.summarizer"
         },
         "('MODELS', 'video-super-resolution', 'msrresnet-lite')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_super_resolution/msrresnet_lite_model.py",
             "imports": [
-                "torch",
                 "functools",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_super_resolution.msrresnet_lite_model"
         },
         "('MODELS', 'video-super-resolution', 'real-basicvsr')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_super_resolution/real_basicvsr_for_video_super_resolution.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.cv.video_super_resolution.real_basicvsr_for_video_super_resolution"
         },
         "('MODELS', 'video-temporal-grounding', 'soonet')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/soonet/model.py",
             "imports": [
                 "torch",
@@ -5569,66 +5532,66 @@
                 "typing"
             ],
             "module": "modelscope.models.cv.vision_efficient_tuning.model"
         },
         "('MODELS', 'visual-entailment', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'visual-grounding', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'visual-question-answering', 'mplug')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/mplug_for_all_tasks.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.multi_modal.mplug_for_all_tasks"
         },
         "('MODELS', 'visual-question-answering', 'ofa')": {
             "filepath": "TEMPLATE_PATH/models/multi_modal/ofa_for_all_tasks.py",
             "imports": [
-                "re",
-                "string",
-                "math",
+                "functools",
+                "typing",
                 "json",
+                "string",
                 "os",
+                "math",
                 "torch",
-                "typing",
-                "functools"
+                "re"
             ],
             "module": "modelscope.models.multi_modal.ofa_for_all_tasks"
         },
         "('MODELS', 'voice-activity-detection', 'generic-asr')": {
             "filepath": "TEMPLATE_PATH/models/audio/asr/generic_automatic_speech_recognition.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.models.audio.asr.generic_automatic_speech_recognition"
         },
         "('MODELS', 'word-alignment', 'bert')": {
             "filepath": "TEMPLATE_PATH/models/nlp/bert/word_alignment.py",
             "imports": [
                 "torch"
@@ -5691,16 +5654,16 @@
                 "torch"
             ],
             "module": "modelscope.models.multi_modal.mgeo.text_classification"
         },
         "('MODELS', 'zero-shot-classification', 'peer')": {
             "filepath": "TEMPLATE_PATH/models/nlp/peer/text_classification.py",
             "imports": [
-                "torch",
-                "copy"
+                "copy",
+                "torch"
             ],
             "module": "modelscope.models.nlp.peer.text_classification"
         },
         "('MODELS', 'zero-shot-classification', 'plug-mental')": {
             "filepath": "TEMPLATE_PATH/models/nlp/plug_mental/text_classification.py",
             "imports": [
                 "torch"
@@ -5713,154 +5676,154 @@
                 "torch"
             ],
             "module": "modelscope.models.nlp.structbert.text_classification"
         },
         "('NECKS', 'default', 'CPFPN')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/necks/cp_fpn.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.necks.cp_fpn"
         },
         "('NECKS', 'default', 'FPNF')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection/mmdet_ms/necks/fpn.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection.mmdet_ms.necks.fpn"
         },
         "('NECKS', 'default', 'MSDeformAttnPixelDecoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/neck/msdeformattn_decoder.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.neck.msdeformattn_decoder"
         },
         "('NECKS', 'default', 'SemanticFPNWrapper')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/head/semantic_fpn_wrapper.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.head.semantic_fpn_wrapper"
         },
         "('OPTIMIZERS', 'default', 'ChildTuningAdamW')": {
             "filepath": "TEMPLATE_PATH/trainers/optimizer/child_tuning_adamw_optimizer.py",
             "imports": [
+                "typing",
                 "types",
                 "numpy",
                 "math",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.trainers.optimizer.child_tuning_adamw_optimizer"
         },
         "('PARALLEL', 'default', 'DistributedDataParallel')": {
             "filepath": "TEMPLATE_PATH/trainers/parallel/builder.py",
             "imports": [
                 "torch"
             ],
             "module": "modelscope.trainers.parallel.builder"
         },
         "('PIPELINES', 'acoustic-echo-cancellation', 'speech-dfsmn-aec-psm-16k')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/linear_aec_pipeline.py",
             "imports": [
+                "typing",
+                "importlib",
                 "numpy",
+                "os",
                 "yaml",
                 "scipy",
-                "importlib",
-                "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.pipelines.audio.linear_aec_pipeline"
         },
         "('PIPELINES', 'acoustic-noise-suppression', 'speech_dfsmn_ans_psm_48k_causal')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/ans_dfsmn_pipeline.py",
             "imports": [
-                "numpy",
-                "io",
+                "sys",
+                "typing",
                 "librosa",
                 "soundfile",
-                "os",
+                "io",
+                "numpy",
                 "torch",
-                "collections",
-                "sys",
-                "typing"
+                "os",
+                "collections"
             ],
             "module": "modelscope.pipelines.audio.ans_dfsmn_pipeline"
         },
         "('PIPELINES', 'acoustic-noise-suppression', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/ans_pipeline.py",
             "imports": [
-                "numpy",
-                "io",
-                "soundfile",
+                "typing",
                 "librosa",
-                "torch",
-                "typing"
+                "soundfile",
+                "io",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.audio.ans_pipeline"
         },
         "('PIPELINES', 'action-detection', 'ResNetC3D-action-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_detection_pipeline.py",
             "imports": [
                 "math",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.action_detection_pipeline"
         },
         "('PIPELINES', 'action-recognition', 'TAdaConv_action-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py",
             "imports": [
                 "math",
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.action_recognition_pipeline"
         },
         "('PIPELINES', 'action-recognition', 'patchshift-action-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/action_recognition_pipeline.py",
             "imports": [
                 "math",
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.action_recognition_pipeline"
         },
         "('PIPELINES', 'animal-recognition', 'resnet101-animal-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/animal_recognition_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.animal_recognition_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'asr-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/asr_inference_pipeline.py",
             "imports": [
                 "yaml",
-                "typing",
+                "json",
                 "os",
-                "json"
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.asr_inference_pipeline"
         },
         "('PIPELINES', 'auto-speech-recognition', 'asr-wenet-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/asr_wenet_inference_pipeline.py",
             "imports": [
                 "typing"
@@ -5874,46 +5837,46 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.asr_pipeline"
         },
         "('PIPELINES', 'bad-image-detecting', 'bad-image-detecting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/bad_image_detecting_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.bad_image_detecting_pipeline"
         },
         "('PIPELINES', 'body-2d-keypoints', 'hrnetv2w32_body-2d-keypoints_image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/body_2d_keypoints_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
                 "json",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.body_2d_keypoints_pipeline"
         },
         "('PIPELINES', 'body-3d-keypoints', 'canonical_body-3d-keypoints_video')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/body_3d_keypoints_pipeline.py",
             "imports": [
-                "numpy",
-                "tempfile",
                 "datetime",
-                "os",
                 "mpl_toolkits",
+                "typing",
+                "cv2",
+                "numpy",
                 "torch",
                 "matplotlib",
-                "typing",
-                "cv2"
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.body_3d_keypoints_pipeline"
         },
         "('PIPELINES', 'card-detection', 'resnet-card-detection-scrfd34gkps')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/card_detection_pipeline.py",
             "imports": [
                 "typing"
@@ -5933,207 +5896,206 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.codegeex_code_translation_pipeline"
         },
         "('PIPELINES', 'competency-aware-translation', 'canmt-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/canmt_translation_pipeline.py",
             "imports": [
-                "torch",
                 "sacremoses",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.canmt_translation_pipeline"
         },
         "('PIPELINES', 'controllable-image-generation', 'controllable-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/controllable_image_generation_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "subprocess",
+                "typing",
                 "cv2",
                 "glob",
-                "tempfile",
-                "os",
+                "numpy",
                 "torch",
-                "typing",
-                "subprocess"
+                "math",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.controllable_image_generation_pipeline"
         },
         "('PIPELINES', 'crowd-counting', 'hrnet-crowd-counting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/crowd_counting_pipeline.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "PIL",
                 "numpy",
                 "math",
-                "PIL",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.crowd_counting_pipeline"
         },
         "('PIPELINES', 'default', 'DefaultFormatBundleV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/formating.py",
             "imports": [
-                "mmdet",
-                "torch",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.formating"
         },
         "('PIPELINES', 'default', 'LoadAnnotationsV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/loading.py",
             "imports": [
-                "mmdet",
                 "pycocotools",
                 "numpy",
-                "os"
+                "os",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.loading"
         },
         "('PIPELINES', 'default', 'LoadMultiViewImageFromMultiSweepsFiles')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/loading.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.loading"
         },
         "('PIPELINES', 'default', 'NormalizeMultiviewImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
-                "mmdet",
                 "mmdet3d",
-                "numpy",
                 "mmcv",
+                "numpy",
                 "torch",
                 "copy",
-                "PIL"
+                "PIL",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'PadMultiViewImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
-                "mmdet",
                 "mmdet3d",
-                "numpy",
                 "mmcv",
+                "numpy",
                 "torch",
                 "copy",
-                "PIL"
+                "PIL",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'RandomFlipV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'RandomSquareCrop')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'ResizeCropFlipImage')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/datasets/pipelines/transform_3d.py",
             "imports": [
-                "mmdet",
                 "mmdet3d",
-                "numpy",
                 "mmcv",
+                "numpy",
                 "torch",
                 "copy",
-                "PIL"
+                "PIL",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d"
         },
         "('PIPELINES', 'default', 'ResizeToMultiple')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_semantic_segmentation/vit_adapter/utils/data_process_func.py",
             "imports": [
-                "mmdet",
-                "mmcv"
+                "mmcv",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.data_process_func"
         },
         "('PIPELINES', 'default', 'ResizeV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/transforms.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "numpy",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms"
         },
         "('PIPELINES', 'default', 'RotateV2')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/mmdet_patch/datasets/pipelines/auto_augment.py",
             "imports": [
-                "mmdet",
-                "numpy",
                 "mmcv",
+                "cv2",
+                "numpy",
                 "copy",
-                "cv2"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.auto_augment"
         },
         "('PIPELINES', 'document-grounded-dialog-generate', 'document-grounded-dialog-generate')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_generate_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_generate_pipeline"
         },
         "('PIPELINES', 'document-grounded-dialog-rerank', 'document-grounded-dialog-rerank')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_rerank_pipeline.py",
             "imports": [
-                "numpy",
-                "re",
+                "ujson",
+                "sys",
                 "pprint",
-                "time",
+                "typing",
                 "transformers",
-                "os",
-                "ujson",
-                "collections",
+                "time",
+                "numpy",
                 "torch",
                 "random",
-                "sys",
-                "typing"
+                "os",
+                "collections",
+                "re"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_rerank_pipeline"
         },
         "('PIPELINES', 'document-grounded-dialog-retrieval', 'document-grounded-dialog-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_grounded_dialog_retrieval_pipeline.py",
             "imports": [
-                "numpy",
-                "faiss",
                 "json",
-                "os",
-                "typing"
+                "typing",
+                "faiss",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.document_grounded_dialog_retrieval_pipeline"
         },
         "('PIPELINES', 'document-segmentation', 'document-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/document_segmentation_pipeline.py",
             "imports": [
+                "typing",
                 "numpy",
-                "re",
-                "transformers",
                 "datasets",
                 "torch",
-                "typing"
+                "re"
             ],
             "module": "modelscope.pipelines.nlp.document_segmentation_pipeline"
         },
         "('PIPELINES', 'document-vl-embedding', 'document-vl-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/document_vl_embedding_pipeline.py",
             "imports": [
                 "torch",
@@ -6144,130 +6106,112 @@
         "('PIPELINES', 'domain-specific-object-detection', 'tinynas-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.tinynas_detection_pipeline"
         },
-        "('PIPELINES', 'domain-specific-object-detection', 'yolox-pai_hand-detection')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py",
-            "imports": [
-                "typing"
-            ],
-            "module": "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline"
-        },
         "('PIPELINES', 'efficient-diffusion-tuning', 'efficient-diffusion-tuning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.multi_modal.efficient_diffusion_tuning_pipeline"
         },
         "('PIPELINES', 'extractive-summarization', 'extractive-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/extractive_summarization_pipeline.py",
             "imports": [
+                "typing",
                 "numpy",
-                "re",
                 "datasets",
                 "torch",
-                "typing"
+                "re"
             ],
             "module": "modelscope.pipelines.nlp.extractive_summarization_pipeline"
         },
         "('PIPELINES', 'face-2d-keypoints', 'manual-facial-landmark-confidence-flcm')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/facial_landmark_confidence_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.facial_landmark_confidence_pipeline"
         },
-        "('PIPELINES', 'face-2d-keypoints', 'mobilenet_face-2d-keypoints_alignment')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py",
-            "imports": [
-                "numpy",
-                "typing",
-                "math",
-                "copy",
-                "cv2"
-            ],
-            "module": "modelscope.pipelines.cv.easycv_pipelines.face_2d_keypoints_pipeline"
-        },
         "('PIPELINES', 'face-attribute-recognition', 'resnet34-face-attribute-recognition-fairface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_attribute_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.face_attribute_recognition_pipeline"
         },
         "('PIPELINES', 'face-detection', 'manual-face-detection-mtcnn')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mtcnn_face_detection_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.mtcnn_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'manual-face-detection-ulfd')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ulfd_face_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.ulfd_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet-face-detection-scrfd10gkps')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet101-face-detection-cvpr22papermogface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mog_face_detection_pipeline.py",
             "imports": [
                 "numpy",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.mog_face_detection_pipeline"
         },
         "('PIPELINES', 'face-detection', 'resnet50-face-detection-retinaface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/retina_face_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.retina_face_detection_pipeline"
         },
         "('PIPELINES', 'face-emotion', 'face-emotion')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_emotion_pipeline.py",
             "imports": [
                 "numpy",
@@ -6282,187 +6226,187 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.face_human_hand_detection_pipeline"
         },
         "('PIPELINES', 'face-image-generation', 'gan-face-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_image_generation_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.face_image_generation_pipeline"
         },
         "('PIPELINES', 'face-liveness', 'manual-face-liveness-flir')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_liveness_ir_pipeline.py",
             "imports": [
-                "numpy",
-                "onnxruntime",
+                "typing",
                 "PIL",
+                "onnxruntime",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.face_liveness_ir_pipeline"
         },
         "('PIPELINES', 'face-liveness', 'manual-face-liveness-flxc')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_liveness_xc_pipeline.py",
             "imports": [
-                "numpy",
-                "onnxruntime",
+                "typing",
                 "PIL",
+                "onnxruntime",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.face_liveness_xc_pipeline"
         },
         "('PIPELINES', 'face-quality-assessment', 'manual-face-quality-assessment-fqa')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_quality_assessment_pipeline.py",
             "imports": [
-                "numpy",
-                "onnxruntime",
+                "typing",
                 "PIL",
+                "onnxruntime",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.face_quality_assessment_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir-face-recognition-rts')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_ood_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_ood_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir101-face-recognition-cfglint')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'ir50-face-recognition-arcface')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/arc_face_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.arc_face_recognition_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'manual-face-recognition-frfm')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_fm_pipeline.py",
             "imports": [
-                "numpy",
-                "onnxruntime",
+                "typing",
                 "PIL",
+                "onnxruntime",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_onnx_fm_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'manual-face-recognition-frir')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_recognition_onnx_ir_pipeline.py",
             "imports": [
-                "numpy",
-                "onnxruntime",
+                "typing",
                 "PIL",
+                "onnxruntime",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.face_recognition_onnx_ir_pipeline"
         },
         "('PIPELINES', 'face-recognition', 'resnet-face-recognition-facemask')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mask_face_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "collections",
-                "typing",
-                "cv2"
+                "os",
+                "collections"
             ],
             "module": "modelscope.pipelines.cv.mask_face_recognition_pipeline"
         },
         "('PIPELINES', 'face-reconstruction', 'resnet50-face-reconstruction')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/face_reconstruction_pipeline.py",
             "imports": [
-                "numpy",
+                "tensorflow",
                 "face_alignment",
-                "io",
                 "shutil",
+                "typing",
                 "PIL",
-                "tensorflow",
-                "scipy",
-                "os",
+                "cv2",
+                "io",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "scipy",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.face_reconstruction_pipeline"
         },
         "('PIPELINES', 'facial-expression-recognition', 'vgg19-facial-expression-recognition-fer')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/facial_expression_recognition_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.facial_expression_recognition_pipeline"
         },
         "('PIPELINES', 'faq-question-answering', 'faq-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/faq_question_answering_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.faq_question_answering_pipeline"
         },
         "('PIPELINES', 'feature-extraction', 'feature-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/feature_extraction_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.feature_extraction_pipeline"
         },
         "('PIPELINES', 'fid-dialogue', 'fid-dialogue')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fid_dialogue_pipeline.py",
             "imports": [
-                "torch",
                 "re",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.fid_dialogue_pipeline"
         },
         "('PIPELINES', 'fill-mask', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fill_mask_pipeline.py",
             "imports": [
@@ -6478,38 +6422,31 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.fill_mask_pipeline"
         },
         "('PIPELINES', 'general-recognition', 'resnet101-general-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/general_recognition_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.general_recognition_pipeline"
         },
         "('PIPELINES', 'generative-multi-modal-embedding', 'generative-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.generative_multi_modal_embedding_pipeline"
         },
-        "('PIPELINES', 'hand-2d-keypoints', 'hrnetv2w18_hand-2d-keypoints_image')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/hand_2d_keypoints_pipeline.py",
-            "imports": [
-                "os"
-            ],
-            "module": "modelscope.pipelines.cv.hand_2d_keypoints_pipeline"
-        },
         "('PIPELINES', 'hand-static', 'hand-static')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/hand_static_pipeline.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.hand_static_pipeline"
@@ -6521,609 +6458,568 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
         "('PIPELINES', 'human-reconstruction', 'human-reconstruction')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/human_reconstruction_pipeline.py",
             "imports": [
-                "numpy",
                 "shutil",
+                "typing",
                 "trimesh",
-                "os",
+                "numpy",
                 "torch",
-                "typing"
-            ],
-            "module": "modelscope.pipelines.cv.human_reconstruction_pipeline"
-        },
-        "('PIPELINES', 'human-wholebody-keypoint', 'hrnetw48_human-wholebody-keypoint_image')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py",
-            "imports": [
-                "typing",
                 "os"
             ],
-            "module": "modelscope.pipelines.cv.easycv_pipelines.human_wholebody_keypoint_pipeline"
+            "module": "modelscope.pipelines.cv.human_reconstruction_pipeline"
         },
         "('PIPELINES', 'image-body-reshaping', 'flow-based-body-reshaping')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_body_reshaping_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_body_reshaping_pipeline"
         },
         "('PIPELINES', 'image-captioning', 'image-captioning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/image_captioning_pipeline.py",
             "imports": [
+                "numpy",
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.image_captioning_pipeline"
         },
         "('PIPELINES', 'image-classification', 'bnext-small_image-classification_ImageNet-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'common-image-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'convnext-base_image-classification_garbage')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'easyrobust-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'image-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'image-structured-model-probing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_structured_model_probing_pipeline.py",
             "imports": [
-                "numpy",
                 "mmcv",
-                "math",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "numpy",
+                "os",
+                "math",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_structured_model_probing_pipeline"
         },
         "('PIPELINES', 'image-classification', 'nextvit-small_image-classification_Dailylife-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'resnet50-image-classification-cc')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/content_check_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.content_check_pipeline"
         },
         "('PIPELINES', 'image-classification', 'tinynas-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_classification_pipeline.py",
             "imports": [
-                "math",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "os",
+                "math",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.tinynas_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'vit-base_image-classification_Dailylife-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-classification', 'vit-base_image-classification_ImageNet-labels')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_classification_pipeline"
         },
         "('PIPELINES', 'image-color-enhancement', 'adaint-image-color-enhance')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-color-enhancement', 'csrnet-image-color-enhance')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-color-enhancement', 'deeplpf-image-color-enhance')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_color_enhance_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_color_enhance_pipeline"
         },
         "('PIPELINES', 'image-colorization', 'ddcolor-image-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ddcolor_image_colorization_pipeline.py",
             "imports": [
-                "numpy",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.ddcolor_image_colorization_pipeline"
         },
         "('PIPELINES', 'image-colorization', 'unet-image-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_colorization_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_colorization_pipeline"
         },
         "('PIPELINES', 'image-debanding', 'rrdb-image-debanding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_debanding_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_debanding_pipeline"
         },
         "('PIPELINES', 'image-deblurring', 'nafnet-image-deblur')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_deblur_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_deblur_pipeline"
         },
         "('PIPELINES', 'image-demoireing', 'uhdm-image-demoireing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_restoration_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_restoration_pipeline"
         },
         "('PIPELINES', 'image-denoising', 'nafnet-image-denoise')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_denoise_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_denoise_pipeline"
         },
         "('PIPELINES', 'image-depth-estimation', 'image-bts-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_bts_depth_estimation_pipeline.py",
             "imports": [
-                "numpy",
-                "albumentations",
-                "torch",
                 "typing",
-                "cv2"
+                "albumentations",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_bts_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-depth-estimation', 'image-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_depth_estimation_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-driving-perception', 'yolopv2_image-driving-percetion_bdd100k')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_driving_perception_pipeline.py",
             "imports": [
+                "cv2",
                 "numpy",
-                "typing",
                 "os",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_driving_perception_pipeline"
         },
         "('PIPELINES', 'image-face-fusion', 'image-face-fusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_face_fusion_pipeline.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_face_fusion_pipeline"
         },
         "('PIPELINES', 'image-fewshot-detection', 'image-fewshot-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_defrcn_fewshot_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_defrcn_fewshot_pipeline"
         },
         "('PIPELINES', 'image-inpainting', 'fft-inpainting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_inpainting_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_inpainting_pipeline"
         },
         "('PIPELINES', 'image-inpainting', 'image-inpainting-sdv2')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_inpainting_sdv2_pipeline.py",
             "imports": [
-                "numpy",
+                "sys",
                 "typing",
-                "math",
-                "tempfile",
-                "os",
                 "diffusers",
+                "cv2",
+                "numpy",
                 "torch",
-                "sys",
-                "cv2"
+                "math",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_inpainting_sdv2_pipeline"
         },
         "('PIPELINES', 'image-matching', 'image-matching')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matching_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_matching_pipeline"
         },
         "('PIPELINES', 'image-multi-view-depth-estimation', 'image-multi-view-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_mvs_depth_estimation_pipeline.py",
             "imports": [
                 "tempfile",
                 "shutil",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_mvs_depth_estimation_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'abnormal-object-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
-        "('PIPELINES', 'image-object-detection', 'easycv-detection')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py",
-            "imports": [
-                "typing"
-            ],
-            "module": "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline"
-        },
         "('PIPELINES', 'image-object-detection', 'tbs-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tbs_detection_pipeline.py",
             "imports": [
-                "numpy",
-                "colorsys",
+                "typing",
                 "PIL",
+                "cv2",
+                "colorsys",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.tbs_detection_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'tinynas-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/tinynas_detection_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.tinynas_detection_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'vidt')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vidt_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.vidt_pipeline"
         },
         "('PIPELINES', 'image-object-detection', 'vit-object-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_detection_pipeline.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_detection_pipeline"
         },
-        "('PIPELINES', 'image-object-detection', 'yolox_image-object-detection-auto')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/detection_pipeline.py",
-            "imports": [
-                "typing"
-            ],
-            "module": "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline"
-        },
         "('PIPELINES', 'image-paintbyexample', 'stablediffusion-paintbyexample')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_paintbyexample_pipeline.py",
             "imports": [
-                "numpy",
-                "einops",
-                "PIL",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "einops",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_paintbyexample_pipeline"
         },
         "('PIPELINES', 'image-portrait-enhancement', 'gpen-image-portrait-enhancement')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_portrait_enhancement_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "typing",
                 "PIL",
+                "cv2",
+                "numpy",
                 "scipy",
-                "torch",
-                "typing",
-                "cv2"
+                "math",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_portrait_enhancement_pipeline"
         },
         "('PIPELINES', 'image-portrait-stylization', 'unet-person-image-cartoon')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_cartoon_pipeline.py",
             "imports": [
-                "numpy",
                 "tensorflow",
-                "os",
                 "typing",
-                "cv2"
+                "cv2",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_cartoon_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-degradation', 'image-quality-assessment-degradation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_degradation_pipeline.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "cv2",
                 "numpy",
                 "math",
                 "tempfile",
-                "torch",
-                "typing",
-                "torchvision",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_degradation_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-mos', 'image-quality-assessment-man')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_man_pipeline.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "cv2",
                 "numpy",
                 "math",
                 "tempfile",
-                "torch",
-                "typing",
-                "torchvision",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_man_pipeline"
         },
         "('PIPELINES', 'image-quality-assessment-mos', 'image-quality-assessment-mos')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_quality_assessment_mos_pipeline.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "cv2",
                 "numpy",
                 "math",
                 "tempfile",
-                "torch",
-                "typing",
-                "torchvision",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_quality_assessment_mos_pipeline"
         },
         "('PIPELINES', 'image-reid-person', 'passvitb-image-reid-person')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_reid_person_pipeline.py",
             "imports": [
-                "math",
+                "typing",
+                "torchvision",
                 "PIL",
-                "os",
                 "torch",
-                "typing",
-                "torchvision"
+                "math",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_reid_person_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'cascade-mask-rcnn-swin-image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_instance_segmentation_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
-                "cv2"
-            ],
-            "module": "modelscope.pipelines.cv.image_instance_segmentation_pipeline"
-        },
-        "('PIPELINES', 'image-segmentation', 'easycv-segmentation')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/easycv_pipelines/segmentation_pipeline.py",
-            "imports": [
-                "numpy",
-                "typing"
-            ],
-            "module": "modelscope.pipelines.cv.easycv_pipelines.segmentation_pipeline"
-        },
-        "('PIPELINES', 'image-segmentation', 'image-panoptic-segmentation')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py",
-            "imports": [
-                "numpy",
                 "PIL",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
-            "module": "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline"
+            "module": "modelscope.pipelines.cv.image_instance_segmentation_pipeline"
         },
-        "('PIPELINES', 'image-segmentation', 'image-panoptic-segmentation-easycv')": {
-            "filepath": "TEMPLATE_PATH/pipelines/cv/image_panoptic_segmentation_pipeline.py",
+        "('PIPELINES', 'image-segmentation', 'fast-instance-segmentation')": {
+            "filepath": "TEMPLATE_PATH/pipelines/cv/fast_instance_segmentation_pipeline.py",
             "imports": [
+                "torchvision",
                 "numpy",
-                "PIL",
                 "torch",
-                "typing",
-                "cv2"
+                "typing"
             ],
-            "module": "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline"
+            "module": "modelscope.pipelines.cv.fast_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'image-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_semantic_segmentation_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_semantic_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'm2fp-image-human-parsing')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_human_parsing_pipeline.py",
             "imports": [
-                "torch",
+                "torchvision",
                 "numpy",
-                "typing",
-                "torchvision"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_human_parsing_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'maskdino-swin-image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/maskdino_instance_segmentation_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.maskdino_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'image-segmentation', 'vision-middleware-multi-task')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vision_middleware_pipeline.py",
             "imports": [
-                "numpy",
                 "mmcv",
-                "math",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "numpy",
+                "os",
+                "math",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.vision_middleware_pipeline"
         },
         "('PIPELINES', 'image-skychange', 'image-skychange')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_skychange_pipeline.py",
             "imports": [
                 "pdb",
-                "numpy",
-                "cv2",
+                "typing",
                 "PIL",
+                "cv2",
                 "time",
-                "typing"
+                "numpy"
             ],
             "module": "modelscope.pipelines.cv.image_skychange_pipeline"
         },
         "('PIPELINES', 'image-style-transfer', 'AAMS-style-transfer')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_style_transfer_pipeline.py",
             "imports": [
+                "cv2",
                 "numpy",
-                "typing",
                 "os",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.image_style_transfer_pipeline"
         },
         "('PIPELINES', 'image-super-resolution', 'mobile-image-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/mobile_image_super_resolution_pipeline.py",
             "imports": [
-                "numpy",
-                "torchvision",
-                "torch",
                 "typing",
-                "skimage"
+                "torchvision",
+                "numpy",
+                "skimage",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.mobile_image_super_resolution_pipeline"
         },
         "('PIPELINES', 'image-super-resolution', 'rrdb-image-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_super_resolution_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_super_resolution_pipeline"
         },
         "('PIPELINES', 'image-text-retrieval', 'image-text-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/image_text_retrieval_pipeline.py",
             "imports": [
                 "torch",
@@ -7137,180 +7033,180 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.multi_modal_embedding_pipeline"
         },
         "('PIPELINES', 'image-to-image-generation', 'image-to-image-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_to_image_generate_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.image_to_image_generate_pipeline"
         },
         "('PIPELINES', 'image-to-image-translation', 'image-to-image-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_to_image_translation_pipeline.py",
             "imports": [
-                "numpy",
+                "sys",
                 "typing",
-                "io",
+                "torchvision",
                 "PIL",
-                "os",
+                "cv2",
+                "io",
+                "numpy",
                 "torch",
-                "sys",
-                "torchvision",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_to_image_translation_pipeline"
         },
         "('PIPELINES', 'indoor-layout-estimation', 'indoor-layout-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/indoor_layout_estimation_pipeline.py",
             "imports": [
+                "cv2",
                 "numpy",
-                "typing",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.indoor_layout_estimation_pipeline"
         },
         "('PIPELINES', 'information-extraction', 'relation-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.information_extraction_pipeline"
         },
         "('PIPELINES', 'inverse-text-processing', 'itn-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/inverse_text_processing_pipeline.py",
             "imports": [
-                "shutil",
                 "yaml",
-                "typing",
-                "os"
+                "shutil",
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.inverse_text_processing_pipeline"
         },
         "('PIPELINES', 'keyword-spotting', 'kws-kwsbp')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/kws_kwsbp_pipeline.py",
             "imports": [
                 "json",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.kws_kwsbp_pipeline"
         },
         "('PIPELINES', 'keyword-spotting', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/kws_farfield_pipeline.py",
             "imports": [
-                "numpy",
-                "io",
-                "soundfile",
+                "wave",
                 "typing",
-                "wave"
+                "soundfile",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.pipelines.audio.kws_farfield_pipeline"
         },
         "('PIPELINES', 'language-guided-video-summarization', 'clip-it-video-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/language_guided_video_summarization_pipeline.py",
             "imports": [
-                "numpy",
                 "shutil",
+                "typing",
+                "clip",
                 "PIL",
-                "tempfile",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "clip",
                 "random",
-                "typing",
-                "cv2"
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.language_guided_video_summarization_pipeline"
         },
         "('PIPELINES', 'language-score-prediction', 'language-score-prediction')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/lm_infer_pipeline.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.lm_infer_pipeline"
         },
         "('PIPELINES', 'license-plate-detection', 'resnet18-license-plate-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/license_plate_detection_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "math",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.license_plate_detection_pipeline"
         },
         "('PIPELINES', 'lineless-table-recognition', 'lore-lineless-table-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/lineless_table_recognition_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "math",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.lineless_table_recognition_pipeline"
         },
         "('PIPELINES', 'live-category', 'live-category')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/live_category_pipeline.py",
             "imports": [
-                "numpy",
-                "decord",
+                "typing",
+                "torchvision",
                 "PIL",
+                "numpy",
                 "os",
                 "torch",
-                "typing",
-                "torchvision"
+                "decord"
             ],
             "module": "modelscope.pipelines.cv.live_category_pipeline"
         },
         "('PIPELINES', 'motion-generation', 'mdm-motion-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/motion_generation_pipeline.py",
             "imports": [
+                "typing",
                 "numpy",
-                "tempfile",
                 "os",
-                "torch",
-                "typing"
+                "tempfile",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.motion_generation_pipeline"
         },
         "('PIPELINES', 'movie-scene-segmentation', 'resnet50-bert-movie-scene-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/movie_scene_segmentation_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.movie_scene_segmentation_pipeline"
         },
         "('PIPELINES', 'multi-modal-embedding', 'gridvlp-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py",
             "imports": [
-                "numpy",
-                "traceback",
-                "PIL",
                 "json",
+                "typing",
                 "transformers",
-                "os",
-                "torch",
+                "PIL",
                 "time",
-                "typing"
+                "numpy",
+                "torch",
+                "os",
+                "traceback"
             ],
             "module": "modelscope.pipelines.multi_modal.gridvlp_pipeline"
         },
         "('PIPELINES', 'multi-modal-embedding', 'multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
@@ -7320,14 +7216,22 @@
         "('PIPELINES', 'multi-modal-similarity', 'multi-modal-similarity')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.team_multi_modal_similarity_pipeline"
         },
+        "('PIPELINES', 'multimodal-dialogue', 'multimodal-dialogue')": {
+            "filepath": "TEMPLATE_PATH/pipelines/multi_modal/multimodal_dialogue_pipeline.py",
+            "imports": [
+                "torch",
+                "typing"
+            ],
+            "module": "modelscope.pipelines.multi_modal.multimodal_dialogue_pipeline"
+        },
         "('PIPELINES', 'named-entity-recognition', 'named-entity-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/named_entity_recognition_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.named_entity_recognition_pipeline"
         },
@@ -7351,44 +7255,44 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.nerf_recon_acc_pipeline"
         },
         "('PIPELINES', 'nli', 'nli')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'object-detection-3d', 'object-detection-3d-depe')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/object_detection_3d_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "tempfile",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "typing",
-                "cv2"
+                "tempfile",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.object_detection_3d_pipeline"
         },
         "('PIPELINES', 'ocr-detection', 'resnet18-ocr-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ocr_detection_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
                 "tensorflow",
-                "os",
-                "torch",
                 "typing",
+                "cv2",
                 "tf_slim",
-                "cv2"
+                "numpy",
+                "os",
+                "math",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.ocr_detection_pipeline"
         },
         "('PIPELINES', 'ocr-recognition', 'convnextTiny-ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ocr_recognition_pipeline.py",
             "imports": [],
             "module": "modelscope.pipelines.cv.ocr_recognition_pipeline"
@@ -7400,151 +7304,151 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.ocr_recognition_pipeline"
         },
         "('PIPELINES', 'open-vocabulary-detection', 'open-vocabulary-detection-vild')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_open_vocabulary_detection_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_open_vocabulary_detection_pipeline"
         },
         "('PIPELINES', 'panorama-depth-estimation', 'panorama-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/panorama_depth_estimation_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "torch",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.panorama_depth_estimation_pipeline"
         },
         "('PIPELINES', 'part-of-speech', 'part-of-speech')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'pedestrian-attribute-recognition', 'resnet50_pedestrian-attribute-recognition_image')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/pedestrian_attribute_recognition_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
                 "json",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.pedestrian_attribute_recognition_pipeline"
         },
         "('PIPELINES', 'pointcloud-sceneflow-estimation', 'pointcloud-sceneflow-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/pointcloud_sceneflow_estimation_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "plyfile"
+                "plyfile",
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.pointcloud_sceneflow_estimation_pipeline"
         },
         "('PIPELINES', 'portrait-matting', 'unet-image-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py",
             "imports": [
-                "numpy",
                 "tensorflow",
-                "os",
                 "typing",
-                "cv2"
+                "cv2",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_matting_pipeline"
         },
         "('PIPELINES', 'product-retrieval-embedding', 'resnet50-product-retrieval-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/product_retrieval_embedding_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.product_retrieval_embedding_pipeline"
         },
         "('PIPELINES', 'product-segmentation', 'product-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/product_segmentation_pipeline.py",
             "imports": [
                 "numpy",
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.product_segmentation_pipeline"
         },
         "('PIPELINES', 'protein-structure', 'unifold-protein-structure')": {
             "filepath": "TEMPLATE_PATH/pipelines/science/protein_structure_pipeline.py",
             "imports": [
-                "numpy",
                 "json",
-                "os",
+                "typing",
                 "unicore",
-                "torch",
                 "time",
-                "typing"
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.science.protein_structure_pipeline"
         },
         "('PIPELINES', 'punctuation', 'punc-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/punctuation_processing_pipeline.py",
             "imports": [
-                "shutil",
                 "yaml",
-                "typing",
-                "os"
+                "shutil",
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.punctuation_processing_pipeline"
         },
         "('PIPELINES', 'referring-video-object-segmentation', 'referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/referring_video_object_segmentation_pipeline.py",
             "imports": [
-                "moviepy",
+                "typing",
+                "torchvision",
+                "tqdm",
+                "PIL",
                 "numpy",
+                "moviepy",
                 "einops",
-                "PIL",
                 "tempfile",
-                "torch",
-                "tqdm",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.referring_video_object_segmentation_pipeline"
         },
         "('PIPELINES', 'relation-extraction', 'relation-extraction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/information_extraction_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.information_extraction_pipeline"
         },
         "('PIPELINES', 'semantic-segmentation', 'ddpm-image-semantic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/ddpm_semantic_segmentation_pipeline.py",
             "imports": [
+                "torchvision",
                 "torch",
-                "typing",
-                "torchvision"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.ddpm_semantic_segmentation_pipeline"
         },
         "('PIPELINES', 'semantic-segmentation', 'res2net-camouflaged-detection')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_salient_detection_pipeline.py",
             "imports": [
                 "typing"
@@ -7564,45 +7468,45 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.image_salient_detection_pipeline"
         },
         "('PIPELINES', 'sentence-embedding', 'sentence-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/sentence_embedding_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.sentence_embedding_pipeline"
         },
         "('PIPELINES', 'sentence-similarity', 'sentence-similarity')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'sentence-similarity', 'translation-quality-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_quality_estimation_pipeline.py",
             "imports": [
-                "io",
+                "typing",
                 "transformers",
+                "io",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.pipelines.nlp.translation_quality_estimation_pipeline"
         },
         "('PIPELINES', 'sentiment-classification', 'sentiment-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'shop-segmentation', 'shop-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/shop_segmentation_pipleline.py",
             "imports": [
@@ -7610,92 +7514,123 @@
             ],
             "module": "modelscope.pipelines.cv.shop_segmentation_pipleline"
         },
         "('PIPELINES', 'siamese-uie', 'siamese-uie')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/siamese_uie_pipeline.py",
             "imports": [
                 "pathlib",
-                "math",
-                "scipy",
-                "time",
                 "json",
-                "os",
                 "logging",
-                "torch",
-                "tqdm",
                 "copy",
-                "typing"
+                "typing",
+                "tqdm",
+                "time",
+                "os",
+                "scipy",
+                "math",
+                "torch"
             ],
             "module": "modelscope.pipelines.nlp.siamese_uie_pipeline"
         },
         "('PIPELINES', 'skin-retouching', 'unet-skin-retouching')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/skin_retouching_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
                 "tensorflow",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.skin_retouching_pipeline"
         },
+        "('PIPELINES', 'speaker-diarization', 'speaker-change-locating')": {
+            "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_change_locating_pipeline.py",
+            "imports": [
+                "typing",
+                "soundfile",
+                "io",
+                "numpy",
+                "torch"
+            ],
+            "module": "modelscope.pipelines.audio.speaker_change_locating_pipeline"
+        },
         "('PIPELINES', 'speaker-diarization', 'speaker-diarization-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_diarization_pipeline.py",
             "imports": [
-                "yaml",
-                "numpy",
                 "shutil",
+                "typing",
                 "json",
-                "os",
-                "typing"
+                "numpy",
+                "yaml",
+                "os"
             ],
             "module": "modelscope.pipelines.audio.speaker_diarization_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'speaker-verification')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_light_pipeline.py",
             "imports": [
+                "soundfile",
+                "torch",
                 "io",
+                "typing"
+            ],
+            "module": "modelscope.pipelines.audio.speaker_verification_light_pipeline"
+        },
+        "('PIPELINES', 'speaker-verification', 'speaker-verification-eres2net')": {
+            "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_eres2net_pipeline.py",
+            "imports": [
+                "soundfile",
                 "torch",
+                "io",
+                "typing"
+            ],
+            "module": "modelscope.pipelines.audio.speaker_verification_eres2net_pipeline"
+        },
+        "('PIPELINES', 'speaker-verification', 'speaker-verification-rdino')": {
+            "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_rdino_pipeline.py",
+            "imports": [
                 "soundfile",
+                "torch",
+                "io",
                 "typing"
             ],
-            "module": "modelscope.pipelines.audio.speaker_verification_light_pipeline"
+            "module": "modelscope.pipelines.audio.speaker_verification_rdino_pipeline"
         },
         "('PIPELINES', 'speaker-verification', 'sv-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/speaker_verification_pipeline.py",
             "imports": [
-                "shutil",
                 "yaml",
-                "typing",
-                "os"
+                "shutil",
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.audio.speaker_verification_pipeline"
         },
         "('PIPELINES', 'speech-separation', 'speech-separation')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/separation_pipeline.py",
             "imports": [
-                "numpy",
-                "io",
+                "typing",
                 "soundfile",
-                "torch",
-                "typing"
+                "io",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.audio.separation_pipeline"
         },
         "('PIPELINES', 'speech-timestamp', 'speech-timestamp-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/timestamp_pipeline.py",
             "imports": [
-                "yaml",
-                "funasr",
                 "json",
-                "os",
-                "typing"
+                "typing",
+                "funasr",
+                "yaml",
+                "os"
             ],
             "module": "modelscope.pipelines.audio.timestamp_pipeline"
         },
         "('PIPELINES', 'sudoku', 'ofa-sudoku')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/sudoku_pipeline.py",
             "imports": [
                 "torch",
@@ -7712,31 +7647,31 @@
             ],
             "module": "modelscope.pipelines.nlp.conversational_text_to_sql_pipeline"
         },
         "('PIPELINES', 'table-question-answering', 'table-question-answering-pipeline')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/table_question_answering_pipeline.py",
             "imports": [
                 "json",
+                "typing",
                 "transformers",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.pipelines.nlp.table_question_answering_pipeline"
         },
         "('PIPELINES', 'table-recognition', 'dla34-table-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/table_recognition_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "math",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.table_recognition_pipeline"
         },
         "('PIPELINES', 'task-oriented-conversation', 'dialog-intent-prediction')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/dialog_intent_prediction_pipeline.py",
             "imports": [
                 "typing"
@@ -7753,77 +7688,85 @@
         "('PIPELINES', 'task-oriented-conversation', 'dialog-state-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/dialog_state_tracking_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.dialog_state_tracking_pipeline"
         },
+        "('PIPELINES', 'task-template', 'pipeline-template')": {
+            "filepath": "TEMPLATE_PATH/pipelines/pipeline_template.py",
+            "imports": [
+                "numpy",
+                "typing"
+            ],
+            "module": "modelscope.pipelines.pipeline_template"
+        },
         "('PIPELINES', 'text-classification', 'domain-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/fasttext_text_classification_pipeline.py",
             "imports": [
-                "numpy",
-                "fasttext",
+                "typing",
                 "sentencepiece",
-                "os",
-                "typing"
+                "fasttext",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.fasttext_text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'language_identification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/language_identification_pipline.py",
             "imports": [
-                "numpy",
-                "re",
                 "tensorflow",
+                "typing",
+                "numpy",
                 "os",
-                "typing"
+                "re"
             ],
             "module": "modelscope.pipelines.nlp.language_identification_pipline"
         },
         "('PIPELINES', 'text-classification', 'sentence-similarity')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'sentiment-analysis')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'sentiment-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'text-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_classification_pipeline"
         },
         "('PIPELINES', 'text-classification', 'user-satisfaction-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/user_satisfaction_estimation_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.user_satisfaction_estimation_pipeline"
         },
         "('PIPELINES', 'text-driven-segmentation', 'text-driven-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/text_driven_segmentation_pipleline.py",
             "imports": [
@@ -7870,24 +7813,24 @@
             ],
             "module": "modelscope.pipelines.nlp.distributed_plug_pipeline"
         },
         "('PIPELINES', 'text-generation', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text-ranking', 'mgeo-ranking')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/mgeo_ranking_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.mgeo_ranking_pipeline"
         },
         "('PIPELINES', 'text-ranking', 'text-ranking')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_ranking_pipeline.py",
             "imports": [
@@ -7895,66 +7838,66 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_ranking_pipeline"
         },
         "('PIPELINES', 'text-summarization', 'mglm-text-summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/mglm_text_summarization_pipeline.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.mglm_text_summarization_pipeline"
         },
         "('PIPELINES', 'text-summarization', 'text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/summarization_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.summarization_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'chinese-stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/chinese_stable_diffusion_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
+                "typing",
                 "transformers",
+                "PIL",
                 "diffusers",
-                "torch",
-                "typing",
-                "cv2"
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.chinese_stable_diffusion_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'diffusers-stable-diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/diffusers_wrapped/stable_diffusion/stable_diffusion_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
                 "diffusers",
-                "torch",
-                "typing",
-                "cv2"
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.stable_diffusion_pipeline"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'disco_guided_diffusion')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/disco_guided_diffusion_pipeline/disco_guided_diffusion.py",
             "imports": [
-                "numpy",
-                "math",
                 "gc",
+                "json",
+                "torchvision",
+                "clip",
                 "PIL",
+                "cv2",
                 "importlib",
-                "json",
-                "os",
+                "numpy",
                 "torch",
-                "clip",
-                "torchvision",
-                "cv2"
+                "math",
+                "os"
             ],
             "module": "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.disco_guided_diffusion"
         },
         "('PIPELINES', 'text-to-image-synthesis', 'text-to-image-synthesis')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text_to_image_synthesis_pipeline.py",
             "imports": [
                 "torch",
@@ -7969,19 +7912,20 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.audio.text_to_speech_pipeline"
         },
         "('PIPELINES', 'text-to-video-synthesis', 'latent-text-to-video-synthesis')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text_to_video_synthesis_pipeline.py",
             "imports": [
+                "typing",
+                "cv2",
+                "os",
                 "einops",
                 "tempfile",
-                "torch",
-                "typing",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.pipelines.multi_modal.text_to_video_synthesis_pipeline"
         },
         "('PIPELINES', 'text2sql', 'ofa-text2sql')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/text2sql_pipeline.py",
             "imports": [
                 "torch",
@@ -7989,454 +7933,454 @@
             ],
             "module": "modelscope.pipelines.multi_modal.text2sql_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'text2text-generation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'translation_en_to_de')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'translation_en_to_fr')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'text2text-generation', 'translation_en_to_ro')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/text_generation_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.nlp.text_generation_pipeline"
         },
         "('PIPELINES', 'token-classification', 'named-entity-recognition')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'token-classification', 'part-of-speech')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'token-classification', 'token-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'token-classification', 'word-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/token_classification_pipeline.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.token_classification_pipeline"
         },
         "('PIPELINES', 'translation', 'automatic-post-editing')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/automatic_post_editing_pipeline.py",
             "imports": [
-                "numpy",
-                "html",
                 "tensorflow",
-                "jieba",
-                "sentencepiece",
-                "os",
                 "sacremoses",
-                "typing"
+                "typing",
+                "sentencepiece",
+                "html",
+                "jieba",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.automatic_post_editing_pipeline"
         },
         "('PIPELINES', 'translation', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_pipeline.py",
             "imports": [
-                "subword_nmt",
-                "numpy",
                 "tensorflow",
-                "jieba",
-                "os",
                 "sacremoses",
-                "typing"
+                "subword_nmt",
+                "typing",
+                "jieba",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.translation_pipeline"
         },
         "('PIPELINES', 'translation', 'interactive-translation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/interactive_translation_pipeline.py",
             "imports": [
-                "subword_nmt",
-                "numpy",
                 "tensorflow",
-                "jieba",
-                "os",
                 "sacremoses",
-                "typing"
+                "subword_nmt",
+                "typing",
+                "jieba",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.nlp.interactive_translation_pipeline"
         },
         "('PIPELINES', 'translation-evaluation', 'translation-evaluation')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/translation_evaluation_pipeline.py",
             "imports": [
+                "typing",
                 "numpy",
-                "enum",
                 "os",
-                "torch",
-                "typing"
+                "enum",
+                "torch"
             ],
             "module": "modelscope.pipelines.nlp.translation_evaluation_pipeline"
         },
         "('PIPELINES', 'universal-matting', 'unet-universal-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/image_matting_pipeline.py",
             "imports": [
-                "numpy",
                 "tensorflow",
-                "os",
                 "typing",
-                "cv2"
+                "cv2",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.image_matting_pipeline"
         },
         "('PIPELINES', 'video-captioning', 'video-captioning')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_captioning_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.video_captioning_pipeline"
         },
         "('PIPELINES', 'video-category', 'video-category')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_category_pipeline.py",
             "imports": [
-                "numpy",
-                "decord",
-                "PIL",
                 "json",
+                "torchvision",
+                "typing",
+                "PIL",
+                "numpy",
                 "os",
                 "torch",
-                "typing",
-                "torchvision"
+                "decord"
             ],
             "module": "modelscope.pipelines.cv.video_category_pipeline"
         },
         "('PIPELINES', 'video-colorization', 'video-colorization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_colorization_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "tempfile",
                 "subprocess",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "torch",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_colorization_pipeline"
         },
         "('PIPELINES', 'video-deinterlace', 'video-deinterlace')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_deinterlace_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
-                "tempfile",
                 "subprocess",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "cv2",
+                "numpy",
+                "torch",
+                "math",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_deinterlace_pipeline"
         },
         "('PIPELINES', 'video-depth-estimation', 'video-depth-estimation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_depth_estimation_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.video_depth_estimation_pipeline"
         },
         "('PIPELINES', 'video-embedding', 'cmdssl-r2p1d_video_embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/cmdssl_video_embedding_pipeline.py",
             "imports": [
-                "numpy",
-                "decord",
+                "typing",
+                "torchvision",
                 "PIL",
+                "numpy",
                 "os",
                 "torch",
-                "typing",
-                "torchvision"
+                "decord"
             ],
             "module": "modelscope.pipelines.cv.cmdssl_video_embedding_pipeline"
         },
         "('PIPELINES', 'video-embedding', 'hicossl-s3dg-video_embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/hicossl_video_embedding_pipeline.py",
             "imports": [
                 "math",
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.hicossl_video_embedding_pipeline"
         },
         "('PIPELINES', 'video-frame-interpolation', 'video-frame-interpolation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_frame_interpolation_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "subprocess",
+                "typing",
+                "torchvision",
                 "cv2",
                 "glob",
-                "tempfile",
-                "os",
+                "numpy",
                 "torch",
-                "typing",
-                "torchvision",
-                "subprocess"
+                "math",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_frame_interpolation_pipeline"
         },
         "('PIPELINES', 'video-human-matting', 'video-human-matting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_human_matting_pipeline.py",
             "imports": [
-                "moviepy",
+                "typing",
+                "cv2",
                 "numpy",
-                "os",
                 "torch",
-                "typing",
-                "cv2"
+                "moviepy",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_human_matting_pipeline"
         },
         "('PIPELINES', 'video-inpainting', 'video-inpainting')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_inpainting_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.cv.video_inpainting_pipeline"
         },
         "('PIPELINES', 'video-instance-segmentation', 'video-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_instance_segmentation_pipeline.py",
             "imports": [
-                "numpy",
                 "mmcv",
-                "os",
-                "torch",
-                "tqdm",
                 "typing",
-                "cv2"
+                "tqdm",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.video_instance_segmentation_pipeline"
         },
         "('PIPELINES', 'video-multi-modal-embedding', 'video-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_multi_modal_embedding_pipeline.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.video_multi_modal_embedding_pipeline"
         },
         "('PIPELINES', 'video-multi-object-tracking', 'video-multi-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_multi_object_tracking_pipeline.py",
             "imports": [
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.video_multi_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-object-detection', 'cspnet_realtime-video-object-detection_streamyolo')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/realtime_video_object_detection_pipeline.py",
             "imports": [
-                "numpy",
-                "PIL",
                 "json",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.realtime_video_object_detection_pipeline"
         },
         "('PIPELINES', 'video-object-segmentation', 'video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_object_segmentation_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
+                "torchvision",
                 "PIL",
-                "os",
+                "numpy",
                 "torch",
-                "typing",
-                "torchvision"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_object_segmentation_pipeline"
         },
         "('PIPELINES', 'video-panoptic-segmentation', 'video-panoptic-segmentation')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_panoptic_segmentation_pipeline.py",
             "imports": [
-                "numpy",
                 "mmcv",
-                "os",
-                "torch",
-                "tqdm",
                 "typing",
-                "cv2"
+                "tqdm",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.video_panoptic_segmentation_pipeline"
         },
         "('PIPELINES', 'video-question-answering', 'video-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/video_question_answering_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.video_question_answering_pipeline"
         },
         "('PIPELINES', 'video-single-object-tracking', 'ostrack-vitb-video-single-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py",
             "imports": [
-                "typing",
+                "cv2",
                 "os",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.video_single_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-single-object-tracking', 'procontext-vitb-video-single-object-tracking')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_single_object_tracking_pipeline.py",
             "imports": [
-                "typing",
+                "cv2",
                 "os",
-                "cv2"
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.video_single_object_tracking_pipeline"
         },
         "('PIPELINES', 'video-stabilization', 'video-stabilization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_stabilization_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
+                "subprocess",
+                "typing",
                 "cv2",
                 "glob",
-                "tempfile",
-                "os",
+                "numpy",
                 "torch",
-                "typing",
-                "subprocess"
+                "math",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_stabilization_pipeline"
         },
         "('PIPELINES', 'video-summarization', 'googlenet_pgl_video_summarization')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_summarization_pipeline.py",
             "imports": [
+                "typing",
+                "tqdm",
+                "cv2",
                 "numpy",
-                "os",
                 "torch",
-                "tqdm",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_summarization_pipeline"
         },
         "('PIPELINES', 'video-super-resolution', 'realbasicvsr-video-super-resolution')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/video_super_resolution_pipeline.py",
             "imports": [
-                "numpy",
-                "math",
-                "tempfile",
                 "subprocess",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "cv2",
+                "numpy",
+                "torch",
+                "math",
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.pipelines.cv.video_super_resolution_pipeline"
         },
         "('PIPELINES', 'video-temporal-grounding', 'soonet-video-temporal-grounding')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py",
             "imports": [
+                "typing",
+                "torchvision",
                 "numpy",
                 "os",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.pipelines.multi_modal.soonet_video_temporal_grounding_pipeline"
         },
         "('PIPELINES', 'video-text-retrieval', 'vop-video-text-retrieval')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vop_retrieval_pipeline.py",
             "imports": [
-                "numpy",
-                "pickle",
-                "gzip",
+                "typing",
+                "random",
                 "tqdm",
+                "pickle",
+                "numpy",
+                "torch",
                 "math",
+                "gzip",
                 "os",
-                "collections",
-                "torch",
-                "random",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.pipelines.cv.vop_retrieval_pipeline"
         },
         "('PIPELINES', 'video-text-retrieval', 'vop-video-text-retrieval-se')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vop_retrieval_se_pipeline.py",
             "imports": [
+                "typing",
                 "numpy",
-                "gzip",
                 "os",
-                "torch",
-                "typing"
+                "gzip",
+                "torch"
             ],
             "module": "modelscope.pipelines.cv.vop_retrieval_se_pipeline"
         },
         "('PIPELINES', 'virtual-try-on', 'virtual-try-on')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/virtual_try_on_pipeline.py",
             "imports": [
-                "numpy",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.pipelines.cv.virtual_try_on_pipeline"
         },
         "('PIPELINES', 'vision-efficient-tuning', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/pipelines/cv/vision_efficient_tuning_pipeline.py",
             "imports": [
-                "torch",
+                "torchvision",
                 "numpy",
-                "typing",
-                "torchvision"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.pipelines.cv.vision_efficient_tuning_pipeline"
         },
         "('PIPELINES', 'visual-entailment', 'visual-entailment')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/visual_entailment_pipeline.py",
             "imports": [
                 "torch",
@@ -8451,42 +8395,42 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.visual_grounding_pipeline"
         },
         "('PIPELINES', 'visual-question-answering', 'gridvlp-multi-modal-classification')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/gridvlp_pipeline.py",
             "imports": [
-                "numpy",
-                "traceback",
-                "PIL",
                 "json",
+                "typing",
                 "transformers",
-                "os",
-                "torch",
+                "PIL",
                 "time",
-                "typing"
+                "numpy",
+                "torch",
+                "os",
+                "traceback"
             ],
             "module": "modelscope.pipelines.multi_modal.gridvlp_pipeline"
         },
         "('PIPELINES', 'visual-question-answering', 'visual-question-answering')": {
             "filepath": "TEMPLATE_PATH/pipelines/multi_modal/visual_question_answering_pipeline.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.pipelines.multi_modal.visual_question_answering_pipeline"
         },
         "('PIPELINES', 'voice-activity-detection', 'vad-inference')": {
             "filepath": "TEMPLATE_PATH/pipelines/audio/voice_activity_detection_pipeline.py",
             "imports": [
-                "yaml",
-                "funasr",
                 "json",
-                "os",
-                "typing"
+                "typing",
+                "funasr",
+                "yaml",
+                "os"
             ],
             "module": "modelscope.pipelines.audio.voice_activity_detection_pipeline"
         },
         "('PIPELINES', 'word-alignment', 'word-alignment')": {
             "filepath": "TEMPLATE_PATH/pipelines/nlp/word_alignment_pipeline.py",
             "imports": [
                 "numpy",
@@ -8526,575 +8470,615 @@
                 "typing"
             ],
             "module": "modelscope.pipelines.nlp.zero_shot_classification_pipeline"
         },
         "('POSITIONAL_ENCODING', 'default', 'SinePositionalEncoding3D')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/positional_encoding.py",
             "imports": [
+                "mmcv",
                 "math",
-                "torch",
-                "mmcv"
+                "torch"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.positional_encoding"
         },
         "('PREPROCESSORS', 'audio', 'LinearAECAndFbank')": {
             "filepath": "TEMPLATE_PATH/preprocessors/audio.py",
             "imports": [
-                "numpy",
+                "typing",
                 "io",
-                "scipy",
-                "os",
+                "numpy",
                 "torch",
-                "typing"
+                "scipy",
+                "os"
             ],
             "module": "modelscope.preprocessors.audio"
         },
         "('PREPROCESSORS', 'audio', 'wav-to-lists')": {
             "filepath": "TEMPLATE_PATH/preprocessors/kws.py",
             "imports": [
                 "yaml",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.kws"
         },
         "('PREPROCESSORS', 'audio', 'wav-to-scp')": {
             "filepath": "TEMPLATE_PATH/preprocessors/asr.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.asr"
         },
         "('PREPROCESSORS', 'cv', 'CenterCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'ImageToTensor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'Normalize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomHorizontalFlip')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'RandomResizedCrop')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'Resize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'bad-image-detecting-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/bad_image_detecting_preprocessor.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "PIL",
                 "numpy",
                 "math",
-                "PIL",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.bad_image_detecting_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'controllable-image-generation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/controllable_image_generation.py",
             "imports": [
-                "numpy",
-                "math",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "math",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.controllable_image_generation"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-bypass-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-mmcv-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/mmcls_preprocessor.py",
             "imports": [
                 "numpy",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.cv.mmcls_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-classification-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_classification_preprocessor.py",
             "imports": [
-                "numpy",
-                "PIL",
-                "os",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "PIL",
+                "cv2",
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_classification_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-color-enhance-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-deblur-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-demoire-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_restoration_preprocessor.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "PIL",
                 "numpy",
                 "math",
-                "PIL",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_restoration_preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-denoise-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-driving-perception-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_driving_perception/preprocessor.py",
             "imports": [
-                "torch",
+                "cv2",
                 "numpy",
-                "typing",
-                "cv2"
+                "torch",
+                "typing"
             ],
             "module": "modelscope.models.cv.image_driving_perception.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'image-instance-segmentation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-portrait-enhancement-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'image-quality_assessment-man-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_man.py",
             "imports": [
+                "typing",
+                "torchvision",
+                "PIL",
                 "numpy",
                 "math",
-                "PIL",
-                "torch",
-                "typing",
-                "torchvision"
+                "torch"
             ],
             "module": "modelscope.preprocessors.cv.image_quality_assessment_man"
         },
         "('PREPROCESSORS', 'cv', 'image-quality_assessment-mos-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/cv/image_quality_assessment_mos.py",
             "imports": [
-                "numpy",
-                "math",
                 "typing",
                 "torchvision",
-                "cv2"
+                "cv2",
+                "numpy",
+                "math"
             ],
             "module": "modelscope.preprocessors.cv.image_quality_assessment_mos"
         },
         "('PREPROCESSORS', 'cv', 'image-sky-change-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/image_skychange/preprocessor.py",
             "imports": [
                 "pdb",
-                "numpy",
                 "json",
-                "numbers",
-                "torch",
                 "typing",
                 "torchvision",
-                "cv2"
+                "numbers",
+                "cv2",
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.models.cv.image_skychange.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'load-image')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'movie-scene-segmentation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/video.py",
             "imports": [
+                "urllib",
+                "torchvision",
                 "numpy",
+                "torch",
                 "uuid",
-                "decord",
                 "math",
-                "urllib",
+                "random",
                 "tempfile",
                 "os",
-                "torch",
-                "random",
-                "torchvision"
+                "decord"
             ],
             "module": "modelscope.preprocessors.video"
         },
         "('PREPROCESSORS', 'cv', 'nerf-recon-acc-preprocessor')": {
             "filepath": "TEMPLATE_PATH/models/cv/nerf_recon_acc/nerf_preprocess.py",
             "imports": [
-                "numpy",
-                "glob",
                 "tensorflow",
                 "subprocess",
-                "os",
                 "typing",
-                "cv2"
+                "cv2",
+                "glob",
+                "numpy",
+                "os"
             ],
             "module": "modelscope.models.cv.nerf_recon_acc.nerf_preprocess"
         },
         "('PREPROCESSORS', 'cv', 'object-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/models/cv/face_detection/scrfd/preprocessor.py",
             "imports": [
                 "numpy",
-                "typing",
-                "PIL"
+                "PIL",
+                "typing"
             ],
             "module": "modelscope.models.cv.face_detection.scrfd.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'object-detection-tinynas-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'cv', 'ocr-detection')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_detection/preprocessor.py",
             "imports": [
-                "numpy",
-                "math",
+                "typing",
                 "PIL",
-                "os",
+                "cv2",
+                "numpy",
                 "torch",
-                "typing",
-                "cv2"
+                "math",
+                "os"
             ],
             "module": "modelscope.models.cv.ocr_detection.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/models/cv/ocr_recognition/preprocessor.py",
             "imports": [
-                "numpy",
                 "PIL",
+                "cv2",
+                "numpy",
                 "os",
-                "torch",
-                "cv2"
+                "torch"
             ],
             "module": "modelscope.models.cv.ocr_recognition.preprocessor"
         },
         "('PREPROCESSORS', 'cv', 'video-summarization-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/image.py",
             "imports": [
-                "numpy",
-                "io",
-                "PIL",
                 "typing",
-                "cv2"
+                "PIL",
+                "cv2",
+                "io",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.image"
         },
         "('PREPROCESSORS', 'default', 'Compose')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
+                "typing",
+                "time",
                 "numpy",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Filter')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
+                "typing",
+                "time",
                 "numpy",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Identity')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
+                "typing",
+                "time",
                 "numpy",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'Rename')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
+                "typing",
+                "time",
                 "numpy",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'ToNumpy')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
+                "typing",
+                "time",
                 "numpy",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'default', 'ToTensor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/common.py",
             "imports": [
+                "typing",
+                "time",
                 "numpy",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.preprocessors.common"
         },
         "('PREPROCESSORS', 'multi-modal', 'clip-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "numpy",
-                "decord",
-                "io",
-                "PIL",
                 "timm",
                 "json",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "PIL",
+                "numpy",
+                "torch",
+                "io",
+                "os",
+                "decord",
+                "re"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'diffusion-image-generation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "numpy",
-                "decord",
-                "io",
-                "PIL",
                 "timm",
                 "json",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "PIL",
+                "numpy",
+                "torch",
+                "io",
+                "os",
+                "decord",
+                "re"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'hitea-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
+                "timm",
+                "json",
+                "typing",
+                "torchvision",
+                "PIL",
                 "numpy",
-                "decord",
+                "torch",
                 "io",
-                "PIL",
+                "os",
+                "decord",
+                "re"
+            ],
+            "module": "modelscope.preprocessors.multi_modal"
+        },
+        "('PREPROCESSORS', 'multi-modal', 'image-captioning-clip-interrogator-preprocessor')": {
+            "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
+            "imports": [
                 "timm",
                 "json",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "PIL",
+                "numpy",
+                "torch",
+                "io",
+                "os",
+                "decord",
+                "re"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
-        "('PREPROCESSORS', 'multi-modal', 'mplug-tasks-preprocessor')": {
+        "('PREPROCESSORS', 'multi-modal', 'mplug-owl-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
+                "timm",
+                "json",
+                "typing",
+                "torchvision",
+                "PIL",
                 "numpy",
-                "decord",
+                "torch",
                 "io",
-                "PIL",
+                "os",
+                "decord",
+                "re"
+            ],
+            "module": "modelscope.preprocessors.multi_modal"
+        },
+        "('PREPROCESSORS', 'multi-modal', 'mplug-tasks-preprocessor')": {
+            "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
+            "imports": [
                 "timm",
                 "json",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "PIL",
+                "numpy",
+                "torch",
+                "io",
+                "os",
+                "decord",
+                "re"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'ofa-tasks-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "numpy",
-                "decord",
-                "io",
-                "PIL",
                 "timm",
                 "json",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "PIL",
+                "numpy",
+                "torch",
+                "io",
+                "os",
+                "decord",
+                "re"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'multi-modal', 'vldoc-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/multi_modal.py",
             "imports": [
-                "numpy",
-                "decord",
-                "io",
-                "PIL",
                 "timm",
                 "json",
-                "os",
-                "torch",
                 "typing",
-                "torchvision"
+                "torchvision",
+                "PIL",
+                "numpy",
+                "torch",
+                "io",
+                "os",
+                "decord",
+                "re"
             ],
             "module": "modelscope.preprocessors.multi_modal"
         },
         "('PREPROCESSORS', 'nlp', 'Tokenize')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/bert_seq_cls_tokenizer.py",
             "imports": [
-                "typing",
-                "transformers"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.bert_seq_cls_tokenizer"
         },
         "('PREPROCESSORS', 'nlp', 'bert-seq-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
                 "numpy",
@@ -9103,62 +9087,62 @@
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'canmt-translation')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/canmt_translation.py",
             "imports": [
                 "sacremoses",
                 "subword_nmt",
+                "typing",
                 "jieba",
-                "os",
                 "torch",
-                "typing"
+                "os"
             ],
             "module": "modelscope.preprocessors.nlp.canmt_translation"
         },
         "('PREPROCESSORS', 'nlp', 'conversational-text-to-sql')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space_T_en/conversational_text_to_sql_preprocessor.py",
             "imports": [
-                "text2sql_lgesql",
                 "json",
+                "typing",
                 "os",
-                "torch",
-                "typing"
+                "text2sql_lgesql",
+                "torch"
             ],
             "module": "modelscope.preprocessors.nlp.space_T_en.conversational_text_to_sql_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-intent-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_intent_prediction_preprocessor.py",
             "imports": [
                 "json",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_intent_prediction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-modeling-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_modeling_preprocessor.py",
             "imports": [
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_modeling_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-state-tracking-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space/dialog_state_tracking_preprocessor.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.space.dialog_state_tracking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'dialog-use-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/dialog_classification_use_preprocessor.py",
             "imports": [
+                "transformers",
                 "torch",
-                "typing",
-                "transformers"
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.dialog_classification_use_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-generate')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_generate_preprocessor.py",
             "imports": [
                 "torch",
@@ -9167,19 +9151,19 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_generate_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-rerank')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_rerank_preprocessor.py",
             "imports": [
+                "typing",
+                "copy",
                 "transformers",
                 "os",
-                "torch",
-                "copy",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.preprocessors.nlp.document_grounded_dialog_rerank_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'document-grounded-dialog-retrieval')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/document_grounded_dialog_retrieval_preprocessor.py",
             "imports": [
                 "torch",
@@ -9211,40 +9195,40 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.feature_extraction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'fill-mask')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py",
             "imports": [
+                "typing",
                 "numpy",
-                "re",
-                "abc",
                 "os",
+                "abc",
                 "torch",
-                "typing"
+                "re"
             ],
             "module": "modelscope.preprocessors.nlp.fill_mask_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'fill-mask-ponet')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/fill_mask_preprocessor.py",
             "imports": [
+                "typing",
                 "numpy",
-                "re",
-                "abc",
                 "os",
+                "abc",
                 "torch",
-                "typing"
+                "re"
             ],
             "module": "modelscope.preprocessors.nlp.fill_mask_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'mgeo-ranking')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/mgeo_ranking_preprocessor.py",
             "imports": [
-                "torch",
                 "transformers",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.mgeo_ranking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'mglm-summarization')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/mglm_summarization_preprocessor.py",
             "imports": [
@@ -9253,16 +9237,16 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.mglm_summarization_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'nli-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
@@ -9270,16 +9254,16 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 're-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/relation_extraction_preprocessor.py",
             "imports": [
-                "typing",
-                "transformers"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.relation_extraction_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sen-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_classification_preprocessor.py",
             "imports": [
                 "numpy",
@@ -9301,35 +9285,35 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.sentence_embedding_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sentence-piece')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'sequence-labeling-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'siamese-uie-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/siamese_uie_preprocessor.py",
             "imports": [
-                "typing",
-                "transformers"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.siamese_uie_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'table-question-answering-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/space_T_cn/table_question_answering_preprocessor.py",
             "imports": [
                 "torch",
@@ -9348,46 +9332,46 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_error_correction"
         },
         "('PREPROCESSORS', 'nlp', 'text-gen-jieba-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text-gen-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text-ranking')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_ranking_preprocessor.py",
             "imports": [
-                "typing",
-                "transformers"
+                "transformers",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_ranking_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'text2text-gen-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/text_generation_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.text_generation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'thai-ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_thai_preprocessor.py",
             "imports": [
                 "typing"
@@ -9400,158 +9384,159 @@
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_thai_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'token-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'translation-evaluation-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/translation_evaluation_preprocessor.py",
             "imports": [
-                "typing",
-                "transformers"
+                "transformers",
+                "torch",
+                "typing"
             ],
             "module": "modelscope.preprocessors.nlp.translation_evaluation_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'viet-ner-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_viet_preprocessor.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_viet_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'word-alignment')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/word_alignment_preprocessor.py",
             "imports": [
+                "typing",
                 "numpy",
                 "os",
-                "torch",
-                "typing",
-                "itertools"
+                "itertools",
+                "torch"
             ],
             "module": "modelscope.preprocessors.nlp.word_alignment_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'word-segment-text-to-label-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/token_classification_preprocessor.py",
             "imports": [
-                "torch",
                 "numpy",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.token_classification_preprocessor"
         },
         "('PREPROCESSORS', 'nlp', 'zero-shot-cls-tokenizer')": {
             "filepath": "TEMPLATE_PATH/preprocessors/nlp/zero_shot_classification_preprocessor.py",
             "imports": [
                 "typing"
             ],
             "module": "modelscope.preprocessors.nlp.zero_shot_classification_preprocessor"
         },
         "('PREPROCESSORS', 'science', 'unifold-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/science/uni_fold.py",
             "imports": [
-                "tarfile",
+                "typing",
+                "json",
+                "logging",
+                "ipdb",
+                "pickle",
+                "time",
+                "requests",
                 "hashlib",
-                "pathlib",
+                "gzip",
+                "random",
                 "os",
-                "logging",
-                "unittest",
                 "torch",
-                "random",
-                "time",
-                "numpy",
                 "re",
-                "pickle",
-                "gzip",
-                "ipdb",
-                "requests",
-                "json",
+                "pathlib",
+                "unittest",
                 "tqdm",
-                "typing"
+                "tarfile",
+                "numpy"
             ],
             "module": "modelscope.preprocessors.science.uni_fold"
         },
         "('PREPROCESSORS', 'text-to-speech', 'kantts-data-preprocessor')": {
             "filepath": "TEMPLATE_PATH/preprocessors/tts.py",
             "imports": [
-                "typing",
+                "kantts",
                 "os",
-                "kantts"
+                "typing"
             ],
             "module": "modelscope.preprocessors.tts"
         },
         "('ROI_EXTRACTORS', 'default', 'SingleRoINExtractor')": {
             "filepath": "TEMPLATE_PATH/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.roi_extractors.single_level_roi_extractor"
         },
         "('TRACKERS', 'default', 'QuasiDenseEmbedTracker')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_panoptic_segmentation/track/quasi_dense_embed_tracker.py",
             "imports": [
-                "mmdet",
+                "mmcv",
                 "torch",
-                "mmcv"
+                "mmdet"
             ],
             "module": "modelscope.models.cv.video_panoptic_segmentation.track.quasi_dense_embed_tracker"
         },
         "('TRAINERS', 'default', 'action-detection')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/action_detection_trainer.py",
             "imports": [
-                "fvcore",
+                "typing",
                 "detectron2",
+                "fvcore",
                 "os",
-                "torch",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.trainers.cv.action_detection_trainer"
         },
         "('TRAINERS', 'default', 'bert-sentiment-analysis')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/sequence_classification_trainer.py",
             "imports": [
-                "numpy",
                 "time",
+                "numpy",
                 "typing"
             ],
             "module": "modelscope.trainers.nlp.sequence_classification_trainer"
         },
         "('TRAINERS', 'default', 'card-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/card_detection_scrfd_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.card_detection_scrfd_trainer"
         },
         "('TRAINERS', 'default', 'cartoon-translation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/cartoon_translation_trainer.py",
             "imports": [
-                "packaging",
-                "numpy",
                 "tensorflow",
-                "os",
+                "typing",
                 "tqdm",
-                "typing"
+                "numpy",
+                "packaging",
+                "os"
             ],
             "module": "modelscope.trainers.cv.cartoon_translation_trainer"
         },
         "('TRAINERS', 'default', 'clip-multi-modal-embedding')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/clip/clip_trainer.py",
             "imports": [
                 "math",
                 "torch",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.trainers.multi_modal.clip.clip_trainer"
         },
         "('TRAINERS', 'default', 'csanmt-translation')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/csanmt_translation_trainer.py",
             "imports": [
                 "tensorflow",
@@ -9561,165 +9546,154 @@
             ],
             "module": "modelscope.trainers.nlp.csanmt_translation_trainer"
         },
         "('TRAINERS', 'default', 'dialog-intent-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/space/dialog_intent_trainer.py",
             "imports": [
                 "numpy",
-                "typing",
-                "os"
+                "os",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp.space.dialog_intent_trainer"
         },
         "('TRAINERS', 'default', 'dialog-modeling-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/space/dialog_modeling_trainer.py",
             "imports": [
-                "numpy",
                 "time",
+                "numpy",
                 "os",
                 "typing"
             ],
             "module": "modelscope.trainers.nlp.space.dialog_modeling_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-generate-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_generate_trainer.py",
             "imports": [
-                "string",
-                "re",
+                "rouge",
                 "json",
+                "tqdm",
+                "string",
                 "transformers",
-                "os",
                 "torch",
                 "sacrebleu",
-                "tqdm",
-                "rouge",
-                "collections"
+                "os",
+                "collections",
+                "re"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_generate_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-rerank-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_rerank_trainer.py",
             "imports": [
-                "numpy",
+                "typing",
                 "transformers",
+                "time",
+                "numpy",
                 "os",
-                "torch",
                 "random",
-                "time",
-                "typing"
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_rerank_trainer"
         },
         "('TRAINERS', 'default', 'document-grounded-dialog-retrieval-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/document_grounded_dialog_retrieval_trainer.py",
             "imports": [
-                "numpy",
-                "faiss",
                 "json",
+                "tqdm",
                 "transformers",
+                "faiss",
+                "numpy",
                 "os",
-                "torch",
-                "tqdm"
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.document_grounded_dialog_retrieval_trainer"
         },
         "('TRAINERS', 'default', 'dummy')": {
             "filepath": "TEMPLATE_PATH/trainers/base.py",
             "imports": [
                 "abc",
                 "time",
                 "os",
                 "typing"
             ],
             "module": "modelscope.trainers.base"
         },
-        "('TRAINERS', 'default', 'easycv')": {
-            "filepath": "TEMPLATE_PATH/trainers/easycv/trainer.py",
-            "imports": [
-                "easycv",
-                "torch",
-                "copy",
-                "functools",
-                "typing"
-            ],
-            "module": "modelscope.trainers.easycv.trainer"
-        },
         "('TRAINERS', 'default', 'efficient-diffusion-tuning')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/efficient_diffusion_tuning/efficient_diffusion_tuning_trainer.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.trainers.multi_modal.efficient_diffusion_tuning.efficient_diffusion_tuning_trainer"
         },
         "('TRAINERS', 'default', 'face-detection-scrfd')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/face_detection_scrfd_trainer.py",
             "imports": [
-                "time",
                 "copy",
+                "time",
                 "os",
                 "typing"
             ],
             "module": "modelscope.trainers.cv.face_detection_scrfd_trainer"
         },
         "('TRAINERS', 'default', 'faq-question-answering-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/faq_question_answering_trainer.py",
             "imports": [
-                "numpy",
+                "functools",
                 "distutils",
+                "typing",
                 "contextlib",
                 "dataclasses",
+                "numpy",
                 "torch",
-                "collections",
-                "typing",
-                "functools"
+                "collections"
             ],
             "module": "modelscope.trainers.nlp.faq_question_answering_trainer"
         },
         "('TRAINERS', 'default', 'image-classification')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_classifition_trainer.py",
             "imports": [
-                "numpy",
+                "typing",
                 "copy",
-                "os",
-                "torch",
                 "time",
-                "typing"
+                "numpy",
+                "torch",
+                "os"
             ],
             "module": "modelscope.trainers.cv.image_classifition_trainer"
         },
         "('TRAINERS', 'default', 'image-classification-team')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/team/team_trainer.py",
             "imports": [
-                "numpy",
+                "typing",
                 "sklearn",
+                "numpy",
                 "os",
                 "torch",
-                "collections",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.trainers.multi_modal.team.team_trainer"
         },
         "('TRAINERS', 'default', 'image-fewshot-detection')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_defrcn_fewshot_detection_trainer.py",
             "imports": [
+                "typing",
                 "detectron2",
                 "os",
                 "torch",
-                "collections",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.trainers.cv.image_defrcn_fewshot_detection_trainer"
         },
         "('TRAINERS', 'default', 'image-inpainting')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_inpainting_trainer.py",
             "imports": [
+                "time",
                 "torch",
-                "collections",
-                "time"
+                "collections"
             ],
             "module": "modelscope.trainers.cv.image_inpainting_trainer"
         },
         "('TRAINERS', 'default', 'image-instance-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_instance_segmentation_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.cv.image_instance_segmentation_trainer"
@@ -9731,16 +9705,16 @@
                 "collections"
             ],
             "module": "modelscope.trainers.cv.image_portrait_enhancement_trainer"
         },
         "('TRAINERS', 'default', 'mgeo-ranking-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/mgeo_ranking_trainer.py",
             "imports": [
-                "torch",
                 "dataclasses",
+                "torch",
                 "typing"
             ],
             "module": "modelscope.trainers.multi_modal.mgeo_ranking_trainer"
         },
         "('TRAINERS', 'default', 'movie-scene-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/movie_scene_segmentation_trainer.py",
             "imports": [],
@@ -9754,390 +9728,408 @@
                 "typing"
             ],
             "module": "modelscope.trainers.multi_modal.mplug.mplug_trainer"
         },
         "('TRAINERS', 'default', 'nerf-recon-acc')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/nerf_recon_acc_trainer.py",
             "imports": [
-                "numpy",
+                "datetime",
                 "typing",
                 "tqdm",
+                "cv2",
+                "time",
                 "glob",
-                "datetime",
-                "os",
+                "numpy",
                 "torch",
                 "random",
-                "time",
-                "cv2"
+                "os"
             ],
             "module": "modelscope.trainers.cv.nerf_recon_acc_trainer"
         },
         "('TRAINERS', 'default', 'nlp-base-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp_trainer.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp_trainer"
         },
         "('TRAINERS', 'default', 'nlp-gpt-moe-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/gpt_moe_trainer.py",
             "imports": [
-                "megatron_util",
-                "os",
+                "typing",
                 "torch",
-                "collections",
-                "typing"
+                "os",
+                "megatron_util",
+                "collections"
             ],
             "module": "modelscope.trainers.nlp.gpt_moe_trainer"
         },
         "('TRAINERS', 'default', 'nlp-gpt3-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/gpt3_trainer.py",
             "imports": [
-                "torch",
                 "copy",
+                "torch",
                 "os",
                 "typing"
             ],
             "module": "modelscope.trainers.nlp.gpt3_trainer"
         },
         "('TRAINERS', 'default', 'nlp-plug-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/plug_trainer.py",
             "imports": [
-                "megatron_util",
-                "os",
-                "deepspeed",
+                "typing",
                 "torch",
-                "typing"
+                "os",
+                "megatron_util",
+                "deepspeed"
             ],
             "module": "modelscope.trainers.nlp.plug_trainer"
         },
         "('TRAINERS', 'default', 'nlp-sentence-embedding-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/sentence_embedding_trainer.py",
             "imports": [
-                "numpy",
-                "dataclasses",
-                "transformers",
-                "torch",
+                "typing",
                 "tqdm",
+                "transformers",
+                "dataclasses",
                 "time",
-                "typing"
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.sentence_embedding_trainer"
         },
         "('TRAINERS', 'default', 'nlp-text-ranking-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/text_ranking_trainer.py",
             "imports": [
-                "numpy",
-                "dataclasses",
-                "torch",
+                "typing",
                 "tqdm",
+                "dataclasses",
                 "time",
-                "typing"
+                "numpy",
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.text_ranking_trainer"
         },
         "('TRAINERS', 'default', 'nlp-veco-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp_trainer.py",
             "imports": [
-                "torch",
                 "numpy",
-                "typing",
-                "os"
+                "torch",
+                "os",
+                "typing"
             ],
             "module": "modelscope.trainers.nlp_trainer"
         },
         "('TRAINERS', 'default', 'ocr-detection-db')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/ocr_detection_db_trainer.py",
             "imports": [
+                "datetime",
+                "typing",
                 "copy",
-                "numpy",
-                "math",
                 "easydict",
-                "datetime",
-                "os",
-                "torch",
                 "tqdm",
                 "time",
-                "typing"
+                "numpy",
+                "torch",
+                "math",
+                "os"
             ],
             "module": "modelscope.trainers.cv.ocr_detection_db_trainer"
         },
         "('TRAINERS', 'default', 'ocr-recognition')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/ocr_recognition_trainer.py",
             "imports": [
+                "time",
                 "torch",
-                "collections",
-                "time"
+                "collections"
             ],
             "module": "modelscope.trainers.cv.ocr_recognition_trainer"
         },
         "('TRAINERS', 'default', 'ofa')": {
             "filepath": "TEMPLATE_PATH/trainers/multi_modal/ofa/ofa_trainer.py",
             "imports": [
-                "math",
+                "functools",
                 "shutil",
-                "tempfile",
+                "typing",
                 "json",
                 "os",
-                "torch",
-                "typing",
-                "functools"
+                "math",
+                "tempfile",
+                "torch"
             ],
             "module": "modelscope.trainers.multi_modal.ofa.ofa_trainer"
         },
         "('TRAINERS', 'default', 'referring-video-object-segmentation')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/referring_video_object_segmentation_trainer.py",
             "imports": [
                 "torch",
                 "os"
             ],
             "module": "modelscope.trainers.cv.referring_video_object_segmentation_trainer"
         },
         "('TRAINERS', 'default', 'siamese-uie-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/siamese_uie_trainer.py",
             "imports": [
+                "typing",
+                "json",
+                "time",
                 "numpy",
+                "os",
                 "math",
-                "json",
                 "random",
-                "os",
                 "torch",
-                "collections",
-                "time",
-                "typing"
+                "collections"
             ],
             "module": "modelscope.trainers.nlp.siamese_uie_trainer"
         },
         "('TRAINERS', 'default', 'speech-asr-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/asr_trainer.py",
             "imports": [
                 "shutil",
+                "json",
+                "typing",
                 "funasr",
                 "tempfile",
-                "json",
-                "os",
-                "typing"
+                "os"
             ],
             "module": "modelscope.trainers.audio.asr_trainer"
         },
         "('TRAINERS', 'default', 'speech-kantts-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/tts_trainer.py",
             "imports": [
-                "shutil",
                 "zipfile",
-                "tempfile",
+                "shutil",
+                "typing",
                 "json",
-                "os",
-                "typing"
+                "tempfile",
+                "os"
             ],
             "module": "modelscope.trainers.audio.tts_trainer"
         },
         "('TRAINERS', 'default', 'speech-separation')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/separation_trainer.py",
             "imports": [
-                "torchaudio",
-                "numpy",
-                "csv",
-                "os",
                 "speechbrain",
-                "torch",
+                "typing",
                 "tqdm",
-                "typing"
+                "numpy",
+                "torchaudio",
+                "os",
+                "csv",
+                "torch"
             ],
             "module": "modelscope.trainers.audio.separation_trainer"
         },
         "('TRAINERS', 'default', 'speech_dfsmn_kws_char_farfield')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/kws_farfield_trainer.py",
             "imports": [
-                "numpy",
+                "datetime",
+                "typing",
                 "pickle",
-                "math",
                 "glob",
-                "datetime",
-                "os",
+                "numpy",
                 "torch",
-                "typing"
+                "math",
+                "os"
             ],
             "module": "modelscope.trainers.audio.kws_farfield_trainer"
         },
         "('TRAINERS', 'default', 'speech_frcrn_ans_cirm_16k')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/ans_trainer.py",
             "imports": [],
             "module": "modelscope.trainers.audio.ans_trainer"
         },
         "('TRAINERS', 'default', 'speech_kws_fsmn_char_ctc_nearfield')": {
             "filepath": "TEMPLATE_PATH/trainers/audio/kws_nearfield_trainer.py",
             "imports": [
-                "yaml",
-                "re",
                 "datetime",
-                "os",
                 "tensorboardX",
-                "torch",
+                "typing",
                 "copy",
-                "typing"
+                "torch",
+                "yaml",
+                "os",
+                "re"
             ],
             "module": "modelscope.trainers.audio.kws_nearfield_trainer"
         },
         "('TRAINERS', 'default', 'table-question-answering-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/table_question_answering_trainer.py",
             "imports": [
-                "numpy",
                 "json",
-                "os",
-                "torch",
+                "typing",
                 "tqdm",
                 "time",
-                "typing"
+                "numpy",
+                "os",
+                "torch"
             ],
             "module": "modelscope.trainers.nlp.table_question_answering_trainer"
         },
         "('TRAINERS', 'default', 'text-generation-trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/nlp/text_generation_trainer.py",
             "imports": [
                 "torch",
                 "collections"
             ],
             "module": "modelscope.trainers.nlp.text_generation_trainer"
         },
         "('TRAINERS', 'default', 'tinynas-damoyolo')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/image_detection_damoyolo_trainer.py",
             "imports": [
-                "math",
-                "easydict",
                 "datetime",
-                "os",
-                "torch",
+                "typing",
+                "easydict",
                 "time",
-                "typing"
+                "os",
+                "math",
+                "torch"
             ],
             "module": "modelscope.trainers.cv.image_detection_damoyolo_trainer"
         },
         "('TRAINERS', 'default', 'trainer')": {
             "filepath": "TEMPLATE_PATH/trainers/trainer.py",
             "imports": [
-                "distutils",
+                "functools",
+                "typing",
+                "copy",
                 "json",
+                "collections",
+                "torch",
                 "inspect",
                 "os",
-                "torch",
-                "collections",
-                "copy",
-                "functools",
-                "typing"
+                "distutils"
             ],
             "module": "modelscope.trainers.trainer"
         },
+        "('TRAINERS', 'default', 'translation-evaluation-trainer')": {
+            "filepath": "TEMPLATE_PATH/trainers/nlp/translation_evaluation_trainer.py",
+            "imports": [
+                "pandas",
+                "typing",
+                "tqdm",
+                "transformers",
+                "os",
+                "math",
+                "random",
+                "torch"
+            ],
+            "module": "modelscope.trainers.nlp.translation_evaluation_trainer"
+        },
         "('TRAINERS', 'default', 'vision-efficient-tuning')": {
             "filepath": "TEMPLATE_PATH/trainers/cv/vision_efficient_tuning_trainer.py",
             "imports": [
                 "torch",
                 "typing"
             ],
             "module": "modelscope.trainers.cv.vision_efficient_tuning_trainer"
         },
         "('TRANSFORMER', 'default', 'PETRDNTransformer')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "mmdet",
                 "mmcv",
-                "math",
-                "torch",
                 "warnings",
+                "typing",
                 "copy",
-                "typing"
+                "math",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER', 'default', 'KernelUpdator')": {
             "filepath": "TEMPLATE_PATH/models/cv/video_instance_segmentation/head/kernel_updator.py",
             "imports": [
-                "torch",
-                "mmcv"
+                "mmcv",
+                "torch"
             ],
             "module": "modelscope.models.cv.video_instance_segmentation.head.kernel_updator"
         },
         "('TRANSFORMER_LAYER', 'default', 'PETRTransformerDecoderLayer')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "mmdet",
                 "mmcv",
-                "math",
-                "torch",
                 "warnings",
+                "typing",
                 "copy",
-                "typing"
+                "math",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER_SEQUENCE', 'default', 'PETRTransformerDecoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "mmdet",
                 "mmcv",
-                "math",
-                "torch",
                 "warnings",
+                "typing",
                 "copy",
-                "typing"
+                "math",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         },
         "('TRANSFORMER_LAYER_SEQUENCE', 'default', 'PETRTransformerEncoder')": {
             "filepath": "TEMPLATE_PATH/models/cv/object_detection_3d/depe/mmdet3d_plugin/models/utils/petr_transformer.py",
             "imports": [
-                "mmdet",
                 "mmcv",
-                "math",
-                "torch",
                 "warnings",
+                "typing",
                 "copy",
-                "typing"
+                "math",
+                "torch",
+                "mmdet"
             ],
             "module": "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer"
         }
     },
-    "md5": "a4533205a073c907e811da89956b0c9c",
+    "md5": "4c7401b7122cd886f5e6e668a32d58c5",
     "modelscope_path": "TEMPLATE_PATH",
     "requirements": {
+        "modelscope.exporters.audio.ans_dfsmn_exporter": [
+            "torch",
+            "os"
+        ],
         "modelscope.exporters.base": [
             "abc",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.exporters.builder": [],
         "modelscope.exporters.cv.cartoon_translation_exporter": [
             "tensorflow",
             "packaging",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.exporters.cv.face_detection_scrfd_exporter": [
-            "numpy",
+            "functools",
             "onnx",
-            "os",
-            "torch",
             "typing",
-            "functools"
+            "numpy",
+            "torch",
+            "os"
         ],
         "modelscope.exporters.cv.object_detection_damoyolo_exporter": [
-            "numpy",
+            "functools",
             "onnx",
-            "os",
-            "torch",
             "typing",
-            "functools"
+            "numpy",
+            "torch",
+            "os"
         ],
         "modelscope.exporters.nlp.csanmt_for_translation_exporter": [
             "tensorflow",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.exporters.nlp.model_for_token_classification_exporter": [
             "torch",
             "collections",
             "typing"
         ],
         "modelscope.exporters.nlp.sbert_for_sequence_classification_exporter": [
@@ -10147,130 +10139,130 @@
         ],
         "modelscope.exporters.nlp.sbert_for_zero_shot_classification_exporter": [
             "collections",
             "typing"
         ],
         "modelscope.exporters.tf_model_exporter": [
             "tensorflow",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.exporters.torch_model_exporter": [
+            "typing",
             "contextlib",
             "os",
-            "torch",
-            "typing",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.metrics.accuracy_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.action_detection_evaluator": [
-            "numpy",
+            "pandas",
+            "copy",
             "logging",
             "detectron2",
+            "numpy",
             "scipy",
             "os",
-            "pandas",
-            "collections",
-            "copy"
+            "collections"
         ],
         "modelscope.metrics.audio_noise_metric": [
             "typing"
         ],
         "modelscope.metrics.base": [
             "abc",
             "typing"
         ],
         "modelscope.metrics.bleu_metric": [
+            "itertools",
             "sacrebleu",
-            "typing",
-            "itertools"
+            "typing"
         ],
         "modelscope.metrics.builder": [
             "typing"
         ],
         "modelscope.metrics.ciderD.ciderD": [
             "__future__"
         ],
         "modelscope.metrics.ciderD.ciderD_scorer": [
             "pdb",
+            "six",
+            "copy",
+            "__future__",
             "numpy",
             "math",
-            "__future__",
             "os",
-            "six",
-            "collections",
-            "copy"
+            "collections"
         ],
         "modelscope.metrics.image_color_enhance_metric": [
+            "cv2",
             "numpy",
-            "typing",
-            "cv2"
+            "typing"
         ],
         "modelscope.metrics.image_colorization_metric": [
-            "numpy",
-            "scipy",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "cv2",
+            "numpy",
+            "scipy",
+            "torch"
         ],
         "modelscope.metrics.image_denoise_metric": [
-            "torch",
+            "cv2",
             "numpy",
-            "typing",
-            "cv2"
+            "torch",
+            "typing"
         ],
         "modelscope.metrics.image_inpainting_metric": [
             "scipy",
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.metrics.image_instance_segmentation_metric": [
+            "typing",
             "pycocotools",
             "numpy",
             "tempfile",
             "os",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.metrics.image_portrait_enhancement_metric": [
+            "cv2",
             "numpy",
-            "typing",
-            "cv2"
+            "typing"
         ],
         "modelscope.metrics.image_quality_assessment_degradation_metric": [
-            "numpy",
+            "sys",
             "typing",
             "tqdm",
+            "cv2",
+            "numpy",
+            "torch",
             "scipy",
             "tempfile",
             "os",
-            "torch",
-            "collections",
-            "sys",
-            "cv2"
+            "collections"
         ],
         "modelscope.metrics.image_quality_assessment_mos_metric": [
-            "numpy",
             "sys",
+            "typing",
+            "tqdm",
+            "cv2",
+            "numpy",
+            "os",
             "scipy",
             "tempfile",
-            "os",
-            "torch",
-            "tqdm",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.metrics.inbatch_recall_metric": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.metrics.loss_metric": [
             "numpy",
             "sklearn",
             "typing"
         ],
@@ -10283,334 +10275,370 @@
             "typing"
         ],
         "modelscope.metrics.ned_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.ocr_recognition_metric": [
+            "numpy",
             "torch",
             "edit_distance",
-            "numpy",
             "typing"
         ],
         "modelscope.metrics.ppl_metric": [
             "math",
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.metrics.prediction_saving_wrapper": [
             "numpy",
             "sklearn",
             "typing"
         ],
         "modelscope.metrics.referring_video_object_segmentation_metric": [
+            "typing",
+            "tqdm",
             "pycocotools",
             "numpy",
-            "torch",
-            "tqdm",
-            "typing"
+            "torch"
         ],
         "modelscope.metrics.sequence_classification_metric": [
             "numpy",
             "sklearn",
             "typing"
         ],
         "modelscope.metrics.text_generation_metric": [
             "nltk",
-            "typing",
-            "rouge"
+            "rouge",
+            "typing"
         ],
         "modelscope.metrics.text_ranking_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.token_classification_metric": [
+            "importlib",
             "numpy",
-            "typing",
-            "importlib"
+            "typing"
+        ],
+        "modelscope.metrics.translation_evaluation_metric": [
+            "pandas",
+            "importlib",
+            "typing"
         ],
         "modelscope.metrics.video_frame_interpolation_metric": [
+            "typing",
             "numpy",
-            "math",
             "lpips",
-            "torch",
-            "typing"
+            "math",
+            "torch"
         ],
         "modelscope.metrics.video_stabilization_metric": [
-            "numpy",
             "sys",
-            "tempfile",
-            "os",
-            "tqdm",
             "typing",
-            "cv2"
+            "tqdm",
+            "cv2",
+            "numpy",
+            "tempfile",
+            "os"
         ],
         "modelscope.metrics.video_summarization_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.metrics.video_super_resolution_metric.matlab_functions": [
             "math",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.metrics.video_super_resolution_metric.metric_util": [
             "numpy"
         ],
         "modelscope.metrics.video_super_resolution_metric.niqe": [
             "scipy",
             "math",
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.metrics.video_super_resolution_metric.video_super_resolution_metric": [
             "numpy",
             "typing"
         ],
         "modelscope.models.audio.aec.layers.activations": [
             "torch"
         ],
         "modelscope.models.audio.aec.layers.affine_transform": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.aec.layers.deep_fsmn": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.aec.layers.layer_base": [
             "abc",
-            "torch",
             "numpy",
+            "torch",
             "re"
         ],
         "modelscope.models.audio.aec.layers.uni_deep_fsmn": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.aec.network.loss": [
             "torch"
         ],
         "modelscope.models.audio.aec.network.modulation_loss": [
             "math",
-            "torch",
-            "torchaudio"
+            "torchaudio",
+            "torch"
         ],
         "modelscope.models.audio.aec.network.se_net": [
             "torch"
         ],
         "modelscope.models.audio.ans.complex_nn": [
             "torch"
         ],
         "modelscope.models.audio.ans.conv_stft": [
             "scipy",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.ans.denoise_net": [
             "torch"
         ],
         "modelscope.models.audio.ans.frcrn": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.ans.layers.activations": [
             "torch"
         ],
         "modelscope.models.audio.ans.layers.affine_transform": [
             "torch"
         ],
         "modelscope.models.audio.ans.layers.layer_base": [
-            "torch",
-            "abc",
             "six",
-            "numpy"
+            "abc",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.ans.layers.uni_deep_fsmn": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.ans.se_module_complex": [
             "torch"
         ],
         "modelscope.models.audio.ans.unet": [
             "torch"
         ],
         "modelscope.models.audio.asr.generic_automatic_speech_recognition": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.asr.wenet_automatic_speech_recognition": [
             "wenetruntime",
             "json",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.itn.generic_inverse_text_processing": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.kws.farfield.fsmn": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.audio.kws.farfield.fsmn_sele_v2": [
             "torch"
         ],
+        "modelscope.models.audio.kws.farfield.fsmn_sele_v3": [
+            "torch"
+        ],
         "modelscope.models.audio.kws.farfield.model": [
             "tempfile",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.kws.farfield.model_def": [
-            "math",
             "enum",
-            "struct"
+            "struct",
+            "math"
         ],
         "modelscope.models.audio.kws.generic_key_word_spotting": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.kws.nearfield.cmvn": [
-            "torch",
             "numpy",
+            "torch",
             "re"
         ],
         "modelscope.models.audio.kws.nearfield.fsmn": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.models.audio.kws.nearfield.model": [
-            "tempfile",
-            "os",
-            "torch",
             "sys",
-            "typing"
+            "typing",
+            "os",
+            "tempfile",
+            "torch"
         ],
         "modelscope.models.audio.punc.generic_punctuation": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.audio.separation.layer_norm": [
-            "torch",
-            "__future__"
+            "__future__",
+            "torch"
         ],
         "modelscope.models.audio.separation.mossformer": [
-            "torch",
             "copy",
+            "torch",
             "os",
             "typing"
         ],
         "modelscope.models.audio.separation.mossformer_block": [
             "torch"
         ],
         "modelscope.models.audio.separation.mossformer_conv_module": [
             "torch"
         ],
         "modelscope.models.audio.sv.DTDNN": [
+            "typing",
             "torchaudio",
             "os",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.audio.sv.DTDNN_layers": [
             "torch"
         ],
+        "modelscope.models.audio.sv.ERes2Net": [
+            "typing",
+            "torchaudio",
+            "os",
+            "math",
+            "torch"
+        ],
         "modelscope.models.audio.sv.ecapa_tdnn": [
+            "typing",
             "torchaudio",
+            "os",
             "math",
+            "torch"
+        ],
+        "modelscope.models.audio.sv.fusion": [
+            "torch"
+        ],
+        "modelscope.models.audio.sv.generic_speaker_verification": [
             "os",
-            "torch",
             "typing"
         ],
-        "modelscope.models.audio.sv.generic_speaker_verification": [
+        "modelscope.models.audio.sv.pooling_layers": [
+            "torch"
+        ],
+        "modelscope.models.audio.sv.rdino": [
             "typing",
-            "os"
+            "torchaudio",
+            "os",
+            "math",
+            "torch"
         ],
-        "modelscope.models.audio.tts.sambert_hifi": [
+        "modelscope.models.audio.sv.speaker_change_locator": [
+            "typing",
+            "torchaudio",
             "numpy",
-            "yaml",
-            "shutil",
+            "os",
+            "torch",
+            "collections"
+        ],
+        "modelscope.models.audio.tts.sambert_hifi": [
+            "datetime",
             "zipfile",
+            "wave",
+            "shutil",
             "json",
-            "datetime",
-            "os",
-            "matplotlib",
             "__future__",
-            "wave"
-        ],
-        "modelscope.models.audio.tts.voice": [
             "numpy",
+            "matplotlib",
             "yaml",
-            "pickle",
-            "threading",
+            "os"
+        ],
+        "modelscope.models.audio.tts.voice": [
             "kantts",
             "json",
-            "os",
+            "time",
+            "pickle",
+            "numpy",
             "torch",
-            "collections",
-            "time"
+            "threading",
+            "yaml",
+            "os",
+            "collections"
         ],
         "modelscope.models.base.base_head": [
             "abc",
             "typing"
         ],
         "modelscope.models.base.base_model": [
             "abc",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.base.base_torch_head": [
             "torch",
             "typing"
         ],
         "modelscope.models.base.base_torch_model": [
+            "functools",
+            "typing",
+            "copy",
             "packaging",
-            "os",
             "torch",
-            "copy",
-            "functools",
-            "typing"
+            "os"
         ],
         "modelscope.models.builder": [],
         "modelscope.models.cv.abnormal_object_detection.mmdet_model": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.mask_scoring_roi_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.abnormal_object_detection.mmdet_ms.roi_head.roi_extractors.single_level_roi_extractor": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.action_detection.action_detection_onnx": [
-            "uuid",
-            "numpy",
-            "urllib",
             "shutil",
+            "subprocess",
+            "urllib",
             "onnxruntime",
+            "cv2",
+            "numpy",
+            "uuid",
             "tempfile",
-            "subprocess",
-            "os",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.action_detection.modules.action_detection_pytorch": [
-            "fvcore",
-            "detectron2",
+            "typing",
             "logging",
-            "torch",
-            "typing"
+            "detectron2",
+            "fvcore",
+            "torch"
         ],
         "modelscope.models.cv.action_detection.modules.resnet": [
             "torch",
             "detectron2"
         ],
         "modelscope.models.cv.action_recognition.models": [
             "torch"
@@ -10619,157 +10647,157 @@
             "torch"
         ],
         "modelscope.models.cv.action_recognition.tada_convnext": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.action_recognition.temporal_patch_shift_transformer": [
-            "numpy",
-            "torchvision",
+            "functools",
             "einops",
+            "torchvision",
+            "operator",
+            "numpy",
             "abc",
-            "timm",
             "torch",
-            "operator",
-            "functools"
+            "timm"
         ],
         "modelscope.models.cv.animal_recognition.resnet": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.animal_recognition.splat": [
             "torch"
         ],
         "modelscope.models.cv.bad_image_detecting.bad_image_detecting": [
+            "typing",
+            "torchvision",
             "numpy",
             "os",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.body_2d_keypoints.hrnet_basic_modules": [
             "torch"
         ],
         "modelscope.models.cv.body_2d_keypoints.hrnet_v2": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.body_2d_keypoints.w48": [],
         "modelscope.models.cv.body_3d_keypoints.cannonical_pose.body_3d_pose": [
+            "typing",
+            "logging",
             "numpy",
             "os",
-            "logging",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.cannonical_pose.canonical_pose_modules": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.backbone": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.block": [
-            "math",
+            "einops",
             "torch",
-            "einops"
+            "math"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.directed_graph": [
-            "numpy",
             "sys",
+            "numpy",
             "typing"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer": [
             "torch"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.hdformer_detector": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.body_3d_keypoints.hdformer.skeleton": [
             "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.LK.lk": [
             "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.config": [
             "easydict",
             "numpy",
             "os"
         ],
         "modelscope.models.cv.cartoon.facelib.face_detector": [
             "tensorflow",
-            "numpy",
             "time",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.face_landmark": [
+            "cv2",
             "tensorflow",
-            "numpy",
-            "cv2"
+            "numpy"
         ],
         "modelscope.models.cv.cartoon.facelib.facer": [
-            "numpy",
+            "cv2",
             "time",
-            "cv2"
+            "numpy"
         ],
         "modelscope.models.cv.cartoon.loss": [
+            "tensorflow",
             "joblib",
             "numpy",
+            "skimage",
             "scipy",
-            "tensorflow",
-            "os",
-            "skimage"
+            "os"
         ],
         "modelscope.models.cv.cartoon.model_tf": [
             "tensorflow",
             "typing"
         ],
         "modelscope.models.cv.cartoon.mtcnn_pytorch.src.align_trans": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.cartoon.mtcnn_pytorch.src.matlab_cp2tform": [
             "numpy"
         ],
         "modelscope.models.cv.cartoon.network": [
             "tensorflow"
         ],
         "modelscope.models.cv.cartoon.utils": [
             "tensorflow",
+            "cv2",
             "numpy",
-            "os",
             "random",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.c3d": [
             "torch"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.resnet2p1d": [
             "torch"
         ],
         "modelscope.models.cv.cmdssl_video_embedding.resnet3d": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.annotator": [
-            "numpy",
-            "mmseg",
             "mmcv",
-            "einops",
+            "mmseg",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "cv2"
+            "einops",
+            "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.api": [
+            "torchvision",
             "cv2",
             "torch",
-            "os",
-            "torchvision"
+            "os"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.base_model": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.blocks": [
             "torch"
         ],
@@ -10779,298 +10807,292 @@
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.midas_net": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.midas_net_custom": [
             "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.transforms": [
+            "cv2",
             "math",
-            "numpy",
-            "cv2"
+            "numpy"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.midas.vit": [
             "types",
             "math",
-            "timm",
-            "torch"
+            "torch",
+            "timm"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.midas.utils": [
+            "cv2",
+            "sys",
             "numpy",
-            "re",
             "torch",
-            "sys",
-            "cv2"
+            "re"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.mlsd.mbv2_mlsd_large": [
-            "torch",
             "sys",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.mlsd.utils": [
-            "torch",
+            "cv2",
             "numpy",
-            "os",
-            "cv2"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.body": [
+            "torchvision",
+            "cv2",
+            "time",
             "numpy",
-            "math",
-            "scipy",
-            "torch",
             "matplotlib",
-            "time",
-            "torchvision",
-            "cv2"
+            "scipy",
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.hand": [
-            "numpy",
-            "math",
-            "scipy",
             "json",
-            "torch",
-            "matplotlib",
+            "cv2",
             "time",
+            "numpy",
+            "matplotlib",
             "skimage",
-            "cv2"
+            "scipy",
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.model": [
             "torch",
             "collections"
         ],
         "modelscope.models.cv.controllable_image_generation.annotator.openpose.util": [
-            "numpy",
+            "cv2",
             "math",
-            "matplotlib",
-            "cv2"
+            "numpy",
+            "matplotlib"
         ],
         "modelscope.models.cv.controllable_image_generation.controlnet": [
-            "numpy",
-            "typing",
-            "math",
             "einops",
+            "sys",
+            "typing",
             "PIL",
-            "control_ldm",
-            "tempfile",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
+            "control_ldm",
+            "math",
             "random",
-            "sys",
-            "cv2"
+            "tempfile",
+            "os"
         ],
         "modelscope.models.cv.crowd_counting.cc_model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.crowd_counting.hrnet_aspp_relu": [
+            "functools",
+            "logging",
             "numpy",
             "os",
-            "logging",
-            "torch",
-            "functools"
-        ],
-        "modelscope.models.cv.easycv_base": [
-            "easycv"
-        ],
-        "modelscope.models.cv.face_2d_keypoints.face_2d_keypoints_align": [
-            "easycv"
+            "torch"
         ],
         "modelscope.models.cv.face_attribute_recognition.fair_face.face_attribute_recognition": [
-            "numpy",
+            "torchvision",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "torchvision",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.face_detection.mogface.models.detectors": [
-            "torch",
+            "cv2",
             "numpy",
-            "os",
-            "cv2"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.face_detection.mogface.models.mogface": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.mogprednet": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.resnet": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.mogface.models.utils": [
             "math",
-            "torch",
             "numpy",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.box_utils": [
             "numpy",
             "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.detector": [
-            "torch",
             "numpy",
-            "os",
-            "PIL"
+            "torch",
+            "PIL",
+            "os"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.first_stage": [
             "math",
-            "torch",
             "numpy",
+            "torch",
             "PIL"
         ],
         "modelscope.models.cv.face_detection.mtcnn.models.get_nets": [
-            "torch",
             "numpy",
+            "torch",
             "collections"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.LK.lk": [
             "numpy"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.face_detector": [
+            "cv2",
             "tensorflow",
-            "numpy",
-            "cv2"
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.face_landmark": [
+            "cv2",
             "tensorflow",
-            "numpy",
-            "cv2"
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.peppa_pig_face.facer": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.face_detection.retinaface.detection": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.face_detection.retinaface.models.net": [
-            "torch",
+            "torchvision",
             "time",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.face_detection.retinaface.models.retinaface": [
+            "torchvision",
             "torch",
-            "collections",
-            "torchvision"
+            "collections"
         ],
         "modelscope.models.cv.face_detection.retinaface.utils": [
             "math",
-            "torch",
             "numpy",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.damofd_detect": [
-            "torch",
             "copy",
+            "torch",
             "os",
             "typing"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.core.bbox.transforms": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.core.post_processing.bbox_nms": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.auto_augment": [
-            "mmdet",
-            "numpy",
             "mmcv",
+            "cv2",
+            "numpy",
             "copy",
-            "cv2"
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.formating": [
-            "mmdet",
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.loading": [
-            "mmdet",
             "pycocotools",
             "numpy",
-            "os"
+            "os",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.pipelines.transforms": [
-            "mmdet",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.datasets.retinaface": [
-            "mmdet",
-            "numpy"
+            "numpy",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.master_net": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.mobilenet": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.backbones.resnet": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.dense_heads.scrfd_head": [
-            "mmdet",
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.base": [
-            "mmdet",
-            "abc",
-            "numpy",
             "mmcv",
+            "numpy",
+            "abc",
             "torch",
-            "collections"
+            "collections",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.scrfd": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.single_stage": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.mmdet_patch.models.detectors.tinymog": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.face_detection.scrfd.preprocessor": [
             "numpy",
-            "typing",
-            "PIL"
+            "PIL",
+            "typing"
         ],
         "modelscope.models.cv.face_detection.scrfd.scrfd_detect": [
+            "typing",
+            "copy",
             "numpy",
             "os",
-            "torch",
-            "copy",
-            "typing"
+            "torch"
         ],
         "modelscope.models.cv.face_detection.scrfd.tinymog_detect": [
-            "torch",
             "copy",
+            "torch",
             "os",
             "typing"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.detection": [
-            "torch",
+            "cv2",
             "numpy",
-            "os",
-            "cv2"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.box_utils": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.mb_tiny": [
             "torch"
@@ -11082,108 +11104,108 @@
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.mb_tiny_fd": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.predictor": [
             "torch"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.ssd.ssd": [
-            "torch",
             "numpy",
+            "torch",
             "collections",
             "typing"
         ],
         "modelscope.models.cv.face_detection.ulfd_slim.vision.transforms": [
             "types",
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.face_emotion.efficient.model": [
             "torch"
         ],
         "modelscope.models.cv.face_emotion.efficient.utils": [
-            "re",
+            "functools",
             "math",
             "torch",
             "collections",
-            "functools"
+            "re"
         ],
         "modelscope.models.cv.face_emotion.emotion_infer": [
-            "torch",
             "torchvision",
+            "torch",
             "PIL"
         ],
         "modelscope.models.cv.face_emotion.emotion_model": [
-            "torch",
             "sys",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.face_emotion.face_alignment.face": [
+            "cv2",
             "tensorflow",
             "numpy",
-            "os",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.face_emotion.face_alignment.face_align": [
-            "numpy",
-            "PIL",
-            "os",
             "sys",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os"
         ],
         "modelscope.models.cv.face_generation.op.conv2d_gradfix": [
-            "torch",
             "warnings",
-            "contextlib"
+            "contextlib",
+            "torch"
         ],
         "modelscope.models.cv.face_generation.op.fused_act": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.face_generation.op.upfirdn2d": [
-            "torch",
             "collections",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.face_generation.stylegan2": [
-            "math",
-            "torch",
+            "functools",
             "operator",
+            "math",
             "random",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.det_infer": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.ghost_pan": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.nanodet_plus_head": [
+            "torchvision",
+            "cv2",
             "numpy",
             "math",
-            "torch",
-            "torchvision",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.one_stage_detector": [
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.shufflenetv2": [
             "torch"
         ],
         "modelscope.models.cv.face_human_hand_detection.utils": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.align_face": [
+            "cv2",
             "numpy",
-            "skimage",
-            "cv2"
+            "skimage"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.arcface_backbone": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.common": [
             "torch"
         ],
@@ -11197,315 +11219,309 @@
         ],
         "modelscope.models.cv.face_recognition.torchkit.backbone.model_resnet": [
             "torch"
         ],
         "modelscope.models.cv.face_recognition.torchkit.rts_backbone": [
             "math",
             "torch",
-            "collections",
-            "os"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.face_reconstruction.models.bfm": [
             "scipy",
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.face_reconstruction.models.de_retouching_module": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.large_base_lmks_infer": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.nets.large_base_lmks_net": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facelandmark.nets.large_eyeball_net": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.facerecon_model": [
+            "cv2",
             "numpy",
             "os",
             "torch",
-            "collections",
-            "cv2"
+            "collections"
         ],
         "modelscope.models.cv.face_reconstruction.models.losses": [
-            "torch",
             "numpy",
-            "kornia"
+            "kornia",
+            "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.networks": [
             "torch",
+            "os",
             "kornia",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.models.cv.face_reconstruction.models.nv_diffrast": [
+            "typing",
             "numpy",
-            "nvdiffrast",
-            "torch",
             "warnings",
-            "typing"
+            "nvdiffrast",
+            "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.opt": [],
         "modelscope.models.cv.face_reconstruction.models.pix2pix.networks": [
-            "torch",
-            "functools"
+            "functools",
+            "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.pix2pix.pix2pix_model": [
             "torch"
         ],
         "modelscope.models.cv.face_reconstruction.models.pix2pix.pix2pix_options": [],
         "modelscope.models.cv.face_reconstruction.models.renderer": [
+            "numpy",
             "imageio",
             "torch",
-            "numpy",
             "skimage"
         ],
         "modelscope.models.cv.face_reconstruction.models.unet": [
-            "torch",
-            "warnings"
+            "warnings",
+            "torch"
         ],
         "modelscope.models.cv.face_reconstruction.utils": [
-            "numpy",
+            "numba",
+            "PIL",
             "argparse",
+            "cv2",
+            "numpy",
+            "torch",
+            "scipy",
             "math",
-            "PIL",
-            "numba",
             "array",
-            "scipy",
-            "os",
-            "torch",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.facial_expression_recognition": [
-            "numpy",
             "PIL",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.transforms": [
-            "types",
-            "numpy",
             "numbers",
+            "numpy",
             "torch",
+            "types",
             "PIL"
         ],
         "modelscope.models.cv.facial_expression_recognition.fer.vgg": [
             "torch"
         ],
         "modelscope.models.cv.facial_landmark_confidence.flc.facial_landmark_confidence": [
-            "numpy",
             "PIL",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.facial_landmark_confidence.flc.manual_landmark_net": [
             "math",
             "torch"
         ],
-        "modelscope.models.cv.hand_2d_keypoints.hand_2d_keypoints": [
-            "easycv"
-        ],
         "modelscope.models.cv.hand_static.hand_model": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
             "sys",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.hand_static.networks": [
+            "torchvision",
             "torch",
-            "os",
-            "torchvision"
+            "os"
         ],
         "modelscope.models.cv.human_reconstruction.Reconstruction": [
-            "numpy",
-            "skimage",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "skimage",
+            "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.Embedding": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.PixToMesh": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.Res_backbone": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.Surface_head": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.detectors": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.geometry": [
             "torch"
         ],
         "modelscope.models.cv.human_reconstruction.models.human_segmenter": [
+            "cv2",
             "tensorflow",
-            "numpy",
-            "cv2"
+            "numpy"
         ],
         "modelscope.models.cv.human_reconstruction.models.networks": [
-            "torch",
+            "functools",
             "numpy",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.human_reconstruction.utils": [
             "mcubes",
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
-        "modelscope.models.cv.human_wholebody_keypoint.human_wholebody_keypoint": [
-            "easycv"
-        ],
         "modelscope.models.cv.image_binary_quant_classification.binary_quant_model": [
             "torch",
-            "collections",
-            "os"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_binary_quant_classification.bnext": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.image_body_reshaping": [
+            "typing",
+            "cv2",
             "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.model": [
             "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.person_info": [
-            "torch",
-            "numpy",
             "copy",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.body": [
+            "cv2",
             "numpy",
-            "math",
             "scipy",
-            "torch",
-            "cv2"
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.model": [
             "torch",
             "collections"
         ],
         "modelscope.models.cv.image_body_reshaping.pose_estimator.util": [
             "numpy"
         ],
         "modelscope.models.cv.image_body_reshaping.slim_utils": [
-            "numba",
+            "cv2",
             "numpy",
+            "numba",
             "os",
             "math",
-            "torch",
             "random",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_classification.backbones.beit_v2": [
+            "functools",
             "mmcv",
-            "math",
-            "warnings",
             "einops",
-            "os",
+            "typing",
             "mmcls",
             "torch",
-            "collections",
-            "functools",
+            "warnings",
+            "math",
             "itertools",
-            "typing"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_classification.backbones.nextvit": [
+            "functools",
             "mmcv",
-            "math",
-            "warnings",
             "einops",
-            "os",
+            "typing",
             "mmcls",
             "torch",
-            "collections",
-            "functools",
+            "warnings",
+            "math",
             "itertools",
-            "typing"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_classification.mmcls_model": [
             "os"
         ],
         "modelscope.models.cv.image_classification.resnet50_cc": [
-            "math",
+            "torchvision",
             "os",
+            "math",
             "torch",
-            "collections",
-            "torchvision"
+            "collections"
         ],
         "modelscope.models.cv.image_classification.utils": [
             "numpy",
+            "torch",
             "os",
-            "mmcls",
             "math",
-            "torch",
-            "collections",
-            "itertools"
+            "itertools",
+            "mmcls",
+            "collections"
         ],
         "modelscope.models.cv.image_color_enhance.adaint.adaint": [
+            "typing",
+            "torchvision",
             "numbers",
             "os",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.image_color_enhance.csrnet": [
+            "functools",
             "math",
-            "torch",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.image_color_enhance.deeplpf.deeplpf_image_color_enhance": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_color_enhance.deeplpf.deeplpfnet": [
             "math",
-            "torch",
-            "matplotlib"
+            "matplotlib",
+            "torch"
         ],
         "modelscope.models.cv.image_color_enhance.image_color_enhance": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.ddcolor": [
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.ddcolor_for_image_colorization": [
+            "typing",
+            "copy",
             "numpy",
             "os",
-            "torch",
-            "copy",
-            "typing"
+            "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.loss": [
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.convnext": [
             "torch",
             "timm"
@@ -11515,322 +11531,322 @@
             "torch"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.transformer_utils": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.unet": [
-            "collections",
+            "enum",
             "torch",
-            "enum"
+            "collections"
         ],
         "modelscope.models.cv.image_colorization.ddcolor.utils.vgg": [
+            "torchvision",
             "torch",
-            "collections",
             "os",
-            "torchvision"
+            "collections"
         ],
         "modelscope.models.cv.image_colorization.unet.unet": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_colorization.unet.utils": [
-            "torch",
+            "functools",
             "enum",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.image_debanding.rrdb.rrdb_image_debanding": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_deblur.nafnet_for_image_deblur": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.defrcn_for_fewshot": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.coco_evaluation": [
-            "pycocotools",
-            "fvcore",
-            "numpy",
-            "tabulate",
+            "json",
+            "copy",
             "contextlib",
+            "logging",
+            "tabulate",
             "detectron2",
+            "fvcore",
             "io",
-            "json",
-            "os",
-            "logging",
+            "numpy",
             "torch",
-            "collections",
-            "copy",
-            "itertools"
+            "pycocotools",
+            "itertools",
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.evaluator": [
-            "detectron2",
             "datetime",
             "logging",
-            "torch",
-            "time"
+            "detectron2",
+            "time",
+            "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.evaluation.pascal_voc_evaluation": [
-            "tempfile",
             "numpy",
-            "detectron2",
             "os",
+            "tempfile",
+            "detectron2",
             "collections"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.calibration_layer": [
-            "torch",
+            "cv2",
             "sklearn",
             "detectron2",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.defrcn": [
             "torch",
+            "detectron2",
             "os",
-            "typing",
-            "detectron2"
+            "typing"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.fast_rcnn": [
-            "torch",
-            "numpy",
             "detectron2",
-            "fvcore"
+            "fvcore",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.gdl": [
             "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.resnet": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.models.roi_heads": [
-            "torch",
-            "detectron2"
+            "detectron2",
+            "torch"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.coco_register": [
-            "pycocotools",
             "fvcore",
-            "detectron2",
-            "contextlib",
+            "io",
+            "pycocotools",
             "os",
-            "io"
+            "contextlib",
+            "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.configuration_mapper": [
             "detectron2"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.model_surgery_op": [
             "argparse",
             "torch",
             "os"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.register_data": [],
         "modelscope.models.cv.image_defrcn_fewshot.utils.requirements_check": [
-            "packaging",
+            "sys",
             "importlib",
             "importlib_metadata",
-            "collections",
-            "sys"
+            "packaging",
+            "collections"
         ],
         "modelscope.models.cv.image_defrcn_fewshot.utils.voc_register": [
             "fvcore",
             "numpy",
-            "xml",
+            "os",
             "detectron2",
-            "os"
+            "xml"
         ],
         "modelscope.models.cv.image_denoise.nafnet.NAFNet_arch": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_denoise.nafnet.arch_util": [
             "torch"
         ],
         "modelscope.models.cv.image_denoise.nafnet_for_image_denoise": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_depth": [
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_layers": [
-            "torch",
             "numpy",
+            "torch",
             "timm"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.newcrf_utils": [
             "importlib",
-            "os",
-            "warnings",
             "torch",
-            "collections",
+            "pkgutil",
+            "warnings",
             "torchvision",
-            "pkgutil"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.swin_transformer": [
-            "torch",
             "numpy",
+            "torch",
             "timm"
         ],
         "modelscope.models.cv.image_depth_estimation.networks.uper_crf_head": [
-            "torch",
-            "mmcv"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.image_depth_estimation.newcrfs_model": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.depth_estimation_bts_model": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.bts_model": [
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.decoder": [
             "torch"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.encoder": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.image_depth_estimation_bts.networks.utils": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_driving_perception.image_driving_percetion_model": [
+            "typing",
+            "cv2",
             "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_driving_perception.preprocessor": [
-            "torch",
+            "cv2",
             "numpy",
-            "typing",
-            "cv2"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.image_driving_perception.utils": [
-            "torch",
-            "numpy",
+            "torchvision",
             "time",
-            "torchvision"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.gan_wrap": [
-            "numpy",
+            "torchvision",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "torchvision",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.model": [
             "math",
-            "torch",
-            "random"
+            "random",
+            "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.conv2d_gradfix": [
-            "torch",
             "warnings",
-            "contextlib"
+            "contextlib",
+            "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.fused_act": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.facegan.op.upfirdn2d": [
             "torch",
             "collections"
         ],
         "modelscope.models.cv.image_face_fusion.facelib.align_trans": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.facelib.matlab_cp2tform": [
             "numpy"
         ],
         "modelscope.models.cv.image_face_fusion.image_face_fusion": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
-            "collections",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch",
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_face_fusion.network.aad_layer": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.aei_flow_net": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.bfm": [
             "scipy",
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.image_face_fusion.network.dense_motion": [
             "torch"
         ],
         "modelscope.models.cv.image_face_fusion.network.facerecon_model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_face_fusion.network.model_irse": [
             "torch",
             "collections"
         ],
         "modelscope.models.cv.image_face_fusion.network.ops": [
             "torch"
         ],
         "modelscope.models.cv.image_human_parsing.backbone.deeplab_resnet": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_human_parsing.m2fp.m2fp_decoder": [
             "torch"
         ],
         "modelscope.models.cv.image_human_parsing.m2fp.m2fp_encoder": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.models.cv.image_human_parsing.m2fp_net": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_human_parsing.parsing_utils": [
-            "torch",
-            "numpy",
             "copy",
-            "PIL"
+            "PIL",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_inpainting.base": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.image_inpainting.default": [
             "bisect",
             "torch"
         ],
         "modelscope.models.cv.image_inpainting.model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_inpainting.modules.ade20k.base": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.image_inpainting.modules.ade20k.resnet": [
             "math",
@@ -11842,514 +11858,525 @@
             "typing"
         ],
         "modelscope.models.cv.image_inpainting.modules.feature_matching": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.image_inpainting.modules.ffc": [
-            "torch",
             "numpy",
-            "kornia"
+            "kornia",
+            "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.inception": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.perceptual": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.image_inpainting.modules.pix2pixhd": [
+            "functools",
             "numpy",
             "logging",
             "torch",
-            "collections",
-            "functools"
+            "collections"
         ],
         "modelscope.models.cv.image_inpainting.refinement": [
-            "numpy",
-            "kornia",
-            "torch",
             "tqdm",
-            "cv2"
+            "kornia",
+            "cv2",
+            "numpy",
+            "torch"
+        ],
+        "modelscope.models.cv.image_instance_segmentation.backbones.resnet": [
+            "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.backbones.swin_transformer": [
-            "torch",
             "numpy",
+            "torch",
             "timm"
         ],
         "modelscope.models.cv.image_instance_segmentation.cascade_mask_rcnn_swin": [
-            "torch",
             "collections",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.image_instance_segmentation.datasets.transforms": [
             "numpy",
             "os"
         ],
+        "modelscope.models.cv.image_instance_segmentation.fastinst.fastinst_decoder": [
+            "math",
+            "torch"
+        ],
+        "modelscope.models.cv.image_instance_segmentation.fastinst.fastinst_encoder": [
+            "logging",
+            "torch",
+            "typing"
+        ],
+        "modelscope.models.cv.image_instance_segmentation.fastinst_model": [
+            "torch",
+            "os",
+            "typing"
+        ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.dino_decoder": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.maskdino_decoder": [
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.maskdino_encoder": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.ms_deform_attn": [
             "mmcv",
-            "math",
-            "torch",
             "warnings",
-            "__future__"
+            "math",
+            "__future__",
+            "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.position_encoding": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino.utils": [
+            "copy",
             "math",
-            "torch",
-            "copy"
+            "torch"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino_model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.maskdino_swin": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.image_instance_segmentation.model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_instance_segmentation.postprocess_utils": [
+            "cv2",
             "pycocotools",
             "numpy",
-            "torch",
             "itertools",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_matching.config.default": [
             "yacs"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.backbone.resnet_fpn": [
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr": [
-            "torch",
-            "einops"
+            "einops",
+            "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.fine_preprocess": [
-            "torch",
-            "einops"
+            "einops",
+            "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.linear_attention": [
             "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.quadtree_attention": [
             "torch",
             "timm"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.loftr_module.transformer": [
+            "copy",
             "math",
             "einops",
-            "timm",
             "torch",
-            "copy"
+            "timm"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.coarse_matching": [
-            "torch",
-            "einops"
+            "einops",
+            "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.fine_matching": [
             "math",
-            "torch",
-            "kornia"
+            "kornia",
+            "torch"
         ],
         "modelscope.models.cv.image_matching.loftr_quadtree.utils.position_encoding": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_matching.quadtree_attention_model": [
-            "numpy",
             "pathlib",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.image_matching.utils.misc": [
             "yacs"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.cas_mvsnet": [
             "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.casmvs_model": [
-            "numpy",
             "easydict",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.colmap2mvsnet": [
-            "numpy",
-            "multiprocessing",
-            "struct",
+            "functools",
             "shutil",
-            "os",
-            "collections",
+            "struct",
             "__future__",
-            "functools",
-            "cv2"
+            "cv2",
+            "multiprocessing",
+            "numpy",
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.depth_filter": [
-            "numpy",
             "plyfile",
             "PIL",
-            "os",
-            "cv2"
+            "cv2",
+            "numpy",
+            "os"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.general_eval_dataset": [
-            "numpy",
-            "re",
-            "os",
             "cv2",
-            "torch",
             "sys",
-            "PIL"
+            "numpy",
+            "torch",
+            "os",
+            "PIL",
+            "re"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.module": [
             "torch"
         ],
         "modelscope.models.cv.image_mvs_depth_estimation.utils": [
-            "torch",
-            "numpy",
+            "torchvision",
             "random",
-            "torchvision"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_paintbyexample.model": [
+            "typing",
             "omegaconf",
             "paint_ldm",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.cv.image_panoptic_segmentation.panseg_model": [
             "torch",
             "os"
         ],
-        "modelscope.models.cv.image_panoptic_segmentation.r50_panseg_model": [
-            "easycv"
-        ],
         "modelscope.models.cv.image_portrait_enhancement.align_faces": [
+            "cv2",
             "numpy",
-            "skimage",
-            "cv2"
+            "skimage"
         ],
         "modelscope.models.cv.image_portrait_enhancement.eqface.fqa": [
-            "torch",
+            "cv2",
             "numpy",
-            "os",
-            "cv2"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.image_portrait_enhancement.eqface.model_resnet": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.gpen": [
-            "math",
-            "itertools",
-            "torch",
+            "functools",
             "operator",
+            "itertools",
+            "math",
             "random",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.image_portrait_enhancement": [
             "math",
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.helpers": [
             "torch",
             "collections"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.losses": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.losses.model_irse": [
             "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.detection": [
-            "torch",
+            "cv2",
             "numpy",
-            "os",
-            "cv2"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.models.net": [
-            "torch",
+            "torchvision",
             "time",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.models.retinaface": [
+            "torchvision",
             "torch",
-            "collections",
-            "torchvision"
+            "collections"
         ],
         "modelscope.models.cv.image_portrait_enhancement.retinaface.utils": [
             "math",
-            "torch",
             "numpy",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.models.cv.image_probing_model.backbone": [
-            "numpy",
+            "functools",
+            "sys",
             "torchvision",
-            "math",
+            "operator",
             "PIL",
+            "numpy",
+            "math",
             "torch",
-            "operator",
-            "collections",
-            "sys",
-            "functools"
+            "collections"
         ],
         "modelscope.models.cv.image_probing_model.model": [
-            "torch",
             "json",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_probing_model.utils": [
             "torch",
             "re"
         ],
         "modelscope.models.cv.image_quality_assessment_degradation.degradation_model": [
+            "cv2",
+            "time",
             "numpy",
             "json",
-            "torch",
-            "collections",
-            "time",
             "torchvision",
-            "cv2"
+            "torch",
+            "collections"
         ],
         "modelscope.models.cv.image_quality_assessment_degradation.image_quality_assessment_degradation": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_quality_assessment_man.image_quality_assessment_man": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_quality_assessment_man.maniqa": [
+            "einops",
             "torch",
-            "timm",
-            "einops"
+            "timm"
         ],
         "modelscope.models.cv.image_quality_assessment_man.swin": [
-            "itertools",
-            "math",
-            "torch",
+            "collections",
             "warnings",
             "einops",
-            "collections"
+            "itertools",
+            "torch",
+            "math"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.backbones.resnet": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.censeo_ivqa_model": [
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.heads.simple_head": [
             "torch"
         ],
         "modelscope.models.cv.image_quality_assessment_mos.image_quality_assessment_mos": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.image_reid_person.pass_model": [
-            "torch",
             "enum",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.image_reid_person.transreid_model": [
-            "torch",
-            "collections",
+            "functools",
             "itertools",
-            "functools"
+            "torch",
+            "collections"
         ],
         "modelscope.models.cv.image_restoration.demoire_models.nets": [
             "torch"
         ],
         "modelscope.models.cv.image_restoration.image_restoration_model": [
-            "torch",
+            "cv2",
             "numpy",
-            "os",
-            "cv2"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.data_util": [],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.feature_extractors": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.pixel_classifier": [
-            "numpy",
             "PIL",
+            "numpy",
             "os",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_seg.utils": [
-            "torch",
-            "numpy",
+            "PIL",
             "random",
-            "PIL"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.image_semantic_segmentation.ddpm_segmentation_model": [
             "torch",
-            "typing",
             "os",
-            "ddpm_guided_diffusion"
+            "ddpm_guided_diffusion",
+            "typing"
         ],
         "modelscope.models.cv.image_semantic_segmentation.pan_merge.base_panoptic_fusion_head": [
-            "mmdet",
+            "mmcv",
             "abc",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.pan_merge.maskformer_semantic_head": [
-            "mmdet",
-            "torch"
-        ],
-        "modelscope.models.cv.image_semantic_segmentation.segformer": [
-            "easycv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.semantic_seg_model": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.adapter_modules": [
-            "mmdet",
-            "timm",
+            "functools",
             "logging",
             "torch",
-            "functools"
+            "timm",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.base.beit": [
-            "mmdet",
-            "timm",
+            "functools",
             "mmcv",
             "math",
             "torch",
-            "functools"
+            "timm",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.backbone.beit_adapter": [
+            "logging",
             "mmdet",
             "math",
-            "timm",
-            "logging",
-            "torch"
+            "torch",
+            "timm"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.base_decode_head": [
-            "mmdet",
+            "mmcv",
             "abc",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.decode_heads.mask2former_head_from_mmseg": [
-            "mmdet",
-            "torch",
             "copy",
-            "mmcv"
+            "mmcv",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.base_segmentor": [
-            "abc",
-            "numpy",
             "mmcv",
+            "numpy",
             "warnings",
+            "abc",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.models.segmentors.encoder_decoder_mask2former": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.builder": [
             "mmcv"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.data_process_func": [
-            "mmdet",
-            "mmcv"
+            "mmcv",
+            "mmdet"
         ],
         "modelscope.models.cv.image_semantic_segmentation.vit_adapter.utils.seg_func": [
-            "torch",
-            "warnings"
+            "warnings",
+            "torch"
         ],
         "modelscope.models.cv.image_skychange.preprocessor": [
             "pdb",
-            "numpy",
             "json",
-            "numbers",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "numbers",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.BlockModules": [
             "torch"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.hrnet_backnone": [
-            "torch",
+            "logging",
             "numpy",
-            "os",
-            "logging"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.hrnet_super_and_ocr": [
-            "torch",
             "numpy",
-            "__future__"
+            "__future__",
+            "torch"
         ],
         "modelscope.models.cv.image_skychange.ptsemseg.unet": [
             "torch"
         ],
         "modelscope.models.cv.image_skychange.skychange": [
             "pdb",
-            "numpy",
-            "PIL",
             "json",
+            "torchvision",
+            "PIL",
             "numbers",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "collections",
-            "torchvision",
-            "cv2"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.image_skychange.skychange_model": [
             "pdb",
-            "math",
-            "time",
+            "typing",
             "json",
+            "cv2",
+            "time",
             "os",
+            "math",
             "torch",
-            "collections",
-            "typing",
-            "cv2"
+            "collections"
         ],
         "modelscope.models.cv.image_to_image_generation.data.transforms": [
+            "torchvision",
             "math",
             "random",
-            "torchvision",
             "PIL"
         ],
         "modelscope.models.cv.image_to_image_generation.model": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_generation.models.autoencoder": [
@@ -12365,17 +12392,17 @@
             "torch"
         ],
         "modelscope.models.cv.image_to_image_generation.ops.losses": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.data.transforms": [
+            "torchvision",
             "math",
             "random",
-            "torchvision",
             "PIL"
         ],
         "modelscope.models.cv.image_to_image_translation.model_translation": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.models.autoencoder": [
@@ -12383,440 +12410,434 @@
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.models.clip": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.apps": [
-            "numpy",
-            "PIL",
             "artist",
-            "os",
+            "torchvision",
+            "PIL",
+            "numpy",
             "torch",
-            "torchvision"
+            "os"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.degradation": [
-            "scipy",
+            "cv2",
             "numpy",
             "os",
+            "scipy",
             "math",
-            "torch",
             "random",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.diffusion": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.losses": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.metrics": [
             "scipy",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.random_color": [
             "colorsys",
             "random"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.random_mask": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.svd": [
             "torch"
         ],
         "modelscope.models.cv.image_to_image_translation.ops.utils": [
-            "binascii",
-            "hashlib",
-            "numpy",
-            "multiprocessing",
-            "math",
-            "io",
             "zipfile",
-            "PIL",
-            "base64",
             "json",
-            "os",
+            "PIL",
+            "cv2",
+            "numpy",
             "torch",
-            "cv2"
+            "io",
+            "base64",
+            "hashlib",
+            "multiprocessing",
+            "binascii",
+            "math",
+            "os"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.backbone.resnet_DA": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.backbone.vit_horizon_pry_image": [
-            "torch",
             "numpy",
+            "torch",
             "timm"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.fourier": [
             "scipy",
             "numpy",
             "PIL"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.panostretch": [
+            "functools",
             "scipy",
-            "numpy",
-            "functools"
+            "numpy"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.misc.post_proc": [
             "scipy",
             "numpy",
             "sklearn"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.modality.layout": [
-            "numpy",
-            "math",
             "shapely",
+            "numpy",
             "scipy",
+            "math",
             "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.panovit": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.networks.utils": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.indoor_layout_estimation.panovit": [
             "torch",
             "numpy",
             "yacs",
             "os"
         ],
         "modelscope.models.cv.language_guided_video_summarization.summarizer": [
-            "numpy",
-            "argparse",
             "bmt_clipit",
+            "typing",
             "videofeatures_clipit",
+            "argparse",
+            "numpy",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.layers": [
             "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.models": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.modules": [
             "torch"
         ],
         "modelscope.models.cv.language_guided_video_summarization.transformer.sub_layers": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.motion_generation.model": [],
         "modelscope.models.cv.motion_generation.modules.cfg_sampler": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.gaussian_diffusion": [
-            "numpy",
             "enum",
+            "numpy",
+            "copy",
             "math",
-            "torch",
-            "copy"
+            "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.mdm": [
-            "torch",
             "numpy",
-            "clip"
+            "clip",
+            "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.respace": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.rotation2xyz": [
             "torch"
         ],
         "modelscope.models.cv.motion_generation.modules.smpl": [
             "numpy",
-            "contextlib",
-            "os",
             "torch",
-            "smplx"
+            "contextlib",
+            "smplx",
+            "os"
         ],
         "modelscope.models.cv.movie_scene_segmentation.get_model": [],
         "modelscope.models.cv.movie_scene_segmentation.model": [
-            "numpy",
-            "math",
+            "typing",
+            "torchvision",
             "shotdetect_scenedetect_lgss",
-            "einops",
+            "tqdm",
             "PIL",
+            "math",
+            "numpy",
             "os",
-            "torch",
-            "tqdm",
-            "typing",
-            "torchvision"
+            "einops",
+            "torch"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.head": [
             "torch"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.save_op": [
-            "numpy",
-            "os",
             "cv2",
+            "numpy",
+            "subprocess",
             "tqdm",
-            "subprocess"
+            "os"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.shot_encoder": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.movie_scene_segmentation.utils.trn": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.cv.nerf_recon_acc.dataloader.nerf_dataset": [
-            "numpy",
-            "math",
-            "PIL",
             "json",
+            "torchvision",
+            "PIL",
+            "numpy",
             "os",
-            "torch",
-            "torchvision"
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.nerf_recon_acc.dataloader.read_write_model": [
+            "argparse",
             "numpy",
-            "os",
             "struct",
-            "argparse",
+            "os",
             "collections"
         ],
         "modelscope.models.cv.nerf_recon_acc.nerf_preprocess": [
-            "numpy",
-            "glob",
             "tensorflow",
             "subprocess",
-            "os",
             "typing",
-            "cv2"
+            "cv2",
+            "glob",
+            "numpy",
+            "os"
         ],
         "modelscope.models.cv.nerf_recon_acc.nerf_recon_acc": [
+            "tqdm",
+            "cv2",
+            "time",
             "numpy",
             "glob",
             "os",
-            "torch",
-            "tqdm",
-            "time",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.nerf": [
+            "torch",
             "nerfacc",
             "tinycudann",
-            "torch",
             "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.segmenter": [
             "tensorflow",
             "numpy"
         ],
         "modelscope.models.cv.nerf_recon_acc.network.utils": [
             "numpy",
-            "mcubes",
-            "tinycudann",
             "torch",
-            "collections",
-            "gc"
-        ],
-        "modelscope.models.cv.object_detection.dino": [
-            "easycv"
+            "gc",
+            "tinycudann",
+            "mcubes",
+            "collections"
         ],
         "modelscope.models.cv.object_detection.mmdet_model": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.backbones.vit": [
+            "functools",
             "mmdet",
             "math",
-            "timm",
             "torch",
-            "functools"
+            "timm"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.anchor_head": [
             "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.dense_heads.rpn_head": [
-            "mmdet",
-            "torch",
             "copy",
-            "mmcv"
+            "mmcv",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.necks.fpn": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.bbox_heads.convfc_bbox_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.roi_heads.mask_heads.fcn_mask_head": [
+            "mmcv",
             "mmdet",
             "numpy",
-            "mmcv",
-            "torch",
-            "warnings"
+            "warnings",
+            "torch"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.utils.checkpoint": [
             "mmcv",
+            "pkgutil",
+            "torchvision",
+            "time",
             "io",
-            "tempfile",
-            "importlib",
-            "os",
-            "collections",
             "torch",
             "warnings",
-            "time",
-            "torchvision",
-            "pkgutil"
+            "importlib",
+            "tempfile",
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.object_detection.mmdet_ms.utils.convModule_norm": [
             "mmcv"
         ],
-        "modelscope.models.cv.object_detection.yolox_pai": [
-            "easycv"
-        ],
         "modelscope.models.cv.object_detection_3d.depe.depe_detect": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.assigners.hungarian_assigner_3d": [
-            "mmdet",
+            "scipy",
             "torch",
-            "scipy"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.coders.nms_free_coder": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.match_costs.match_cost": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.core.bbox.util": [
             "mmdet3d",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.nuscenes_dataset": [
-            "mmdet",
+            "numpy",
             "mmdet3d",
-            "numpy"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.loading": [
-            "mmdet",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.datasets.pipelines.transform_3d": [
-            "mmdet",
             "mmdet3d",
-            "numpy",
             "mmcv",
+            "numpy",
             "torch",
             "copy",
-            "PIL"
+            "PIL",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.backbones.vovnet": [
-            "mmdet",
+            "mmcv",
             "torch",
             "collections",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.depth_net": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.dense_heads.petrv2_dednhead": [
+            "mmcv",
+            "copy",
             "mmdet",
             "numpy",
-            "mmcv",
-            "math",
-            "mmdet3d",
             "torch",
-            "copy"
+            "math",
+            "mmdet3d"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.detectors.petr3d": [
+            "mmcv",
             "mmdet",
             "numpy",
-            "mmcv",
-            "mmdet3d",
-            "torch"
+            "torch",
+            "mmdet3d"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.necks.cp_fpn": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.petr_transformer": [
-            "mmdet",
             "mmcv",
-            "math",
-            "torch",
             "warnings",
+            "typing",
             "copy",
-            "typing"
+            "math",
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.object_detection_3d.depe.mmdet3d_plugin.models.utils.positional_encoding": [
+            "mmcv",
             "math",
-            "torch",
-            "mmcv"
+            "torch"
         ],
         "modelscope.models.cv.object_detection_3d.depe.result_vis": [
-            "numpy",
-            "pickle",
-            "argparse",
-            "mmdet3d",
             "json",
             "pyquaternion",
-            "os",
-            "cv2"
+            "mmdet3d",
+            "argparse",
+            "pickle",
+            "cv2",
+            "numpy",
+            "os"
         ],
         "modelscope.models.cv.ocr_detection.model": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.ocr_detection.modules.dbnet": [
-            "os",
-            "math",
+            "sys",
             "torch",
-            "collections",
-            "sys"
+            "math",
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.ocr_detection.modules.seg_detector_loss": [
-            "torch",
-            "sys"
+            "sys",
+            "torch"
         ],
         "modelscope.models.cv.ocr_detection.preprocessor": [
-            "numpy",
-            "math",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "math",
+            "os"
         ],
         "modelscope.models.cv.ocr_detection.utils": [
-            "pyclipper",
+            "cv2",
             "numpy",
             "shapely",
-            "cv2"
+            "pyclipper"
         ],
         "modelscope.models.cv.ocr_recognition.model": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.ocr_recognition.modules.convnext": [
             "torch"
@@ -12824,181 +12845,181 @@
         "modelscope.models.cv.ocr_recognition.modules.convnextvit": [
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.crnn": [
             "torch"
         ],
         "modelscope.models.cv.ocr_recognition.modules.timm_tinyc": [
-            "logging",
             "functools",
+            "copy",
+            "logging",
             "math",
+            "itertools",
             "torch",
-            "collections",
-            "copy",
-            "itertools"
+            "collections"
         ],
         "modelscope.models.cv.ocr_recognition.modules.vitstr": [
-            "__future__",
-            "logging",
-            "torch",
+            "functools",
             "copy",
-            "functools"
+            "logging",
+            "__future__",
+            "torch"
         ],
         "modelscope.models.cv.ocr_recognition.preprocessor": [
-            "numpy",
             "PIL",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.open_vocabulary_detection_vild.vild": [
-            "numpy",
             "tensorflow",
-            "scipy",
+            "typing",
             "clip",
-            "os",
+            "numpy",
             "torch",
-            "typing"
+            "scipy",
+            "os"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.equi": [
-            "torch",
             "numpy",
-            "collections",
-            "__future__"
+            "__future__",
+            "torch",
+            "collections"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.layers": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.mobilenet": [
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.resnet": [
             "torch"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.unifuse": [
-            "torch",
             "numpy",
-            "collections",
-            "__future__"
+            "__future__",
+            "torch",
+            "collections"
         ],
         "modelscope.models.cv.panorama_depth_estimation.networks.util": [
-            "scipy",
+            "cv2",
             "numpy",
-            "cv2"
+            "scipy"
         ],
         "modelscope.models.cv.panorama_depth_estimation.unifuse_model": [
-            "torch",
+            "torchvision",
             "numpy",
-            "os",
-            "torchvision"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.pedestrian_attribute_recognition.model": [
-            "torch",
+            "torchvision",
             "numpy",
-            "os",
-            "torchvision"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.common": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.pointnet2_utils": [
+            "pointnet2_cuda",
             "torch",
-            "typing",
-            "pointnet2_cuda"
+            "typing"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.rcp_model": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.pointcloud_sceneflow_estimation.sf_rcp": [
             "torch"
         ],
         "modelscope.models.cv.product_retrieval_embedding.item_detection": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.product_retrieval_embedding.item_embedding": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.product_retrieval_embedding.item_model": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.product_segmentation.net": [
             "torch"
         ],
         "modelscope.models.cv.product_segmentation.seg_infer": [
             "cv2",
-            "torch",
             "numpy",
+            "torch",
             "PIL"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.backbone": [
-            "torch",
+            "torchvision",
             "einops",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.criterion": [
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.matcher": [
             "scipy",
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.misc": [
-            "torch",
-            "typing",
+            "torchvision",
             "pickle",
-            "torchvision"
+            "torch",
+            "typing"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.mttr": [
-            "torch",
-            "einops"
+            "einops",
+            "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.multimodal_transformer": [
-            "einops",
+            "typing",
+            "copy",
             "transformers",
-            "os",
             "torch",
-            "copy",
-            "typing"
+            "einops",
+            "os"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.position_encoding_2d": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.postprocessing": [
             "pycocotools",
-            "torch",
+            "einops",
             "numpy",
-            "einops"
+            "torch"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.segmentation": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.referring_video_object_segmentation.utils.swin_transformer": [
-            "timm",
+            "functools",
+            "einops",
             "numpy",
-            "torch",
             "operator",
-            "functools",
-            "einops"
+            "torch",
+            "timm"
         ],
         "modelscope.models.cv.robust_image_classification.easyrobust_model": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.salient_detection.models.backbone.Res2Net_v1b": [
             "math",
@@ -13013,145 +13034,145 @@
         "modelscope.models.cv.salient_detection.models.u2net": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.models.utils": [
             "torch"
         ],
         "modelscope.models.cv.salient_detection.salient_model": [
+            "torchvision",
             "PIL",
+            "cv2",
             "os",
-            "torch",
-            "torchvision",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.shop_segmentation.common": [
-            "torch",
-            "warnings"
+            "warnings",
+            "torch"
         ],
         "modelscope.models.cv.shop_segmentation.head_fpn": [
-            "torch",
-            "timm",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "timm"
         ],
         "modelscope.models.cv.shop_segmentation.models": [
             "timm",
             "math",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.shop_segmentation.neck_fpn": [
+            "mmcv",
             "torch",
-            "timm",
-            "mmcv"
+            "timm"
         ],
         "modelscope.models.cv.shop_segmentation.shop_seg_base": [
             "torch"
         ],
         "modelscope.models.cv.shop_segmentation.shop_seg_model": [
-            "numpy",
-            "PIL",
             "json",
-            "os",
+            "typing",
+            "PIL",
+            "numpy",
             "torch",
-            "typing"
+            "os"
         ],
         "modelscope.models.cv.shop_segmentation.utils": [
-            "gzip",
-            "html",
-            "ftfy",
+            "functools",
+            "typing",
             "regex",
-            "os",
+            "html",
             "torch",
-            "typing",
-            "functools"
+            "ftfy",
+            "gzip",
+            "os"
         ],
         "modelscope.models.cv.skin_retouching.detection_model.detection_module": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.detection_model.detection_unet_in": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.inpainting_model.gconv": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.inpainting_model.inpainting_unet": [
             "torch"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.box_utils": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.net": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.network": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.predict_single": [
-            "numpy",
-            "albumentations",
-            "torch",
             "typing",
-            "torchvision"
+            "torchvision",
+            "albumentations",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.prior_box": [
             "math",
-            "torch",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.models.cv.skin_retouching.retinaface.utils": [
+            "cv2",
             "numpy",
-            "re",
             "pathlib",
-            "torch",
             "typing",
-            "cv2"
+            "torch",
+            "re"
         ],
         "modelscope.models.cv.skin_retouching.unet_deploy": [
-            "torch",
-            "warnings"
+            "warnings",
+            "torch"
         ],
         "modelscope.models.cv.skin_retouching.utils": [
-            "numpy",
             "cv2",
-            "torch",
             "time",
+            "numpy",
+            "typing",
             "einops",
-            "typing"
+            "torch"
         ],
         "modelscope.models.cv.skin_retouching.weights_init": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.data.data_augment": [
-            "math",
-            "numpy",
             "random",
-            "cv2"
+            "cv2",
+            "math",
+            "numpy"
         ],
         "modelscope.models.cv.stream_yolo.exp.base_exp": [
             "abc",
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.exp.build": [
             "sys",
             "os"
         ],
         "modelscope.models.cv.stream_yolo.exp.default.streamyolo": [
-            "torch",
             "sys",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.stream_yolo.exp.yolox_base": [
-            "torch",
             "random",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.stream_yolo.models.darknet": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.dfp_pafpn": [
             "torch"
@@ -13162,183 +13183,183 @@
         "modelscope.models.cv.stream_yolo.models.streamyolo": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.models.tal_head": [
             "torch"
         ],
         "modelscope.models.cv.stream_yolo.realtime_video_detector": [
-            "numpy",
-            "argparse",
             "json",
-            "os",
             "logging",
-            "torch",
             "tqdm",
+            "argparse",
             "time",
-            "cv2"
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.models.cv.stream_yolo.utils.boxes": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.stream_yolo.utils.format": [
             "math"
         ],
         "modelscope.models.cv.super_resolution.arch_util": [
+            "warnings",
             "torchvision",
             "math",
-            "torch",
-            "warnings",
             "itertools",
+            "torch",
             "collections"
         ],
         "modelscope.models.cv.super_resolution.ecb": [
             "torch"
         ],
         "modelscope.models.cv.super_resolution.ecbsr_model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.super_resolution.rrdbnet_arch": [
             "torch"
         ],
         "modelscope.models.cv.table_recognition.lineless_table_process": [
-            "torch",
+            "cv2",
             "numpy",
-            "shapely",
-            "cv2"
+            "torch",
+            "shapely"
         ],
         "modelscope.models.cv.table_recognition.model_lore": [
+            "typing",
+            "copy",
             "numpy",
-            "math",
-            "os",
             "torch",
-            "copy",
-            "typing"
+            "math",
+            "os"
         ],
         "modelscope.models.cv.table_recognition.modules.lore_detector": [
             "numpy",
-            "os",
-            "math",
             "torch",
-            "copy"
+            "copy",
+            "math",
+            "os"
         ],
         "modelscope.models.cv.table_recognition.modules.lore_processor": [
             "numpy",
-            "os",
-            "math",
             "torch",
-            "copy"
+            "copy",
+            "math",
+            "os"
         ],
         "modelscope.models.cv.text_driven_segmentation.clip": [
-            "hashlib",
-            "tqdm",
             "urllib",
+            "typing",
+            "torchvision",
+            "tqdm",
             "PIL",
-            "pkg_resources",
-            "os",
-            "torch",
             "warnings",
-            "typing",
-            "torchvision"
+            "os",
+            "hashlib",
+            "pkg_resources",
+            "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_base": [
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_blocks": [
             "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_model": [
-            "numpy",
-            "PIL",
             "json",
-            "os",
+            "typing",
+            "PIL",
+            "numpy",
             "torch",
-            "typing"
+            "os"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_net": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.text_driven_segmentation.lseg_vit": [
             "types",
             "math",
-            "timm",
-            "torch"
+            "torch",
+            "timm"
         ],
         "modelscope.models.cv.text_driven_segmentation.model": [
-            "torch",
             "numpy",
+            "torch",
             "collections",
             "typing"
         ],
         "modelscope.models.cv.text_driven_segmentation.simple_tokenizer": [
+            "functools",
+            "html",
             "ftfy",
-            "regex",
-            "os",
             "gzip",
-            "html",
-            "functools"
+            "regex",
+            "os"
         ],
         "modelscope.models.cv.tinynas_classfication.basic_blocks": [
-            "torch",
+            "uuid",
             "numpy",
-            "uuid"
+            "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.global_utils": [],
         "modelscope.models.cv.tinynas_classfication.master_net": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.model_zoo": [],
         "modelscope.models.cv.tinynas_classfication.plain_net_utils": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.super_blocks": [
-            "torch",
-            "uuid"
+            "uuid",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.super_res_idwexkx": [
-            "torch",
-            "uuid"
+            "uuid",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.super_res_k1kxk1": [
-            "torch",
-            "uuid"
+            "uuid",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_classfication.super_res_kxkx": [
-            "torch",
-            "uuid"
+            "uuid",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.apis.detector_evaluater": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.tinynas_detection.damo.apis.detector_inference": [
-            "torch",
             "tqdm",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.box_level_augs": [
-            "numpy",
-            "random"
+            "random",
+            "numpy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.color_augs": [
-            "torch",
-            "random"
+            "random",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.gaussian_maps": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.box_level_augs.geometric_augs": [
-            "torch",
-            "random",
             "copy",
-            "torchvision"
+            "random",
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.augmentations.scale_aware_aug": [
             "copy"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.backbones.darknet": [
             "torch"
         ],
@@ -13349,128 +13370,128 @@
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.base_ops": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.neck_ops": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.ops": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.ota_assigner": [
-            "torch",
-            "warnings"
+            "warnings",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.repvgg_block": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.utils": [
-            "torch",
-            "functools"
+            "functools",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.core.weight_init": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.heads.gfocal_v2_tiny": [
-            "torch",
+            "functools",
             "numpy",
-            "functools"
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.heads.zero_head": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.losses.distill_loss": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.losses.gfocal_loss": [
-            "torch",
-            "functools"
+            "functools",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_config": [
-            "collections",
-            "networkx"
+            "networkx",
+            "collections"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_fpn": [
+            "functools",
+            "typing",
+            "collections",
             "numpy",
             "math",
-            "timm",
             "torch",
-            "collections",
-            "typing",
-            "functools"
+            "timm"
         ],
         "modelscope.models.cv.tinynas_detection.damo.base_models.necks.giraffe_fpn_btn": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.detectors.detector": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.bounding_box": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.boxlist_ops": [
             "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.structures.image_list": [
-            "torch",
-            "__future__"
+            "__future__",
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.boxes": [
-            "torch",
+            "torchvision",
             "numpy",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.model_utils": [
+            "time",
+            "thop",
             "copy",
             "math",
-            "torch",
-            "thop",
-            "time"
+            "torch"
         ],
         "modelscope.models.cv.tinynas_detection.damo.utils.scheduler": [
             "math"
         ],
         "modelscope.models.cv.tinynas_detection.detector": [
-            "torch",
-            "os",
+            "torchvision",
             "pickle",
-            "torchvision"
+            "torch",
+            "os"
         ],
         "modelscope.models.cv.tinynas_detection.tinynas_damoyolo": [],
         "modelscope.models.cv.tinynas_detection.tinynas_detector": [],
         "modelscope.models.cv.tinynas_detection.utils": [
-            "easydict",
-            "tempfile",
             "importlib",
-            "os",
+            "sys",
             "shutil",
-            "sys"
+            "easydict",
+            "tempfile",
+            "os"
         ],
         "modelscope.models.cv.video_deinterlace.UNet_for_video_deinterlace": [
-            "torch",
             "copy",
+            "torch",
             "os",
             "typing"
         ],
         "modelscope.models.cv.video_deinterlace.deinterlace_arch": [
             "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.archs": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.deep_fourier_upsampling": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.enh": [
             "torch"
         ],
         "modelscope.models.cv.video_deinterlace.models.fre": [
             "torch"
         ],
@@ -13478,198 +13499,198 @@
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.configs.default_config": [
             "yacs",
             "os"
         ],
         "modelscope.models.cv.video_depth_estimation.dro_model": [
-            "numpy",
+            "tqdm",
+            "cv2",
             "glob",
-            "os",
+            "numpy",
             "torch",
-            "tqdm",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.camera": [
-            "torch",
-            "functools"
+            "functools",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.camera_utils": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.pose": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.geometry.pose_utils": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.models.model_checkpoint": [
-            "torch",
             "numpy",
-            "re",
-            "os"
+            "torch",
+            "os",
+            "re"
         ],
         "modelscope.models.cv.video_depth_estimation.models.model_utils": [],
         "modelscope.models.cv.video_depth_estimation.models.model_wrapper": [
-            "numpy",
             "importlib",
+            "numpy",
             "random",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.video_depth_estimation.models.sfm_model_mf": [
-            "torch",
-            "random"
+            "random",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.models.sup_model_mf": [],
         "modelscope.models.cv.video_depth_estimation.networks.depth_pose.depth_pose_net": [
-            "torch",
-            "functools"
+            "functools",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.depth_decoder": [
-            "torch",
             "numpy",
-            "collections",
-            "__future__"
+            "__future__",
+            "torch",
+            "collections"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.layers": [
-            "torch",
-            "__future__"
+            "__future__",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.pose_decoder": [
+            "__future__",
             "torch",
-            "collections",
-            "__future__"
+            "collections"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.layers.resnet.resnet_encoder": [
-            "torch",
+            "torchvision",
             "numpy",
             "__future__",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.optim.extractor": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.networks.optim.update": [
             "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.augmentations": [
-            "numpy",
-            "PIL",
-            "random",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "random"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.config": [
-            "torch",
-            "yacs",
             "datetime",
-            "os"
+            "yacs",
+            "os",
+            "torch"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.depth": [
-            "matplotlib",
-            "torch",
+            "torchvision",
             "numpy",
-            "torchvision"
+            "torch",
+            "matplotlib"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.horovod": [
             "horovod"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.image": [
-            "numpy",
+            "functools",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "functools",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.image_gt": [
+            "functools",
             "cv2",
             "torch",
-            "functools",
             "PIL"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.load": [
+            "logging",
             "importlib",
-            "inspect",
+            "warnings",
             "os",
-            "logging",
-            "collections",
+            "inspect",
             "torch",
-            "warnings"
+            "collections"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.misc": [
             "termcolor"
         ],
         "modelscope.models.cv.video_depth_estimation.utils.types": [
-            "torch",
             "numpy",
-            "yacs"
+            "yacs",
+            "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.VFINet_arch": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.VFINet_for_video_frame_interpolation": [
-            "torch",
             "copy",
+            "torch",
             "os",
             "typing"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.corr": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.extractor": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.raft": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.flow_model.update": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.IFNet_swin": [
-            "torch",
             "numpy",
+            "torch",
             "timm"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.UNet": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.flow_reversal": [
             "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.refinenet_arch": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.interp_model.transformer_layers": [
-            "timm",
+            "functools",
+            "sys",
             "math",
             "torch",
-            "sys",
-            "functools"
+            "timm"
         ],
         "modelscope.models.cv.video_frame_interpolation.utils.scene_change_detection": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_frame_interpolation.utils.utils": [
             "scipy",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_human_matting.model": [
+            "typing",
+            "torchvision",
             "numpy",
             "os",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.video_human_matting.models.decoder": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.video_human_matting.models.deep_guided_filter": [
             "torch"
@@ -13681,122 +13702,122 @@
             "torch"
         ],
         "modelscope.models.cv.video_human_matting.models.matting": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.video_inpainting.inpainting": [
-            "numpy",
-            "os",
             "cv2",
-            "torch",
             "time",
+            "numpy",
+            "torch",
+            "os",
             "torchvision",
             "PIL"
         ],
         "modelscope.models.cv.video_inpainting.inpainting_model": [
+            "torchvision",
             "math",
-            "torch",
             "numpy",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_frame_iter_head": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_head": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_iter_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_update_head": [
-            "mmdet",
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.head.kernel_updator": [
-            "torch",
-            "mmcv"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.video_instance_segmentation.neck.msdeformattn_decoder": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.track.kernel_update_head": [
-            "mmdet",
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.track.mask_hungarian_assigner": [
-            "mmdet",
-            "torch",
+            "scipy",
             "numpy",
-            "scipy"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.utils": [
-            "mmdet",
+            "numpy",
             "torch",
-            "numpy"
+            "mmdet"
         ],
         "modelscope.models.cv.video_instance_segmentation.video_knet": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.common": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.decode": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.model": [
             "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.models.yolo": [
+            "copy",
             "math",
-            "torch",
-            "copy"
+            "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.basetrack": [
             "numpy",
             "collections"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.matching": [
             "lap",
             "scipy",
             "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.tracker.multitracker": [
-            "torch",
             "numpy",
+            "torch",
             "collections"
         ],
         "modelscope.models.cv.video_multi_object_tracking.utils.image": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.utils.kalman_filter": [
             "scipy",
             "numpy"
         ],
         "modelscope.models.cv.video_multi_object_tracking.utils.utils": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.video_multi_object_tracking.utils.visualization": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.models.cv.video_object_segmentation.aggregate": [
             "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.cbam": [
             "torch"
         ],
@@ -13813,93 +13834,93 @@
         "modelscope.models.cv.video_object_segmentation.mod_resnet": [
             "math",
             "torch",
             "collections"
         ],
         "modelscope.models.cv.video_object_segmentation.model": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.video_object_segmentation.modules": [
-            "torch",
-            "torchvision"
+            "torchvision",
+            "torch"
         ],
         "modelscope.models.cv.video_object_segmentation.network": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.backbone.swin_checkpoint": [
             "importlib",
-            "os",
             "torch",
-            "collections",
+            "pkgutil",
             "torchvision",
-            "pkgutil"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.backbone.swin_transformer": [
-            "mmdet",
+            "numpy",
             "torch",
             "timm",
-            "numpy"
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_head": [
-            "torch",
-            "mmcv"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_iter_head": [
-            "mmdet",
-            "torch"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_update_head": [
-            "mmdet",
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.kernel_updator": [
-            "torch",
-            "mmcv"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.mask": [
-            "pycocotools",
+            "cv2",
             "numpy",
-            "torch",
+            "pycocotools",
             "__future__",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.semantic_fpn_wrapper": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.head.track_heads": [
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.neck.fpn": [
-            "torch",
-            "mmcv"
+            "mmcv",
+            "torch"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.track.quasi_dense_embed_tracker": [
-            "mmdet",
+            "mmcv",
             "torch",
-            "mmcv"
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.video_k_net": [
-            "mmdet",
-            "torch",
+            "mmcv",
             "numpy",
-            "mmcv"
+            "torch",
+            "mmdet"
         ],
         "modelscope.models.cv.video_panoptic_segmentation.visualizer": [
+            "cv2",
             "numpy",
-            "hashlib",
-            "cv2"
+            "hashlib"
         ],
         "modelscope.models.cv.video_single_object_tracking.config.ostrack": [
             "easydict"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.layers.attn": [
             "torch"
         ],
@@ -13922,142 +13943,142 @@
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.ostrack": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.utils": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.ostrack.vit_ce": [
+            "functools",
             "torch",
-            "timm",
-            "functools"
+            "timm"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.procontext": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.utils": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.models.procontext.vit_ce": [
+            "functools",
             "torch",
-            "timm",
-            "functools"
+            "timm"
         ],
         "modelscope.models.cv.video_single_object_tracking.tracker.ostrack": [
             "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.tracker.procontext": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.cv.video_single_object_tracking.utils.utils": [
+            "cv2",
             "numpy",
-            "math",
-            "torch",
             "typing",
-            "cv2"
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.DUT_raft": [
-            "torch",
-            "numpy",
+            "cv2",
             "sys",
-            "cv2"
+            "torch",
+            "numpy"
         ],
         "modelscope.models.cv.video_stabilization.DUT.MotionPro": [
+            "cv2",
             "numpy",
-            "math",
             "os",
-            "torch",
-            "cv2"
+            "math",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.corr": [
             "alt_cuda_corr",
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.extractor": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.raft": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.RAFT.update": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.Smoother": [
             "math",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.config": [
             "easydict",
             "__future__"
         ],
         "modelscope.models.cv.video_stabilization.DUT.rf_det_module": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUT.rf_det_so": [
             "torch"
         ],
         "modelscope.models.cv.video_stabilization.DUTRAFTStabilizer": [
-            "numpy",
+            "sys",
             "typing",
+            "cv2",
+            "numpy",
+            "torch",
             "math",
             "tempfile",
-            "os",
-            "torch",
-            "sys",
-            "cv2"
+            "os"
         ],
         "modelscope.models.cv.video_stabilization.utils.IterativeSmooth": [
             "math",
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.video_stabilization.utils.MedianFilter": [
+            "cv2",
             "math",
-            "torch",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.ProjectionUtils": [
+            "cv2",
             "math",
-            "torch",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.RAFTUtils": [
             "scipy",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.WarpUtils": [
-            "torch",
             "numpy",
-            "tqdm"
+            "tqdm",
+            "torch"
         ],
         "modelscope.models.cv.video_stabilization.utils.image_utils": [
             "torch",
             "skimage"
         ],
         "modelscope.models.cv.video_stabilization.utils.math_utils": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.exp.longshortnet_base": [],
         "modelscope.models.cv.video_streaming_perception.longshortnet.longshortnet": [
-            "numpy",
-            "argparse",
             "json",
-            "os",
             "logging",
-            "torch",
             "tqdm",
+            "argparse",
             "time",
-            "cv2"
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.dfp_pafpn_long": [
             "torch",
             "collections"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.dfp_pafpn_short": [
             "torch",
@@ -14066,430 +14087,465 @@
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.longshort": [
             "torch"
         ],
         "modelscope.models.cv.video_streaming_perception.longshortnet.models.longshort_backbone_neck": [
             "torch"
         ],
         "modelscope.models.cv.video_summarization.base_model": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.models.cv.video_summarization.kts.cpd_auto": [
             "numpy"
         ],
         "modelscope.models.cv.video_summarization.kts.cpd_nonlin": [
             "numpy"
         ],
         "modelscope.models.cv.video_summarization.pgl_sum": [
             "math",
             "torch"
         ],
         "modelscope.models.cv.video_summarization.summarizer": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.video_super_resolution.basicvsr_net": [
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.common": [
             "torch"
         ],
         "modelscope.models.cv.video_super_resolution.msrresnet_lite_model": [
-            "torch",
             "functools",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.video_super_resolution.real_basicvsr_for_video_super_resolution": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.video_super_resolution.real_basicvsr_net": [
             "torch"
         ],
         "modelscope.models.cv.vidt.backbone": [
-            "timm",
             "numpy",
-            "os",
+            "torch",
             "math",
-            "torch"
+            "os",
+            "timm"
         ],
         "modelscope.models.cv.vidt.deformable_transformer": [
-            "timm",
+            "warnings",
+            "copy",
             "math",
             "torch",
-            "warnings",
-            "copy"
+            "timm"
         ],
         "modelscope.models.cv.vidt.fpn_fusion": [
             "torch"
         ],
         "modelscope.models.cv.vidt.head": [
+            "copy",
             "math",
-            "torch",
-            "copy"
+            "torch"
         ],
         "modelscope.models.cv.vidt.model": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.virual_tryon.sdafnet": [
+            "random",
             "torch",
-            "numpy",
-            "random"
+            "numpy"
         ],
         "modelscope.models.cv.vision_efficient_tuning.backbone": [
-            "torch",
-            "functools"
+            "functools",
+            "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.head": [
             "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.model": [
             "torch",
             "typing"
         ],
         "modelscope.models.cv.vision_efficient_tuning.petl": [
+            "torchvision",
             "math",
             "torch",
-            "collections",
-            "torchvision"
+            "collections"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_helpers": [
             "math",
+            "itertools",
             "torch",
-            "typing",
-            "itertools"
+            "typing"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_vision_transformer": [
-            "math",
+            "functools",
             "logging",
+            "math",
+            "itertools",
             "torch",
-            "collections",
-            "functools",
-            "itertools"
+            "collections"
         ],
         "modelscope.models.cv.vision_efficient_tuning.timm_weight_init": [
+            "warnings",
             "math",
-            "torch",
-            "warnings"
+            "torch"
         ],
         "modelscope.models.cv.vision_efficient_tuning.vision_efficient_tuning": [
-            "torch",
             "collections",
+            "torch",
             "os"
         ],
         "modelscope.models.cv.vision_middleware.backbone": [
+            "typing",
             "numpy",
-            "math",
             "os",
+            "math",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.cv.vision_middleware.head": [
+            "mmcv",
             "abc",
-            "torch",
             "numpy",
-            "mmcv"
+            "torch"
         ],
         "modelscope.models.cv.vision_middleware.model": [
-            "torch",
             "json",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.models.cv.vision_middleware.vim": [
-            "math",
+            "einops",
             "torch",
-            "einops"
+            "math"
         ],
         "modelscope.models.cv.vop_retrieval.backbone": [
-            "numpy",
-            "hashlib",
-            "tqdm",
             "urllib",
-            "os",
-            "collections",
+            "typing",
+            "tqdm",
+            "numpy",
             "torch",
             "warnings",
-            "typing"
+            "hashlib",
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.vop_retrieval.basic_utils": [
-            "numpy",
-            "pickle",
-            "shutil",
+            "ujson",
             "zipfile",
+            "shutil",
+            "torchvision",
             "PIL",
-            "os",
-            "ujson",
-            "collections",
+            "cv2",
+            "pickle",
+            "numpy",
             "torch",
             "random",
-            "torchvision",
-            "cv2"
+            "os",
+            "collections"
         ],
         "modelscope.models.cv.vop_retrieval.model": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.vop_retrieval.model_se": [
             "torch",
             "os"
         ],
         "modelscope.models.cv.vop_retrieval.tokenization_clip": [
-            "ftfy",
-            "regex",
+            "functools",
+            "html",
             "os",
+            "ftfy",
             "gzip",
-            "torch",
-            "html",
-            "functools"
+            "regex",
+            "torch"
         ],
         "modelscope.models.multi_modal.clip.bert_tokenizer": [
-            "re",
-            "os",
             "six",
-            "collections",
+            "unicodedata",
             "__future__",
-            "unicodedata"
+            "os",
+            "collections",
+            "re"
         ],
         "modelscope.models.multi_modal.clip.configuration_bert": [
-            "__future__",
-            "logging"
+            "logging",
+            "__future__"
         ],
         "modelscope.models.multi_modal.clip.model": [
-            "numpy",
+            "typing",
             "json",
+            "numpy",
             "os",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.multi_modal.clip.modeling_bert": [
-            "math",
-            "__future__",
-            "io",
+            "sys",
             "json",
-            "os",
             "logging",
+            "__future__",
+            "io",
             "torch",
-            "sys"
+            "math",
+            "os"
+        ],
+        "modelscope.models.multi_modal.clip_interrogator.model": [
+            "safetensors",
+            "typing",
+            "transformers",
+            "PIL",
+            "dataclasses",
+            "time",
+            "requests",
+            "hashlib",
+            "torch",
+            "os",
+            "torchvision",
+            "tqdm",
+            "open_clip",
+            "numpy",
+            "math"
         ],
         "modelscope.models.multi_modal.diffusion.diffusion": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.diffusion.model": [
-            "numpy",
             "json",
+            "typing",
+            "numpy",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.multi_modal.diffusion.structbert": [
+            "six",
             "numpy",
             "json",
             "copy",
             "math",
-            "torch",
             "__future__",
-            "six"
+            "torch"
         ],
         "modelscope.models.multi_modal.diffusion.tokenizer": [
             "six",
-            "collections",
+            "unicodedata",
             "__future__",
-            "unicodedata"
+            "collections"
         ],
         "modelscope.models.multi_modal.diffusion.unet_generator": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.diffusion.unet_upsampler_1024": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.diffusion.unet_upsampler_256": [
+            "functools",
             "math",
-            "torch",
-            "functools"
+            "torch"
         ],
         "modelscope.models.multi_modal.dpm_solver_pytorch": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.efficient_diffusion_tuning.efficient_stable_diffusion": [
+            "functools",
+            "typing",
             "transformers",
-            "os",
             "diffusers",
             "torch",
-            "typing",
-            "functools"
+            "os"
         ],
         "modelscope.models.multi_modal.gemm.gemm_base": [
-            "numpy",
+            "typing",
             "json",
+            "numpy",
             "os",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.multi_modal.gemm.gemm_model": [
-            "numpy",
-            "PIL",
+            "typing",
             "json",
+            "torchvision",
+            "PIL",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.models.multi_modal.gemm.tokenizer": [
-            "ftfy",
-            "regex",
+            "functools",
+            "html",
             "os",
+            "ftfy",
             "gzip",
-            "torch",
-            "html",
-            "functools"
+            "regex",
+            "torch"
         ],
         "modelscope.models.multi_modal.guided_diffusion.gaussian_diffusion": [
-            "math",
-            "torch",
+            "enum",
             "numpy",
-            "enum"
+            "torch",
+            "math"
         ],
         "modelscope.models.multi_modal.guided_diffusion.respace": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.multi_modal.guided_diffusion.script": [],
         "modelscope.models.multi_modal.guided_diffusion.unet": [
-            "abc",
             "numpy",
-            "transformers",
+            "abc",
             "math",
+            "transformers",
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.backbone": [
-            "math",
-            "warnings",
-            "dataclasses",
+            "typing",
             "transformers",
+            "dataclasses",
+            "warnings",
             "os",
-            "torch",
+            "math",
             "random",
-            "typing"
+            "torch"
         ],
         "modelscope.models.multi_modal.mgeo.text_classification": [
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.text_ranking": [
             "torch"
         ],
         "modelscope.models.multi_modal.mgeo.token_classification": [
             "torch"
         ],
         "modelscope.models.multi_modal.mmr.dataloaders.rawvideo_util": [
-            "numpy",
-            "PIL",
-            "torch",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.multi_modal.mmr.models.clip_for_mm_video_embedding": [
-            "numpy",
-            "uuid",
-            "decord",
+            "json",
+            "typing",
             "urllib",
             "PIL",
-            "tempfile",
-            "json",
-            "os",
+            "numpy",
             "torch",
+            "uuid",
             "random",
-            "typing"
+            "tempfile",
+            "os",
+            "decord"
         ],
         "modelscope.models.multi_modal.mmr.models.dynamic_inverted_softmax": [
             "numpy"
         ],
         "modelscope.models.multi_modal.mmr.models.modeling": [
-            "types",
             "platform",
+            "types",
             "os",
             "torch",
             "collections"
         ],
         "modelscope.models.multi_modal.mmr.models.module_clip": [
-            "hashlib",
-            "tqdm",
             "urllib",
+            "typing",
+            "tqdm",
+            "warnings",
             "os",
-            "collections",
+            "hashlib",
             "torch",
-            "warnings",
-            "typing"
+            "collections"
         ],
         "modelscope.models.multi_modal.mmr.models.module_cross": [
             "json",
             "logging",
+            "__future__",
             "torch",
-            "collections",
-            "__future__"
+            "collections"
         ],
         "modelscope.models.multi_modal.mmr.models.tokenization_clip": [
+            "functools",
+            "html",
             "ftfy",
-            "regex",
-            "os",
             "gzip",
-            "html",
-            "functools"
+            "regex",
+            "os"
         ],
         "modelscope.models.multi_modal.mmr.models.until_module": [
+            "logging",
             "math",
-            "torch",
             "numpy",
-            "logging"
+            "torch"
         ],
         "modelscope.models.multi_modal.mplug.clip.clip": [
             "torch",
             "collections",
             "typing"
         ],
         "modelscope.models.multi_modal.mplug.configuration_mplug": [
             "yaml",
             "transformers",
             "os",
             "typing"
         ],
         "modelscope.models.multi_modal.mplug.modeling_mplug": [
-            "math",
+            "typing",
             "transformers",
             "os",
-            "torch",
-            "typing"
+            "math",
+            "torch"
         ],
         "modelscope.models.multi_modal.mplug.mvit": [
+            "functools",
             "timm",
             "numpy",
             "fairscale",
             "torch",
-            "collections",
-            "functools"
+            "collections"
         ],
         "modelscope.models.multi_modal.mplug.predictor": [
-            "torch",
-            "__future__"
+            "__future__",
+            "torch"
         ],
         "modelscope.models.multi_modal.mplug_for_all_tasks": [
+            "os",
+            "typing"
+        ],
+        "modelscope.models.multi_modal.mplug_owl.configuration_mplug_owl": [
+            "copy",
+            "transformers",
+            "os",
+            "typing"
+        ],
+        "modelscope.models.multi_modal.mplug_owl.modeling_mplug_owl": [
             "typing",
+            "copy",
+            "logging",
+            "transformers",
+            "dataclasses",
+            "io",
+            "torch",
+            "math",
+            "random",
             "os"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.clip": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.decoder": [
@@ -14497,599 +14553,599 @@
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.gaussian_diffusion": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.model": [
-            "numpy",
-            "math",
-            "PIL",
             "json",
+            "typing",
+            "PIL",
+            "numpy",
             "os",
-            "torch",
-            "typing"
+            "math",
+            "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.prior": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.tokenizer": [
-            "ftfy",
-            "regex",
+            "functools",
+            "html",
             "transformers",
+            "ftfy",
             "gzip",
-            "torch",
-            "html",
-            "functools"
+            "regex",
+            "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.upsampler": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.multi_stage_diffusion.xglm": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.configuration_mmspeech": [
-            "warnings",
-            "transformers"
+            "transformers",
+            "warnings"
         ],
         "modelscope.models.multi_modal.ofa.configuration_ofa": [
-            "warnings",
-            "transformers"
+            "transformers",
+            "warnings"
         ],
         "modelscope.models.multi_modal.ofa.generate.incremental_decoding_utils": [
-            "torch",
             "uuid",
+            "torch",
             "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.multihead_attention": [
+            "fairseq",
             "math",
             "torch",
-            "typing",
-            "fairseq"
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.ngram_repeat_block": [
-            "math",
-            "torch",
             "warnings",
             "typing",
+            "math",
+            "torch",
             "fairseq"
         ],
         "modelscope.models.multi_modal.ofa.generate.search": [
             "math",
             "torch",
             "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.sequence_generator": [
             "math",
-            "torch",
             "sys",
+            "torch",
             "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.token_generation_constraints": [
             "torch",
             "collections",
             "typing"
         ],
         "modelscope.models.multi_modal.ofa.generate.utils": [
-            "amp_C",
             "torch_xla",
+            "amp_C",
+            "itertools",
             "torch",
-            "collections",
-            "itertools"
+            "collections"
         ],
         "modelscope.models.multi_modal.ofa.modeling_mmspeech": [
-            "packaging",
-            "numpy",
-            "math",
-            "dataclasses",
             "apex",
-            "transformers",
-            "torch",
             "typing",
-            "fairseq"
-        ],
-        "modelscope.models.multi_modal.ofa.modeling_ofa": [
+            "transformers",
+            "fairseq",
+            "dataclasses",
+            "numpy",
             "packaging",
             "math",
-            "dataclasses",
+            "torch"
+        ],
+        "modelscope.models.multi_modal.ofa.modeling_ofa": [
             "apex",
+            "typing",
             "transformers",
-            "torch",
+            "dataclasses",
+            "packaging",
+            "math",
             "random",
-            "typing"
+            "torch"
         ],
         "modelscope.models.multi_modal.ofa.resnet": [
             "torch"
         ],
         "modelscope.models.multi_modal.ofa.tokenization_ofa": [
-            "collections",
             "transformers",
             "os",
+            "collections",
             "typing"
         ],
         "modelscope.models.multi_modal.ofa.tokenization_ofa_fast": [
             "json",
-            "typing",
             "transformers",
-            "tokenizers"
+            "tokenizers",
+            "typing"
         ],
         "modelscope.models.multi_modal.ofa.utils.constant": [],
         "modelscope.models.multi_modal.ofa.utils.utils": [
             "torch",
             "typing"
         ],
         "modelscope.models.multi_modal.ofa.vit": [
             "torch",
             "collections",
             "fairseq"
         ],
         "modelscope.models.multi_modal.ofa_for_all_tasks": [
-            "re",
-            "string",
-            "math",
+            "functools",
+            "typing",
             "json",
+            "string",
             "os",
+            "math",
             "torch",
-            "typing",
-            "functools"
+            "re"
         ],
         "modelscope.models.multi_modal.ofa_for_text_to_image_synthesis_model": [
+            "typing",
+            "json",
+            "torchvision",
+            "PIL",
             "numpy",
             "taming",
-            "PIL",
-            "pkg_resources",
-            "json",
             "os",
-            "torch",
-            "typing",
-            "torchvision"
+            "pkg_resources",
+            "torch"
         ],
         "modelscope.models.multi_modal.rleg.model": [
             "torch",
-            "json",
-            "os"
+            "os",
+            "json"
         ],
         "modelscope.models.multi_modal.rleg.rleg": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.models.multi_modal.soonet.blocks": [
             "math",
             "torch"
         ],
         "modelscope.models.multi_modal.soonet.clip": [
             "numpy",
-            "collections",
-            "torch",
             "warnings",
-            "typing"
+            "typing",
+            "torch",
+            "collections"
         ],
         "modelscope.models.multi_modal.soonet.model": [
             "torch",
             "os"
         ],
         "modelscope.models.multi_modal.soonet.swin_transformer": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.multi_modal.soonet.tokenizer": [
+            "functools",
+            "html",
             "ftfy",
-            "regex",
             "gzip",
-            "torch",
-            "html",
-            "functools"
+            "regex",
+            "torch"
         ],
         "modelscope.models.multi_modal.soonet.utils": [
+            "copy",
             "numpy",
             "tqdm",
-            "copy",
             "decord"
         ],
         "modelscope.models.multi_modal.team.team_model": [
-            "numpy",
-            "tokenizers",
-            "PIL",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "tokenizers",
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.models.multi_modal.team.utils": [
             "numpy",
+            "typing",
             "transformers",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.multi_modal.video_synthesis.autoencoder": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.diffusion": [
             "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model": [
-            "einops",
-            "os",
-            "torch",
+            "typing",
             "open_clip",
-            "typing"
+            "os",
+            "einops",
+            "torch"
         ],
         "modelscope.models.multi_modal.video_synthesis.unet_sd": [
-            "math",
+            "einops",
             "torch",
-            "einops"
+            "math"
         ],
         "modelscope.models.multi_modal.vldoc.conv_fpn_trans": [
-            "timm",
             "apex",
             "collections",
+            "random",
             "torch",
-            "random"
+            "timm"
         ],
         "modelscope.models.multi_modal.vldoc.convnext": [
-            "torch",
             "timm",
+            "torch",
             "os"
         ],
         "modelscope.models.multi_modal.vldoc.model": [
-            "re",
             "sys",
-            "math",
             "json",
-            "os",
+            "copy",
             "logging",
+            "torchvision",
             "torch",
-            "copy",
-            "torchvision"
+            "math",
+            "os",
+            "re"
         ],
         "modelscope.models.multi_modal.vldoc.modeling_layout_roberta": [
             "packaging",
-            "transformers",
-            "os",
+            "torch",
             "math",
-            "torch"
+            "transformers",
+            "os"
         ],
         "modelscope.models.multi_modal.vldoc.processing": [
-            "numpy",
-            "PIL",
             "timm",
-            "torch",
-            "collections",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch",
+            "collections"
         ],
         "modelscope.models.multi_modal.vldoc.tokenization": [
             "transformers",
             "os"
         ],
         "modelscope.models.multi_modal.vldoc.transformer_local": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.T5.backbone": [
-            "math",
+            "typing",
+            "copy",
             "transformers",
-            "os",
             "torch",
             "warnings",
-            "copy",
-            "typing"
+            "math",
+            "os"
         ],
         "modelscope.models.nlp.T5.configuration": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.T5.text2text_generation": [
+            "typing",
+            "copy",
             "transformers",
-            "torch",
             "warnings",
-            "copy",
-            "typing"
+            "torch"
         ],
         "modelscope.models.nlp.bart.text_error_correction": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.bert.backbone": [
             "packaging",
-            "torch",
             "math",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.bert.configuration": [
+            "transformers",
             "collections",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.nlp.bert.document_segmentation": [
             "torch",
             "typing"
         ],
         "modelscope.models.nlp.bert.fill_mask": [],
         "modelscope.models.nlp.bert.sentence_embedding": [
             "torch"
         ],
         "modelscope.models.nlp.bert.siamese_uie": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.bert.text_classification": [],
         "modelscope.models.nlp.bert.text_ranking": [],
         "modelscope.models.nlp.bert.token_classification": [],
         "modelscope.models.nlp.bert.word_alignment": [
             "torch"
         ],
         "modelscope.models.nlp.bloom.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.canmt.canmt_model": [
             "numpy",
+            "typing",
             "math",
             "torch",
-            "typing",
             "fairseq"
         ],
         "modelscope.models.nlp.canmt.canmt_translation": [
+            "typing",
             "numpy",
-            "math",
             "os",
-            "torch",
-            "typing"
+            "math",
+            "torch"
         ],
         "modelscope.models.nlp.canmt.sequence_generator": [
+            "sys",
             "numpy",
+            "typing",
             "math",
             "torch",
-            "sys",
-            "fairseq",
-            "typing"
+            "fairseq"
         ],
         "modelscope.models.nlp.codegeex.codegeex": [
             "math",
             "torch"
         ],
         "modelscope.models.nlp.codegeex.codegeex_for_code_generation": [
-            "torch",
             "copy",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.codegeex.codegeex_for_code_translation": [
-            "torch",
             "copy",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.codegeex.inference": [
             "torch",
             "typing"
         ],
         "modelscope.models.nlp.codegeex.tokenizer": [
+            "transformers",
             "torch",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.nlp.csanmt.translation": [
             "tensorflow",
             "math",
             "collections",
             "typing"
         ],
         "modelscope.models.nlp.deberta_v2.backbone": [
+            "transformers",
             "torch",
             "collections",
-            "transformers",
             "typing"
         ],
         "modelscope.models.nlp.deberta_v2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.deberta_v2.fill_mask": [
-            "torch",
             "transformers",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.deberta_v2.tokenization": [
-            "sentencepiece",
-            "transformers",
+            "unicodedata",
             "os",
             "typing",
-            "unicodedata"
+            "transformers",
+            "sentencepiece"
         ],
         "modelscope.models.nlp.deberta_v2.tokenization_fast": [
-            "shutil",
             "transformers",
+            "shutil",
             "os",
             "typing"
         ],
         "modelscope.models.nlp.dgds.backbone": [
-            "__future__",
             "torch",
+            "__future__",
             "transformers",
             "os"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_generate": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_rerank": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.dgds.document_grounded_dialog_retrieval": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.fid_T5.text_generation": [
-            "io",
             "torch",
+            "io",
             "transformers",
             "os"
         ],
         "modelscope.models.nlp.fid_plug.backbone": [
-            "numpy",
-            "math",
-            "dataclasses",
+            "typing",
+            "copy",
             "transformers",
+            "dataclasses",
+            "numpy",
             "os",
-            "torch",
-            "copy",
-            "typing"
+            "math",
+            "torch"
         ],
         "modelscope.models.nlp.fid_plug.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.fid_plug.text_generation": [
-            "io",
             "torch",
+            "io",
             "transformers",
             "os"
         ],
         "modelscope.models.nlp.glm_130b.generation.strategies": [
-            "torch",
+            "SwissArmyTransformer",
             "numpy",
-            "SwissArmyTransformer"
+            "torch"
         ],
         "modelscope.models.nlp.glm_130b.initialize": [
-            "argparse",
-            "torch",
             "SwissArmyTransformer",
-            "time"
+            "argparse",
+            "time",
+            "torch"
         ],
         "modelscope.models.nlp.glm_130b.quantization.functional": [
             "torch"
         ],
         "modelscope.models.nlp.glm_130b.quantization.layers": [
-            "torch",
-            "SwissArmyTransformer"
+            "SwissArmyTransformer",
+            "torch"
         ],
         "modelscope.models.nlp.glm_130b.text_generation": [
-            "stat",
-            "re",
-            "sys",
+            "functools",
             "SwissArmyTransformer",
+            "sys",
+            "typing",
+            "copy",
+            "stat",
             "time",
-            "os",
             "torch",
             "random",
-            "copy",
-            "functools",
-            "typing"
+            "os",
+            "re"
         ],
         "modelscope.models.nlp.gpt2.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.gpt3.backbone": [
-            "addict",
-            "math",
+            "typing",
             "transformers",
-            "os",
             "torch",
-            "typing"
+            "addict",
+            "math",
+            "os"
         ],
         "modelscope.models.nlp.gpt3.configuration": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.gpt3.distributed_gpt3": [
+            "typing",
             "megatron_util",
-            "math",
             "transformers",
             "os",
+            "math",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.nlp.gpt3.text_generation": [
+            "transformers",
             "torch",
             "collections",
-            "transformers",
             "typing"
         ],
         "modelscope.models.nlp.gpt3.tokenizer": [
-            "typing",
-            "tokenizers"
+            "tokenizers",
+            "typing"
         ],
         "modelscope.models.nlp.gpt_moe.backbone": [
-            "addict",
-            "math",
+            "typing",
             "transformers",
-            "os",
             "torch",
-            "typing"
+            "addict",
+            "math",
+            "os"
         ],
         "modelscope.models.nlp.gpt_moe.checkpointing": [
             "torch",
-            "os",
-            "megatron_util"
+            "megatron_util",
+            "os"
         ],
         "modelscope.models.nlp.gpt_moe.configuration": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.distributed_gpt_moe": [
-            "torch",
             "math",
             "megatron_util",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.experts": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.gpt_moe.moe.layer": [
             "torch",
-            "typing",
-            "megatron_util"
+            "megatron_util",
+            "typing"
         ],
         "modelscope.models.nlp.gpt_moe.moe.mappings": [
             "torch",
             "megatron_util"
         ],
         "modelscope.models.nlp.gpt_moe.moe.sharded_moe": [
             "tutel",
-            "megatron_util",
-            "math",
-            "scipy",
             "apex",
+            "typing",
             "torch",
-            "typing"
+            "scipy",
+            "math",
+            "megatron_util"
         ],
         "modelscope.models.nlp.gpt_moe.moe.utils": [
             "torch",
             "typing"
         ],
         "modelscope.models.nlp.gpt_moe.text_generation": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.gpt_moe.tokenizer": [
             "tokenizers"
         ],
         "modelscope.models.nlp.gpt_neo.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.heads.crf_head": [
+            "transformers",
             "torch",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.nlp.heads.fill_mask_head": [
+            "transformers",
             "torch",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.nlp.heads.infromation_extraction_head": [
             "torch"
         ],
         "modelscope.models.nlp.heads.text_classification_head": [
             "torch",
             "typing"
@@ -15103,2594 +15159,2573 @@
             "typing"
         ],
         "modelscope.models.nlp.heads.token_classification_head": [
             "torch",
             "typing"
         ],
         "modelscope.models.nlp.heads.torch_pretrain_head": [
+            "transformers",
             "torch",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.nlp.hf_transformers.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.llama.backbone": [
             "math",
-            "torch",
             "transformers",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.llama.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.llama.convert_llama_weights_to_hf": [
-            "argparse",
-            "math",
             "shutil",
             "gc",
             "json",
-            "os",
-            "torch"
+            "argparse",
+            "torch",
+            "math",
+            "os"
         ],
         "modelscope.models.nlp.llama.text_generation": [
             "torch",
             "typing"
         ],
         "modelscope.models.nlp.llama.tokenization": [
             "shutil",
-            "sentencepiece",
+            "typing",
             "transformers",
-            "os",
-            "typing"
+            "sentencepiece",
+            "os"
         ],
         "modelscope.models.nlp.llama.tokenization_fast": [
-            "shutil",
             "transformers",
+            "shutil",
             "os",
             "typing"
         ],
         "modelscope.models.nlp.lstm.backbone": [
             "torch"
         ],
         "modelscope.models.nlp.lstm.token_classification": [],
         "modelscope.models.nlp.megatron_bert.backbone": [
             "math",
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.megatron_bert.configuration": [
+            "transformers",
             "collections",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.nlp.megatron_bert.fill_mask": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.arguments": [
-            "argparse",
             "json",
+            "argparse",
             "os",
-            "deepspeed",
-            "torch"
+            "torch",
+            "deepspeed"
         ],
         "modelscope.models.nlp.mglm.blocklm_utils": [
-            "scipy",
-            "megatron_util",
             "numpy",
-            "math",
             "torch",
+            "copy",
+            "scipy",
+            "math",
             "random",
-            "copy"
+            "megatron_util"
         ],
         "modelscope.models.nlp.mglm.configure_data": [
+            "bisect",
+            "copy",
             "megatron_util",
             "numpy",
-            "bisect",
+            "itertools",
             "os",
-            "torch",
             "random",
-            "copy",
-            "itertools"
+            "torch"
         ],
         "modelscope.models.nlp.mglm.data_utils.corpora": [
-            "multiprocessing",
-            "tqdm",
             "json",
+            "queue",
+            "tqdm",
+            "multiprocessing",
             "os",
-            "collections",
-            "torch",
             "random",
-            "queue"
+            "torch",
+            "collections"
         ],
         "modelscope.models.nlp.mglm.data_utils.datasets": [
-            "math",
-            "csv",
-            "nltk",
-            "bisect",
-            "os",
             "pandas",
-            "torch",
+            "json",
             "operator",
-            "random",
+            "math",
             "time",
+            "csv",
             "itertools",
+            "random",
+            "torch",
+            "os",
+            "bisect",
+            "tqdm",
             "numpy",
-            "json",
-            "tqdm"
+            "nltk"
         ],
         "modelscope.models.nlp.mglm.data_utils.extraction": [
-            "nltk",
-            "json",
             "glob",
-            "os"
+            "os",
+            "nltk",
+            "json"
         ],
         "modelscope.models.nlp.mglm.data_utils.file_utils": [
-            "hashlib",
-            "pathlib",
+            "functools",
+            "botocore",
+            "sys",
             "shutil",
-            "os",
+            "json",
             "logging",
-            "botocore",
-            "__future__",
-            "io",
-            "urllib",
             "requests",
-            "boto3",
+            "hashlib",
             "tempfile",
-            "json",
+            "os",
+            "pathlib",
+            "urllib",
+            "__future__",
+            "boto3",
             "tqdm",
-            "sys",
-            "functools"
+            "io"
         ],
         "modelscope.models.nlp.mglm.data_utils.lazy_loader": [
-            "mmap",
-            "numpy",
+            "time",
             "pickle",
+            "numpy",
+            "mmap",
             "os",
-            "torch",
-            "time",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.data_utils.samplers": [
+            "sys",
             "numpy",
-            "os",
-            "math",
             "torch",
-            "sys"
+            "math",
+            "os"
         ],
         "modelscope.models.nlp.mglm.data_utils.sp_tokenizer": [
             "os"
         ],
         "modelscope.models.nlp.mglm.data_utils.tokenization": [
-            "csv",
-            "nltk",
-            "sentencepiece",
             "regex",
-            "os",
-            "collections",
+            "sentencepiece",
+            "itertools",
             "torch",
+            "csv",
+            "nltk",
             "random",
-            "itertools"
+            "os",
+            "collections"
         ],
         "modelscope.models.nlp.mglm.data_utils.tokenization_gpt2": [
-            "io",
-            "__future__",
+            "functools",
+            "sys",
             "json",
-            "regex",
-            "os",
             "logging",
-            "sys",
-            "functools"
+            "regex",
+            "__future__",
+            "io",
+            "os"
         ],
         "modelscope.models.nlp.mglm.data_utils.wordpiece": [
-            "io",
             "unicodedata",
-            "os",
             "logging",
-            "collections",
-            "__future__"
+            "__future__",
+            "io",
+            "os",
+            "collections"
         ],
         "modelscope.models.nlp.mglm.generation_utils": [
             "abc",
             "torch",
             "collections",
             "typing"
         ],
         "modelscope.models.nlp.mglm.mglm_for_text_summarization": [
-            "megatron_util",
+            "typing",
             "numpy",
-            "os",
             "torch",
+            "os",
             "random",
-            "typing"
+            "megatron_util"
         ],
         "modelscope.models.nlp.mglm.model.distributed": [
             "torch",
             "megatron_util"
         ],
         "modelscope.models.nlp.mglm.model.downstream": [
             "torch"
         ],
         "modelscope.models.nlp.mglm.model.modeling_bert": [
-            "tarfile",
-            "megatron_util",
-            "math",
-            "__future__",
+            "apex",
             "shutil",
-            "data_utils",
-            "tempfile",
             "json",
-            "apex",
-            "os",
+            "copy",
             "logging",
+            "data_utils",
+            "__future__",
+            "megatron_util",
+            "tarfile",
             "torch",
-            "copy"
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.models.nlp.mglm.model.modeling_glm": [
             "torch",
             "megatron_util"
         ],
         "modelscope.models.nlp.mglm.model.prompt": [
-            "torch",
-            "random"
+            "random",
+            "torch"
         ],
         "modelscope.models.nlp.mglm.model.transformer": [
-            "megatron_util",
             "apex",
-            "deepspeed",
             "math",
-            "torch"
+            "megatron_util",
+            "torch",
+            "deepspeed"
         ],
         "modelscope.models.nlp.mglm.process_grid": [
+            "glob",
+            "sys",
             "json",
-            "statistics",
             "os",
-            "sys",
-            "glob"
+            "statistics"
         ],
         "modelscope.models.nlp.mglm.run_test": [
             "test",
             "sys"
         ],
         "modelscope.models.nlp.mglm.tasks.data_utils": [
+            "json",
+            "copy",
+            "typing",
             "megatron_util",
-            "numpy",
-            "re",
             "pickle",
-            "json",
+            "numpy",
             "torch",
-            "copy",
-            "typing"
+            "re"
         ],
         "modelscope.models.nlp.mglm.tasks.eval_utils": [
+            "datetime",
+            "typing",
+            "finetune_glm",
             "megatron_util",
             "sklearn",
-            "tasks",
             "utils",
-            "finetune_glm",
-            "datetime",
+            "time",
+            "tasks",
             "os",
-            "collections",
-            "torch",
             "random",
-            "time",
-            "typing"
+            "torch",
+            "collections"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.dataset": [
-            "numpy",
-            "tasks",
-            "math",
-            "utils",
             "bisect",
             "json",
+            "tasks",
+            "numpy",
+            "math",
+            "itertools",
             "torch",
-            "itertools"
+            "utils"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.detokenizer": [
             "re"
         ],
         "modelscope.models.nlp.mglm.tasks.language_model.finetune": [
-            "megatron_util",
+            "functools",
             "tasks",
-            "math",
             "torch",
             "pretrain_glm",
-            "functools",
-            "finetune_glm"
+            "math",
+            "finetune_glm",
+            "megatron_util"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.dataset": [
-            "numpy",
+            "json",
+            "data_utils",
             "tqdm",
             "tasks",
-            "utils",
-            "data_utils",
-            "json",
+            "numpy",
             "os",
+            "random",
             "torch",
-            "random"
+            "utils"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.evaluate": [
-            "megatron_util",
-            "string",
             "datetime",
             "generation_utils",
             "torch",
+            "string",
             "random",
+            "megatron_util",
             "rouge_score"
         ],
         "modelscope.models.nlp.mglm.tasks.seq2seq.finetune": [
-            "megatron_util",
+            "functools",
             "tasks",
             "torch",
             "pretrain_glm",
-            "collections",
-            "functools",
-            "finetune_glm"
+            "finetune_glm",
+            "megatron_util",
+            "collections"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.dataset": [
-            "utils",
+            "pandas",
+            "json",
+            "typing",
+            "glob",
             "csv",
             "abc",
-            "os",
-            "pandas",
-            "torch",
-            "collections",
             "random",
-            "typing",
-            "numpy",
+            "torch",
+            "utils",
+            "os",
             "re",
-            "glob",
+            "copy",
             "data_utils",
-            "json",
             "tqdm",
-            "copy"
+            "numpy",
+            "collections"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.evaluate": [
-            "string",
-            "re",
+            "functools",
             "tasks",
-            "collections",
+            "typing",
             "__future__",
-            "functools",
-            "typing"
+            "string",
+            "collections",
+            "re"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.finetune": [
-            "collections",
+            "tasks",
             "finetune_glm",
-            "tasks"
+            "collections"
         ],
         "modelscope.models.nlp.mglm.tasks.superglue.pvp": [
-            "numpy",
+            "typing",
+            "copy",
+            "abc",
             "string",
+            "utils",
             "tasks",
+            "numpy",
             "math",
-            "utils",
-            "abc",
-            "collections",
             "random",
-            "copy",
-            "typing"
+            "collections"
         ],
         "modelscope.models.nlp.mglm.test.test_block": [
             "argparse",
-            "numpy",
+            "blocklm_utils",
             "random",
-            "blocklm_utils"
+            "numpy"
         ],
         "modelscope.models.nlp.mglm.test.test_rel_shift": [
+            "learning_rates",
             "numpy",
             "torch",
-            "matplotlib",
-            "learning_rates"
+            "matplotlib"
         ],
         "modelscope.models.nlp.mglm.train_utils": [
-            "torch",
-            "megatron_util",
             "apex",
+            "megatron_util",
+            "torch",
             "deepspeed"
         ],
         "modelscope.models.nlp.mglm.utils": [
+            "json",
+            "subprocess",
             "megatron_util",
+            "time",
             "numpy",
-            "json",
-            "os",
             "torch",
             "random",
-            "time",
-            "subprocess"
+            "os"
         ],
         "modelscope.models.nlp.palm_v2.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.palm_v2.dureader_eval": [
-            "copy",
-            "numpy",
-            "re",
-            "argparse",
-            "math",
+            "sys",
             "zipfile",
             "rouge",
             "json",
+            "copy",
+            "argparse",
+            "numpy",
+            "math",
             "collections",
-            "sys"
+            "re"
         ],
         "modelscope.models.nlp.palm_v2.text_generation": [
-            "numpy",
-            "typing",
-            "codecs",
-            "math",
+            "subprocess",
+            "copy",
             "json",
-            "dataclasses",
+            "typing",
             "transformers",
-            "os",
+            "dataclasses",
+            "numpy",
             "torch",
-            "copy",
-            "subprocess"
+            "codecs",
+            "math",
+            "os"
         ],
         "modelscope.models.nlp.peer.backbone": [
-            "math",
-            "dataclasses",
+            "typing",
             "transformers",
-            "torch",
-            "typing"
+            "dataclasses",
+            "math",
+            "torch"
         ],
         "modelscope.models.nlp.peer.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.peer.sas_utils": [
             "nltk",
+            "random",
             "torch",
-            "numpy",
-            "random"
+            "numpy"
         ],
         "modelscope.models.nlp.peer.text_classification": [
-            "torch",
-            "copy"
+            "copy",
+            "torch"
         ],
         "modelscope.models.nlp.plug.AnnealingLR": [
             "math",
             "torch"
         ],
         "modelscope.models.nlp.plug.backbone": [
-            "megatron_util",
-            "math",
             "logging",
+            "__future__",
             "torch",
-            "__future__"
+            "math",
+            "megatron_util"
         ],
         "modelscope.models.nlp.plug.configuration": [
-            "json",
             "copy",
-            "transformers"
+            "transformers",
+            "json"
         ],
         "modelscope.models.nlp.plug.distributed_plug": [
-            "torch",
             "megatron_util",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.plug.generator": [
             "torch"
         ],
         "modelscope.models.nlp.plug_mental.adv_utils": [
             "torch"
         ],
         "modelscope.models.nlp.plug_mental.backbone": [
+            "typing",
+            "transformers",
+            "dataclasses",
             "packaging",
             "math",
-            "dataclasses",
-            "transformers",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.nlp.plug_mental.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.plug_mental.text_classification": [
             "torch"
         ],
         "modelscope.models.nlp.ponet.backbone": [
+            "transformers",
             "packaging",
-            "distutils",
             "math",
-            "transformers",
-            "torch"
+            "torch",
+            "distutils"
         ],
         "modelscope.models.nlp.ponet.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.ponet.document_segmentation": [
             "torch",
             "typing"
         ],
         "modelscope.models.nlp.ponet.fill_mask": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.ponet.tokenization": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.models.nlp.space.configuration": [],
         "modelscope.models.nlp.space.dialog_intent_prediction": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.space.dialog_modeling": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.space.dialog_state_tracking": [
-            "torch",
             "transformers",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.space.model.gen_unified_transformer": [
             "torch"
         ],
         "modelscope.models.nlp.space.model.generator": [
             "math",
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.nlp.space.model.intent_unified_transformer": [
             "torch"
         ],
         "modelscope.models.nlp.space.model.model_base": [
             "torch",
             "os"
         ],
         "modelscope.models.nlp.space.model.tokenization_space": [
             "transformers"
         ],
         "modelscope.models.nlp.space.model.unified_transformer": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.nlp.space.modules.embedder": [
             "torch"
         ],
         "modelscope.models.nlp.space.modules.feedforward": [
             "torch"
         ],
         "modelscope.models.nlp.space.modules.functions": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.models.nlp.space.modules.multihead_attention": [
             "torch"
         ],
         "modelscope.models.nlp.space.modules.transformer_block": [
             "torch"
         ],
         "modelscope.models.nlp.space_T_cn.backbone": [
+            "shutil",
+            "copy",
+            "__future__",
             "tarfile",
             "numpy",
+            "torch",
             "math",
-            "__future__",
-            "shutil",
             "tempfile",
-            "os",
-            "torch",
-            "copy"
+            "os"
         ],
         "modelscope.models.nlp.space_T_cn.configuration": [
-            "__future__",
-            "json",
             "copy",
+            "json",
+            "__future__",
             "logging"
         ],
         "modelscope.models.nlp.space_T_cn.table_question_answering": [
-            "numpy",
+            "typing",
             "transformers",
+            "numpy",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.nlp.space_T_en.text_to_sql": [
-            "text2sql_lgesql",
             "torch",
-            "typing",
-            "os"
+            "text2sql_lgesql",
+            "os",
+            "typing"
         ],
         "modelscope.models.nlp.structbert.adv_utils": [
             "torch"
         ],
         "modelscope.models.nlp.structbert.backbone": [
+            "typing",
+            "transformers",
+            "dataclasses",
             "packaging",
             "math",
-            "dataclasses",
-            "transformers",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.nlp.structbert.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.structbert.faq_question_answering": [
-            "math",
+            "typing",
             "os",
+            "math",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.nlp.structbert.fill_mask": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.structbert.text_classification": [
             "torch"
         ],
         "modelscope.models.nlp.structbert.token_classification": [
             "torch"
         ],
         "modelscope.models.nlp.task_models.feature_extraction": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.fill_mask": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.task_models.information_extraction": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.task_model": [
-            "re",
-            "abc",
+            "typing",
             "os",
+            "abc",
             "torch",
             "collections",
-            "typing"
+            "re"
         ],
         "modelscope.models.nlp.task_models.text_classification": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.text_generation": [
-            "torch",
             "numpy",
             "transformers",
+            "torch",
             "typing"
         ],
         "modelscope.models.nlp.task_models.text_ranking": [
             "numpy",
             "typing"
         ],
         "modelscope.models.nlp.task_models.token_classification": [
             "torch",
             "typing"
         ],
-        "modelscope.models.nlp.unite.configuration_unite": [
+        "modelscope.models.nlp.unite.configuration": [
             "enum"
         ],
-        "modelscope.models.nlp.unite.modeling_unite": [
-            "packaging",
-            "numpy",
-            "math",
-            "dataclasses",
+        "modelscope.models.nlp.unite.translation_evaluation": [
+            "typing",
             "transformers",
-            "torch",
+            "dataclasses",
+            "numpy",
+            "packaging",
             "warnings",
-            "typing"
+            "math",
+            "torch"
         ],
         "modelscope.models.nlp.use.transformer": [
             "math",
             "torch"
         ],
         "modelscope.models.nlp.use.user_satisfaction_estimation": [
-            "numpy",
+            "typing",
             "transformers",
+            "numpy",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.models.nlp.veco.backbone": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.configuration": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.fill_mask": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.text_classification": [
             "transformers"
         ],
         "modelscope.models.nlp.veco.token_classification": [
-            "torch",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.xlm_roberta.backbone": [
             "packaging",
-            "torch",
             "math",
-            "transformers"
+            "transformers",
+            "torch"
         ],
         "modelscope.models.nlp.xlm_roberta.configuration": [
+            "transformers",
             "collections",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.models.science.unifold.config": [
             "copy",
-            "typing",
-            "ml_collections"
+            "ml_collections",
+            "typing"
         ],
         "modelscope.models.science.unifold.data.data_ops": [
+            "functools",
+            "typing",
+            "operator",
+            "unicore",
             "numpy",
             "itertools",
-            "unicore",
-            "torch",
-            "operator",
-            "typing",
-            "functools"
+            "torch"
         ],
         "modelscope.models.science.unifold.data.msa_pairing": [
+            "pandas",
+            "typing",
             "numpy",
             "scipy",
-            "pandas",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.models.science.unifold.data.process": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.models.science.unifold.data.process_multimer": [
             "numpy",
             "collections",
             "typing"
         ],
         "modelscope.models.science.unifold.data.protein": [
-            "numpy",
-            "io",
             "Bio",
+            "typing",
             "dataclasses",
-            "typing"
+            "io",
+            "numpy"
         ],
         "modelscope.models.science.unifold.data.residue_constants": [
-            "numpy",
-            "os",
+            "functools",
             "unicore",
-            "collections",
+            "numpy",
             "typing",
-            "functools"
+            "os",
+            "collections"
         ],
         "modelscope.models.science.unifold.data.utils": [
-            "numpy",
-            "pickle",
-            "gzip",
-            "scipy",
+            "functools",
             "json",
             "copy",
-            "functools",
-            "typing"
+            "typing",
+            "pickle",
+            "numpy",
+            "scipy",
+            "gzip"
         ],
         "modelscope.models.science.unifold.dataset": [
-            "numpy",
-            "typing",
             "json",
-            "os",
+            "copy",
+            "ml_collections",
             "logging",
-            "torch",
+            "typing",
             "unicore",
-            "copy",
-            "ml_collections"
+            "numpy",
+            "torch",
+            "os"
         ],
         "modelscope.models.science.unifold.model": [
             "argparse",
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.alphafold": [
-            "torch",
-            "unicore"
+            "unicore",
+            "torch"
         ],
         "modelscope.models.science.unifold.modules.attentions": [
-            "torch",
-            "typing",
             "functools",
-            "unicore"
+            "unicore",
+            "torch",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.auxillary_heads": [
+            "unicore",
             "torch",
-            "typing",
-            "unicore"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.common": [
-            "torch",
-            "typing",
             "functools",
-            "unicore"
+            "unicore",
+            "torch",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.confidence": [
             "torch",
             "typing"
         ],
         "modelscope.models.science.unifold.modules.embedders": [
+            "unicore",
             "torch",
-            "typing",
-            "unicore"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.evoformer": [
-            "torch",
-            "typing",
             "functools",
-            "unicore"
+            "unicore",
+            "torch",
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.featurization": [
+            "unicore",
             "torch",
-            "typing",
-            "unicore"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.frame": [
-            "torch",
             "numpy",
             "__future__",
+            "torch",
             "typing"
         ],
         "modelscope.models.science.unifold.modules.structure_module": [
+            "unicore",
             "math",
             "torch",
-            "typing",
-            "unicore"
+            "typing"
         ],
         "modelscope.models.science.unifold.modules.template": [
-            "math",
-            "unicore",
-            "torch",
+            "functools",
             "typing",
-            "functools"
+            "unicore",
+            "math",
+            "torch"
         ],
         "modelscope.models.science.unifold.modules.triangle_multiplication": [
-            "torch",
-            "typing",
             "functools",
-            "unicore"
+            "unicore",
+            "torch",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.mmcif": [
+            "functools",
+            "Bio",
             "dataclasses",
-            "absl",
             "io",
-            "Bio",
-            "collections",
             "typing",
-            "functools"
+            "absl",
+            "collections"
         ],
         "modelscope.models.science.unifold.msa.msa_identifiers": [
             "dataclasses",
             "re",
             "typing"
         ],
         "modelscope.models.science.unifold.msa.parsers": [
             "dataclasses",
+            "typing",
+            "itertools",
             "string",
-            "re",
             "collections",
-            "typing",
-            "itertools"
+            "re"
         ],
         "modelscope.models.science.unifold.msa.pipeline": [
-            "numpy",
             "absl",
-            "typing",
-            "os"
+            "numpy",
+            "os",
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.templates": [
-            "numpy",
+            "functools",
+            "datetime",
+            "typing",
             "absl",
-            "re",
+            "dataclasses",
             "glob",
+            "numpy",
             "abc",
-            "dataclasses",
-            "datetime",
             "os",
-            "typing",
-            "functools"
+            "re"
         ],
         "modelscope.models.science.unifold.msa.tools.hhblits": [
+            "subprocess",
+            "typing",
             "absl",
             "glob",
-            "os",
-            "typing",
-            "subprocess"
+            "os"
         ],
         "modelscope.models.science.unifold.msa.tools.hhsearch": [
+            "subprocess",
+            "typing",
             "absl",
             "glob",
-            "os",
-            "typing",
-            "subprocess"
+            "os"
         ],
         "modelscope.models.science.unifold.msa.tools.hmmbuild": [
             "absl",
             "re",
             "os",
             "subprocess"
         ],
         "modelscope.models.science.unifold.msa.tools.hmmsearch": [
             "absl",
-            "typing",
+            "subprocess",
             "os",
-            "subprocess"
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.tools.jackhmmer": [
+            "typing",
+            "subprocess",
+            "urllib",
             "absl",
             "concurrent",
-            "urllib",
             "glob",
-            "os",
-            "typing",
-            "subprocess"
+            "os"
         ],
         "modelscope.models.science.unifold.msa.tools.kalign": [
             "absl",
-            "typing",
+            "subprocess",
             "os",
-            "subprocess"
+            "typing"
         ],
         "modelscope.models.science.unifold.msa.tools.utils": [
-            "tempfile",
-            "absl",
-            "contextlib",
-            "shutil",
             "time",
-            "typing"
+            "shutil",
+            "typing",
+            "contextlib",
+            "tempfile",
+            "absl"
         ],
         "modelscope.models.science.unifold.msa.utils": [
-            "json",
             "absl",
-            "typing",
-            "os"
+            "json",
+            "os",
+            "typing"
         ],
         "modelscope.msdatasets.audio.asr_dataset": [],
         "modelscope.msdatasets.auth.auth_config": [
             "http",
             "typing"
         ],
         "modelscope.msdatasets.context.dataset_context_config": [
             "typing"
         ],
         "modelscope.msdatasets.data_files.data_files_manager": [
+            "os",
             "datasets",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.msdatasets.data_loader.data_loader": [
             "abc",
+            "os",
             "datasets",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.msdatasets.data_loader.data_loader_manager": [
             "abc",
-            "datasets",
             "enum",
-            "os"
+            "os",
+            "datasets"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.asr_dataset": [
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_farfield_dataset": [
+            "queue",
             "numpy",
-            "math",
-            "os",
-            "torch",
             "threading",
-            "queue"
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_dataset": [
-            "torch",
-            "random"
+            "random",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.audio.kws_nearfield_processor": [
-            "torchaudio",
+            "json",
             "numpy",
+            "torchaudio",
             "kaldiio",
-            "json",
-            "torch",
-            "random"
+            "random",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.bad_image_detecting.bad_image_detecting_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.builder": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.build": [
+            "copy",
             "math",
-            "torch",
             "bisect",
-            "copy"
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.collate_batch": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.datasets.coco": [
-            "torch",
-            "numpy",
             "torchvision",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.datasets.mosaic_wrapper": [
+            "cv2",
             "numpy",
             "math",
-            "torch",
             "random",
-            "cv2"
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.evaluation.coco.coco_eval": [
             "torch",
             "tempfile",
-            "collections",
-            "os"
+            "os",
+            "collections"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.distributed": [
             "math",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.grouped_batch_sampler": [
-            "torch",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.samplers.iteration_based_batch_sampler": [
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.transforms.build": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.damoyolo.transforms.transforms": [
+            "cv2",
             "numpy",
-            "torch",
-            "random",
             "torchvision",
-            "cv2"
+            "random",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.easycv_base": [
             "os"
         ],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.face_2d_keypoins.face_2d_keypoints_dataset": [
-            "easycv"
-        ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.gopro_image_deblurring_dataset": [
-            "numpy",
-            "cv2"
-        ],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.hand_2d_keypoints.hand_2d_keypoints_dataset": [
-            "easycv"
-        ],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.human_wholebody_keypoint.human_wholebody_keypoint_dataset": [
-            "easycv"
-        ],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.image_classification.classification_dataset": [
-            "easycv"
+            "cv2",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_colorization.image_colorization_dataset": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.aug": [
             "imgaug",
             "albumentations"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_inpainting.image_inpainting_dataset": [
-            "numpy",
             "albumentations",
+            "cv2",
             "glob",
+            "numpy",
             "enum",
-            "os",
-            "cv2"
+            "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_instance_segmentation_coco_dataset": [
             "pycocotools",
             "numpy",
             "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_portrait_enhancement.data_utils": [
-            "torch",
-            "cv2"
+            "cv2",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_portrait_enhancement.image_portrait_enhancement_dataset": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assessment_degradation.image_quality_assessment_degradation_dataset": [
             "torchvision"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.image_quality_assmessment_mos.image_quality_assessment_mos_dataset": [],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.image_semantic_segmentation.segmentation_dataset": [
-            "easycv"
-        ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.language_guided_video_summarization_dataset": [
-            "numpy",
             "json",
             "h5py",
+            "numpy",
             "os",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.mgeo_ranking_dataset": [
-            "torch",
             "json",
             "random",
+            "torch",
             "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.movie_scene_segmentation_dataset": [
             "json",
-            "os",
+            "copy",
+            "torchvision",
             "torch",
             "random",
-            "copy",
-            "torchvision"
+            "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.movie_scene_segmentation.sampler": [
-            "numpy",
-            "random"
-        ],
-        "modelscope.msdatasets.dataset_cls.custom_datasets.object_detection.detection_dataset": [
-            "easycv"
+            "random",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.augmenter": [
             "imgaug"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.data_loader": [
+            "bisect",
+            "imgaug",
             "numpy",
             "math",
-            "bisect",
-            "torch",
-            "imgaug"
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.image_dataset": [
-            "numpy",
-            "math",
-            "glob",
+            "functools",
             "bisect",
-            "os",
             "logging",
+            "cv2",
+            "glob",
+            "numpy",
             "torch",
-            "functools",
-            "cv2"
+            "math",
+            "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.measures.iou_evaluator": [
             "numpy",
             "collections",
             "shapely"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.measures.quad_measurer": [
             "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.augment_data": [
-            "math",
-            "numpy",
             "imgaug",
-            "cv2"
+            "cv2",
+            "math",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.data_process": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_border_map": [
-            "pyclipper",
+            "cv2",
             "numpy",
             "shapely",
-            "cv2"
+            "pyclipper"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_icdar_data": [
-            "torch",
+            "cv2",
             "numpy",
-            "collections",
-            "cv2"
+            "torch",
+            "collections"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.make_seg_detection_data": [
-            "pyclipper",
+            "cv2",
             "numpy",
             "shapely",
-            "cv2"
+            "pyclipper"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.normalize_image": [
-            "torch",
-            "numpy"
+            "numpy",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_detection.processes.random_crop_data": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.ocr_recognition_dataset": [
-            "numpy",
-            "PIL",
-            "lmdb",
+            "six",
             "json",
-            "os",
+            "PIL",
+            "cv2",
+            "numpy",
             "torch",
-            "six",
-            "cv2"
+            "lmdb",
+            "os"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.reds_image_deblurring_dataset": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.referring_video_object_segmentation_dataset": [
-            "pycocotools",
-            "numpy",
-            "glob",
+            "pandas",
             "json",
+            "torchvision",
+            "tqdm",
             "h5py",
+            "glob",
+            "numpy",
+            "pycocotools",
             "os",
-            "pandas",
-            "torch",
-            "tqdm",
-            "torchvision"
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.referring_video_object_segmentation.transformers": [
-            "torch",
-            "random",
             "torchvision",
+            "random",
+            "torch",
             "PIL"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.data_utils": [
-            "torch",
-            "cv2"
+            "cv2",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.sidd_image_denoising_dataset": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.sidd_image_denoising.transforms": [
             "random"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.text_ranking_dataset": [
-            "torch",
             "random",
+            "torch",
             "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.torch_custom_dataset": [
             "torch",
             "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.veco_dataset": [
-            "datasets",
             "numpy",
+            "datasets",
             "typing"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_frame_interpolation.data_utils": [
-            "torch",
-            "cv2"
+            "cv2",
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_frame_interpolation.video_frame_interpolation_dataset": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_stabilization.video_stabilization_dataset": [],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_summarization_dataset": [
-            "numpy",
             "json",
             "h5py",
+            "numpy",
             "os",
             "torch"
         ],
         "modelscope.msdatasets.dataset_cls.custom_datasets.video_super_resolution.video_super_resolution_dataset": [
-            "torch",
+            "cv2",
             "numpy",
-            "collections",
-            "cv2"
+            "torch",
+            "collections"
         ],
         "modelscope.msdatasets.dataset_cls.dataset": [
-            "PIL",
-            "os",
             "pandas",
-            "datasets",
-            "copy"
+            "os",
+            "copy",
+            "datasets"
         ],
         "modelscope.msdatasets.download.dataset_builder": [
-            "pyarrow",
-            "os",
             "pandas",
+            "typing",
+            "pyarrow",
             "datasets",
-            "typing"
+            "os"
         ],
         "modelscope.msdatasets.download.download_config": [
             "datasets",
             "typing"
         ],
         "modelscope.msdatasets.download.download_manager": [
             "datasets"
         ],
         "modelscope.msdatasets.meta.data_meta_config": [],
         "modelscope.msdatasets.meta.data_meta_manager": [
             "shutil",
             "json",
-            "os",
             "datasets",
+            "os",
             "collections"
         ],
         "modelscope.msdatasets.ms_dataset": [
+            "typing",
             "numpy",
-            "os",
-            "datasets",
             "warnings",
-            "typing"
+            "datasets",
+            "os"
         ],
         "modelscope.msdatasets.task_datasets.gopro_image_deblurring_dataset": [],
         "modelscope.msdatasets.task_datasets.reds_image_deblurring_dataset": [],
         "modelscope.msdatasets.task_datasets.sidd_image_denoising": [],
         "modelscope.msdatasets.task_datasets.torch_base_dataset": [],
         "modelscope.msdatasets.task_datasets.video_summarization_dataset": [],
         "modelscope.msdatasets.utils.dataset_utils": [
+            "os",
             "collections",
-            "typing",
-            "os"
+            "typing"
         ],
         "modelscope.msdatasets.utils.delete_utils": [],
+        "modelscope.msdatasets.utils.maxcompute_utils": [
+            "pandas",
+            "math"
+        ],
         "modelscope.msdatasets.utils.oss_utils": [
+            "__future__",
             "multiprocessing",
-            "os",
             "oss2",
             "datasets",
-            "__future__"
+            "os"
         ],
         "modelscope.msdatasets.utils.upload_utils": [
-            "os",
             "multiprocessing",
-            "tqdm"
+            "tqdm",
+            "os"
         ],
         "modelscope.pipelines.audio.ans_dfsmn_pipeline": [
-            "numpy",
-            "io",
+            "sys",
+            "typing",
             "librosa",
             "soundfile",
-            "os",
+            "io",
+            "numpy",
             "torch",
-            "collections",
-            "sys",
-            "typing"
+            "os",
+            "collections"
         ],
         "modelscope.pipelines.audio.ans_pipeline": [
-            "numpy",
-            "io",
-            "soundfile",
+            "typing",
             "librosa",
-            "torch",
-            "typing"
+            "soundfile",
+            "io",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.audio.asr_inference_pipeline": [
             "yaml",
-            "typing",
+            "json",
             "os",
-            "json"
+            "typing"
         ],
         "modelscope.pipelines.audio.asr_wenet_inference_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.audio.inverse_text_processing_pipeline": [
-            "shutil",
             "yaml",
-            "typing",
-            "os"
+            "shutil",
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.audio.kws_farfield_pipeline": [
-            "numpy",
-            "io",
-            "soundfile",
+            "wave",
             "typing",
-            "wave"
+            "soundfile",
+            "io",
+            "numpy"
         ],
         "modelscope.pipelines.audio.kws_kwsbp_pipeline": [
             "json",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.audio.linear_aec_pipeline": [
+            "typing",
+            "importlib",
             "numpy",
+            "os",
             "yaml",
             "scipy",
-            "importlib",
-            "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.pipelines.audio.lm_infer_pipeline": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.audio.punctuation_processing_pipeline": [
-            "shutil",
             "yaml",
-            "typing",
-            "os"
+            "shutil",
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.audio.separation_pipeline": [
-            "numpy",
+            "typing",
+            "soundfile",
             "io",
+            "numpy",
+            "torch"
+        ],
+        "modelscope.pipelines.audio.speaker_change_locating_pipeline": [
+            "typing",
             "soundfile",
-            "torch",
-            "typing"
+            "io",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.audio.speaker_diarization_pipeline": [
-            "yaml",
-            "numpy",
             "shutil",
+            "typing",
             "json",
-            "os",
+            "numpy",
+            "yaml",
+            "os"
+        ],
+        "modelscope.pipelines.audio.speaker_verification_eres2net_pipeline": [
+            "soundfile",
+            "torch",
+            "io",
             "typing"
         ],
         "modelscope.pipelines.audio.speaker_verification_light_pipeline": [
-            "io",
-            "torch",
             "soundfile",
+            "torch",
+            "io",
             "typing"
         ],
         "modelscope.pipelines.audio.speaker_verification_pipeline": [
-            "shutil",
             "yaml",
-            "typing",
-            "os"
+            "shutil",
+            "os",
+            "typing"
+        ],
+        "modelscope.pipelines.audio.speaker_verification_rdino_pipeline": [
+            "soundfile",
+            "torch",
+            "io",
+            "typing"
         ],
         "modelscope.pipelines.audio.text_to_speech_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.audio.timestamp_pipeline": [
-            "yaml",
-            "funasr",
             "json",
-            "os",
-            "typing"
+            "typing",
+            "funasr",
+            "yaml",
+            "os"
         ],
         "modelscope.pipelines.audio.voice_activity_detection_pipeline": [
-            "yaml",
-            "funasr",
             "json",
-            "os",
-            "typing"
+            "typing",
+            "funasr",
+            "yaml",
+            "os"
         ],
         "modelscope.pipelines.base": [
-            "packaging",
+            "functools",
+            "typing",
             "numpy",
             "multiprocessing",
             "threading",
-            "abc",
             "os",
-            "torch",
+            "packaging",
+            "abc",
             "random",
-            "typing",
-            "functools"
+            "torch"
         ],
         "modelscope.pipelines.builder": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.action_detection_pipeline": [
             "math",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.action_recognition_pipeline": [
             "math",
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.animal_recognition_pipeline": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.arc_face_recognition_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.bad_image_detecting_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.cv.body_2d_keypoints_pipeline": [
-            "numpy",
-            "PIL",
             "json",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.body_3d_keypoints_pipeline": [
-            "numpy",
-            "tempfile",
             "datetime",
-            "os",
             "mpl_toolkits",
+            "typing",
+            "cv2",
+            "numpy",
             "torch",
             "matplotlib",
-            "typing",
-            "cv2"
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.card_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.cmdssl_video_embedding_pipeline": [
-            "numpy",
-            "decord",
+            "typing",
+            "torchvision",
             "PIL",
+            "numpy",
             "os",
             "torch",
-            "typing",
-            "torchvision"
+            "decord"
         ],
         "modelscope.pipelines.cv.content_check_pipeline": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.controllable_image_generation_pipeline": [
-            "numpy",
-            "math",
+            "subprocess",
+            "typing",
             "cv2",
             "glob",
-            "tempfile",
-            "os",
+            "numpy",
             "torch",
-            "typing",
-            "subprocess"
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.crowd_counting_pipeline": [
+            "typing",
+            "torchvision",
+            "PIL",
             "numpy",
             "math",
-            "PIL",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.pipelines.cv.ddcolor_image_colorization_pipeline": [
-            "numpy",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.ddpm_semantic_segmentation_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
-        ],
-        "modelscope.pipelines.cv.easycv_pipelines.base": [
-            "numpy",
-            "easycv",
-            "glob",
-            "PIL",
-            "os",
-            "typing"
-        ],
-        "modelscope.pipelines.cv.easycv_pipelines.detection_pipeline": [
-            "typing"
-        ],
-        "modelscope.pipelines.cv.easycv_pipelines.face_2d_keypoints_pipeline": [
-            "numpy",
-            "typing",
-            "math",
-            "copy",
-            "cv2"
-        ],
-        "modelscope.pipelines.cv.easycv_pipelines.human_wholebody_keypoint_pipeline": [
-            "typing",
-            "os"
-        ],
-        "modelscope.pipelines.cv.easycv_pipelines.segmentation_pipeline": [
-            "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.face_attribute_recognition_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.face_detection_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.face_emotion_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.face_human_hand_detection_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.face_image_generation_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.face_liveness_ir_pipeline": [
-            "numpy",
-            "onnxruntime",
+            "typing",
             "PIL",
+            "onnxruntime",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.face_liveness_xc_pipeline": [
-            "numpy",
-            "onnxruntime",
+            "typing",
             "PIL",
+            "onnxruntime",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.face_processing_base_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.face_quality_assessment_pipeline": [
-            "numpy",
-            "onnxruntime",
+            "typing",
             "PIL",
+            "onnxruntime",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.face_recognition_onnx_fm_pipeline": [
-            "numpy",
-            "onnxruntime",
+            "typing",
             "PIL",
+            "onnxruntime",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.face_recognition_onnx_ir_pipeline": [
-            "numpy",
-            "onnxruntime",
+            "typing",
             "PIL",
+            "onnxruntime",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.face_recognition_ood_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.face_recognition_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.face_reconstruction_pipeline": [
-            "numpy",
+            "tensorflow",
             "face_alignment",
-            "io",
             "shutil",
+            "typing",
             "PIL",
-            "tensorflow",
-            "scipy",
-            "os",
+            "cv2",
+            "io",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "scipy",
+            "os"
         ],
         "modelscope.pipelines.cv.facial_expression_recognition_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.facial_landmark_confidence_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
-        "modelscope.pipelines.cv.general_recognition_pipeline": [
+        "modelscope.pipelines.cv.fast_instance_segmentation_pipeline": [
+            "torchvision",
             "numpy",
-            "PIL",
-            "os",
             "torch",
+            "typing"
+        ],
+        "modelscope.pipelines.cv.general_recognition_pipeline": [
             "typing",
             "torchvision",
-            "cv2"
-        ],
-        "modelscope.pipelines.cv.hand_2d_keypoints_pipeline": [
-            "os"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.hand_static_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.hicossl_video_embedding_pipeline": [
             "math",
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.human_reconstruction_pipeline": [
-            "numpy",
             "shutil",
+            "typing",
             "trimesh",
-            "os",
+            "numpy",
             "torch",
-            "typing"
+            "os"
         ],
         "modelscope.pipelines.cv.image_body_reshaping_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_bts_depth_estimation_pipeline": [
-            "numpy",
-            "albumentations",
-            "torch",
             "typing",
-            "cv2"
+            "albumentations",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_cartoon_pipeline": [
-            "numpy",
             "tensorflow",
-            "os",
             "typing",
-            "cv2"
+            "cv2",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.cv.image_classification_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.cv.image_color_enhance_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_colorization_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_debanding_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_deblur_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_defrcn_fewshot_pipeline": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_denoise_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_depth_estimation_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_detection_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.image_driving_perception_pipeline": [
+            "cv2",
             "numpy",
-            "typing",
             "os",
-            "cv2"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_face_fusion_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.image_human_parsing_pipeline": [
-            "torch",
+            "torchvision",
             "numpy",
-            "typing",
-            "torchvision"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_inpainting_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_inpainting_sdv2_pipeline": [
-            "numpy",
+            "sys",
             "typing",
-            "math",
-            "tempfile",
-            "os",
             "diffusers",
+            "cv2",
+            "numpy",
             "torch",
-            "sys",
-            "cv2"
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.image_instance_segmentation_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.image_matching_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_matting_pipeline": [
-            "numpy",
             "tensorflow",
-            "os",
             "typing",
-            "cv2"
+            "cv2",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.cv.image_mvs_depth_estimation_pipeline": [
             "tempfile",
             "shutil",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.image_open_vocabulary_detection_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.image_paintbyexample_pipeline": [
-            "numpy",
-            "einops",
-            "PIL",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
-        ],
-        "modelscope.pipelines.cv.image_panoptic_segmentation_pipeline": [
-            "numpy",
             "PIL",
-            "torch",
-            "typing",
-            "cv2"
+            "cv2",
+            "numpy",
+            "einops",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_portrait_enhancement_pipeline": [
-            "numpy",
-            "math",
+            "typing",
             "PIL",
+            "cv2",
+            "numpy",
             "scipy",
-            "torch",
-            "typing",
-            "cv2"
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_degradation_pipeline": [
+            "typing",
+            "torchvision",
+            "cv2",
             "numpy",
             "math",
             "tempfile",
-            "torch",
-            "typing",
-            "torchvision",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_man_pipeline": [
+            "typing",
+            "torchvision",
+            "cv2",
             "numpy",
             "math",
             "tempfile",
-            "torch",
-            "typing",
-            "torchvision",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.image_quality_assessment_mos_pipeline": [
+            "typing",
+            "torchvision",
+            "cv2",
             "numpy",
             "math",
             "tempfile",
-            "torch",
-            "typing",
-            "torchvision",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.image_reid_person_pipeline": [
-            "math",
+            "typing",
+            "torchvision",
             "PIL",
-            "os",
             "torch",
-            "typing",
-            "torchvision"
+            "math",
+            "os"
         ],
         "modelscope.pipelines.cv.image_restoration_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_salient_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.image_semantic_segmentation_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_skychange_pipeline": [
             "pdb",
-            "numpy",
-            "cv2",
+            "typing",
             "PIL",
+            "cv2",
             "time",
-            "typing"
+            "numpy"
         ],
         "modelscope.pipelines.cv.image_structured_model_probing_pipeline": [
-            "numpy",
             "mmcv",
-            "math",
-            "os",
-            "torch",
             "typing",
-            "torchvision"
+            "torchvision",
+            "numpy",
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_style_transfer_pipeline": [
+            "cv2",
             "numpy",
-            "typing",
             "os",
-            "cv2"
+            "typing"
         ],
         "modelscope.pipelines.cv.image_super_resolution_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_to_image_generate_pipeline": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.image_to_image_translation_pipeline": [
-            "numpy",
+            "sys",
             "typing",
-            "io",
+            "torchvision",
             "PIL",
-            "os",
+            "cv2",
+            "io",
+            "numpy",
             "torch",
-            "sys",
-            "torchvision",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.indoor_layout_estimation_pipeline": [
+            "cv2",
             "numpy",
-            "typing",
-            "cv2"
+            "typing"
         ],
         "modelscope.pipelines.cv.language_guided_video_summarization_pipeline": [
-            "numpy",
             "shutil",
+            "typing",
+            "clip",
             "PIL",
-            "tempfile",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "clip",
             "random",
-            "typing",
-            "cv2"
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.license_plate_detection_pipeline": [
-            "numpy",
-            "math",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "math",
+            "os"
         ],
         "modelscope.pipelines.cv.lineless_table_recognition_pipeline": [
-            "numpy",
-            "math",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "math",
+            "os"
         ],
         "modelscope.pipelines.cv.live_category_pipeline": [
-            "numpy",
-            "decord",
+            "typing",
+            "torchvision",
             "PIL",
+            "numpy",
             "os",
             "torch",
-            "typing",
-            "torchvision"
+            "decord"
         ],
         "modelscope.pipelines.cv.mask_face_recognition_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "collections",
-            "typing",
-            "cv2"
+            "os",
+            "collections"
         ],
         "modelscope.pipelines.cv.maskdino_instance_segmentation_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.pipelines.cv.mobile_image_super_resolution_pipeline": [
-            "numpy",
-            "torchvision",
-            "torch",
             "typing",
-            "skimage"
+            "torchvision",
+            "numpy",
+            "skimage",
+            "torch"
         ],
         "modelscope.pipelines.cv.mog_face_detection_pipeline": [
             "numpy",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.motion_generation_pipeline": [
+            "typing",
             "numpy",
-            "tempfile",
             "os",
-            "torch",
-            "typing"
+            "tempfile",
+            "torch"
         ],
         "modelscope.pipelines.cv.movie_scene_segmentation_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.cv.mtcnn_face_detection_pipeline": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.nerf_recon_acc_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.object_detection_3d_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "tempfile",
+            "cv2",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "tempfile",
+            "torch"
         ],
         "modelscope.pipelines.cv.ocr_detection_pipeline": [
-            "numpy",
-            "math",
             "tensorflow",
-            "os",
-            "torch",
             "typing",
+            "cv2",
             "tf_slim",
-            "cv2"
+            "numpy",
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.cv.ocr_recognition_pipeline": [],
         "modelscope.pipelines.cv.ocr_utils.model_convnext_transformer": [
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_dla34": [
             "math",
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_resnet18_half": [
             "torch",
             "os"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_resnet_mutex_v4_linewithchar": [
             "tensorflow",
             "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.model_vlpt": [
             "math",
-            "torch",
             "sys",
+            "torch",
             "os"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.convnext": [
             "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.timm_tinyc": [
-            "logging",
             "functools",
+            "copy",
+            "logging",
             "math",
+            "itertools",
             "torch",
-            "collections",
-            "copy",
-            "itertools"
+            "collections"
         ],
         "modelscope.pipelines.cv.ocr_utils.ocr_modules.vitstr": [
-            "__future__",
-            "logging",
-            "torch",
+            "functools",
             "copy",
-            "functools"
+            "logging",
+            "__future__",
+            "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.ops": [
-            "uuid",
+            "tensorflow",
+            "sys",
+            "shutil",
             "absl",
+            "cv2",
             "numpy",
+            "uuid",
             "math",
-            "shutil",
-            "tensorflow",
-            "os",
-            "sys",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.ocr_utils.resnet18_v1": [
             "tensorflow",
             "tf_slim"
         ],
         "modelscope.pipelines.cv.ocr_utils.resnet_utils": [
             "tensorflow",
-            "collections",
-            "tf_slim"
+            "tf_slim",
+            "collections"
         ],
         "modelscope.pipelines.cv.ocr_utils.table_process": [
+            "cv2",
             "numpy",
+            "copy",
             "math",
-            "torch",
             "random",
-            "copy",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.ocr_utils.utils": [
-            "pyclipper",
+            "cv2",
             "numpy",
             "shapely",
-            "cv2"
+            "pyclipper"
         ],
         "modelscope.pipelines.cv.panorama_depth_estimation_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.cv.pedestrian_attribute_recognition_pipeline": [
-            "numpy",
-            "PIL",
             "json",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.pointcloud_sceneflow_estimation_pipeline": [
-            "torch",
             "numpy",
-            "typing",
-            "plyfile"
+            "plyfile",
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.cv.product_retrieval_embedding_pipeline": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.product_segmentation_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.cv.realtime_video_object_detection_pipeline": [
-            "numpy",
-            "PIL",
             "json",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.referring_video_object_segmentation_pipeline": [
-            "moviepy",
+            "typing",
+            "torchvision",
+            "tqdm",
+            "PIL",
             "numpy",
+            "moviepy",
             "einops",
-            "PIL",
             "tempfile",
-            "torch",
-            "tqdm",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.pipelines.cv.retina_face_detection_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.shop_segmentation_pipleline": [
             "typing"
         ],
         "modelscope.pipelines.cv.skin_retouching_pipeline": [
-            "numpy",
-            "PIL",
             "tensorflow",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch",
+            "os"
         ],
         "modelscope.pipelines.cv.table_recognition_pipeline": [
-            "numpy",
-            "math",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "math",
+            "os"
         ],
         "modelscope.pipelines.cv.tbs_detection_pipeline": [
-            "numpy",
-            "colorsys",
+            "typing",
             "PIL",
+            "cv2",
+            "colorsys",
+            "numpy",
             "os",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.cv.tbs_detection_utils.utils": [
-            "numpy",
-            "colorsys",
-            "PIL",
-            "os",
             "pandas",
-            "torch",
-            "matplotlib",
+            "torchvision",
             "__future__",
-            "torchvision"
+            "PIL",
+            "colorsys",
+            "numpy",
+            "matplotlib",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.text_driven_segmentation_pipleline": [
             "typing"
         ],
         "modelscope.pipelines.cv.tinynas_classification_pipeline": [
-            "math",
-            "os",
-            "torch",
             "typing",
-            "torchvision"
+            "torchvision",
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.cv.tinynas_detection_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.ulfd_face_detection_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.video_category_pipeline": [
-            "numpy",
-            "decord",
-            "PIL",
             "json",
+            "torchvision",
+            "typing",
+            "PIL",
+            "numpy",
             "os",
             "torch",
-            "typing",
-            "torchvision"
+            "decord"
         ],
         "modelscope.pipelines.cv.video_colorization_pipeline": [
-            "numpy",
-            "PIL",
-            "tempfile",
             "subprocess",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
-        ],
-        "modelscope.pipelines.cv.video_deinterlace_pipeline": [
+            "PIL",
+            "cv2",
             "numpy",
-            "math",
+            "torch",
             "tempfile",
+            "os"
+        ],
+        "modelscope.pipelines.cv.video_deinterlace_pipeline": [
             "subprocess",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch",
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.video_depth_estimation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.video_frame_interpolation_pipeline": [
-            "numpy",
-            "math",
+            "subprocess",
+            "typing",
+            "torchvision",
             "cv2",
             "glob",
-            "tempfile",
-            "os",
+            "numpy",
             "torch",
-            "typing",
-            "torchvision",
-            "subprocess"
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.video_human_matting_pipeline": [
-            "moviepy",
+            "typing",
+            "cv2",
             "numpy",
-            "os",
             "torch",
-            "typing",
-            "cv2"
+            "moviepy",
+            "os"
         ],
         "modelscope.pipelines.cv.video_inpainting_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.cv.video_instance_segmentation_pipeline": [
-            "numpy",
             "mmcv",
-            "os",
-            "torch",
-            "tqdm",
             "typing",
-            "cv2"
+            "tqdm",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.video_multi_object_tracking_pipeline": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.cv.video_object_segmentation_pipeline": [
-            "numpy",
+            "typing",
+            "torchvision",
             "PIL",
-            "os",
+            "numpy",
             "torch",
-            "typing",
-            "torchvision"
+            "os"
         ],
         "modelscope.pipelines.cv.video_panoptic_segmentation_pipeline": [
-            "numpy",
             "mmcv",
-            "os",
-            "torch",
-            "tqdm",
             "typing",
-            "cv2"
+            "tqdm",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.cv.video_single_object_tracking_pipeline": [
-            "typing",
+            "cv2",
             "os",
-            "cv2"
+            "typing"
         ],
         "modelscope.pipelines.cv.video_stabilization_pipeline": [
-            "numpy",
-            "math",
+            "subprocess",
+            "typing",
             "cv2",
             "glob",
-            "tempfile",
-            "os",
+            "numpy",
             "torch",
-            "typing",
-            "subprocess"
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.video_summarization_pipeline": [
+            "typing",
+            "tqdm",
+            "cv2",
             "numpy",
-            "os",
             "torch",
-            "tqdm",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.video_super_resolution_pipeline": [
-            "numpy",
-            "math",
-            "tempfile",
             "subprocess",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch",
+            "math",
+            "tempfile",
+            "os"
         ],
         "modelscope.pipelines.cv.vidt_pipeline": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.pipelines.cv.virtual_try_on_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
-            "os",
+            "cv2",
+            "numpy",
             "torch",
-            "typing",
-            "cv2"
+            "os"
         ],
         "modelscope.pipelines.cv.vision_efficient_tuning_pipeline": [
-            "torch",
+            "torchvision",
             "numpy",
-            "typing",
-            "torchvision"
+            "torch",
+            "typing"
         ],
         "modelscope.pipelines.cv.vision_middleware_pipeline": [
-            "numpy",
             "mmcv",
-            "math",
-            "os",
-            "torch",
             "typing",
-            "torchvision"
+            "torchvision",
+            "numpy",
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.cv.vop_retrieval_pipeline": [
-            "numpy",
-            "pickle",
-            "gzip",
+            "typing",
+            "random",
             "tqdm",
+            "pickle",
+            "numpy",
+            "torch",
             "math",
+            "gzip",
             "os",
-            "collections",
-            "torch",
-            "random",
-            "typing"
+            "collections"
         ],
         "modelscope.pipelines.cv.vop_retrieval_se_pipeline": [
+            "typing",
             "numpy",
-            "gzip",
             "os",
-            "torch",
-            "typing"
+            "gzip",
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.asr_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.diffusers_pipeline": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.chinese_stable_diffusion_pipeline": [
-            "numpy",
-            "PIL",
+            "typing",
             "transformers",
+            "PIL",
             "diffusers",
-            "torch",
-            "typing",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.diffusers_wrapped.stable_diffusion.stable_diffusion_pipeline": [
-            "numpy",
+            "typing",
             "PIL",
             "diffusers",
-            "torch",
-            "typing",
-            "cv2"
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.disco_guided_diffusion": [
-            "numpy",
-            "math",
             "gc",
+            "json",
+            "torchvision",
+            "clip",
             "PIL",
+            "cv2",
             "importlib",
-            "json",
-            "os",
+            "numpy",
             "torch",
-            "clip",
-            "torchvision",
-            "cv2"
+            "math",
+            "os"
         ],
         "modelscope.pipelines.multi_modal.disco_guided_diffusion_pipeline.utils": [
             "numpy",
-            "math",
-            "torch",
             "warnings",
-            "fractions"
+            "fractions",
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.document_vl_embedding_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.efficient_diffusion_tuning_pipeline": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.generative_multi_modal_embedding_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.multi_modal.gridvlp_pipeline": [
-            "numpy",
-            "traceback",
-            "PIL",
             "json",
+            "typing",
             "transformers",
-            "os",
-            "torch",
+            "PIL",
             "time",
-            "typing"
+            "numpy",
+            "torch",
+            "os",
+            "traceback"
         ],
         "modelscope.pipelines.multi_modal.image_captioning_pipeline": [
+            "numpy",
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.image_text_retrieval_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.mgeo_ranking_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.multi_modal_embedding_pipeline": [
             "typing"
         ],
+        "modelscope.pipelines.multi_modal.multimodal_dialogue_pipeline": [
+            "torch",
+            "typing"
+        ],
         "modelscope.pipelines.multi_modal.ocr_recognition_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.soonet_video_temporal_grounding_pipeline": [
+            "typing",
+            "torchvision",
             "numpy",
             "os",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.sudoku_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.team_multi_modal_similarity_pipeline": [
             "typing"
@@ -17700,19 +17735,20 @@
             "typing"
         ],
         "modelscope.pipelines.multi_modal.text_to_image_synthesis_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.text_to_video_synthesis_pipeline": [
+            "typing",
+            "cv2",
+            "os",
             "einops",
             "tempfile",
-            "torch",
-            "typing",
-            "cv2"
+            "torch"
         ],
         "modelscope.pipelines.multi_modal.video_captioning_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.multi_modal.video_multi_modal_embedding_pipeline": [
             "typing"
@@ -17730,28 +17766,28 @@
             "typing"
         ],
         "modelscope.pipelines.multi_modal.visual_question_answering_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.automatic_post_editing_pipeline": [
-            "numpy",
-            "html",
             "tensorflow",
-            "jieba",
-            "sentencepiece",
-            "os",
             "sacremoses",
-            "typing"
+            "typing",
+            "sentencepiece",
+            "html",
+            "jieba",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.nlp.canmt_translation_pipeline": [
-            "torch",
             "sacremoses",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.nlp.codegeex_code_generation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.codegeex_code_translation_pipeline": [
             "typing"
         ],
@@ -17781,181 +17817,180 @@
             "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_generate_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_rerank_pipeline": [
-            "numpy",
-            "re",
+            "ujson",
+            "sys",
             "pprint",
-            "time",
+            "typing",
             "transformers",
-            "os",
-            "ujson",
-            "collections",
+            "time",
+            "numpy",
             "torch",
             "random",
-            "sys",
-            "typing"
+            "os",
+            "collections",
+            "re"
         ],
         "modelscope.pipelines.nlp.document_grounded_dialog_retrieval_pipeline": [
-            "numpy",
-            "faiss",
             "json",
-            "os",
-            "typing"
+            "typing",
+            "faiss",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.nlp.document_segmentation_pipeline": [
+            "typing",
             "numpy",
-            "re",
-            "transformers",
             "datasets",
             "torch",
-            "typing"
+            "re"
         ],
         "modelscope.pipelines.nlp.extractive_summarization_pipeline": [
+            "typing",
             "numpy",
-            "re",
             "datasets",
             "torch",
-            "typing"
+            "re"
         ],
         "modelscope.pipelines.nlp.faq_question_answering_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.fasttext_text_classification_pipeline": [
-            "numpy",
-            "fasttext",
+            "typing",
             "sentencepiece",
-            "os",
-            "typing"
+            "fasttext",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.nlp.feature_extraction_pipeline": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.nlp.fid_dialogue_pipeline": [
-            "torch",
             "re",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.fill_mask_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.nlp.glm130b_text_generation_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.information_extraction_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.interactive_translation_pipeline": [
-            "subword_nmt",
-            "numpy",
             "tensorflow",
-            "jieba",
-            "os",
             "sacremoses",
-            "typing"
+            "subword_nmt",
+            "typing",
+            "jieba",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.nlp.language_identification_pipline": [
-            "numpy",
-            "re",
             "tensorflow",
+            "typing",
+            "numpy",
             "os",
-            "typing"
+            "re"
         ],
         "modelscope.pipelines.nlp.mglm_text_summarization_pipeline": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.nlp.named_entity_recognition_pipeline": [
             "typing"
         ],
         "modelscope.pipelines.nlp.sentence_embedding_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.siamese_uie_pipeline": [
             "pathlib",
-            "math",
-            "scipy",
-            "time",
             "json",
-            "os",
             "logging",
-            "torch",
-            "tqdm",
             "copy",
-            "typing"
+            "typing",
+            "tqdm",
+            "time",
+            "os",
+            "scipy",
+            "math",
+            "torch"
         ],
         "modelscope.pipelines.nlp.summarization_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.table_question_answering_pipeline": [
             "json",
+            "typing",
             "transformers",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.pipelines.nlp.text_classification_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.text_error_correction_pipeline": [
             "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.text_generation_pipeline": [
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.pipelines.nlp.text_ranking_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.nlp.token_classification_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.translation_evaluation_pipeline": [
+            "typing",
             "numpy",
-            "enum",
             "os",
-            "torch",
-            "typing"
+            "enum",
+            "torch"
         ],
         "modelscope.pipelines.nlp.translation_pipeline": [
-            "subword_nmt",
-            "numpy",
             "tensorflow",
-            "jieba",
-            "os",
             "sacremoses",
-            "typing"
+            "subword_nmt",
+            "typing",
+            "jieba",
+            "numpy",
+            "os"
         ],
         "modelscope.pipelines.nlp.translation_quality_estimation_pipeline": [
-            "io",
+            "typing",
             "transformers",
+            "io",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.pipelines.nlp.user_satisfaction_estimation_pipeline": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.pipelines.nlp.word_alignment_pipeline": [
             "numpy",
             "typing"
         ],
         "modelscope.pipelines.nlp.word_segmentation_pipeline": [
@@ -17963,207 +17998,212 @@
             "typing"
         ],
         "modelscope.pipelines.nlp.zero_shot_classification_pipeline": [
             "scipy",
             "torch",
             "typing"
         ],
-        "modelscope.pipelines.science.protein_structure_pipeline": [
+        "modelscope.pipelines.pipeline_template": [
             "numpy",
+            "typing"
+        ],
+        "modelscope.pipelines.science.protein_structure_pipeline": [
             "json",
-            "os",
+            "typing",
             "unicore",
-            "torch",
             "time",
-            "typing"
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.pipelines.util": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.asr": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.audio": [
-            "numpy",
+            "typing",
             "io",
-            "scipy",
-            "os",
+            "numpy",
             "torch",
-            "typing"
+            "scipy",
+            "os"
         ],
         "modelscope.preprocessors.base": [
             "abc",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.builder": [],
         "modelscope.preprocessors.common": [
+            "typing",
+            "time",
             "numpy",
             "torch",
-            "collections",
-            "time",
-            "typing"
+            "collections"
         ],
         "modelscope.preprocessors.cv.action_detection_mapper": [
-            "scipy",
             "numpy",
-            "decord",
-            "detectron2",
             "torch",
+            "copy",
+            "scipy",
             "random",
-            "copy"
+            "detectron2",
+            "decord"
         ],
         "modelscope.preprocessors.cv.bad_image_detecting_preprocessor": [
+            "typing",
+            "torchvision",
+            "PIL",
             "numpy",
             "math",
-            "PIL",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.preprocessors.cv.controllable_image_generation": [
-            "numpy",
-            "math",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.preprocessors.cv.cv2_transforms": [
-            "numpy",
             "numbers",
             "cv2",
+            "numpy",
             "math",
-            "torch",
             "random",
+            "torch",
             "collections"
         ],
         "modelscope.preprocessors.cv.image_classification_preprocessor": [
-            "numpy",
-            "PIL",
-            "os",
-            "torch",
             "typing",
             "torchvision",
-            "cv2"
+            "PIL",
+            "cv2",
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.preprocessors.cv.image_quality_assessment_man": [
+            "typing",
+            "torchvision",
+            "PIL",
             "numpy",
             "math",
-            "PIL",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.preprocessors.cv.image_quality_assessment_mos": [
-            "numpy",
-            "math",
             "typing",
             "torchvision",
-            "cv2"
+            "cv2",
+            "numpy",
+            "math"
         ],
         "modelscope.preprocessors.cv.image_restoration_preprocessor": [
+            "typing",
+            "torchvision",
+            "PIL",
             "numpy",
             "math",
-            "PIL",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.preprocessors.cv.mmcls_preprocessor": [
             "numpy",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.cv.timer": [
             "time"
         ],
         "modelscope.preprocessors.cv.util": [
-            "shutil",
-            "collections",
             "sys",
-            "os"
+            "shutil",
+            "os",
+            "collections"
         ],
         "modelscope.preprocessors.cv.video_stabilization": [
-            "torch",
+            "cv2",
             "numpy",
-            "cv2"
+            "torch"
         ],
         "modelscope.preprocessors.cv.video_super_resolution": [
-            "collections",
+            "cv2",
             "os",
-            "cv2"
+            "collections"
         ],
         "modelscope.preprocessors.image": [
-            "numpy",
-            "io",
-            "PIL",
             "typing",
-            "cv2"
+            "PIL",
+            "cv2",
+            "io",
+            "numpy"
         ],
         "modelscope.preprocessors.kws": [
             "yaml",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.movie_scene_segmentation.transforms": [
-            "numpy",
+            "typing",
+            "torchvision",
             "PIL",
             "numbers",
+            "numpy",
             "os",
-            "torch",
             "random",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.preprocessors.multi_modal": [
-            "numpy",
-            "decord",
-            "io",
-            "PIL",
             "timm",
             "json",
-            "os",
-            "torch",
             "typing",
-            "torchvision"
+            "torchvision",
+            "PIL",
+            "numpy",
+            "torch",
+            "io",
+            "os",
+            "decord",
+            "re"
         ],
         "modelscope.preprocessors.nlp.bert_seq_cls_tokenizer": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.canmt_translation": [
             "sacremoses",
             "subword_nmt",
+            "typing",
             "jieba",
-            "os",
             "torch",
-            "typing"
+            "os"
         ],
         "modelscope.preprocessors.nlp.dialog_classification_use_preprocessor": [
+            "transformers",
             "torch",
-            "typing",
-            "transformers"
+            "typing"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_generate_preprocessor": [
             "torch",
             "transformers",
             "os",
             "typing"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_rerank_preprocessor": [
+            "typing",
+            "copy",
             "transformers",
             "os",
-            "torch",
-            "copy",
-            "typing"
+            "torch"
         ],
         "modelscope.preprocessors.nlp.document_grounded_dialog_retrieval_preprocessor": [
             "torch",
             "transformers",
             "os",
             "typing"
         ],
@@ -18175,93 +18215,93 @@
             "typing"
         ],
         "modelscope.preprocessors.nlp.feature_extraction_preprocessor": [
             "numpy",
             "typing"
         ],
         "modelscope.preprocessors.nlp.fill_mask_preprocessor": [
+            "typing",
             "numpy",
-            "re",
-            "abc",
             "os",
+            "abc",
             "torch",
-            "typing"
+            "re"
         ],
         "modelscope.preprocessors.nlp.mgeo_ranking_preprocessor": [
-            "torch",
             "transformers",
+            "torch",
             "typing"
         ],
         "modelscope.preprocessors.nlp.mglm_summarization_preprocessor": [
             "re",
             "os",
             "typing"
         ],
         "modelscope.preprocessors.nlp.relation_extraction_preprocessor": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.sentence_embedding_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.siamese_uie_preprocessor": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.space.args": [
             "argparse",
             "json"
         ],
         "modelscope.preprocessors.nlp.space.batch": [],
         "modelscope.preprocessors.nlp.space.data_loader": [
             "math",
             "numpy",
             "os"
         ],
         "modelscope.preprocessors.nlp.space.dialog_intent_prediction_preprocessor": [
             "json",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.space.dialog_modeling_preprocessor": [
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.space.dialog_state_tracking_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.space.dst_processors": [
+            "six",
             "numpy",
             "json",
-            "re",
             "logging",
-            "six",
-            "tqdm"
+            "tqdm",
+            "re"
         ],
         "modelscope.preprocessors.nlp.space.fields.gen_field": [
-            "numpy",
-            "asyncio",
             "json",
-            "os",
-            "collections",
+            "asyncio",
+            "numpy",
+            "itertools",
             "random",
-            "itertools"
+            "os",
+            "collections"
         ],
         "modelscope.preprocessors.nlp.space.fields.intent_field": [
-            "numpy",
-            "multiprocessing",
-            "re",
+            "json",
             "tqdm",
             "glob",
-            "json",
+            "multiprocessing",
+            "time",
+            "numpy",
+            "itertools",
+            "random",
             "os",
             "collections",
-            "random",
-            "time",
-            "itertools"
+            "re"
         ],
         "modelscope.preprocessors.nlp.space.lazy_dataset": [
             "json"
         ],
         "modelscope.preprocessors.nlp.space.preprocess": [
             "glob",
             "os"
@@ -18269,851 +18309,862 @@
         "modelscope.preprocessors.nlp.space.sampler": [
             "numpy"
         ],
         "modelscope.preprocessors.nlp.space.tensorlistdataset": [
             "torch"
         ],
         "modelscope.preprocessors.nlp.space.tokenizer": [
-            "__future__",
+            "functools",
             "unicodedata",
+            "sys",
             "json",
+            "logging",
             "regex",
+            "__future__",
             "os",
-            "logging",
-            "collections",
-            "sys",
-            "functools"
+            "collections"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.database": [
             "sqlite3",
-            "json",
-            "tqdm"
+            "tqdm",
+            "json"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.schema_link": [
             "re"
         ],
         "modelscope.preprocessors.nlp.space_T_cn.fields.struct": [],
         "modelscope.preprocessors.nlp.space_T_cn.table_question_answering_preprocessor": [
             "torch",
             "transformers",
             "os",
             "typing"
         ],
         "modelscope.preprocessors.nlp.space_T_en.conversational_text_to_sql_preprocessor": [
-            "text2sql_lgesql",
             "json",
+            "typing",
             "os",
-            "torch",
-            "typing"
+            "text2sql_lgesql",
+            "torch"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.common_utils": [
             "numpy",
-            "text2sql_lgesql",
             "sqlite3",
             "nltk",
-            "os",
-            "itertools"
+            "itertools",
+            "text2sql_lgesql",
+            "os"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.parse": [],
         "modelscope.preprocessors.nlp.space_T_en.fields.preprocess_dataset": [
             "text2sql_lgesql"
         ],
         "modelscope.preprocessors.nlp.space_T_en.fields.process_dataset": [
             "pickle",
-            "text2sql_lgesql",
             "sys",
+            "text2sql_lgesql",
             "os"
         ],
         "modelscope.preprocessors.nlp.text_classification_preprocessor": [
             "numpy",
             "typing"
         ],
         "modelscope.preprocessors.nlp.text_clean": [
-            "re",
             "sys",
-            "codecs"
+            "codecs",
+            "re"
         ],
         "modelscope.preprocessors.nlp.text_error_correction": [
             "torch",
             "transformers",
             "os",
             "typing"
         ],
         "modelscope.preprocessors.nlp.text_generation_preprocessor": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.text_ranking_preprocessor": [
-            "typing",
-            "transformers"
+            "transformers",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_preprocessor": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_thai_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.nlp.token_classification_viet_preprocessor": [
             "torch",
             "typing"
         ],
         "modelscope.preprocessors.nlp.transformers_tokenizer": [
-            "json",
-            "collections",
             "transformers",
-            "os"
+            "os",
+            "collections",
+            "json"
         ],
         "modelscope.preprocessors.nlp.translation_evaluation_preprocessor": [
-            "typing",
-            "transformers"
+            "transformers",
+            "torch",
+            "typing"
         ],
         "modelscope.preprocessors.nlp.utils": [
-            "numpy",
             "json",
+            "typing",
             "transformers",
+            "numpy",
             "os",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.preprocessors.nlp.word_alignment_preprocessor": [
+            "typing",
             "numpy",
             "os",
-            "torch",
-            "typing",
-            "itertools"
+            "itertools",
+            "torch"
         ],
         "modelscope.preprocessors.nlp.zero_shot_classification_preprocessor": [
             "typing"
         ],
         "modelscope.preprocessors.ofa.asr": [
             "pathlib",
+            "typing",
             "librosa",
             "soundfile",
-            "os",
+            "fairseq",
             "torch",
             "random",
-            "typing",
-            "fairseq"
+            "os"
         ],
         "modelscope.preprocessors.ofa.base": [
-            "torchaudio",
-            "numpy",
+            "json",
+            "PIL",
             "string",
-            "re",
             "io",
-            "PIL",
-            "json",
+            "numpy",
+            "torchaudio",
             "os",
-            "torch"
+            "torch",
+            "re"
         ],
         "modelscope.preprocessors.ofa.image_captioning": [
+            "torchvision",
             "torch",
-            "typing",
-            "torchvision"
+            "typing"
         ],
         "modelscope.preprocessors.ofa.image_classification": [
-            "PIL",
-            "timm",
-            "torch",
             "functools",
             "typing",
-            "torchvision"
+            "torchvision",
+            "PIL",
+            "torch",
+            "timm"
         ],
         "modelscope.preprocessors.ofa.ocr_recognition": [
+            "typing",
+            "torchvision",
             "unicodedata2",
             "zhconv",
-            "torch",
-            "typing",
-            "torchvision"
+            "torch"
         ],
         "modelscope.preprocessors.ofa.sudoku": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.preprocessors.ofa.summarization": [
             "torch",
             "typing"
         ],
         "modelscope.preprocessors.ofa.text2sql": [
-            "re",
+            "typing",
             "os",
-            "torch",
             "random",
-            "typing"
+            "torch",
+            "re"
         ],
         "modelscope.preprocessors.ofa.text_classification": [
             "torch",
             "typing"
         ],
         "modelscope.preprocessors.ofa.text_to_image_synthesis": [
             "torch",
             "typing"
         ],
         "modelscope.preprocessors.ofa.utils.audio_helper": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.preprocessors.ofa.utils.bridge_content_encoder": [
+            "functools",
+            "typing",
             "sqlite3",
             "difflib",
-            "rapidfuzz",
-            "typing",
-            "functools"
+            "rapidfuzz"
         ],
         "modelscope.preprocessors.ofa.utils.collate": [
-            "torch",
             "numpy",
+            "torch",
             "typing"
         ],
         "modelscope.preprocessors.ofa.utils.constant": [],
         "modelscope.preprocessors.ofa.utils.get_tables": [
-            "sys",
             "sqlite3",
+            "sys",
             "traceback"
         ],
         "modelscope.preprocessors.ofa.utils.random_help": [
             "torch",
             "torch_xla"
         ],
         "modelscope.preprocessors.ofa.utils.text2phone": [],
         "modelscope.preprocessors.ofa.utils.transforms": [
             "numpy",
             "torch",
-            "random",
             "torchvision",
+            "random",
             "PIL"
         ],
         "modelscope.preprocessors.ofa.utils.vision_helper": [
-            "numpy",
-            "cv2"
+            "cv2",
+            "numpy"
         ],
         "modelscope.preprocessors.ofa.visual_entailment": [
-            "torch",
-            "typing",
             "torchvision",
-            "PIL"
+            "torch",
+            "PIL",
+            "typing"
         ],
         "modelscope.preprocessors.ofa.visual_grounding": [
-            "numpy",
-            "PIL",
-            "torch",
             "typing",
-            "torchvision"
+            "torchvision",
+            "PIL",
+            "numpy",
+            "torch"
         ],
         "modelscope.preprocessors.ofa.visual_question_answering": [
-            "torch",
-            "typing",
             "torchvision",
-            "PIL"
+            "torch",
+            "PIL",
+            "typing"
         ],
         "modelscope.preprocessors.science.uni_fold": [
-            "tarfile",
+            "typing",
+            "json",
+            "logging",
+            "ipdb",
+            "pickle",
+            "time",
+            "requests",
             "hashlib",
-            "pathlib",
+            "gzip",
+            "random",
             "os",
-            "logging",
-            "unittest",
             "torch",
-            "random",
-            "time",
-            "numpy",
             "re",
-            "pickle",
-            "gzip",
-            "ipdb",
-            "requests",
-            "json",
+            "pathlib",
+            "unittest",
             "tqdm",
-            "typing"
+            "tarfile",
+            "numpy"
         ],
         "modelscope.preprocessors.tts": [
-            "typing",
+            "kantts",
             "os",
-            "kantts"
+            "typing"
         ],
         "modelscope.preprocessors.video": [
+            "urllib",
+            "torchvision",
             "numpy",
+            "torch",
             "uuid",
-            "decord",
             "math",
-            "urllib",
+            "random",
             "tempfile",
             "os",
-            "torch",
-            "random",
-            "torchvision"
+            "decord"
         ],
         "modelscope.trainers.audio.ans_trainer": [],
         "modelscope.trainers.audio.asr_trainer": [
             "shutil",
+            "json",
+            "typing",
             "funasr",
             "tempfile",
-            "json",
-            "os",
-            "typing"
+            "os"
         ],
         "modelscope.trainers.audio.kws_farfield_trainer": [
-            "numpy",
+            "datetime",
+            "typing",
             "pickle",
-            "math",
             "glob",
-            "datetime",
-            "os",
+            "numpy",
             "torch",
-            "typing"
+            "math",
+            "os"
         ],
         "modelscope.trainers.audio.kws_nearfield_trainer": [
-            "yaml",
-            "re",
             "datetime",
-            "os",
             "tensorboardX",
-            "torch",
+            "typing",
             "copy",
-            "typing"
+            "torch",
+            "yaml",
+            "os",
+            "re"
         ],
         "modelscope.trainers.audio.kws_utils.batch_utils": [
+            "datetime",
+            "sys",
+            "typing",
             "numpy",
+            "torch",
             "math",
-            "datetime",
             "os",
-            "torch",
-            "collections",
-            "sys",
-            "typing"
+            "collections"
         ],
         "modelscope.trainers.audio.kws_utils.det_utils": [
-            "numpy",
+            "json",
             "glob",
+            "numpy",
+            "threading",
             "kaldiio",
-            "json",
-            "os",
-            "torch",
             "matplotlib",
-            "threading"
+            "os",
+            "torch"
         ],
         "modelscope.trainers.audio.kws_utils.file_utils": [
             "re"
         ],
         "modelscope.trainers.audio.kws_utils.model_utils": [
-            "numpy",
-            "yaml",
-            "re",
             "shutil",
             "glob",
+            "numpy",
             "os",
-            "torch"
+            "yaml",
+            "torch",
+            "re"
         ],
         "modelscope.trainers.audio.kws_utils.runtime_utils": [
-            "stat",
-            "re",
-            "codecs",
+            "sys",
             "shutil",
             "json",
+            "stat",
+            "codecs",
             "os",
             "collections",
-            "sys"
+            "re"
         ],
         "modelscope.trainers.audio.separation_trainer": [
-            "torchaudio",
-            "numpy",
-            "csv",
-            "os",
             "speechbrain",
-            "torch",
+            "typing",
             "tqdm",
-            "typing"
+            "numpy",
+            "torchaudio",
+            "os",
+            "csv",
+            "torch"
         ],
         "modelscope.trainers.audio.tts_trainer": [
-            "shutil",
             "zipfile",
-            "tempfile",
+            "shutil",
+            "typing",
             "json",
-            "os",
-            "typing"
+            "tempfile",
+            "os"
         ],
         "modelscope.trainers.base": [
             "abc",
             "time",
             "os",
             "typing"
         ],
         "modelscope.trainers.builder": [],
+        "modelscope.trainers.cli_argument_parser": [
+            "argparse",
+            "dataclasses",
+            "typing"
+        ],
         "modelscope.trainers.cv.action_detection_trainer": [
-            "fvcore",
+            "typing",
             "detectron2",
+            "fvcore",
             "os",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.trainers.cv.card_detection_scrfd_trainer": [],
         "modelscope.trainers.cv.cartoon_translation_trainer": [
-            "packaging",
-            "numpy",
             "tensorflow",
-            "os",
+            "typing",
             "tqdm",
-            "typing"
+            "numpy",
+            "packaging",
+            "os"
         ],
         "modelscope.trainers.cv.face_detection_scrfd_trainer": [
-            "time",
             "copy",
+            "time",
             "os",
             "typing"
         ],
         "modelscope.trainers.cv.image_classifition_trainer": [
-            "numpy",
+            "typing",
             "copy",
-            "os",
-            "torch",
             "time",
-            "typing"
+            "numpy",
+            "torch",
+            "os"
         ],
         "modelscope.trainers.cv.image_defrcn_fewshot_detection_trainer": [
+            "typing",
             "detectron2",
             "os",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.trainers.cv.image_detection_damoyolo_trainer": [
-            "math",
-            "easydict",
             "datetime",
-            "os",
-            "torch",
+            "typing",
+            "easydict",
             "time",
-            "typing"
+            "os",
+            "math",
+            "torch"
         ],
         "modelscope.trainers.cv.image_inpainting_trainer": [
+            "time",
             "torch",
-            "collections",
-            "time"
+            "collections"
         ],
         "modelscope.trainers.cv.image_instance_segmentation_trainer": [],
         "modelscope.trainers.cv.image_portrait_enhancement_trainer": [
             "torch",
             "collections"
         ],
         "modelscope.trainers.cv.movie_scene_segmentation_trainer": [],
         "modelscope.trainers.cv.nerf_recon_acc_trainer": [
-            "numpy",
+            "datetime",
             "typing",
             "tqdm",
+            "cv2",
+            "time",
             "glob",
-            "datetime",
-            "os",
+            "numpy",
             "torch",
             "random",
-            "time",
-            "cv2"
+            "os"
         ],
         "modelscope.trainers.cv.ocr_detection_db_trainer": [
+            "datetime",
+            "typing",
             "copy",
-            "numpy",
-            "math",
             "easydict",
-            "datetime",
-            "os",
-            "torch",
             "tqdm",
             "time",
-            "typing"
+            "numpy",
+            "torch",
+            "math",
+            "os"
         ],
         "modelscope.trainers.cv.ocr_recognition_trainer": [
+            "time",
             "torch",
-            "collections",
-            "time"
+            "collections"
         ],
         "modelscope.trainers.cv.referring_video_object_segmentation_trainer": [
             "torch",
             "os"
         ],
         "modelscope.trainers.cv.vision_efficient_tuning_trainer": [
             "torch",
             "typing"
         ],
         "modelscope.trainers.default_config": [
             "typing"
         ],
-        "modelscope.trainers.easycv.trainer": [
-            "easycv",
-            "torch",
-            "copy",
-            "functools",
-            "typing"
-        ],
-        "modelscope.trainers.easycv.utils.hooks": [],
-        "modelscope.trainers.easycv.utils.metric": [
-            "torch",
-            "numpy",
+        "modelscope.trainers.hooks.builder": [],
+        "modelscope.trainers.hooks.checkpoint.checkpoint_hook": [
             "typing",
-            "itertools"
+            "time",
+            "numpy",
+            "torch",
+            "random",
+            "os"
         ],
-        "modelscope.trainers.easycv.utils.register_util": [
-            "inspect",
-            "logging"
+        "modelscope.trainers.hooks.checkpoint.checkpoint_processor": [
+            "shutil",
+            "os",
+            "re"
         ],
-        "modelscope.trainers.hooks.builder": [],
-        "modelscope.trainers.hooks.checkpoint_hook": [
-            "packaging",
+        "modelscope.trainers.hooks.checkpoint.load_checkpoint_hook": [
+            "typing",
             "numpy",
-            "re",
-            "os",
-            "torch",
+            "packaging",
             "random",
-            "time"
+            "torch"
         ],
         "modelscope.trainers.hooks.clip_clamp_logit_scale_hook": [
             "torch"
         ],
         "modelscope.trainers.hooks.compression.sparsity_hook": [
             "os"
         ],
         "modelscope.trainers.hooks.compression.utils": [
             "torch"
         ],
-        "modelscope.trainers.hooks.ddp_hook": [],
-        "modelscope.trainers.hooks.deepspeed_hook": [
-            "megatron_util",
+        "modelscope.trainers.hooks.distributed.ddp_hook": [],
+        "modelscope.trainers.hooks.distributed.deepspeed_hook": [
             "shutil",
+            "megatron_util",
             "os",
-            "deepspeed",
-            "torch"
+            "torch",
+            "deepspeed"
+        ],
+        "modelscope.trainers.hooks.distributed.megatron_hook": [
+            "torch",
+            "shutil",
+            "megatron_util",
+            "os"
         ],
         "modelscope.trainers.hooks.early_stop_hook": [
             "numpy"
         ],
         "modelscope.trainers.hooks.evaluation_hook": [
-            "collections"
+            "collections",
+            "typing"
         ],
         "modelscope.trainers.hooks.hook": [
             "functools"
         ],
         "modelscope.trainers.hooks.iter_timer_hook": [
             "time"
         ],
         "modelscope.trainers.hooks.logger.base": [
+            "numbers",
             "abc",
-            "torch",
             "numpy",
-            "numbers"
+            "torch"
         ],
         "modelscope.trainers.hooks.logger.tensorboard_hook": [
-            "torch",
             "numpy",
+            "torch",
             "os"
         ],
         "modelscope.trainers.hooks.logger.text_logger_hook": [
-            "json",
             "datetime",
+            "json",
             "os",
             "torch",
             "collections"
         ],
         "modelscope.trainers.hooks.lr_scheduler_hook": [],
-        "modelscope.trainers.hooks.megatron_hook": [
-            "torch",
-            "megatron_util",
-            "copy",
-            "os"
-        ],
         "modelscope.trainers.hooks.optimizer.apex_optimizer_hook": [
+            "logging",
             "packaging",
-            "torch",
-            "logging"
+            "torch"
         ],
         "modelscope.trainers.hooks.optimizer.base": [
-            "torch",
-            "logging"
+            "logging",
+            "torch"
         ],
         "modelscope.trainers.hooks.optimizer.torch_optimizer_hook": [
             "logging"
         ],
         "modelscope.trainers.hooks.priority": [
             "enum",
             "typing"
         ],
         "modelscope.trainers.lrscheduler.builder": [
+            "inspect",
             "packaging",
-            "torch",
-            "inspect"
+            "torch"
         ],
         "modelscope.trainers.lrscheduler.warmup.base": [
             "torch"
         ],
         "modelscope.trainers.lrscheduler.warmup.warmup": [],
         "modelscope.trainers.multi_modal.clip.clip_trainer": [
             "math",
             "torch",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.trainers.multi_modal.clip.clip_trainer_utils": [
+            "functools",
+            "os",
             "math",
             "inspect",
-            "os",
-            "torch",
-            "functools"
+            "torch"
         ],
         "modelscope.trainers.multi_modal.efficient_diffusion_tuning.efficient_diffusion_tuning_trainer": [
             "torch",
             "typing"
         ],
         "modelscope.trainers.multi_modal.mgeo_ranking_trainer": [
-            "torch",
             "dataclasses",
+            "torch",
             "typing"
         ],
         "modelscope.trainers.multi_modal.mplug.mplug_trainer": [
             "torch",
             "collections",
             "typing"
         ],
         "modelscope.trainers.multi_modal.ofa.ofa_trainer": [
-            "math",
+            "functools",
             "shutil",
-            "tempfile",
+            "typing",
             "json",
             "os",
-            "torch",
-            "typing",
-            "functools"
+            "math",
+            "tempfile",
+            "torch"
         ],
         "modelscope.trainers.multi_modal.ofa.ofa_trainer_utils": [
             "numpy",
-            "transformers",
-            "os",
-            "math",
+            "shutil",
             "torch",
-            "shutil"
+            "math",
+            "transformers",
+            "os"
         ],
         "modelscope.trainers.multi_modal.team.team_trainer": [
-            "numpy",
+            "typing",
             "sklearn",
+            "numpy",
             "os",
             "torch",
-            "collections",
-            "typing"
+            "collections"
         ],
         "modelscope.trainers.multi_modal.team.team_trainer_utils": [
-            "torch",
             "torchvision",
+            "torch",
             "PIL"
         ],
         "modelscope.trainers.nlp.csanmt_translation_trainer": [
             "tensorflow",
             "time",
             "os",
             "typing"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_generate_trainer": [
-            "string",
-            "re",
+            "rouge",
             "json",
+            "tqdm",
+            "string",
             "transformers",
-            "os",
             "torch",
             "sacrebleu",
-            "tqdm",
-            "rouge",
-            "collections"
+            "os",
+            "collections",
+            "re"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_rerank_trainer": [
-            "numpy",
+            "typing",
             "transformers",
+            "time",
+            "numpy",
             "os",
-            "torch",
             "random",
-            "time",
-            "typing"
+            "torch"
         ],
         "modelscope.trainers.nlp.document_grounded_dialog_retrieval_trainer": [
-            "numpy",
-            "faiss",
             "json",
+            "tqdm",
             "transformers",
+            "faiss",
+            "numpy",
             "os",
-            "torch",
-            "tqdm"
+            "torch"
         ],
         "modelscope.trainers.nlp.faq_question_answering_trainer": [
-            "numpy",
+            "functools",
             "distutils",
+            "typing",
             "contextlib",
             "dataclasses",
+            "numpy",
             "torch",
-            "collections",
-            "typing",
-            "functools"
+            "collections"
         ],
         "modelscope.trainers.nlp.gpt3_trainer": [
-            "torch",
             "copy",
+            "torch",
             "os",
             "typing"
         ],
         "modelscope.trainers.nlp.gpt_moe_trainer": [
-            "megatron_util",
-            "os",
+            "typing",
             "torch",
-            "collections",
-            "typing"
+            "os",
+            "megatron_util",
+            "collections"
         ],
         "modelscope.trainers.nlp.plug_trainer": [
-            "megatron_util",
-            "os",
-            "deepspeed",
+            "typing",
             "torch",
-            "typing"
+            "os",
+            "megatron_util",
+            "deepspeed"
         ],
         "modelscope.trainers.nlp.sentence_embedding_trainer": [
-            "numpy",
-            "dataclasses",
-            "transformers",
-            "torch",
+            "typing",
             "tqdm",
+            "transformers",
+            "dataclasses",
             "time",
-            "typing"
+            "numpy",
+            "torch"
         ],
         "modelscope.trainers.nlp.sequence_classification_trainer": [
-            "numpy",
             "time",
+            "numpy",
             "typing"
         ],
         "modelscope.trainers.nlp.siamese_uie_trainer": [
+            "typing",
+            "json",
+            "time",
             "numpy",
+            "os",
             "math",
-            "json",
             "random",
-            "os",
             "torch",
-            "collections",
-            "time",
-            "typing"
+            "collections"
         ],
         "modelscope.trainers.nlp.space.dialog_intent_trainer": [
             "numpy",
-            "typing",
-            "os"
+            "os",
+            "typing"
         ],
         "modelscope.trainers.nlp.space.dialog_modeling_trainer": [
-            "numpy",
             "time",
+            "numpy",
             "os",
             "typing"
         ],
         "modelscope.trainers.nlp.space.eval": [
-            "numpy",
+            "json",
             "sklearn",
-            "math",
+            "numpy",
             "nltk",
-            "json",
+            "math",
             "collections"
         ],
         "modelscope.trainers.nlp.space.metrics.metrics_tracker": [
             "math",
             "collections"
         ],
         "modelscope.trainers.nlp.space.trainer.gen_trainer": [
-            "numpy",
-            "tqdm",
             "json",
+            "tqdm",
             "transformers",
+            "time",
+            "numpy",
             "os",
             "torch",
-            "collections",
-            "time"
+            "collections"
         ],
         "modelscope.trainers.nlp.space.trainer.intent_trainer": [
-            "numpy",
-            "tqdm",
             "json",
+            "tqdm",
             "transformers",
+            "time",
+            "numpy",
             "os",
             "torch",
-            "collections",
-            "time"
+            "collections"
         ],
         "modelscope.trainers.nlp.table_question_answering_trainer": [
-            "numpy",
             "json",
-            "os",
-            "torch",
+            "typing",
             "tqdm",
             "time",
-            "typing"
+            "numpy",
+            "os",
+            "torch"
         ],
         "modelscope.trainers.nlp.text_generation_trainer": [
             "torch",
             "collections"
         ],
         "modelscope.trainers.nlp.text_ranking_trainer": [
-            "numpy",
-            "dataclasses",
-            "torch",
+            "typing",
             "tqdm",
+            "dataclasses",
             "time",
-            "typing"
+            "numpy",
+            "torch"
+        ],
+        "modelscope.trainers.nlp.translation_evaluation_trainer": [
+            "pandas",
+            "typing",
+            "tqdm",
+            "transformers",
+            "os",
+            "math",
+            "random",
+            "torch"
         ],
         "modelscope.trainers.nlp_trainer": [
-            "torch",
             "numpy",
-            "typing",
-            "os"
+            "torch",
+            "os",
+            "typing"
         ],
         "modelscope.trainers.optimizer.builder": [
-            "torch",
             "inspect",
+            "torch",
             "typing"
         ],
         "modelscope.trainers.optimizer.child_tuning_adamw_optimizer": [
+            "typing",
             "types",
             "numpy",
             "math",
-            "torch",
-            "typing"
+            "torch"
         ],
         "modelscope.trainers.parallel.builder": [
             "torch"
         ],
         "modelscope.trainers.parallel.utils": [],
         "modelscope.trainers.trainer": [
-            "distutils",
+            "functools",
+            "typing",
+            "copy",
             "json",
+            "collections",
+            "torch",
             "inspect",
             "os",
-            "torch",
-            "collections",
-            "copy",
-            "functools",
-            "typing"
+            "distutils"
         ],
         "modelscope.trainers.training_args": [
-            "re",
-            "argparse",
-            "dataclasses",
             "typing",
-            "functools"
+            "copy",
+            "json",
+            "dataclasses",
+            "addict",
+            "re"
         ],
         "modelscope.trainers.utils.inference": [
-            "pickle",
-            "tqdm",
             "shutil",
-            "os",
             "logging",
+            "tqdm",
+            "pickle",
+            "os",
             "torch",
             "collections"
         ],
         "modelscope.trainers.utils.log_buffer": [
             "numpy",
             "collections"
         ]
     },
-    "version": "1.5.2"
+    "version": "1.6.0"
 }
```

### Comparing `modelscope-1.5.2/modelscope/utils/ast_utils.py` & `modelscope-1.6.0/modelscope/utils/ast_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,24 +1,22 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import ast
-import contextlib
 import hashlib
 import os
 import os.path as osp
 import time
 import traceback
 from functools import reduce
 from pathlib import Path
-from typing import Generator, Union
+from typing import Union
 
 import gast
 import json
 
-from modelscope import __version__
 from modelscope.fileio.file import LocalStorage
 from modelscope.metainfo import (CustomDatasets, Heads, Hooks, LR_Schedulers,
                                  Metrics, Models, Optimizers, Pipelines,
                                  Preprocessors, TaskModels, Trainers)
 from modelscope.utils.constant import Fields, Tasks
 from modelscope.utils.file_utils import get_default_cache_dir
 from modelscope.utils.logger import get_logger
@@ -570,14 +568,15 @@
 
 file_scanner = FilesAstScanning()
 
 
 def _save_index(index, file_path, file_list=None, with_template=False):
     # convert tuple key to str key
     index[INDEX_KEY] = {str(k): v for k, v in index[INDEX_KEY].items()}
+    from modelscope.version import __version__
     index[VERSION_KEY] = __version__
     index[MD5_KEY], index[FILES_MTIME_KEY] = file_scanner.files_mtime_md5(
         file_list=file_list)
     index[MODELSCOPE_PATH_KEY] = MODELSCOPE_PATH.as_posix()
     json_index = json.dumps(index)
     if with_template:
         json_index = json_index.replace(MODELSCOPE_PATH.as_posix(),
@@ -678,14 +677,15 @@
     file_path = os.path.join(cache_dir, index_file)
     logger.info(f'Loading ast index from {file_path}')
     index = None
     local_changed = False
     if not force_rebuild and os.path.exists(file_path):
         wrapped_index = _load_index(file_path)
         md5, files_mtime = file_scanner.files_mtime_md5(file_list=file_list)
+        from modelscope.version import __version__
         if (wrapped_index[VERSION_KEY] == __version__):
             index = wrapped_index
             if (wrapped_index[MD5_KEY] != md5):
                 local_changed = True
     full_index_flag = False
 
     if index is None:
```

### Comparing `modelscope-1.5.2/modelscope/utils/audio/audio_utils.py` & `modelscope-1.6.0/modelscope/utils/audio/audio_utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 import tempfile
 from typing import Union
 from urllib.parse import urlparse
 
 import numpy as np
 
 from modelscope.fileio.file import HTTPStorage
+from modelscope.hub.utils.utils import get_cache_dir
 from modelscope.utils.hub import snapshot_download
 from modelscope.utils.logger import get_logger
 
 logger = get_logger()
 
 SEGMENT_LENGTH_TRAIN = 16000
 SUPPORT_AUDIO_TYPE_SETS = ('.flac', '.mp3', '.ogg', '.opus', '.wav', '.pcm')
@@ -319,38 +320,35 @@
             else:
                 raise ValueError("Can't download from {}.".format(url))
         audio_scps.append(audio_scp)
     return audio_scps
 
 
 def update_local_model(model_config, model_path, extra_args):
+    if 'update_model' in extra_args and not extra_args['update_model']:
+        return
+    model_revision = None
     if 'update_model' in extra_args:
         if extra_args['update_model'] == 'latest':
             model_revision = None
         else:
             model_revision = extra_args['update_model']
-        if model_config.__contains__('model'):
-            model_name = model_config['model']
-            if isinstance(model_path, str) and os.path.exists(model_path):
-                try:
-                    logger.info(
-                        'Download the model to local path {0} ...'.format(
-                            model_path))
-                    src_path = snapshot_download(
-                        model_name, revision=model_revision)
-                    # cp to model_path
-                    if src_path == model_path:
-                        logger.warning('src_path is the same with model_path')
-                        return
-                    for filename in os.listdir(src_path):
-                        src_file = os.path.join(src_path, filename)
-                        dst_file = os.path.join(model_path, filename)
-                        if os.path.isfile(src_file):
-                            shutil.copy2(src_file, model_path)
-                        elif os.path.isdir(src_file):
-                            if os.path.exists(dst_file):
-                                shutil.rmtree(dst_file)
-                            shutil.copytree(src_file, dst_file)
-                except Exception as e:
-                    logger.warning(str(e))
-        else:
-            logger.warning('Can not find model name in configuration')
+    if model_config.__contains__('model'):
+        model_name = model_config['model']
+        dst_dir_root = get_cache_dir()
+        if isinstance(model_path, str) and os.path.exists(
+                model_path) and not model_path.startswith(dst_dir_root):
+            try:
+                dst = os.path.join(dst_dir_root, '.cache/' + model_name)
+                dst_dir = os.path.dirname(dst)
+                os.makedirs(dst_dir, exist_ok=True)
+                if not os.path.exists(dst):
+                    os.symlink(os.path.abspath(model_path), dst)
+
+                snapshot_download(
+                    model_name,
+                    cache_dir=dst_dir_root,
+                    revision=model_revision)
+            except Exception as e:
+                logger.warning(str(e))
+    else:
+        logger.warning('Can not find model name in configuration')
```

### Comparing `modelscope-1.5.2/modelscope/utils/audio/tts_exceptions.py` & `modelscope-1.6.0/modelscope/utils/audio/tts_exceptions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/checkpoint.py` & `modelscope-1.6.0/modelscope/utils/checkpoint.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,25 +1,23 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import io
 import os
 import re
 import time
 from collections import OrderedDict
-from functools import partial
 from shutil import copytree, ignore_patterns, rmtree
 from typing import Callable, Dict, Optional, Union
 
 import json
 import torch
 from torch import nn
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler
 
-from modelscope import __version__
 from modelscope.fileio import File, LocalStorage
 from modelscope.utils.config import Config, JSONIteratorEncoder
 from modelscope.utils.constant import ConfigFields, ModelFile
 from modelscope.utils.logger import get_logger
 from modelscope.utils.torch_utils import is_master
 
 logger = get_logger()
@@ -72,14 +70,15 @@
 
     if with_meta:
         if meta is None:
             meta = {}
         elif not isinstance(meta, dict):
             raise TypeError(
                 f'meta must be a dict or None, but got {type(meta)}')
+        from modelscope import __version__
         meta.update(modelscope=__version__, time=time.asctime())
 
         if isinstance(model, torch.nn.parallel.DistributedDataParallel):
             model = model.module
 
         if hasattr(model, 'CLASSES') and model.CLASSES is not None:
             # save class name to the meta
```

### Comparing `modelscope-1.5.2/modelscope/utils/chinese_utils.py` & `modelscope-1.6.0/modelscope/utils/chinese_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/config.py` & `modelscope-1.6.0/modelscope/utils/config.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/config_ds.py` & `modelscope-1.6.0/modelscope/utils/config_ds.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/constant.py` & `modelscope-1.6.0/modelscope/utils/constant.py`

 * *Files 2% similar despite different names*

```diff
@@ -243,14 +243,15 @@
     image_text_retrieval = 'image-text-retrieval'
     document_vl_embedding = 'document-vl-embedding'
     video_captioning = 'video-captioning'
     video_question_answering = 'video-question-answering'
     video_temporal_grounding = 'video-temporal-grounding'
     text_to_video_synthesis = 'text-to-video-synthesis'
     efficient_diffusion_tuning = 'efficient-diffusion-tuning'
+    multimodal_dialogue = 'multimodal-dialogue'
 
 
 class ScienceTasks(object):
     protein_structure = 'protein-structure'
 
 
 class TasksIODescriptions(object):
@@ -273,14 +274,15 @@
 class Tasks(CVTasks, NLPTasks, AudioTasks, MultiModalTasks, ScienceTasks):
     """ Names for tasks supported by modelscope.
 
     Holds the standard task name to use for identifying different tasks.
     This should be used to register models, pipelines, trainers.
     """
     reverse_field_index = {}
+    task_template = 'task-template'
 
     @staticmethod
     def find_field_by_task(task_name):
         if len(Tasks.reverse_field_index) == 0:
             # Lazy init, not thread safe
             field_dict = {
                 Fields.cv: [
@@ -556,7 +558,21 @@
 
     # Columns for meta content
     col_id = 'id'
     col_meta_info = 'meta_info'
     col_analysis_result = 'analysis_result'
     col_external_info = 'external_info'
     col_cache_file = 'cache_file'
+
+
+DEFAULT_MAXCOMPUTE_ENDPOINT = 'http://service-corp.odps.aliyun-inc.com/api'
+
+
+class MaxComputeEnvs:
+
+    ACCESS_ID = 'ODPS_ACCESS_ID'
+
+    ACCESS_SECRET_KEY = 'ODPS_ACCESS_SECRET_KEY'
+
+    PROJECT_NAME = 'ODPS_PROJECT_NAME'
+
+    ENDPOINT = 'ODPS_ENDPOINT'
```

### Comparing `modelscope-1.5.2/modelscope/utils/cv/image_utils.py` & `modelscope-1.6.0/modelscope/utils/cv/image_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/cv/motion_utils/motion_process.py` & `modelscope-1.6.0/modelscope/utils/cv/motion_utils/motion_process.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/cv/motion_utils/plot_script.py` & `modelscope-1.6.0/modelscope/utils/cv/motion_utils/plot_script.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/cv/motion_utils/rotation_conversions.py` & `modelscope-1.6.0/modelscope/utils/cv/motion_utils/rotation_conversions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/data_collators.py` & `modelscope-1.6.0/modelscope/utils/data_collators.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/data_utils.py` & `modelscope-1.6.0/modelscope/utils/data_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/device.py` & `modelscope-1.6.0/modelscope/utils/device.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/error.py` & `modelscope-1.6.0/modelscope/utils/error.py`

 * *Files 2% similar despite different names*

```diff
@@ -158,7 +158,13 @@
 OPENCLIP_IMPORT_ERROR = """
 {0} requires the fasttext library but it was not found in your environment.
 You can install it with pip on linux or mac:
 `pip install open_clip_torch`
 Or you can checkout the instructions on the
 installation page: https://github.com/mlfoundations/open_clip and follow the ones that match your environment.
 """
+
+# docstyle-ignore
+TAMING_IMPORT_ERROR = """
+{0} requires the timm library but it was not found in your environment. You can install it with pip:
+`pip install taming-transformers-rom1504`
+"""
```

### Comparing `modelscope-1.5.2/modelscope/utils/file_utils.py` & `modelscope-1.6.0/modelscope/utils/file_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/hub.py` & `modelscope-1.6.0/modelscope/utils/hub.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/import_utils.py` & `modelscope-1.6.0/modelscope/utils/import_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -301,14 +301,15 @@
     ('fasttext', (is_package_available('fasttext'), FASTTEXT_IMPORT_ERROR)),
     ('megatron_util', (is_package_available('megatron_util'),
                        MEGATRON_UTIL_IMPORT_ERROR)),
     ('text2sql_lgesql', (is_package_available('text2sql_lgesql'),
                          TEXT2SQL_LGESQL_IMPORT_ERROR)),
     ('mpi4py', (is_package_available('mpi4py'), MPI4PY_IMPORT_ERROR)),
     ('open_clip', (is_package_available('open_clip'), OPENCLIP_IMPORT_ERROR)),
+    ('taming', (is_package_available('taming'), TAMING_IMPORT_ERROR)),
 ])
 
 SYSTEM_PACKAGE = set(['os', 'sys', 'typing'])
 
 
 def requires(obj, requirements):
     if not isinstance(requirements, (list, tuple)):
```

### Comparing `modelscope-1.5.2/modelscope/utils/logger.py` & `modelscope-1.6.0/modelscope/utils/logger.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/megatron_utils.py` & `modelscope-1.6.0/modelscope/utils/megatron_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -92,23 +92,24 @@
     target_num_partitions = int(os.getenv('WORLD_SIZE'))
 
     _check_origin_dir(checkpoint_dir)
     _check_target_num_partitions(target_num_partitions)
     log_master(
         f'origin_num_partitions: {origin_num_partitions}, target_num_partitions: {target_num_partitions}'
     )
-    os.makedirs(target_dir, exist_ok=True)
 
     if origin_num_partitions < target_num_partitions:
+        os.makedirs(target_dir, exist_ok=True)
         state_dict = _split_checkpoint(
             model, checkpoint_dir,
             target_num_partitions // origin_num_partitions)
         _save_converted_checkpoint(state_dict, target_dir)
         log_master('Split checkpoints succeeded.')
     elif origin_num_partitions > target_num_partitions:
+        os.makedirs(target_dir, exist_ok=True)
         state_dict = _merge_checkpoint(
             model, checkpoint_dir,
             origin_num_partitions // target_num_partitions)
         _save_converted_checkpoint(state_dict, target_dir)
         log_master('Merge checkpoints succeeded.')
     else:
         shutil.copytree(checkpoint_dir, target_dir)
```

### Comparing `modelscope-1.5.2/modelscope/utils/metric.py` & `modelscope-1.6.0/modelscope/utils/metric.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/model_tag.py` & `modelscope-1.6.0/modelscope/utils/model_tag.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/distributed.py` & `modelscope-1.6.0/modelscope/utils/nlp/distributed.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/load_checkpoint.py` & `modelscope-1.6.0/modelscope/utils/nlp/load_checkpoint.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/args.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/args.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/clean_dataset.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/clean_dataset.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/criterions.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/criterions.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/db_ops.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/db_ops.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/ontology.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/ontology.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/utils.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space/utils_dst.py` & `modelscope-1.6.0/modelscope/utils/nlp/space/utils_dst.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/space_T_en/utils.py` & `modelscope-1.6.0/modelscope/utils/nlp/space_T_en/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/nlp/utils.py` & `modelscope-1.6.0/modelscope/utils/nlp/utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/plugins.py` & `modelscope-1.6.0/modelscope/utils/plugins.py`

 * *Files 4% similar despite different names*

```diff
@@ -259,20 +259,19 @@
         except Exception as e:
             logger.warning(f'{package_name} not imported: {str(e)}')
             if len(package_name.split('.')) == 1:
                 raise ModuleNotFoundError('Package not installed')
 
 
 def install_module_from_requirements(requirement_path, ):
-    """
+    """ install module from requirements
     Args:
         requirement_path: The path of requirement file
 
-    Returns:
-
+    No returns, raise error if failed
     """
 
     install_list = []
     with open(requirement_path, 'r', encoding='utf-8') as f:
         requirements = f.read().splitlines()
         for req in requirements:
             if req == '':
@@ -288,21 +287,38 @@
         )
         if status_code != 0:
             raise ImportError(
                 f'Failed to install requirements from {requirement_path}')
 
 
 def import_module_from_file(module_name, file_path):
+    """ install module by name with file path
+
+    Args:
+        module_name: the module name need to be import
+        file_path: the related file path that matched with the module name
+
+    Returns: return the module class
+
+    """
     spec = importlib.util.spec_from_file_location(module_name, file_path)
     module = importlib.util.module_from_spec(spec)
     spec.loader.exec_module(module)
     return module
 
 
 def import_module_from_model_dir(model_dir):
+    """ import all the necessary module from a model dir
+
+    Args:
+        model_dir: model file location
+
+     No returns, raise error if failed
+
+    """
     from pathlib import Path
     file_scanner = FilesAstScanning()
     file_scanner.traversal_files(model_dir)
     file_dirs = file_scanner.file_dirs
     requirements = file_scanner.requirement_dirs
 
     # install the requirements firstly
@@ -313,14 +329,22 @@
     sys.path.insert(0, model_dir)
     for file in file_dirs:
         module_name = Path(file).stem
         import_module_from_file(module_name, file)
 
 
 def install_requirements_by_names(plugins: List[str]):
+    """ install the requirements by names
+
+    Args:
+        plugins: name of plugins (pai-easyscv, transformers)
+
+     No returns, raise error if failed
+
+    """
     plugins_manager = PluginsManager()
     uninstalled_plugins = []
     for plugin in plugins:
         plugin_installed, version = plugins_manager.check_plugin_installed(
             plugin)
         if not plugin_installed:
             uninstalled_plugins.append(plugin)
@@ -329,25 +353,34 @@
         raise EnvironmentError(
             f'The required packages {",".join(uninstalled_plugins)} are not installed.',
             f'Please run the command `modelscope plugin install {" ".join(uninstalled_plugins)}` to install them.'
         )
 
 
 def install_requirements_by_files(requirements: List[str]):
+    """ install the requriements by files
+
+    Args:
+        requirements: a list of files including requirements info (requirements.txt)
+
+     No returns, raise error if failed
+
+    """
     for requirement in requirements:
         install_module_from_requirements(requirement)
 
 
 def register_plugins_repo(plugins: List[str]) -> None:
     """ Try to install and import plugins from repo"""
     if plugins is not None:
         install_requirements_by_names(plugins)
         modules = []
         for plugin in plugins:
-            modules.extend(get_modules_from_package(plugin))
+            module_name, module_version, _ = get_modules_from_package(plugin)
+            modules.extend(module_name)
         import_plugins(modules)
 
 
 def register_modelhub_repo(model_dir, allow_remote=False) -> None:
     """ Try to install and import remote model from modelhub"""
     if allow_remote:
         try:
@@ -358,32 +391,33 @@
             pass
 
 
 DEFAULT_INDEX = 'https://pypi.org/simple/'
 
 
 def get_modules_from_package(package):
-    """ to get the modules from a installed package
+    """ to get the modules from an installed package
 
     Args:
         package: The distribution name or package name
 
     Returns:
+        import_names: The modules that in the package distribution
+        import_version: The version of those modules, should be same and identical
+        package_name: The package name, if installed by whl file, the package is unknown, should be passed
 
     """
     from zipfile import ZipFile
     from tempfile import mkdtemp
     from subprocess import check_output, STDOUT
     from glob import glob
     import hashlib
     from urllib.parse import urlparse
     from urllib import request as urllib2
     from pip._internal.utils.packaging import get_requirement
-    req = get_requirement(package)
-    package = req.name
 
     def urlretrieve(url, filename, data=None, auth=None):
         if auth is not None:
             # https://docs.python.org/2.7/howto/urllib2.html#id6
             password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
 
             # Add the username and password.
@@ -587,32 +621,66 @@
                                                   extra_index_url)
             checksum = compute_checksum(target=target, algorithm=algorithm)
             checksum = '='.join([algorithm, checksum])
         result = {'path': whl, 'url': url, 'checksum': checksum}
         return result
 
     def discover_import_names(whl_file):
+        import re
         logger.debug('finding import names')
         zipfile = ZipFile(file=whl_file)
         namelist = zipfile.namelist()
         [top_level_fname
          ] = [x for x in namelist if x.endswith('top_level.txt')]
+        [metadata_fname
+         ] = [x for x in namelist if x.endswith('.dist-info/METADATA')]
         all_names = zipfile.read(top_level_fname).decode(
             'utf-8').strip().splitlines()
+        metadata = zipfile.read(metadata_fname).decode('utf-8')
         public_names = [n for n in all_names if not n.startswith('_')]
-        return public_names
+
+        version_pattern = re.compile(r'^Version: (?P<version>.+)$',
+                                     re.MULTILINE)
+        name_pattern = re.compile(r'^Name: (?P<name>.+)$', re.MULTILINE)
+
+        version_match = version_pattern.search(metadata)
+        name_match = name_pattern.search(metadata)
+
+        module_version = version_match.group('version')
+        module_name = name_match.group('name')
+
+        return public_names, module_version, module_name
 
     tmpdir = mkdtemp()
-    data = get(package, tmpdir=tmpdir)
-    import_names = discover_import_names(data['path'])
+    if package.endswith('.whl'):
+        """if user using .whl file then parse the whl to get the module name"""
+        if not os.path.isfile(package):
+            file_name = os.path.basename(package)
+            file_path = os.path.join(tmpdir, file_name)
+            whl_file, _ = _download_dist(package, file_path, None, None)
+        else:
+            whl_file = package
+    else:
+        """if user using package name then generate whl file and parse the file to get the module name by
+        the discover_import_names method
+        """
+        req = get_requirement(package)
+        package = req.name
+        data = get(package, tmpdir=tmpdir)
+        whl_file = data['path']
+    import_names, import_version, package_name = discover_import_names(
+        whl_file)
     shutil.rmtree(tmpdir)
-    return import_names
+    return import_names, import_version, package_name
 
 
 class PluginsManager(object):
+    """
+    plugins manager class
+    """
 
     def __init__(self,
                  cache_dir=MODELSCOPE_FILE_DIR,
                  plugins_file=PLUGINS_FILENAME):
         cache_dir = os.getenv('MODELSCOPE_CACHE', cache_dir)
         plugins_file = os.getenv('MODELSCOPE_PLUGINS_FILE', plugins_file)
         self._file_path = os.path.join(cache_dir, plugins_file)
@@ -629,20 +697,34 @@
     def check_plugin_installed(package):
         """ Check if the plugin is installed, and if the version is valid
 
         Args:
             package: the package name need to be installed
 
         Returns:
+            if_installed: True if installed
+            version: the version of installed or None if not installed
 
         """
 
         if package.split('.')[-1] == 'whl':
-            return False, ''
+            # install from whl should test package name instead of module name
+            _, module_version, package_name = get_modules_from_package(package)
+            local_installed, version = PluginsManager._check_plugin_installed(
+                package_name)
+            if local_installed and module_version != version:
+                return False, version
+            elif not local_installed:
+                return False, version
+            return True, module_version
+        else:
+            return PluginsManager._check_plugin_installed(package)
 
+    @staticmethod
+    def _check_plugin_installed(package, verified_version=None):
         from pip._internal.utils.packaging import get_requirement, specifiers
         req = get_requirement(package)
 
         try:
             importlib.reload(pkg_resources)
             package_meta_info = pkg_resources.working_set.by_key[req.name]
             version = package_meta_info.version
@@ -652,33 +734,40 @@
 
             # If installed, test if the version is correct
             for spec in req.specifier:
                 installed_valid_version = spec.contains(version)
                 if not installed_valid_version:
                     installed = False
                     break
+
         except KeyError:
             version = ''
             installed = False
 
-        return installed, version
+        if installed and verified_version is not None and verified_version != version:
+            return False, verified_version
+        else:
+            return installed, version
 
     @staticmethod
     def pip_command(
         command,
         command_args: List[str],
     ):
         """
 
         Args:
             command: install, uninstall command
             command_args: the args to be used with command, should be in list
               such as ['-r', 'requirements']
 
         Returns:
+            status_code: The pip command status code, 0 if success, else is failed
+            options: parsed option from system args by pip command
+            args: the unknown args that could be parsed by pip command
 
         """
         from pip._internal.commands import create_command
         importlib.reload(pkg_resources)
         if command == 'install':
             command_args.append('-f')
             command_args.append(
@@ -698,14 +787,15 @@
                         install_args: List[str],
                         index_url: Optional[str] = None,
                         force_update=False) -> Any:
         """Install packages via pip
             Args:
             install_args (list): List of arguments passed to `pip install`.
             index_url (str, optional): The pypi index url.
+            force_update: If force update on or off
         """
 
         if len(install_args) == 0:
             return 0, []
 
         if index_url is not None:
             install_args += ['-i', index_url]
@@ -726,14 +816,24 @@
             # Add the plugins info to the local record
             installed_package = self.parse_args_info(args, options)
             self.update_plugins_file(installed_package)
 
         return status_code, install_args
 
     def parse_args_info(self, args: List[str], options):
+        """
+        parse arguments input info
+        Args:
+            args: the list of args from pip command output
+            options: the options that parsed from system args by pip command method
+
+        Returns:
+            installed_package: generate installed package info in order to store in the file
+                                the info includes: name, url and desc of the package
+        """
         installed_package = []
 
         # the case of install with requirements
         if len(args) == 0:
             src_dir = options.src_dir
             requirements = options.requirments
             for requirement in requirements:
@@ -777,14 +877,23 @@
             installed_package.append(package_info)
 
         return installed_package
 
     def uninstall_plugins(self,
                           uninstall_args: Union[str, List],
                           is_yes=False):
+        """
+        uninstall plugins
+        Args:
+            uninstall_args: args used to uninstall by pip command
+            is_yes: force yes without verified
+
+        Returns: status code, and uninstall args
+
+        """
         if is_yes is not None:
             uninstall_args += ['-y']
 
         status_code, options, args = PluginsManager.pip_command(
             'uninstall',
             uninstall_args,
         )
@@ -858,14 +967,15 @@
     ):
         """
 
         Args:
             show_all: show installed and official supported if True, else only those installed
 
         Returns:
+            local_plugins_info: show the list of plugins info
 
         """
         local_plugins_info = self._get_plugins_from_file()
 
         # update plugins with default
 
         local_official_plugins = copy.deepcopy(OFFICIAL_PLUGINS)
@@ -897,14 +1007,15 @@
 
         Args:
             plugins_list: The plugins list contain the information of plugins
                 name, version, introduction, install url and the status of delete or update
             override: Override the file by the list if True, else only update.
 
         Returns:
+            local_plugins_info_json: the json version of updated plugins info
 
         """
         local_plugins_info = self._get_plugins_from_file()
 
         # local_plugins_info is empty if first time loading, should add OFFICIAL_PLUGINS information
         if local_plugins_info == {}:
             plugins_list.extend(copy.deepcopy(OFFICIAL_PLUGINS))
@@ -917,20 +1028,20 @@
 
         return local_plugins_info_json
 
     def remove_plugins_from_file(
         self,
         package_names: Union[str, list],
     ):
-        """
-
+        """remove the plugins from file
         Args:
             package_names:  package name
 
         Returns:
+            local_plugins_info_json: the json version of updated plugins info
 
         """
         local_plugins_info = self._get_plugins_from_file()
 
         if type(package_names) is str:
             package_names = list(package_names)
 
@@ -1008,8 +1119,9 @@
                 'running the cmd: {} failed, with message: {}'.format(
                     cmd, result))
         return result
 
 
 if __name__ == '__main__':
     install_requirements_by_files(['adaseq'])
-    import_name = get_modules_from_package('pai-easycv')
+    import_name, import_version, package_name = get_modules_from_package(
+        'pai-easycv')
```

### Comparing `modelscope-1.5.2/modelscope/utils/registry.py` & `modelscope-1.6.0/modelscope/utils/registry.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/regress_test_utils.py` & `modelscope-1.6.0/modelscope/utils/regress_test_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -479,17 +479,17 @@
             numpify_tensor_nested(t, reduction, clip_value) for t in tensors)
     if isinstance(tensors, torch.Tensor):
         t: np.ndarray = tensors.cpu().numpy()
         if clip_value is not None:
             t = np.where(t > clip_value, clip_value, t)
             t = np.where(t < -clip_value, -clip_value, t)
         if reduction == 'sum':
-            return t.sum(dtype=np.float)
+            return t.sum(dtype=float)
         elif reduction == 'mean':
-            return t.mean(dtype=np.float)
+            return t.mean(dtype=float)
         return t
     return tensors
 
 
 def detach_tensor_nested(tensors):
     try:
         from modelscope.outputs import ModelOutputBase
```

### Comparing `modelscope-1.5.2/modelscope/utils/service_utils.py` & `modelscope-1.6.0/modelscope/utils/service_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -152,21 +152,24 @@
         return decode_base64_to_image(input_data)
     elif input_type == InputType.AUDIO:
         return decode_base64_to_binary(input_data)[0]
     elif input_type == InputType.TEXT:
         return input_data
     elif isinstance(input_type, dict):
         input_data = {}
+        data = json.loads(data)
         for key, val in input_type.items():
             if val == InputType.IMAGE:
                 input_data[key] = decode_base64_to_image(data[key])
             elif val == InputType.AUDIO:
                 input_data[key] = decode_base64_to_binary(data[key])[0]
             elif val == InputType.TEXT:
                 input_data[key] = data[key]
+            else:
+                return data
 
     return input_data
 
 
 def service_data_encoder(task, data):
     if CustomEncoder.get(task) is not None:
         return CustomEncoder[task](data)
```

### Comparing `modelscope-1.5.2/modelscope/utils/task_utils.py` & `modelscope-1.6.0/modelscope/utils/task_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/tensor_utils.py` & `modelscope-1.6.0/modelscope/utils/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/test_utils.py` & `modelscope-1.6.0/modelscope/utils/test_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -146,15 +146,15 @@
             print(
                 f'{print_content}, type not equal:{type1.__name__} and {type2.__name__}'
             )
         return False
 
     if arg1 is None:
         return True
-    elif isinstance(arg1, (int, str, bool, np.bool, np.integer, np.str)):
+    elif isinstance(arg1, (int, str, bool, np.bool_, np.integer, np.str_)):
         if arg1 != arg2:
             if print_content is not None:
                 print(f'{print_content}, arg1:{arg1}, arg2:{arg2}')
             return False
         return True
     elif isinstance(arg1, (float, np.floating)):
         if not np.isclose(arg1, arg2, rtol=rtol, atol=atol, equal_nan=True):
@@ -197,18 +197,16 @@
                 for key in keys1
         ]):
             if print_content is not None:
                 print(f'{print_content}')
             return False
         return True
     elif isinstance(arg1, np.ndarray):
-        arg1 = np.where(np.equal(arg1, None), np.NaN,
-                        arg1).astype(dtype=np.float)
-        arg2 = np.where(np.equal(arg2, None), np.NaN,
-                        arg2).astype(dtype=np.float)
+        arg1 = np.where(np.equal(arg1, None), np.NaN, arg1).astype(dtype=float)
+        arg2 = np.where(np.equal(arg2, None), np.NaN, arg2).astype(dtype=float)
         if not all(
                 np.isclose(arg1, arg2, rtol=rtol, atol=atol,
                            equal_nan=True).flatten()):
             if print_content is not None:
                 print(f'{print_content}')
             return False
         return True
```

### Comparing `modelscope-1.5.2/modelscope/utils/timer.py` & `modelscope-1.6.0/modelscope/utils/timer.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/torch_utils.py` & `modelscope-1.6.0/modelscope/utils/torch_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/trie.py` & `modelscope-1.6.0/modelscope/utils/trie.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/type_assert.py` & `modelscope-1.6.0/modelscope/utils/type_assert.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope/utils/url_utils.py` & `modelscope-1.6.0/modelscope/utils/url_utils.py`

 * *Files identical despite different names*

### Comparing `modelscope-1.5.2/modelscope.egg-info/PKG-INFO` & `modelscope-1.6.0/PKG-INFO`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,307 @@
 Metadata-Version: 2.1
 Name: modelscope
-Version: 1.5.2
-Summary: UNKNOWN
+Version: 1.6.0
+Summary: ModelScope: bring the notion of Model-as-a-Service to life.
 Home-page: https://github.com/modelscope/modelscope
-Author: Alibaba ModelScope team
-Author-email: modelscope@list.alibaba-inc.com
+Author: ModelScope team
+Author-email: contact@modelscope.cn
 License: Apache License 2.0
+Description: 
+        <p align="center">
+            <br>
+            <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
+            <br>
+        <p>
+        
+        <div align="center">
+        
+        [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
+        <!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
+        [![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
+        [![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
+        [![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
+        [![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
+        [![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
+        
+        <!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
+        <!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
+        
+        <h4 align="center">
+            <p>
+                <b>English</b> |
+                <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md"></a>
+            <p>
+        </h4>
+        
+        
+        </div>
+        
+        # Introduction
+        
+        [ModelScope]( https://www.modelscope.cn) is built upon the notion of Model-as-a-Service (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
+        
+        
+        In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
+        
+        Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
+        
+        # Models and Online Accessibility
+        
+        Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
+        
+        
+        <p align="center">
+            <br>
+            <img src="data/resource/inference.gif" width="1024"/>
+            <br>
+        <p>
+        
+        Some representative examples include:
+        
+        NLP:
+        
+        * [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
+        
+        * [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
+        
+        * [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
+        
+        * [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
+        
+        * [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
+        
+        * [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
+        
+        * [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
+        
+        * [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
+        
+        Multi-Modal:
+        
+        * [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
+        
+        * [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
+        
+        * [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
+        
+        * [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
+        
+        CV:
+        
+        * [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
+        
+        * [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
+        
+        * [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
+        
+        * [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
+        
+        * [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
+        
+        * [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
+        
+        * [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
+        
+        * [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
+        
+        * [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
+        
+        
+        Audio:
+        
+        * [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
+        
+        * [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
+        
+        * [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
+        
+        * [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
+        
+        * [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)
+        
+        * [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)
+        
+        * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
+        
+        * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
+        
+        
+        
+        AI for Science:
+        
+        * [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
+        
+        * [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+        
+        **Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
+        
+        # QuickTour
+        
+        We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
+        
+        For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
+        
+        ```python
+        >>> from modelscope.pipelines import pipeline
+        >>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
+        >>> word_segmentation('')
+        {'output': '      '}
+        ```
+        
+        Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
+        
+        ![image](data/resource/portrait_input.png)
+        
+        ```python
+        >>> import cv2
+        >>> from modelscope.pipelines import pipeline
+        
+        >>> portrait_matting = pipeline('portrait-matting')
+        >>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
+        >>> cv2.imwrite('result.png', result['output_img'])
+        ```
+        
+        The output image with the background removed is:
+        ![image](data/resource/portrait_output.png)
+        
+        
+        Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
+        `trainer.evaluate()`  interfaces.
+        
+        For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
+        
+        ```python
+        >>> from modelscope.metainfo import Trainers
+        >>> from modelscope.msdatasets import MsDataset
+        >>> from modelscope.trainers import build_trainer
+        
+        >>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
+        >>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
+        >>> max_epochs = 10
+        >>> tmp_dir = './gpt3_poetry'
+        
+        >>> kwargs = dict(
+             model='damo/nlp_gpt3_text-generation_1.3B',
+             train_dataset=train_dataset,
+             eval_dataset=eval_dataset,
+             max_epochs=max_epochs,
+             work_dir=tmp_dir)
+        
+        >>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
+        >>> trainer.train()
+        ```
+        
+        # Why should I use ModelScope library
+        
+        1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
+        
+        2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
+        
+        3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
+        
+        4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
+        
+        # Installation
+        
+        ## Docker
+        
+        ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
+        
+        To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
+        
+        CPU docker image
+        ```shell
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0
+        ```
+        
+        GPU docker image
+        ```shell
+        registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0
+        ```
+        
+        ## Setup Local Python Environment
+        
+        One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
+        
+        ```shell
+        conda create -n modelscope python=3.7
+        conda activate modelscope
+        ```
+        
+        PyTorch or TensorFlow can be installed separately according to each model's requirements.
+        * Install pytorch [doc](https://pytorch.org/get-started/locally/)
+        * Install tensorflow [doc](https://www.tensorflow.org/install/pip)
+        
+        After installing the necessary machine-learning framework, you can install modelscope library as follows:
+        
+        If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
+        ```shell
+        pip install modelscope
+        ```
+        
+        If you want to use multi-modal models:
+        ```shell
+        pip install modelscope[multi-modal]
+        ```
+        
+        If you want to use nlp models:
+        ```shell
+        pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use cv models:
+        ```shell
+        pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use audio models:
+        ```shell
+        pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        If you want to use science models:
+        ```shell
+        pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
+        ```
+        
+        `Notes`:
+        1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
+        
+        2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
+            ```shell
+            sudo apt-get update
+            sudo apt-get install libsndfile1
+            ```
+        
+        3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
+        
+            ```shell
+            pip uninstall mmcv # if you have installed mmcv, uninstall it
+            pip install -U openmim
+            mim install mmcv-full
+            ```
+        
+        
+        
+        # Learn More
+        
+        We  provide additional documentations including:
+        * [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
+        * [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
+        * [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
+        * [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
+        * [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
+        * [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
+        * [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
+        
+        # License
+        
+        This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+        
 Keywords: python,nlp,science,cv,speech,multi-modal
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
@@ -23,297 +315,7 @@
 Provides-Extra: nlp
 Provides-Extra: science
 Provides-Extra: audio_asr
 Provides-Extra: audio_kws
 Provides-Extra: audio_signal
 Provides-Extra: audio_tts
 Provides-Extra: all
-
-
-<p align="center">
-    <br>
-    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
-    <br>
-<p>
-
-<div align="center">
-
-[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)
-<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->
-[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
-[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)
-[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)
-[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)
-[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)
-
-<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->
-<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->
-
-<h4 align="center">
-    <p>
-        <b>English</b> |
-        <a href="https://github.com/modelscope/modelscope/blob/master/README_zh.md"></a>
-    <p>
-</h4>
-
-
-</div>
-
-# Introduction
-
-[ModelScope]( https://www.modelscope.cn) is built upon the notion of Model-as-a-Service (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.
-
-
-In particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.
-
-Apart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.
-
-# Models and Online Accessibility
-
-Hundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).
-
-
-<p align="center">
-    <br>
-    <img src="data/resource/inference.gif" width="1024"/>
-    <br>
-<p>
-
-Some representative examples include:
-
-NLP:
-
-* [nlp_gpt3_text-generation_2.7B](https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B)
-
-* [ChatYuan-large](https://modelscope.cn/models/ClueAI/ChatYuan-large)
-
-* [mengzi-t5-base](https://modelscope.cn/models/langboat/mengzi-t5-base)
-
-* [nlp_csanmt_translation_en2zh](https://modelscope.cn/models/damo/nlp_csanmt_translation_en2zh)
-
-* [nlp_raner_named-entity-recognition_chinese-base-news](https://modelscope.cn/models/damo/nlp_raner_named-entity-recognition_chinese-base-news)
-
-* [nlp_structbert_word-segmentation_chinese-base](https://modelscope.cn/models/damo/nlp_structbert_word-segmentation_chinese-base)
-
-* [Erlangshen-RoBERTa-330M-Sentiment](https://modelscope.cn/models/fengshenbang/Erlangshen-RoBERTa-330M-Sentiment)
-
-* [nlp_convai_text2sql_pretrain_cn](https://modelscope.cn/models/damo/nlp_convai_text2sql_pretrain_cn)
-
-Multi-Modal:
-
-* [multi-modal_clip-vit-base-patch16_zh](https://modelscope.cn/models/damo/multi-modal_clip-vit-base-patch16_zh)
-
-* [ofa_pretrain_base_zh](https://modelscope.cn/models/damo/ofa_pretrain_base_zh)
-
-* [Taiyi-Stable-Diffusion-1B-Chinese-v0.1](https://modelscope.cn/models/fengshenbang/Taiyi-Stable-Diffusion-1B-Chinese-v0.1)
-
-* [mplug_visual-question-answering_coco_large_en](https://modelscope.cn/models/damo/mplug_visual-question-answering_coco_large_en)
-
-CV:
-
-* [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)
-
-* [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)
-
-* [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)
-
-* [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)
-
-* [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)
-
-* [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)
-
-* [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)
-
-* [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)
-
-* [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)
-
-
-Audio:
-
-* [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)
-
-* [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)
-
-* [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)
-
-* [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)
-
-* [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)
-
-* [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)
-
-
-
-AI for Science:
-
-* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)
-
-* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
-
-**Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.
-
-# QuickTour
-
-We provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.
-
-For any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:
-
-```python
->>> from modelscope.pipelines import pipeline
->>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')
->>> word_segmentation('')
-{'output': '      '}
-```
-
-Given an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:
-
-![image](data/resource/portrait_input.png)
-
-```python
->>> import cv2
->>> from modelscope.pipelines import pipeline
-
->>> portrait_matting = pipeline('portrait-matting')
->>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')
->>> cv2.imwrite('result.png', result['output_img'])
-```
-
-The output image with the background removed is:
-![image](data/resource/portrait_output.png)
-
-
-Fine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and
-`trainer.evaluate()`  interfaces.
-
-For example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.
-
-```python
->>> from modelscope.metainfo import Trainers
->>> from modelscope.msdatasets import MsDataset
->>> from modelscope.trainers import build_trainer
-
->>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})
->>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})
->>> max_epochs = 10
->>> tmp_dir = './gpt3_poetry'
-
->>> kwargs = dict(
-     model='damo/nlp_gpt3_text-generation_1.3B',
-     train_dataset=train_dataset,
-     eval_dataset=eval_dataset,
-     max_epochs=max_epochs,
-     work_dir=tmp_dir)
-
->>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)
->>> trainer.train()
-```
-
-# Why should I use ModelScope library
-
-1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.
-
-2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.
-
-3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.
-
-4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.
-
-# Installation
-
-## Docker
-
-ModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.
-
-To allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:
-
-CPU docker image
-```shell
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0
-```
-
-GPU docker image
-```shell
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0
-```
-
-## Setup Local Python Environment
-
-One can also set up local ModelScope environment using pip and conda.  We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:
-
-```shell
-conda create -n modelscope python=3.7
-conda activate modelscope
-```
-
-PyTorch or TensorFlow can be installed separately according to each model's requirements.
-* Install pytorch [doc](https://pytorch.org/get-started/locally/)
-* Install tensorflow [doc](https://www.tensorflow.org/install/pip)
-
-After installing the necessary machine-learning framework, you can install modelscope library as follows:
-
-If you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:
-```shell
-pip install modelscope
-```
-
-If you want to use multi-modal models:
-```shell
-pip install modelscope[multi-modal]
-```
-
-If you want to use nlp models:
-```shell
-pip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use cv models:
-```shell
-pip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use audio models:
-```shell
-pip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-If you want to use science models:
-```shell
-pip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html
-```
-
-`Notes`:
-1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).
-
-2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:
-    ```shell
-    sudo apt-get update
-    sudo apt-get install libsndfile1
-    ```
-
-3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:
-
-    ```shell
-    pip uninstall mmcv # if you have installed mmcv, uninstall it
-    pip install -U openmim
-    mim install mmcv-full
-    ```
-
-
-
-# Learn More
-
-We  provide additional documentations including:
-* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)
-* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)
-* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)
-* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
-* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)
-* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)
-* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
-
-# License
-
-This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
-
-
```

#### html2text {}

```diff
@@ -1,20 +1,11 @@
-Metadata-Version: 2.1 Name: modelscope Version: 1.5.2 Summary: UNKNOWN Home-
-page: https://github.com/modelscope/modelscope Author: Alibaba ModelScope team
-Author-email: modelscope@list.alibaba-inc.com License: Apache License 2.0
-Keywords: python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta Classifier: License :: OSI Approved
-:: Apache Software License Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3 Classifier: Programming
-Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
-Language :: Python :: 3.10 Description-Content-Type: text/markdown Provides-
-Extra: audio Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp
-Provides-Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws
-Provides-Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
+Metadata-Version: 2.1 Name: modelscope Version: 1.6.0 Summary: ModelScope:
+bring the notion of Model-as-a-Service to life. Home-page: https://github.com/
+modelscope/modelscope Author: ModelScope team Author-email:
+contact@modelscope.cn License: Apache License 2.0 Description:
 
        [https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif]
  [![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/
   modelscope/)  [![license](https://img.shields.io/github/license/modelscope/
 modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)
        [![open issues](https://isitmaintained.com/badge/open/modelscope/
   modelscope.svg)](https://github.com/modelscope/modelscope/issues) [![GitHub
@@ -95,107 +86,110 @@
 cv_resnest101_general_recognition) Audio: * [speech_paraformer-large_asr_nat-
 zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/
 speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch) *
 [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/
 speech_sambert-hifigan_tts_zh-cn_16k) * [speech_charctc_kws_phone-xiaoyun]
 (https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun) *
 [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/
-u2pp_conformer-asr-cn-16k-online) * [speech_frcrn_ans_cirm_16k](https://
-modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k) *
-[speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/
-speech_dfsmn_aec_psm_16k) AI for Science: * [uni-fold-monomer](https://
-modelscope.cn/models/DPTech/uni-fold-monomer/summary) * [uni-fold-multimer]
-(https://modelscope.cn/models/DPTech/uni-fold-multimer/summary) **Note:** Most
-models on ModelScope are public and can be downloaded without account
-registration on modelscope website([www.modelscope.cn](www.modelscope.cn)),
-please refer to instructions for [model download](https://modelscope.cn/docs/
-%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api
-provided by modelscope library or git. # QuickTour We provide unified interface
-for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for
-different tasks. For any given task with any type of input (image, text, audio,
-video...), inference pipeline can be implemented with only a few lines of code,
-which will automatically load the underlying model to get inference result, as
-is exemplified below: ```python >>> from modelscope.pipelines import pipeline
->>> word_segmentation = pipeline('word-segmentation',model='damo/
-nlp_structbert_word-segmentation_chinese-base') >>> word_segmentation
-('') {'output': '  
-   '} ``` Given an image, portrait matting (aka.
-background-removal) can be accomplished with the following code snippet: !
-[image](data/resource/portrait_input.png) ```python >>> import cv2 >>> from
-modelscope.pipelines import pipeline >>> portrait_matting = pipeline('portrait-
-matting') >>> result = portrait_matting('https://modelscope.oss-cn-
-beijing.aliyuncs.com/test/images/image_matting.png') >>> cv2.imwrite
-('result.png', result['output_img']) ``` The output image with the background
-removed is: ![image](data/resource/portrait_output.png) Fine-tuning and
-evaluation can also be done with a few more lines of code to set up training
-dataset and trainer, with the heavy-lifting work of training and evaluation a
-model encapsulated in the implementation of `traner.train()` and
-`trainer.evaluate()` interfaces. For example, the gpt3 base model (1.3B) can be
-fine-tuned with the chinese-poetry dataset, resulting in a model that can be
-used for chinese-poetry generation. ```python >>> from modelscope.metainfo
-import Trainers >>> from modelscope.msdatasets import MsDataset >>> from
-modelscope.trainers import build_trainer >>> train_dataset = MsDataset.load
-('chinese-poetry-collection', split='train'). remap_columns({'text1':
-'src_txt'}) >>> eval_dataset = MsDataset.load('chinese-poetry-collection',
-split='test').remap_columns({'text1': 'src_txt'}) >>> max_epochs = 10 >>>
-tmp_dir = './gpt3_poetry' >>> kwargs = dict( model='damo/nlp_gpt3_text-
-generation_1.3B', train_dataset=train_dataset, eval_dataset=eval_dataset,
-max_epochs=max_epochs, work_dir=tmp_dir) >>> trainer = build_trainer
-(name=Trainers.gpt3_trainer, default_args=kwargs) >>> trainer.train() ``` # Why
-should I use ModelScope library 1. A unified and concise user interface is
-abstracted for different tasks and different models. Model inferences and
-training can be implemented by as few as 3 and 10 lines of code, respectively.
-It is convenient for users to explore models in different fields in the
-ModelScope community. All models integrated into ModelScope are ready to use,
-which makes it easy to get started with AI, in both educational and industrial
-settings. 2. ModelScope offers a model-centric development and application
-experience. It streamlines the support for model training, inference, export
-and deployment, and facilitates users to build their own MLOps based on the
-ModelScope ecosystem. 3. For the model inference and training process, a
-modular design is put in place, and a wealth of functional module
-implementations are provided, which is convenient for users to customize their
-own model inference, training and other processes. 4. For distributed model
-training, especially for large models, it provides rich training strategy
-support, including data parallel, model parallel, hybrid parallel and so on. #
-Installation ## Docker ModelScope Library currently supports popular deep
-learning framework for model training and inference, including PyTorch,
-TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch
-1.8+, Tensorflow1.15 or Tensorflow2.0+. To allow out-of-box usage for all the
-models on ModelScope, official docker images are provided for all releases.
-Based on the docker image, developers can skip all environment installation and
-configuration and use it directly. Currently, the latest version of the CPU
-image and GPU image can be obtained from: CPU docker image ```shell
-registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-
-torch1.11.0-tf1.15.5-1.3.0 ``` GPU docker image ```shell registry.cn-
-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-
-torch1.11.0-tf1.15.5-1.3.0 ``` ## Setup Local Python Environment One can also
-set up local ModelScope environment using pip and conda. We suggest [anaconda]
-(https://docs.anaconda.com/anaconda/install/) for creating local python
-environment: ```shell conda create -n modelscope python=3.7 conda activate
-modelscope ``` PyTorch or TensorFlow can be installed separately according to
-each model's requirements. * Install pytorch [doc](https://pytorch.org/get-
-started/locally/) * Install tensorflow [doc](https://www.tensorflow.org/
-install/pip) After installing the necessary machine-learning framework, you can
-install modelscope library as follows: If you only want to play around with the
-modelscope framework, of trying out model/dataset download, you can install the
-core modelscope components: ```shell pip install modelscope ``` If you want to
-use multi-modal models: ```shell pip install modelscope[multi-modal] ``` If you
-want to use nlp models: ```shell pip install modelscope[nlp] -f https://
+u2pp_conformer-asr-cn-16k-online) * [speech_fsmn_vad_zh-cn-16k-common-pytorch]
+(https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/
+summary) * [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://
+modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/
+summary) * [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/
+speech_frcrn_ans_cirm_16k) * [speech_dfsmn_aec_psm_16k](https://modelscope.cn/
+models/damo/speech_dfsmn_aec_psm_16k) AI for Science: * [uni-fold-monomer]
+(https://modelscope.cn/models/DPTech/uni-fold-monomer/summary) * [uni-fold-
+multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)
+**Note:** Most models on ModelScope are public and can be downloaded without
+account registration on modelscope website([www.modelscope.cn]
+(www.modelscope.cn)), please refer to instructions for [model download](https:/
+/modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for
+dowloading models with api provided by modelscope library or git. # QuickTour
+We provide unified interface for inference using `pipeline`, fine-tuning and
+evaluation using `Trainer` for different tasks. For any given task with any
+type of input (image, text, audio, video...), inference pipeline can be
+implemented with only a few lines of code, which will automatically load the
+underlying model to get inference result, as is exemplified below: ```python
+>>> from modelscope.pipelines import pipeline >>> word_segmentation = pipeline
+('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-
+base') >>> word_segmentation('')
+{'output': '      '} ``` Given an image,
+portrait matting (aka. background-removal) can be accomplished with the
+following code snippet: ![image](data/resource/portrait_input.png) ```python
+>>> import cv2 >>> from modelscope.pipelines import pipeline >>>
+portrait_matting = pipeline('portrait-matting') >>> result = portrait_matting
+('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/
+image_matting.png') >>> cv2.imwrite('result.png', result['output_img']) ``` The
+output image with the background removed is: ![image](data/resource/
+portrait_output.png) Fine-tuning and evaluation can also be done with a few
+more lines of code to set up training dataset and trainer, with the heavy-
+lifting work of training and evaluation a model encapsulated in the
+implementation of `traner.train()` and `trainer.evaluate()` interfaces. For
+example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry
+dataset, resulting in a model that can be used for chinese-poetry generation.
+```python >>> from modelscope.metainfo import Trainers >>> from
+modelscope.msdatasets import MsDataset >>> from modelscope.trainers import
+build_trainer >>> train_dataset = MsDataset.load('chinese-poetry-collection',
+split='train'). remap_columns({'text1': 'src_txt'}) >>> eval_dataset =
+MsDataset.load('chinese-poetry-collection', split='test').remap_columns(
+{'text1': 'src_txt'}) >>> max_epochs = 10 >>> tmp_dir = './gpt3_poetry' >>>
+kwargs = dict( model='damo/nlp_gpt3_text-generation_1.3B',
+train_dataset=train_dataset, eval_dataset=eval_dataset, max_epochs=max_epochs,
+work_dir=tmp_dir) >>> trainer = build_trainer(name=Trainers.gpt3_trainer,
+default_args=kwargs) >>> trainer.train() ``` # Why should I use ModelScope
+library 1. A unified and concise user interface is abstracted for different
+tasks and different models. Model inferences and training can be implemented by
+as few as 3 and 10 lines of code, respectively. It is convenient for users to
+explore models in different fields in the ModelScope community. All models
+integrated into ModelScope are ready to use, which makes it easy to get started
+with AI, in both educational and industrial settings. 2. ModelScope offers a
+model-centric development and application experience. It streamlines the
+support for model training, inference, export and deployment, and facilitates
+users to build their own MLOps based on the ModelScope ecosystem. 3. For the
+model inference and training process, a modular design is put in place, and a
+wealth of functional module implementations are provided, which is convenient
+for users to customize their own model inference, training and other processes.
+4. For distributed model training, especially for large models, it provides
+rich training strategy support, including data parallel, model parallel, hybrid
+parallel and so on. # Installation ## Docker ModelScope Library currently
+supports popular deep learning framework for model training and inference,
+including PyTorch, TensorFlow and ONNX. All releases are tested and run on
+Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+. To allow out-of-
+box usage for all the models on ModelScope, official docker images are provided
+for all releases. Based on the docker image, developers can skip all
+environment installation and configuration and use it directly. Currently, the
+latest version of the CPU image and GPU image can be obtained from: CPU docker
+image ```shell registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:
+ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.3.0 ``` GPU docker image ```shell
+registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-
+cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.3.0 ``` ## Setup Local Python
+Environment One can also set up local ModelScope environment using pip and
+conda. We suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for
+creating local python environment: ```shell conda create -n modelscope
+python=3.7 conda activate modelscope ``` PyTorch or TensorFlow can be installed
+separately according to each model's requirements. * Install pytorch [doc]
+(https://pytorch.org/get-started/locally/) * Install tensorflow [doc](https://
+www.tensorflow.org/install/pip) After installing the necessary machine-learning
+framework, you can install modelscope library as follows: If you only want to
+play around with the modelscope framework, of trying out model/dataset
+download, you can install the core modelscope components: ```shell pip install
+modelscope ``` If you want to use multi-modal models: ```shell pip install
+modelscope[multi-modal] ``` If you want to use nlp models: ```shell pip install
+modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/
+repo.html ``` If you want to use cv models: ```shell pip install modelscope[cv]
+-f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html ``` If you
+want to use audio models: ```shell pip install modelscope[audio] -f https://
 modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html ``` If you want to
-use cv models: ```shell pip install modelscope[cv] -f https://modelscope.oss-
-cn-beijing.aliyuncs.com/releases/repo.html ``` If you want to use audio models:
-```shell pip install modelscope[audio] -f https://modelscope.oss-cn-
-beijing.aliyuncs.com/releases/repo.html ``` If you want to use science models:
-```shell pip install modelscope[science] -f https://modelscope.oss-cn-
-beijing.aliyuncs.com/releases/repo.html ``` `Notes`: 1. Currently, some audio-
-task models only support python3.7, tensorflow1.15.4 Linux environments. Most
-other models can be installed and used on Windows and Mac (x86). 2. Some models
-in the audio field use the third-party library SoundFile for wav file
-processing. On the Linux system, users need to manually install libsndfile of
-SoundFile([doc link](https://github.com/bastibe/python-
+use science models: ```shell pip install modelscope[science] -f https://
+modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html ``` `Notes`: 1.
+Currently, some audio-task models only support python3.7, tensorflow1.15.4
+Linux environments. Most other models can be installed and used on Windows and
+Mac (x86). 2. Some models in the audio field use the third-party library
+SoundFile for wav file processing. On the Linux system, users need to manually
+install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-
 soundfile#installation)). On Windows and MacOS, it will be installed
 automatically without user operation. For example, on Ubuntu, you can use
 following commands: ```shell sudo apt-get update sudo apt-get install
 libsndfile1 ``` 3. Some models in computer vision need mmcv-full, you can refer
 to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation),
 a minimal installation is as follows: ```shell pip uninstall mmcv # if you have
 installed mmcv, uninstall it pip install -U openmim mim install mmcv-full ``` #
@@ -208,8 +202,18 @@
 (https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)
 * [Preprocessing of data](https://modelscope.cn/docs/
 %E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86) * [Evaluation](https://
 modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0) * [Contribute
 your own model to ModelScope](https://modelscope.cn/docs/
 ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)
 # License This project is licensed under the [Apache License (Version 2.0)]
-(https://github.com/modelscope/modelscope/blob/master/LICENSE).
+(https://github.com/modelscope/modelscope/blob/master/LICENSE). Keywords:
+python,nlp,science,cv,speech,multi-modal Platform: UNKNOWN Classifier:
+Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
+Software License Classifier: Operating System :: OS Independent Classifier:
+Programming Language :: Python :: 3 Classifier: Programming Language :: Python
+:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
+Programming Language :: Python :: 3.9 Classifier: Programming Language ::
+Python :: 3.10 Description-Content-Type: text/markdown Provides-Extra: audio
+Provides-Extra: cv Provides-Extra: multi-modal Provides-Extra: nlp Provides-
+Extra: science Provides-Extra: audio_asr Provides-Extra: audio_kws Provides-
+Extra: audio_signal Provides-Extra: audio_tts Provides-Extra: all
```

### Comparing `modelscope-1.5.2/modelscope.egg-info/SOURCES.txt` & `modelscope-1.6.0/modelscope.egg-info/SOURCES.txt`

 * *Files 1% similar despite different names*

```diff
@@ -20,14 +20,16 @@
 modelscope/cli/plugins.py
 modelscope/configs/examples/configuration.py
 modelscope/exporters/__init__.py
 modelscope/exporters/base.py
 modelscope/exporters/builder.py
 modelscope/exporters/tf_model_exporter.py
 modelscope/exporters/torch_model_exporter.py
+modelscope/exporters/audio/__init__.py
+modelscope/exporters/audio/ans_dfsmn_exporter.py
 modelscope/exporters/cv/__init__.py
 modelscope/exporters/cv/cartoon_translation_exporter.py
 modelscope/exporters/cv/face_detection_scrfd_exporter.py
 modelscope/exporters/cv/object_detection_damoyolo_exporter.py
 modelscope/exporters/nlp/__init__.py
 modelscope/exporters/nlp/csanmt_for_translation_exporter.py
 modelscope/exporters/nlp/model_for_token_classification_exporter.py
@@ -79,14 +81,15 @@
 modelscope/metrics/ppl_metric.py
 modelscope/metrics/prediction_saving_wrapper.py
 modelscope/metrics/referring_video_object_segmentation_metric.py
 modelscope/metrics/sequence_classification_metric.py
 modelscope/metrics/text_generation_metric.py
 modelscope/metrics/text_ranking_metric.py
 modelscope/metrics/token_classification_metric.py
+modelscope/metrics/translation_evaluation_metric.py
 modelscope/metrics/video_frame_interpolation_metric.py
 modelscope/metrics/video_stabilization_metric.py
 modelscope/metrics/video_summarization_metric.py
 modelscope/metrics/ciderD/__init__.py
 modelscope/metrics/ciderD/ciderD.py
 modelscope/metrics/ciderD/ciderD_scorer.py
 modelscope/metrics/video_super_resolution_metric/__init__.py
@@ -126,14 +129,15 @@
 modelscope/models/audio/itn/__init__.py
 modelscope/models/audio/itn/generic_inverse_text_processing.py
 modelscope/models/audio/kws/__init__.py
 modelscope/models/audio/kws/generic_key_word_spotting.py
 modelscope/models/audio/kws/farfield/__init__.py
 modelscope/models/audio/kws/farfield/fsmn.py
 modelscope/models/audio/kws/farfield/fsmn_sele_v2.py
+modelscope/models/audio/kws/farfield/fsmn_sele_v3.py
 modelscope/models/audio/kws/farfield/model.py
 modelscope/models/audio/kws/farfield/model_def.py
 modelscope/models/audio/kws/nearfield/__init__.py
 modelscope/models/audio/kws/nearfield/cmvn.py
 modelscope/models/audio/kws/nearfield/fsmn.py
 modelscope/models/audio/kws/nearfield/model.py
 modelscope/models/audio/punc/__init__.py
@@ -141,27 +145,31 @@
 modelscope/models/audio/separation/__init__.py
 modelscope/models/audio/separation/layer_norm.py
 modelscope/models/audio/separation/mossformer.py
 modelscope/models/audio/separation/mossformer_block.py
 modelscope/models/audio/separation/mossformer_conv_module.py
 modelscope/models/audio/sv/DTDNN.py
 modelscope/models/audio/sv/DTDNN_layers.py
+modelscope/models/audio/sv/ERes2Net.py
 modelscope/models/audio/sv/__init__.py
 modelscope/models/audio/sv/ecapa_tdnn.py
+modelscope/models/audio/sv/fusion.py
 modelscope/models/audio/sv/generic_speaker_verification.py
+modelscope/models/audio/sv/pooling_layers.py
+modelscope/models/audio/sv/rdino.py
+modelscope/models/audio/sv/speaker_change_locator.py
 modelscope/models/audio/tts/__init__.py
 modelscope/models/audio/tts/sambert_hifi.py
 modelscope/models/audio/tts/voice.py
 modelscope/models/base/__init__.py
 modelscope/models/base/base_head.py
 modelscope/models/base/base_model.py
 modelscope/models/base/base_torch_head.py
 modelscope/models/base/base_torch_model.py
 modelscope/models/cv/__init__.py
-modelscope/models/cv/easycv_base.py
 modelscope/models/cv/abnormal_object_detection/__init__.py
 modelscope/models/cv/abnormal_object_detection/mmdet_model.py
 modelscope/models/cv/abnormal_object_detection/mmdet_ms/__init__.py
 modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/__init__.py
 modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/mask_scoring_roi_head.py
 modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/__init__.py
 modelscope/models/cv/abnormal_object_detection/mmdet_ms/roi_head/roi_extractors/single_level_roi_extractor.py
@@ -237,16 +245,14 @@
 modelscope/models/cv/controllable_image_generation/annotator/openpose/body.py
 modelscope/models/cv/controllable_image_generation/annotator/openpose/hand.py
 modelscope/models/cv/controllable_image_generation/annotator/openpose/model.py
 modelscope/models/cv/controllable_image_generation/annotator/openpose/util.py
 modelscope/models/cv/crowd_counting/__init__.py
 modelscope/models/cv/crowd_counting/cc_model.py
 modelscope/models/cv/crowd_counting/hrnet_aspp_relu.py
-modelscope/models/cv/face_2d_keypoints/__init__.py
-modelscope/models/cv/face_2d_keypoints/face_2d_keypoints_align.py
 modelscope/models/cv/face_attribute_recognition/__init__.py
 modelscope/models/cv/face_attribute_recognition/fair_face/__init__.py
 modelscope/models/cv/face_attribute_recognition/fair_face/face_attribute_recognition.py
 modelscope/models/cv/face_detection/__init__.py
 modelscope/models/cv/face_detection/mogface/__init__.py
 modelscope/models/cv/face_detection/mogface/models/__init__.py
 modelscope/models/cv/face_detection/mogface/models/detectors.py
@@ -372,16 +378,14 @@
 modelscope/models/cv/facial_expression_recognition/fer/facial_expression_recognition.py
 modelscope/models/cv/facial_expression_recognition/fer/transforms.py
 modelscope/models/cv/facial_expression_recognition/fer/vgg.py
 modelscope/models/cv/facial_landmark_confidence/__init__.py
 modelscope/models/cv/facial_landmark_confidence/flc/__init__.py
 modelscope/models/cv/facial_landmark_confidence/flc/facial_landmark_confidence.py
 modelscope/models/cv/facial_landmark_confidence/flc/manual_landmark_net.py
-modelscope/models/cv/hand_2d_keypoints/__init__.py
-modelscope/models/cv/hand_2d_keypoints/hand_2d_keypoints.py
 modelscope/models/cv/hand_static/__init__.py
 modelscope/models/cv/hand_static/hand_model.py
 modelscope/models/cv/hand_static/networks.py
 modelscope/models/cv/human_reconstruction/Reconstruction.py
 modelscope/models/cv/human_reconstruction/__init__.py
 modelscope/models/cv/human_reconstruction/utils.py
 modelscope/models/cv/human_reconstruction/models/Embedding.py
@@ -389,16 +393,14 @@
 modelscope/models/cv/human_reconstruction/models/Res_backbone.py
 modelscope/models/cv/human_reconstruction/models/Surface_head.py
 modelscope/models/cv/human_reconstruction/models/__init__.py
 modelscope/models/cv/human_reconstruction/models/detectors.py
 modelscope/models/cv/human_reconstruction/models/geometry.py
 modelscope/models/cv/human_reconstruction/models/human_segmenter.py
 modelscope/models/cv/human_reconstruction/models/networks.py
-modelscope/models/cv/human_wholebody_keypoint/__init__.py
-modelscope/models/cv/human_wholebody_keypoint/human_wholebody_keypoint.py
 modelscope/models/cv/image_binary_quant_classification/__init__.py
 modelscope/models/cv/image_binary_quant_classification/binary_quant_model.py
 modelscope/models/cv/image_binary_quant_classification/bnext.py
 modelscope/models/cv/image_body_reshaping/__init__.py
 modelscope/models/cv/image_body_reshaping/image_body_reshaping.py
 modelscope/models/cv/image_body_reshaping/model.py
 modelscope/models/cv/image_body_reshaping/person_info.py
@@ -526,22 +528,27 @@
 modelscope/models/cv/image_inpainting/modules/perceptual.py
 modelscope/models/cv/image_inpainting/modules/pix2pixhd.py
 modelscope/models/cv/image_inpainting/modules/ade20k/__init__.py
 modelscope/models/cv/image_inpainting/modules/ade20k/base.py
 modelscope/models/cv/image_inpainting/modules/ade20k/resnet.py
 modelscope/models/cv/image_instance_segmentation/__init__.py
 modelscope/models/cv/image_instance_segmentation/cascade_mask_rcnn_swin.py
+modelscope/models/cv/image_instance_segmentation/fastinst_model.py
 modelscope/models/cv/image_instance_segmentation/maskdino_model.py
 modelscope/models/cv/image_instance_segmentation/maskdino_swin.py
 modelscope/models/cv/image_instance_segmentation/model.py
 modelscope/models/cv/image_instance_segmentation/postprocess_utils.py
 modelscope/models/cv/image_instance_segmentation/backbones/__init__.py
+modelscope/models/cv/image_instance_segmentation/backbones/resnet.py
 modelscope/models/cv/image_instance_segmentation/backbones/swin_transformer.py
 modelscope/models/cv/image_instance_segmentation/datasets/__init__.py
 modelscope/models/cv/image_instance_segmentation/datasets/transforms.py
+modelscope/models/cv/image_instance_segmentation/fastinst/__init__.py
+modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_decoder.py
+modelscope/models/cv/image_instance_segmentation/fastinst/fastinst_encoder.py
 modelscope/models/cv/image_instance_segmentation/maskdino/__init__.py
 modelscope/models/cv/image_instance_segmentation/maskdino/dino_decoder.py
 modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_decoder.py
 modelscope/models/cv/image_instance_segmentation/maskdino/maskdino_encoder.py
 modelscope/models/cv/image_instance_segmentation/maskdino/ms_deform_attn.py
 modelscope/models/cv/image_instance_segmentation/maskdino/position_encoding.py
 modelscope/models/cv/image_instance_segmentation/maskdino/utils.py
@@ -572,15 +579,14 @@
 modelscope/models/cv/image_mvs_depth_estimation/general_eval_dataset.py
 modelscope/models/cv/image_mvs_depth_estimation/module.py
 modelscope/models/cv/image_mvs_depth_estimation/utils.py
 modelscope/models/cv/image_paintbyexample/__init__.py
 modelscope/models/cv/image_paintbyexample/model.py
 modelscope/models/cv/image_panoptic_segmentation/__init__.py
 modelscope/models/cv/image_panoptic_segmentation/panseg_model.py
-modelscope/models/cv/image_panoptic_segmentation/r50_panseg_model.py
 modelscope/models/cv/image_portrait_enhancement/__init__.py
 modelscope/models/cv/image_portrait_enhancement/align_faces.py
 modelscope/models/cv/image_portrait_enhancement/gpen.py
 modelscope/models/cv/image_portrait_enhancement/image_portrait_enhancement.py
 modelscope/models/cv/image_portrait_enhancement/eqface/__init__.py
 modelscope/models/cv/image_portrait_enhancement/eqface/fqa.py
 modelscope/models/cv/image_portrait_enhancement/eqface/model_resnet.py
@@ -617,15 +623,14 @@
 modelscope/models/cv/image_reid_person/transreid_model.py
 modelscope/models/cv/image_restoration/__init__.py
 modelscope/models/cv/image_restoration/image_restoration_model.py
 modelscope/models/cv/image_restoration/demoire_models/__init__.py
 modelscope/models/cv/image_restoration/demoire_models/nets.py
 modelscope/models/cv/image_semantic_segmentation/__init__.py
 modelscope/models/cv/image_semantic_segmentation/ddpm_segmentation_model.py
-modelscope/models/cv/image_semantic_segmentation/segformer.py
 modelscope/models/cv/image_semantic_segmentation/semantic_seg_model.py
 modelscope/models/cv/image_semantic_segmentation/ddpm_seg/__init__.py
 modelscope/models/cv/image_semantic_segmentation/ddpm_seg/data_util.py
 modelscope/models/cv/image_semantic_segmentation/ddpm_seg/feature_extractors.py
 modelscope/models/cv/image_semantic_segmentation/ddpm_seg/pixel_classifier.py
 modelscope/models/cv/image_semantic_segmentation/ddpm_seg/utils.py
 modelscope/models/cv/image_semantic_segmentation/pan_merge/__init__.py
@@ -729,17 +734,15 @@
 modelscope/models/cv/nerf_recon_acc/dataloader/nerf_dataset.py
 modelscope/models/cv/nerf_recon_acc/dataloader/read_write_model.py
 modelscope/models/cv/nerf_recon_acc/network/__init__.py
 modelscope/models/cv/nerf_recon_acc/network/nerf.py
 modelscope/models/cv/nerf_recon_acc/network/segmenter.py
 modelscope/models/cv/nerf_recon_acc/network/utils.py
 modelscope/models/cv/object_detection/__init__.py
-modelscope/models/cv/object_detection/dino.py
 modelscope/models/cv/object_detection/mmdet_model.py
-modelscope/models/cv/object_detection/yolox_pai.py
 modelscope/models/cv/object_detection/mmdet_ms/__init__.py
 modelscope/models/cv/object_detection/mmdet_ms/backbones/__init__.py
 modelscope/models/cv/object_detection/mmdet_ms/backbones/vit.py
 modelscope/models/cv/object_detection/mmdet_ms/dense_heads/__init__.py
 modelscope/models/cv/object_detection/mmdet_ms/dense_heads/anchor_head.py
 modelscope/models/cv/object_detection/mmdet_ms/dense_heads/rpn_head.py
 modelscope/models/cv/object_detection/mmdet_ms/necks/__init__.py
@@ -1205,14 +1208,16 @@
 modelscope/models/multi_modal/ofa_for_all_tasks.py
 modelscope/models/multi_modal/ofa_for_text_to_image_synthesis_model.py
 modelscope/models/multi_modal/clip/__init__.py
 modelscope/models/multi_modal/clip/bert_tokenizer.py
 modelscope/models/multi_modal/clip/configuration_bert.py
 modelscope/models/multi_modal/clip/model.py
 modelscope/models/multi_modal/clip/modeling_bert.py
+modelscope/models/multi_modal/clip_interrogator/__init__.py
+modelscope/models/multi_modal/clip_interrogator/model.py
 modelscope/models/multi_modal/diffusion/__init__.py
 modelscope/models/multi_modal/diffusion/diffusion.py
 modelscope/models/multi_modal/diffusion/model.py
 modelscope/models/multi_modal/diffusion/structbert.py
 modelscope/models/multi_modal/diffusion/tokenizer.py
 modelscope/models/multi_modal/diffusion/unet_generator.py
 modelscope/models/multi_modal/diffusion/unet_upsampler_1024.py
@@ -1247,14 +1252,17 @@
 modelscope/models/multi_modal/mplug/__init__.py
 modelscope/models/multi_modal/mplug/configuration_mplug.py
 modelscope/models/multi_modal/mplug/modeling_mplug.py
 modelscope/models/multi_modal/mplug/mvit.py
 modelscope/models/multi_modal/mplug/predictor.py
 modelscope/models/multi_modal/mplug/clip/__init__.py
 modelscope/models/multi_modal/mplug/clip/clip.py
+modelscope/models/multi_modal/mplug_owl/__init__.py
+modelscope/models/multi_modal/mplug_owl/configuration_mplug_owl.py
+modelscope/models/multi_modal/mplug_owl/modeling_mplug_owl.py
 modelscope/models/multi_modal/multi_stage_diffusion/__init__.py
 modelscope/models/multi_modal/multi_stage_diffusion/clip.py
 modelscope/models/multi_modal/multi_stage_diffusion/decoder.py
 modelscope/models/multi_modal/multi_stage_diffusion/gaussian_diffusion.py
 modelscope/models/multi_modal/multi_stage_diffusion/model.py
 modelscope/models/multi_modal/multi_stage_diffusion/prior.py
 modelscope/models/multi_modal/multi_stage_diffusion/tokenizer.py
@@ -1508,16 +1516,16 @@
 modelscope/models/nlp/task_models/information_extraction.py
 modelscope/models/nlp/task_models/task_model.py
 modelscope/models/nlp/task_models/text_classification.py
 modelscope/models/nlp/task_models/text_generation.py
 modelscope/models/nlp/task_models/text_ranking.py
 modelscope/models/nlp/task_models/token_classification.py
 modelscope/models/nlp/unite/__init__.py
-modelscope/models/nlp/unite/configuration_unite.py
-modelscope/models/nlp/unite/modeling_unite.py
+modelscope/models/nlp/unite/configuration.py
+modelscope/models/nlp/unite/translation_evaluation.py
 modelscope/models/nlp/use/__init__.py
 modelscope/models/nlp/use/transformer.py
 modelscope/models/nlp/use/user_satisfaction_estimation.py
 modelscope/models/nlp/veco/__init__.py
 modelscope/models/nlp/veco/backbone.py
 modelscope/models/nlp/veco/configuration.py
 modelscope/models/nlp/veco/fill_mask.py
@@ -1614,41 +1622,29 @@
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/distributed.py
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/grouped_batch_sampler.py
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/samplers/iteration_based_batch_sampler.py
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/build.py
 modelscope/msdatasets/dataset_cls/custom_datasets/damoyolo/transforms/transforms.py
-modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/__init__.py
-modelscope/msdatasets/dataset_cls/custom_datasets/face_2d_keypoins/face_2d_keypoints_dataset.py
-modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/__init__.py
-modelscope/msdatasets/dataset_cls/custom_datasets/hand_2d_keypoints/hand_2d_keypoints_dataset.py
-modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/__init__.py
-modelscope/msdatasets/dataset_cls/custom_datasets/human_wholebody_keypoint/human_wholebody_keypoint_dataset.py
-modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/__init__.py
-modelscope/msdatasets/dataset_cls/custom_datasets/image_classification/classification_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_colorization/image_colorization_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/aug.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_inpainting/image_inpainting_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/data_utils.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_portrait_enhancement/image_portrait_enhancement_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assessment_degradation/image_quality_assessment_degradation_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/image_quality_assmessment_mos/image_quality_assessment_mos_dataset.py
-modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/__init__.py
-modelscope/msdatasets/dataset_cls/custom_datasets/image_semantic_segmentation/segmentation_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/movie_scene_segmentation_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/movie_scene_segmentation/sampler.py
-modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/__init__.py
-modelscope/msdatasets/dataset_cls/custom_datasets/object_detection/detection_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/augmenter.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/data_loader.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/image_dataset.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/__init__.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/iou_evaluator.py
 modelscope/msdatasets/dataset_cls/custom_datasets/ocr_detection/measures/quad_measurer.py
@@ -1686,60 +1682,55 @@
 modelscope/msdatasets/task_datasets/reds_image_deblurring_dataset.py
 modelscope/msdatasets/task_datasets/sidd_image_denoising.py
 modelscope/msdatasets/task_datasets/torch_base_dataset.py
 modelscope/msdatasets/task_datasets/video_summarization_dataset.py
 modelscope/msdatasets/utils/__init__.py
 modelscope/msdatasets/utils/dataset_utils.py
 modelscope/msdatasets/utils/delete_utils.py
+modelscope/msdatasets/utils/maxcompute_utils.py
 modelscope/msdatasets/utils/oss_utils.py
 modelscope/msdatasets/utils/upload_utils.py
 modelscope/ops/__init__.py
 modelscope/ops/ailut/__init__.py
 modelscope/ops/ailut/pyinterfaces.py
 modelscope/ops/ailut/Ailut/__init__.py
 modelscope/ops/ailut/Ailut/csrc/__init__.py
-modelscope/ops/ailut/Ailut/csrc/ailut_transform.cpp
-modelscope/ops/ailut/Ailut/csrc/ailut_transform_cpu.cpp
-modelscope/ops/ailut/Ailut/csrc/ailut_transform_cuda.cu
 modelscope/ops/quadtree_attention/__init__.py
 modelscope/ops/quadtree_attention/functions/__init__.py
 modelscope/ops/quadtree_attention/functions/quadtree_attention.py
 modelscope/ops/quadtree_attention/modules/__init__.py
 modelscope/ops/quadtree_attention/modules/quadtree_attention.py
 modelscope/ops/quadtree_attention/src/__init__.py
-modelscope/ops/quadtree_attention/src/score_computation.cpp
-modelscope/ops/quadtree_attention/src/score_computation.h
-modelscope/ops/quadtree_attention/src/score_computation_kernal.cu
-modelscope/ops/quadtree_attention/src/utils.h
-modelscope/ops/quadtree_attention/src/value_aggregation.cpp
-modelscope/ops/quadtree_attention/src/value_aggregation.h
-modelscope/ops/quadtree_attention/src/value_aggregation_kernel.cu
 modelscope/outputs/__init__.py
 modelscope/outputs/cv_outputs.py
 modelscope/outputs/nlp_outputs.py
 modelscope/outputs/outputs.py
 modelscope/pipelines/__init__.py
 modelscope/pipelines/base.py
 modelscope/pipelines/builder.py
+modelscope/pipelines/pipeline_template.py
 modelscope/pipelines/util.py
 modelscope/pipelines/audio/__init__.py
 modelscope/pipelines/audio/ans_dfsmn_pipeline.py
 modelscope/pipelines/audio/ans_pipeline.py
 modelscope/pipelines/audio/asr_inference_pipeline.py
 modelscope/pipelines/audio/asr_wenet_inference_pipeline.py
 modelscope/pipelines/audio/inverse_text_processing_pipeline.py
 modelscope/pipelines/audio/kws_farfield_pipeline.py
 modelscope/pipelines/audio/kws_kwsbp_pipeline.py
 modelscope/pipelines/audio/linear_aec_pipeline.py
 modelscope/pipelines/audio/lm_infer_pipeline.py
 modelscope/pipelines/audio/punctuation_processing_pipeline.py
 modelscope/pipelines/audio/separation_pipeline.py
+modelscope/pipelines/audio/speaker_change_locating_pipeline.py
 modelscope/pipelines/audio/speaker_diarization_pipeline.py
+modelscope/pipelines/audio/speaker_verification_eres2net_pipeline.py
 modelscope/pipelines/audio/speaker_verification_light_pipeline.py
 modelscope/pipelines/audio/speaker_verification_pipeline.py
+modelscope/pipelines/audio/speaker_verification_rdino_pipeline.py
 modelscope/pipelines/audio/text_to_speech_pipeline.py
 modelscope/pipelines/audio/timestamp_pipeline.py
 modelscope/pipelines/audio/voice_activity_detection_pipeline.py
 modelscope/pipelines/cv/__init__.py
 modelscope/pipelines/cv/action_detection_pipeline.py
 modelscope/pipelines/cv/action_recognition_pipeline.py
 modelscope/pipelines/cv/animal_recognition_pipeline.py
@@ -1766,16 +1757,16 @@
 modelscope/pipelines/cv/face_recognition_onnx_fm_pipeline.py
 modelscope/pipelines/cv/face_recognition_onnx_ir_pipeline.py
 modelscope/pipelines/cv/face_recognition_ood_pipeline.py
 modelscope/pipelines/cv/face_recognition_pipeline.py
 modelscope/pipelines/cv/face_reconstruction_pipeline.py
 modelscope/pipelines/cv/facial_expression_recognition_pipeline.py
 modelscope/pipelines/cv/facial_landmark_confidence_pipeline.py
+modelscope/pipelines/cv/fast_instance_segmentation_pipeline.py
 modelscope/pipelines/cv/general_recognition_pipeline.py
-modelscope/pipelines/cv/hand_2d_keypoints_pipeline.py
 modelscope/pipelines/cv/hand_static_pipeline.py
 modelscope/pipelines/cv/hicossl_video_embedding_pipeline.py
 modelscope/pipelines/cv/human_reconstruction_pipeline.py
 modelscope/pipelines/cv/image_body_reshaping_pipeline.py
 modelscope/pipelines/cv/image_bts_depth_estimation_pipeline.py
 modelscope/pipelines/cv/image_cartoon_pipeline.py
 modelscope/pipelines/cv/image_classification_pipeline.py
@@ -1794,15 +1785,14 @@
 modelscope/pipelines/cv/image_inpainting_sdv2_pipeline.py
 modelscope/pipelines/cv/image_instance_segmentation_pipeline.py
 modelscope/pipelines/cv/image_matching_pipeline.py
 modelscope/pipelines/cv/image_matting_pipeline.py
 modelscope/pipelines/cv/image_mvs_depth_estimation_pipeline.py
 modelscope/pipelines/cv/image_open_vocabulary_detection_pipeline.py
 modelscope/pipelines/cv/image_paintbyexample_pipeline.py
-modelscope/pipelines/cv/image_panoptic_segmentation_pipeline.py
 modelscope/pipelines/cv/image_portrait_enhancement_pipeline.py
 modelscope/pipelines/cv/image_quality_assessment_degradation_pipeline.py
 modelscope/pipelines/cv/image_quality_assessment_man_pipeline.py
 modelscope/pipelines/cv/image_quality_assessment_mos_pipeline.py
 modelscope/pipelines/cv/image_reid_person_pipeline.py
 modelscope/pipelines/cv/image_restoration_pipeline.py
 modelscope/pipelines/cv/image_salient_detection_pipeline.py
@@ -1862,20 +1852,14 @@
 modelscope/pipelines/cv/video_super_resolution_pipeline.py
 modelscope/pipelines/cv/vidt_pipeline.py
 modelscope/pipelines/cv/virtual_try_on_pipeline.py
 modelscope/pipelines/cv/vision_efficient_tuning_pipeline.py
 modelscope/pipelines/cv/vision_middleware_pipeline.py
 modelscope/pipelines/cv/vop_retrieval_pipeline.py
 modelscope/pipelines/cv/vop_retrieval_se_pipeline.py
-modelscope/pipelines/cv/easycv_pipelines/__init__.py
-modelscope/pipelines/cv/easycv_pipelines/base.py
-modelscope/pipelines/cv/easycv_pipelines/detection_pipeline.py
-modelscope/pipelines/cv/easycv_pipelines/face_2d_keypoints_pipeline.py
-modelscope/pipelines/cv/easycv_pipelines/human_wholebody_keypoint_pipeline.py
-modelscope/pipelines/cv/easycv_pipelines/segmentation_pipeline.py
 modelscope/pipelines/cv/ocr_utils/__init__.py
 modelscope/pipelines/cv/ocr_utils/model_convnext_transformer.py
 modelscope/pipelines/cv/ocr_utils/model_dla34.py
 modelscope/pipelines/cv/ocr_utils/model_resnet18_half.py
 modelscope/pipelines/cv/ocr_utils/model_resnet_mutex_v4_linewithchar.py
 modelscope/pipelines/cv/ocr_utils/model_vlpt.py
 modelscope/pipelines/cv/ocr_utils/ops.py
@@ -1895,14 +1879,15 @@
 modelscope/pipelines/multi_modal/efficient_diffusion_tuning_pipeline.py
 modelscope/pipelines/multi_modal/generative_multi_modal_embedding_pipeline.py
 modelscope/pipelines/multi_modal/gridvlp_pipeline.py
 modelscope/pipelines/multi_modal/image_captioning_pipeline.py
 modelscope/pipelines/multi_modal/image_text_retrieval_pipeline.py
 modelscope/pipelines/multi_modal/mgeo_ranking_pipeline.py
 modelscope/pipelines/multi_modal/multi_modal_embedding_pipeline.py
+modelscope/pipelines/multi_modal/multimodal_dialogue_pipeline.py
 modelscope/pipelines/multi_modal/ocr_recognition_pipeline.py
 modelscope/pipelines/multi_modal/soonet_video_temporal_grounding_pipeline.py
 modelscope/pipelines/multi_modal/sudoku_pipeline.py
 modelscope/pipelines/multi_modal/team_multi_modal_similarity_pipeline.py
 modelscope/pipelines/multi_modal/text2sql_pipeline.py
 modelscope/pipelines/multi_modal/text_to_image_synthesis_pipeline.py
 modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py
@@ -2080,14 +2065,15 @@
 modelscope/tools/__init__.py
 modelscope/tools/eval.py
 modelscope/tools/speech_tts_autolabel.py
 modelscope/tools/train.py
 modelscope/trainers/__init__.py
 modelscope/trainers/base.py
 modelscope/trainers/builder.py
+modelscope/trainers/cli_argument_parser.py
 modelscope/trainers/default_config.py
 modelscope/trainers/nlp_trainer.py
 modelscope/trainers/trainer.py
 modelscope/trainers/training_args.py
 modelscope/trainers/audio/__init__.py
 modelscope/trainers/audio/ans_trainer.py
 modelscope/trainers/audio/asr_trainer.py
@@ -2114,36 +2100,34 @@
 modelscope/trainers/cv/image_portrait_enhancement_trainer.py
 modelscope/trainers/cv/movie_scene_segmentation_trainer.py
 modelscope/trainers/cv/nerf_recon_acc_trainer.py
 modelscope/trainers/cv/ocr_detection_db_trainer.py
 modelscope/trainers/cv/ocr_recognition_trainer.py
 modelscope/trainers/cv/referring_video_object_segmentation_trainer.py
 modelscope/trainers/cv/vision_efficient_tuning_trainer.py
-modelscope/trainers/easycv/__init__.py
-modelscope/trainers/easycv/trainer.py
-modelscope/trainers/easycv/utils/__init__.py
-modelscope/trainers/easycv/utils/hooks.py
-modelscope/trainers/easycv/utils/metric.py
-modelscope/trainers/easycv/utils/register_util.py
 modelscope/trainers/hooks/__init__.py
 modelscope/trainers/hooks/builder.py
-modelscope/trainers/hooks/checkpoint_hook.py
 modelscope/trainers/hooks/clip_clamp_logit_scale_hook.py
-modelscope/trainers/hooks/ddp_hook.py
-modelscope/trainers/hooks/deepspeed_hook.py
 modelscope/trainers/hooks/early_stop_hook.py
 modelscope/trainers/hooks/evaluation_hook.py
 modelscope/trainers/hooks/hook.py
 modelscope/trainers/hooks/iter_timer_hook.py
 modelscope/trainers/hooks/lr_scheduler_hook.py
-modelscope/trainers/hooks/megatron_hook.py
 modelscope/trainers/hooks/priority.py
+modelscope/trainers/hooks/checkpoint/__init__.py
+modelscope/trainers/hooks/checkpoint/checkpoint_hook.py
+modelscope/trainers/hooks/checkpoint/checkpoint_processor.py
+modelscope/trainers/hooks/checkpoint/load_checkpoint_hook.py
 modelscope/trainers/hooks/compression/__init__.py
 modelscope/trainers/hooks/compression/sparsity_hook.py
 modelscope/trainers/hooks/compression/utils.py
+modelscope/trainers/hooks/distributed/__init__.py
+modelscope/trainers/hooks/distributed/ddp_hook.py
+modelscope/trainers/hooks/distributed/deepspeed_hook.py
+modelscope/trainers/hooks/distributed/megatron_hook.py
 modelscope/trainers/hooks/logger/__init__.py
 modelscope/trainers/hooks/logger/base.py
 modelscope/trainers/hooks/logger/tensorboard_hook.py
 modelscope/trainers/hooks/logger/text_logger_hook.py
 modelscope/trainers/hooks/optimizer/__init__.py
 modelscope/trainers/hooks/optimizer/apex_optimizer_hook.py
 modelscope/trainers/hooks/optimizer/base.py
@@ -2179,14 +2163,15 @@
 modelscope/trainers/nlp/plug_trainer.py
 modelscope/trainers/nlp/sentence_embedding_trainer.py
 modelscope/trainers/nlp/sequence_classification_trainer.py
 modelscope/trainers/nlp/siamese_uie_trainer.py
 modelscope/trainers/nlp/table_question_answering_trainer.py
 modelscope/trainers/nlp/text_generation_trainer.py
 modelscope/trainers/nlp/text_ranking_trainer.py
+modelscope/trainers/nlp/translation_evaluation_trainer.py
 modelscope/trainers/nlp/space/__init__.py
 modelscope/trainers/nlp/space/dialog_intent_trainer.py
 modelscope/trainers/nlp/space/dialog_modeling_trainer.py
 modelscope/trainers/nlp/space/eval.py
 modelscope/trainers/nlp/space/metrics/__init__.py
 modelscope/trainers/nlp/space/metrics/metrics_tracker.py
 modelscope/trainers/nlp/space/trainer/__init__.py
@@ -2211,20 +2196,20 @@
 modelscope/utils/checkpoint.py
 modelscope/utils/chinese_utils.py
 modelscope/utils/config.py
 modelscope/utils/config_ds.py
 modelscope/utils/constant.py
 modelscope/utils/data_collators.py
 modelscope/utils/data_utils.py
-modelscope/utils/demo_utils.py
 modelscope/utils/device.py
 modelscope/utils/error.py
 modelscope/utils/file_utils.py
 modelscope/utils/hub.py
 modelscope/utils/import_utils.py
+modelscope/utils/input_output.py
 modelscope/utils/json_utils.py
 modelscope/utils/logger.py
 modelscope/utils/megatron_utils.py
 modelscope/utils/metric.py
 modelscope/utils/model_tag.py
 modelscope/utils/plugins.py
 modelscope/utils/registry.py
```

### Comparing `modelscope-1.5.2/modelscope.egg-info/requires.txt` & `modelscope-1.6.0/modelscope.egg-info/requires.txt`

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 addict
 attrs
 datasets<=2.8.0,>=2.7.0
 einops
 filelock>=3.3.0
 gast>=0.2.2
-mmdet<=2.28.2
-numpy<1.24.0
+numpy<=1.22.0
 oss2
+pandas<=1.5.3
 Pillow>=6.2.0
 pyarrow!=9.0.0,>=6.0.0
 python-dateutil>=2.1
 pyyaml
 requests
 scipy
-setuptools==59.8.0
+setuptools
 simplejson>=3.3.0
 sortedcontainers>=1.5.9
 tqdm>=4.64.0
 yapf
 
 [all]
 accelerate
 albumentations>=1.0.3
 av>=9.2.0
 bmt_clipit>=1.0
 chumpy
 clip>=1.0
 control_ldm
 ddpm_guided_diffusion
-diffusers
+diffusers<0.15.0,>=0.13.1
 easydict
 easyrobust
 edit_distance
 face_alignment>=1.3.5
 fairscale>=0.4.1
 fastai>=1.0.51
 ffmpeg>=1.4
@@ -43,28 +43,27 @@
 imgaug>=0.4.0
 kornia>=0.5.0
 lap
 lmdb
 lpips
 ml_collections
 mmcls>=0.21.0
-mmdet>=2.25.0
+mmdet<=2.28.2,>=2.25.0
 mmdet3d==1.0.0a1
 mmsegmentation<=0.30.0
 moviepy>=1.0.3
 nerfacc==0.2.2
 networkx
 numba
 omegaconf
 onnx
 onnxruntime>=1.10
 onnxsim
 open-clip-torch>=2.7.0
 opencv-python
-pai-easycv<0.10.0,>=0.8
 paint_ldm
 pandas
 panopticapi
 plyfile>=0.7.4
 psutil
 PyMCubes
 pytorch-lightning
@@ -84,28 +83,29 @@
 trimesh
 ujson
 utils
 videofeatures_clipit>=1.0
 accelerate
 diffusers<0.15.0,>=0.13.1
 ftfy>=6.0.3
-librosa<=0.9.2
+librosa==0.9.2
 opencv-python
 pycocoevalcap>=1.2
 pycocotools>=2.0.4
 pytorch_lightning<=1.7.7
 rapidfuzz
 rouge_score<=0.0.4
 sacrebleu
+safetensors
 soundfile
 taming-transformers-rom1504
 timm
 tokenizers
 torchvision
-transformers>=4.12.0
+transformers>=4.27.1
 unicodedata2
 zhconv
 boto3
 en_core_web_sm>=2.3.5
 filelock
 ftfy
 jieba>=0.42.1
@@ -135,40 +135,40 @@
 ml_collections
 scipy
 tensorboardX
 tokenizers
 
 [audio]
 easyasr>=0.0.2
-funasr>=0.4.0
+funasr>=0.5.0
 kaldiio
 kwsbp>=0.0.6
 matplotlib
 numpy
 py_sound_connect>=0.1
 scipy
 SoundFile>0.10
 tensorboardX
 hyperpyyaml
-librosa<=0.9.2
+librosa==0.9.2
 MinDAEC
 mir_eval>=0.7
 numpy
 rotary_embedding_torch>=0.1.5
 scipy
 SoundFile>0.10
-speechbrain>=0.5.7
+speechbrain>=0.5.12
 torchaudio
 tqdm
 bitstring
 greenlet>=1.1.2
 inflect
 jedi>=0.18.1
 kantts
-librosa<=0.9.2
+librosa==0.9.2
 lxml
 matplotlib
 msgpack>=1.0.4
 parso>=0.8.3
 pexpect>=4.8.0
 pickleshare>=0.7.5
 prompt-toolkit>=3.0.30
@@ -186,46 +186,46 @@
 traitlets>=5.3.0
 ttsfrd>=0.1.2
 unidecode
 wcwidth>=0.2.5
 
 [audio_asr]
 easyasr>=0.0.2
-funasr>=0.4.0
+funasr>=0.5.0
 
 [audio_kws]
 kaldiio
 kwsbp>=0.0.6
 matplotlib
 numpy
 py_sound_connect>=0.1
 scipy
 SoundFile>0.10
 tensorboardX
 
 [audio_signal]
 hyperpyyaml
-librosa<=0.9.2
+librosa==0.9.2
 MinDAEC
 mir_eval>=0.7
 numpy
 rotary_embedding_torch>=0.1.5
 scipy
 SoundFile>0.10
-speechbrain>=0.5.7
+speechbrain>=0.5.12
 torchaudio
 tqdm
 
 [audio_tts]
 bitstring
 greenlet>=1.1.2
 inflect
 jedi>=0.18.1
 kantts
-librosa<=0.9.2
+librosa==0.9.2
 lxml
 matplotlib
 msgpack>=1.0.4
 parso>=0.8.3
 pexpect>=4.8.0
 pickleshare>=0.7.5
 prompt-toolkit>=3.0.30
@@ -250,15 +250,15 @@
 albumentations>=1.0.3
 av>=9.2.0
 bmt_clipit>=1.0
 chumpy
 clip>=1.0
 control_ldm
 ddpm_guided_diffusion
-diffusers
+diffusers<0.15.0,>=0.13.1
 easydict
 easyrobust
 edit_distance
 face_alignment>=1.3.5
 fairscale>=0.4.1
 fastai>=1.0.51
 ffmpeg>=1.4
@@ -269,28 +269,27 @@
 imgaug>=0.4.0
 kornia>=0.5.0
 lap
 lmdb
 lpips
 ml_collections
 mmcls>=0.21.0
-mmdet>=2.25.0
+mmdet<=2.28.2,>=2.25.0
 mmdet3d==1.0.0a1
 mmsegmentation<=0.30.0
 moviepy>=1.0.3
 nerfacc==0.2.2
 networkx
 numba
 omegaconf
 onnx
 onnxruntime>=1.10
 onnxsim
 open-clip-torch>=2.7.0
 opencv-python
-pai-easycv<0.10.0,>=0.8
 paint_ldm
 pandas
 panopticapi
 plyfile>=0.7.4
 psutil
 PyMCubes
 pytorch-lightning
@@ -312,28 +311,29 @@
 utils
 videofeatures_clipit>=1.0
 
 [multi-modal]
 accelerate
 diffusers<0.15.0,>=0.13.1
 ftfy>=6.0.3
-librosa<=0.9.2
+librosa==0.9.2
 opencv-python
 pycocoevalcap>=1.2
 pycocotools>=2.0.4
 pytorch_lightning<=1.7.7
 rapidfuzz
 rouge_score<=0.0.4
 sacrebleu
+safetensors
 soundfile
 taming-transformers-rom1504
 timm
 tokenizers
 torchvision
-transformers>=4.12.0
+transformers>=4.27.1
 unicodedata2
 zhconv
 
 [nlp]
 boto3
 en_core_web_sm>=2.3.5
 filelock
```

